{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import pyvo as vo\n",
    "import torch\n",
    "from astroquery.gaia import Gaia\n",
    "from astropy.coordinates import SkyCoord\n",
    "import astropy.units as u\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "from tqdm import tqdm\n",
    "from astropy.io import fits\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.preprocessing import MinMaxScaler, PowerTransformer\n",
    "import torch.nn as nn\n",
    "from mambapy.mamba import Mamba, MambaConfig\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import seaborn as sns\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_star_labels(gaia_ids, model_path, lamost_catalogue, gaia_transformer_path):\n",
    "    \"\"\"\n",
    "    Given a list of Gaia DR3 IDs, this function:\n",
    "    1) Queries Gaia for star parameters.\n",
    "    2) Cross-matches with LAMOST spectra.\n",
    "    3) Downloads and processes LAMOST spectra.\n",
    "    4) Normalizes both Gaia and LAMOST data.\n",
    "    5) Applies a trained StarClassifierFusion model to predict labels.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with Gaia IDs and predicted multi-label classifications.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\nüöÄ Step 1: Querying Gaia data...\")\n",
    "    print(\"üîó Gaia IDs:\", len(gaia_ids))\n",
    "    df_gaia = query_gaia_data(gaia_ids)\n",
    "    if df_gaia.empty:\n",
    "        print(\"‚ö†Ô∏è No Gaia data found. Exiting.\")\n",
    "        return None\n",
    "    print(\"üîó Gaia data:\", df_gaia.shape)\n",
    "\n",
    "    print(\"\\nüîÑ Step 2: Cross-matching with LAMOST catalog...\")\n",
    "    \n",
    "    df_matched = crossmatch_lamost(df_gaia, lamost_catalogue)\n",
    "    if df_matched.empty:\n",
    "        print(\"‚ö†Ô∏è No LAMOST matches found. Exiting.\")\n",
    "        return None\n",
    "\n",
    "    print(\"\\nüì• Step 3: Downloading LAMOST spectra (if needed)...\")\n",
    "    obsids = df_matched[\"obsid\"].unique()\n",
    "    spectra_folder = \"lamost_spectra_uniques\"\n",
    "    download_lamost_spectra(obsids, save_folder=spectra_folder, num_workers=500)\n",
    "\n",
    "    print(\"\\nüîß Step 4: Converting from FITS LAMOST spectra...\")\n",
    "    #process_lamost_fits_files(folder_path=spectra_folder, output_file=\"Pickles/lamost_data.csv\")\n",
    "    process_lamost_fits_files(folder_path=\"lamost_spectra_uniques\", \n",
    "                          output_file=\"Pickles/lamost_data.csv\", \n",
    "                          matched_obsids=obsids)\n",
    "\n",
    "    print(\"\\nüìä Step 5: Extracting and saving flux & frequency values...\")\n",
    "    extract_flux_frequency_from_csv(csv_path=\"Pickles/lamost_data.csv\")\n",
    "\n",
    "    print(\"\\nüìä Step 6: Interpolating and normalizing LAMOST spectra...\")\n",
    "    nan_files = interpolate_spectrum(\"Pickles/flux_values.pkl\", \"Pickles/freq_values.pkl\", \"Pickles/lamost_data_interpolated.pkl\")\n",
    "    spectrum_interpolated = pd.read_pickle(\"Pickles/lamost_data_interpolated.pkl\")\n",
    "    spectrum_normalized = normalize_lamost_spectra(spectrum_interpolated)\n",
    "\n",
    "    if spectrum_normalized.empty:\n",
    "        print(\"‚ö†Ô∏è No processed LAMOST spectra found. Exiting.\")\n",
    "        return None\n",
    "\n",
    "    print(\"\\nüìä Step 7: Normalizing Gaia data...\")\n",
    "    with open(gaia_transformer_path, \"rb\") as f:\n",
    "        gaia_transformers = pickle.load(f)   # Dict of {col_name: fitted PowerTransformer}\n",
    "    gaia_normalized = apply_gaia_transforms(df_gaia, gaia_transformers)\n",
    "\n",
    "    print(\"\\nüîó Step 8: Merging Gaia and LAMOST data...\")\n",
    "    gaia_lamost_match = df_matched[[\"source_id\", \"obsid\"]]\n",
    "    spectrum_normalized[\"obsid\"] = spectrum_normalized[\"obsid\"].astype(int)\n",
    "    gaia_lamost_match[\"obsid\"] = gaia_lamost_match[\"obsid\"].astype(int)\n",
    "\n",
    "    # Identify and remove all obsid values that appear more than once\n",
    "    obsid_counts = gaia_lamost_match[\"obsid\"].value_counts()\n",
    "    unique_obsids = obsid_counts[obsid_counts == 1].index  # Keep only obsid values that appear once\n",
    "\n",
    "    # Filter dataset to keep only unique obsid values\n",
    "    gaia_lamost_match = gaia_lamost_match[gaia_lamost_match[\"obsid\"].isin(unique_obsids)]\n",
    "\n",
    "    # Now, map the cleaned obsid-to-source_id mapping\n",
    "    spectrum_normalized[\"source_id\"] = spectrum_normalized[\"obsid\"].astype(int).map(\n",
    "        gaia_lamost_match.set_index(\"obsid\")[\"source_id\"]\n",
    "    )\n",
    "\n",
    "    # Merge Gaia and LAMOST data\n",
    "    gaia_lamost_merged = pd.merge(gaia_normalized, spectrum_normalized, on=\"source_id\", how=\"inner\")\n",
    "\n",
    "    if gaia_lamost_merged.empty:\n",
    "        print(\"‚ö†Ô∏è No valid data after merging. Exiting.\")\n",
    "        return None\n",
    "    \n",
    "    print(\"\\nü§ñ Step 9: Predicting labels using the trained model...\")\n",
    "    predictions = process_star_data_fusion(model_path, gaia_lamost_merged, \"Pickles/Updated_list_of_Classes.pkl\", sigmoid_constant=0.5)\n",
    "\n",
    "    print(\"\\nüíæ Step 10: Saving predictions...\")\n",
    "    df_predictions = pd.DataFrame(predictions, columns=pd.read_pickle(\"Pickles/Updated_list_of_Classes.pkl\"))\n",
    "    df_predictions[\"source_id\"] = gaia_lamost_merged[\"source_id\"].values\n",
    "\n",
    "    return df_predictions, gaia_lamost_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "def query_gaia_data(gaia_id_list):\n",
    "    \"\"\"\n",
    "    Given a list of Gaia DR3 source IDs, queries the Gaia archive\n",
    "    for the relevant columns used during training.\n",
    "    Returns a concatenated DataFrame of results.\n",
    "    \"\"\"\n",
    "    # Columns you actually need (adapt to match your pipeline!)\n",
    "    # e.g. ra, dec, pmra, pmdec, phot_g_mean_flux, ...\n",
    "    desired_cols = [\n",
    "        \"source_id\", \"ra\", \"ra_error\", \"dec\", \"dec_error\",\n",
    "        \"pmra\", \"pmra_error\", \"pmdec\", \"pmdec_error\",\n",
    "        \"parallax\", \"parallax_error\",\n",
    "        \"phot_g_mean_flux\", \"phot_g_mean_flux_error\",\n",
    "        \"phot_bp_mean_flux\", \"phot_bp_mean_flux_error\",\n",
    "        \"phot_rp_mean_flux\", \"phot_rp_mean_flux_error\"\n",
    "    ]\n",
    "\n",
    "    all_dfs = []\n",
    "    chunks = split_ids_into_chunks(gaia_id_list, chunk_size=30000)\n",
    "    for chunk in chunks:\n",
    "        query = f\"\"\"\n",
    "        SELECT {', '.join(desired_cols)}\n",
    "        FROM gaiadr3.gaia_source\n",
    "        WHERE source_id IN ({chunk})\n",
    "        \"\"\"\n",
    "        job = Gaia.launch_job_async(query)\n",
    "        tbl = job.get_results()\n",
    "        df_tmp = tbl.to_pandas()\n",
    "        all_dfs.append(df_tmp)\n",
    "    \n",
    "\n",
    "    # Print a warning if some IDs were not found\n",
    "    all_ids = pd.concat(all_dfs)[\"source_id\"].values\n",
    "    missing_ids = set(gaia_id_list) - set(all_ids)\n",
    "    if missing_ids:\n",
    "        print(f\"Warning: {len(missing_ids)} IDs not found in Gaia DR3.\")\n",
    "        print(f\"Missing IDs: {missing_ids}\")\n",
    "\n",
    "    if not all_dfs:\n",
    "        return pd.DataFrame(columns=desired_cols)\n",
    "    else:\n",
    "        return pd.concat(all_dfs, ignore_index=True)\n",
    "      \n",
    "def split_ids_into_chunks(gaia_id_list, chunk_size=50000):\n",
    "    \"\"\"\n",
    "    Takes a Python list of Gaia IDs (strings or ints),\n",
    "    returns a list of comma-joined strings, each containing up to `chunk_size` IDs.\n",
    "    \"\"\"\n",
    "    # Convert everything to string for the SQL query\n",
    "    gaia_id_list = [str(x) for x in gaia_id_list]\n",
    "    chunks = []\n",
    "    for i in range(0, len(gaia_id_list), chunk_size):\n",
    "        chunk = \", \".join(gaia_id_list[i:i+chunk_size])\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def crossmatch_lamost(gaia_df, lamost_df, match_radius=3*u.arcsec):\n",
    "    \"\"\"\n",
    "    Cross-matches Gaia sources with a local LAMOST catalogue.\n",
    "    Returns a merged DataFrame of matched objects.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure RA/Dec are numeric\n",
    "    gaia_df['ra'] = pd.to_numeric(gaia_df['ra'], errors='coerce')\n",
    "    gaia_df['dec'] = pd.to_numeric(gaia_df['dec'], errors='coerce')\n",
    "    lamost_df['ra'] = pd.to_numeric(lamost_df['ra'], errors='coerce')\n",
    "    lamost_df['dec'] = pd.to_numeric(lamost_df['dec'], errors='coerce')\n",
    "\n",
    "    # Drop NaN values\n",
    "    gaia_df = gaia_df.dropna(subset=['ra', 'dec'])\n",
    "    lamost_df = lamost_df.dropna(subset=['ra', 'dec'])\n",
    "\n",
    "    print(f\"After NaN removal: Gaia={gaia_df.shape}, LAMOST={lamost_df.shape}\")\n",
    "\n",
    "    # Check if LAMOST coordinates are in arcseconds (convert if necessary)\n",
    "    if lamost_df['ra'].max() > 360:  # RA should not exceed 360 degrees\n",
    "        print(\"‚ö†Ô∏è LAMOST RA/Dec seem to be in arcseconds. Converting to degrees.\")\n",
    "        lamost_df['ra'] /= 3600\n",
    "        lamost_df['dec'] /= 3600\n",
    "\n",
    "    # Convert to SkyCoord objects (ensuring same frame)\n",
    "    gaia_coords = SkyCoord(ra=gaia_df['ra'].values*u.deg,\n",
    "                           dec=gaia_df['dec'].values*u.deg,\n",
    "                           frame='icrs')\n",
    "\n",
    "    lamost_coords = SkyCoord(ra=lamost_df['ra'].values*u.deg,\n",
    "                             dec=lamost_df['dec'].values*u.deg,\n",
    "                             frame='icrs')\n",
    "\n",
    "    # Perform crossmatch\n",
    "    idx, d2d, _ = gaia_coords.match_to_catalog_sky(lamost_coords)\n",
    "\n",
    "    # Apply matching radius filter\n",
    "    matches = d2d < match_radius\n",
    "    #print(f\"Match distances (arcsec): {d2d.to(u.arcsec).value[matches]}\")\n",
    "\n",
    "    if matches.sum() == 0:\n",
    "        print(\"‚ö†Ô∏è No matches found! Try increasing `match_radius`.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Extract matched rows correctly\n",
    "    gaia_matched = gaia_df.iloc[matches].copy().reset_index(drop=True)\n",
    "    lamost_matched = lamost_df.iloc[idx[matches]].copy().reset_index(drop=True)\n",
    "\n",
    "    print(f\"Matched Gaia Objects: {gaia_matched.shape}\")\n",
    "    print(f\"Matched LAMOST Objects: {lamost_matched.shape}\")\n",
    "\n",
    "    # Merge matches into final DataFrame\n",
    "    final = pd.concat([gaia_matched, lamost_matched], axis=1)\n",
    "\n",
    "    return final\n",
    "\n",
    "\n",
    "def download_one_spectrum(obsid, session, save_folder):\n",
    "    \"\"\"\n",
    "    Helper function to download one spectrum file given an obsid.\n",
    "    Uses the same session to get the file and saves it locally.\n",
    "    \"\"\"\n",
    "    url = f\"https://www.lamost.org/dr7/v2.0/spectrum/fits/{obsid}\"\n",
    "    local_path = os.path.join(save_folder, str(obsid))\n",
    "\n",
    "    # If already downloaded, skip\n",
    "    if os.path.exists(local_path):\n",
    "        return obsid, True, None\n",
    "\n",
    "    try:\n",
    "        resp = session.get(url, timeout=30)\n",
    "        resp.raise_for_status()\n",
    "        with open(local_path, \"wb\") as f:\n",
    "            f.write(resp.content)\n",
    "        return obsid, True, None\n",
    "    except Exception as e:\n",
    "        return obsid, False, str(e)\n",
    "    \n",
    "def process_lamost_fits_files(folder_path=\"lamost_spectra_uniques\", output_file=\"Pickles/lamost_data.csv\", batch_size=10000, matched_obsids=None):\n",
    "    \"\"\"\n",
    "    Processes LAMOST FITS spectra by extracting flux and frequency data.\n",
    "    Saves data in a CSV file with batching to optimize memory usage.\n",
    "    \n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing FITS files.\n",
    "        output_file (str): Path to the output CSV file.\n",
    "        batch_size (int): Number of records to process before writing to the CSV.\n",
    "        matched_obsids (set): Set of obsids to process. If None, all FITS files are processed.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\nüìÇ Processing LAMOST FITS files...\")\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "\n",
    "    # Define column headers\n",
    "    columns = [f'col_{i}' for i in range(3748)] + ['file_name', 'row']\n",
    "\n",
    "    # Initialize the CSV file with headers\n",
    "    with open(output_file, 'w') as f:\n",
    "        pd.DataFrame(columns=columns).to_csv(f, index=False)\n",
    "\n",
    "    # Get list of FITS files in the folder\n",
    "    all_files = os.listdir(folder_path)\n",
    "\n",
    "    # Filter files based on matched_obsids\n",
    "    if matched_obsids is not None:\n",
    "        all_files = [f for f in all_files if int(f.split(\".\")[0]) in matched_obsids]\n",
    "\n",
    "    total_files = len(all_files)\n",
    "    if total_files == 0:\n",
    "        print(\"‚ö†Ô∏è No matching FITS files found for processing. Exiting.\")\n",
    "        return\n",
    "\n",
    "    batch_list = []\n",
    "\n",
    "    # Process FITS files\n",
    "    with tqdm(total=total_files, desc='Processing FITS files') as pbar:\n",
    "        for filename in all_files:\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "            try:\n",
    "                with fits.open(file_path) as hdul:\n",
    "                    data = hdul[0].data[:3, :3748]  # Extract first 3 rows and 3748 columns\n",
    "\n",
    "                    for i, row_data in enumerate(data):\n",
    "                        data_dict = {f'col_{j}': value for j, value in enumerate(row_data)}\n",
    "                        data_dict['file_name'] = filename\n",
    "                        data_dict['row'] = i  # Track which row from the FITS file\n",
    "                        batch_list.append(data_dict)\n",
    "\n",
    "                # Write batch to CSV\n",
    "                if len(batch_list) >= batch_size:\n",
    "                    pd.DataFrame(batch_list).to_csv(output_file, mode='a', header=False, index=False)\n",
    "                    batch_list.clear()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error processing {filename}: {e}\")\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "        # Write any remaining data\n",
    "        if batch_list:\n",
    "            pd.DataFrame(batch_list).to_csv(output_file, mode='a', header=False, index=False)\n",
    "\n",
    "\n",
    "\n",
    "def extract_flux_frequency_from_csv(csv_path=\"Pickles/lamost_data.csv\", flux_pickle=\"Pickles/flux_values.pkl\", freq_pickle=\"Pickles/freq_values.pkl\", chunk_size=10000):\n",
    "    \"\"\"\n",
    "    Extracts flux and frequency data from a CSV file and saves them as separate pickle files.\n",
    "\n",
    "    Args:\n",
    "        csv_path (str): Path to the CSV file containing processed spectra.\n",
    "        flux_pickle (str): Path to save extracted flux values.\n",
    "        freq_pickle (str): Path to save extracted frequency values.\n",
    "        chunk_size (int): Number of rows to process per chunk.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\nüìä Extracting flux and frequency values...\")\n",
    "\n",
    "    flux_values = pd.DataFrame()\n",
    "    freq_values = pd.DataFrame()\n",
    "\n",
    "    # Count total rows for progress tracking\n",
    "    total_rows = sum(1 for _ in open(csv_path)) - 1  # Subtract header row\n",
    "\n",
    "    for chunk in tqdm(pd.read_csv(csv_path, chunksize=chunk_size), total=total_rows // chunk_size):\n",
    "        flux_mask = chunk['row'] == 0  # Select only flux values\n",
    "        freq_mask = chunk['row'] == 2  # Select only frequency values\n",
    "\n",
    "        flux_values = pd.concat([flux_values, chunk[flux_mask].drop(columns=['row'])])\n",
    "        freq_values = pd.concat([freq_values, chunk[freq_mask].drop(columns=['row'])])\n",
    "\n",
    "    print(f\"‚úÖ Flux values shape: {flux_values.shape}, Frequency values shape: {freq_values.shape}\")\n",
    "\n",
    "    # Save extracted values\n",
    "    flux_values.to_pickle(flux_pickle)\n",
    "    freq_values.to_pickle(freq_pickle)\n",
    "\n",
    "    \n",
    "\n",
    "def normalize_lamost_spectra(spectra_df):\n",
    "    \"\"\"\n",
    "    Reads LAMOST FITS spectra, applies interpolation, normalization, and transformation.\n",
    "    Returns a DataFrame of final spectral features (one row per spectrum).\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    spectra = spectra_df.iloc[:, 100:-1].values  # Exclude the last column (file_name)\n",
    "\n",
    "    #print(f\"Shape of the spectra array: {spectra.shape}\")\n",
    "\n",
    "    # Normalize the spectra between 0 and 1\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    spectra_normalized = min_max_scaler.fit_transform(spectra.T).T\n",
    "\n",
    "    #print(f\"Shape of the normalized spectra array: {spectra_normalized.shape}\")\n",
    "\n",
    "    # Apply the Yeo-Johnson transformation to the spectra\n",
    "    pt = PowerTransformer(method='yeo-johnson', standardize=True)\n",
    "    spectra_transformed = pt.fit_transform(spectra_normalized.T).T\n",
    "\n",
    "    # Create a new DataFrame with the transformed spectra\n",
    "    df_transformed = pd.DataFrame(spectra_transformed, columns=spectra_df.columns[100:-1]) # Exclude the first 100+3 columns and the last column\n",
    "\n",
    "    #print(f\"Shape of the transformed spectra array: {spectra_transformed.shape}\")\n",
    "\n",
    "    # Add the file_name column back to the DataFrame\n",
    "    #print(f\"Available columns in spectra_df: {spectra_df.columns}\")\n",
    "    df_transformed['obsid'] = spectra_df['file_name']\n",
    "\n",
    "    return df_transformed\n",
    "\n",
    "def apply_gaia_transforms(gaia_df, transformers_dict):\n",
    "    \"\"\"\n",
    "    Applies the same Yeo-Johnson (or other) transformations used in training\n",
    "    to the relevant Gaia columns. \n",
    "    \"\"\"\n",
    "    # Fill the same NaN values or set the same flags as in training\n",
    "    # e.g. if you flagged parallax=NaN => set parallax=0, error=10\n",
    "    # do that here too, to keep consistent with your training pipeline\n",
    "    #\n",
    "    # Example based on your code:\n",
    "    gaia_df['flagnopllx'] = np.where(gaia_df['parallax'].isna(), 1, 0)\n",
    "    gaia_df['parallax']       = gaia_df['parallax'].fillna(0)\n",
    "    gaia_df['parallax_error'] = gaia_df['parallax_error'].fillna(10)\n",
    "    gaia_df['pmra']           = gaia_df['pmra'].fillna(0)\n",
    "    gaia_df['pmra_error']     = gaia_df['pmra_error'].fillna(10)\n",
    "    gaia_df['pmdec']          = gaia_df['pmdec'].fillna(0)\n",
    "    gaia_df['pmdec_error']    = gaia_df['pmdec_error'].fillna(10)\n",
    "\n",
    "    gaia_df['flagnoflux'] = 0\n",
    "    # If G or BP or RP is missing\n",
    "    missing_flux = gaia_df['phot_g_mean_flux'].isna() | gaia_df['phot_bp_mean_flux'].isna() \n",
    "    gaia_df.loc[missing_flux, 'flagnoflux'] = 1\n",
    "\n",
    "    # fill flux with 0 and error with large number\n",
    "    gaia_df['phot_g_mean_flux']       = gaia_df['phot_g_mean_flux'].fillna(0)\n",
    "    gaia_df['phot_g_mean_flux_error'] = gaia_df['phot_g_mean_flux_error'].fillna(50000)\n",
    "    gaia_df['phot_bp_mean_flux']      = gaia_df['phot_bp_mean_flux'].fillna(0)\n",
    "    gaia_df['phot_bp_mean_flux_error']= gaia_df['phot_bp_mean_flux_error'].fillna(50000)\n",
    "    gaia_df['phot_rp_mean_flux']      = gaia_df['phot_rp_mean_flux'].fillna(0)\n",
    "    gaia_df['phot_rp_mean_flux_error']= gaia_df['phot_rp_mean_flux_error'].fillna(50000)\n",
    "\n",
    "    # Drop any rows that are incomplete, if that was your final approach:\n",
    "    gaia_df.dropna(axis=0, inplace=True)\n",
    "    print(f\"Dropped {len(gaia_df) - len(gaia_df.dropna())} rows with NaN values.\")\n",
    "\n",
    "    # Remove source_id and other columns not to be transformed to be added back later\n",
    "    source_id = gaia_df['source_id']\n",
    "    gaia_df = gaia_df.drop(columns=[\"source_id\"])\n",
    "\n",
    "    # Now apply the stored transformations:\n",
    "    for col, transformer in transformers_dict.items():\n",
    "        if col in gaia_df.columns:\n",
    "            #print(f\"Transforming column: {col}\")\n",
    "            gaia_df[col] = transformer.transform(gaia_df[[col]])\n",
    "            #print(f\"Transformed column: {col}\")\n",
    "        else:\n",
    "            # If the column didn't exist, maybe set to 0 or skip?\n",
    "            print(f\"Warning: column {col} not found in new data, skipping transform.\")\n",
    "\n",
    "    # Add back the source_id column\n",
    "    gaia_df['source_id'] = source_id\n",
    "    return gaia_df\n",
    "\n",
    "def interpolate_spectrum(fluxes_loc, frequencies_loc, output_dir, limit=10, edge_limit=20):\n",
    "    \"\"\"Interpolates the flux values to fill in missing data points.\"\"\"\n",
    "    # Load the data from the pickle file    \n",
    "    df_freq = pd.read_pickle(frequencies_loc).reset_index(drop=True)      \n",
    "    df_flux = pd.read_pickle(fluxes_loc).reset_index(drop=True)  # Reset index for zero-based iteration\n",
    "\n",
    "    # Initialize an empty list to store the results before concatenating into a DataFrame\n",
    "    results_list = []\n",
    "\n",
    "    # Initialize lists to store problematic file_names\n",
    "    nan_files = []  \n",
    "\n",
    "    # Count the number of successful interpolations\n",
    "    cnt_success = 0\n",
    "\n",
    "    # Debugging counters\n",
    "    cnt_total_skipped = 0\n",
    "    cnt_nan_skipped = 0\n",
    "    cnt_zero_skipped = 0\n",
    "\n",
    "    # Overwrite the output file at the beginning\n",
    "    if os.path.exists(output_dir):\n",
    "        os.remove(output_dir)\n",
    "\n",
    "    # Loop through each row in the DataFrame (each row is a spectrum) with tqdm for progress bar\n",
    "    for index, row in tqdm(df_flux.iterrows(), total=len(df_flux), desc='Interpolating spectra'):\n",
    "\n",
    "        # Extract the fluxes (assuming they start at column 0 and continue to the last column)\n",
    "        fluxes = row[:-2].values  # Exclude the last columns (file_name, label)\n",
    "\n",
    "        # Extract the frequencies\n",
    "        frequencies = df_freq.iloc[int(index), :-2].values  # Exclude the last columns (file_name, label)\n",
    "\n",
    "        # Count the number of NaN and 0 values in the fluxes and frequencies\n",
    "        fluxes = pd.to_numeric(row[:-2], errors='coerce').values  # Exclude and convert to numeric\n",
    "        frequencies = pd.to_numeric(df_freq.iloc[index, :-2], errors='coerce').values  # Same for frequencies\n",
    "        num_nan = np.isnan(fluxes).sum() + np.isnan(frequencies).sum()  # Count NaN values\n",
    "        num_zero = (fluxes == 0).sum() + (frequencies == 0).sum()  # Count zero values\n",
    "        num_freq_nan = np.isnan(frequencies).sum() + (frequencies == 0).sum()\n",
    "        if num_freq_nan > 0:\n",
    "            print(f\"Number of NaN or zero frequency values: {num_freq_nan}\")\n",
    "        # Special handling for NaN values, counting nans in sequence, except for the first and last 10\n",
    "        if num_nan > limit and index > edge_limit and index < len(fluxes)-edge_limit:\n",
    "            cnt_nan_skipped += 1  # Debug: count NaN-skipped rows\n",
    "            nan_files.append(row['file_name'])\n",
    "            continue\n",
    "        \n",
    "        if num_zero > limit and index > edge_limit and index < len(fluxes)-edge_limit:\n",
    "            cnt_zero_skipped += 1  # Debug: count zero-skipped rows\n",
    "            nan_files.append(row['file_name'])\n",
    "            continue\n",
    "\n",
    "        # Deal with NaN values\n",
    "        fluxes = fluxes[~np.isnan(fluxes)]\n",
    "        frequencies = frequencies[~np.isnan(fluxes)]\n",
    "\n",
    "        # Interpolate to fill in missing values\n",
    "        f = interp1d(frequencies, fluxes, kind='linear', fill_value=\"extrapolate\")\n",
    "        new_frequencies = np.linspace(frequencies.min(), frequencies.max(), len(row[:-2].values))\n",
    "\n",
    "        # Interpolated flux values\n",
    "        interpolated_fluxes = f(new_frequencies)\n",
    "\n",
    "        # Store the interpolated data along with labels and other metadata\n",
    "        # Create a dictionary for the interpolated spectrum\n",
    "        interpolated_data = {f'flux_{i}': value for i, value in enumerate(interpolated_fluxes)}\n",
    "\n",
    "        # Add the original metadata back (e.g., file_name, label, row)\n",
    "        interpolated_data['file_name'] = row['file_name']\n",
    "                \n",
    "        # Append the interpolated data to the results list\n",
    "        results_list.append(interpolated_data)\n",
    "\n",
    "        if index % 2000 == 0:  # Save every 5000 rows\n",
    "            if os.path.exists(output_dir):\n",
    "                existing_df = pd.read_pickle(output_dir)  # Load existing data\n",
    "                new_df = pd.DataFrame(results_list)\n",
    "                # Concatenate existing and new data\n",
    "                combined_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "                combined_df.to_pickle(output_dir)  # Save combined DataFrame\n",
    "            else:\n",
    "                # If the file doesn't exist, create a new DataFrame and save\n",
    "                pd.DataFrame(results_list).to_pickle(output_dir)\n",
    "            cnt_success += len(results_list)  # Increment the count of successful interpolations\n",
    "            results_list = []  # Clear list to free memory\n",
    "\n",
    "    print(f\"Initial number of rows: {len(df_flux)}\")\n",
    "\n",
    "    # After the loop, save any remaining results\n",
    "    if results_list:\n",
    "        if os.path.exists(output_dir):\n",
    "            existing_df = pd.read_pickle(output_dir)\n",
    "            new_df = pd.DataFrame(results_list)\n",
    "            combined_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "            combined_df.to_pickle(output_dir)\n",
    "        else:\n",
    "            pd.DataFrame(results_list).to_pickle(output_dir)\n",
    "        cnt_success += len(results_list)\n",
    "\n",
    "    # Debugging information\n",
    "    cnt_total_skipped = len(nan_files)\n",
    "    print(f\"Total successful interpolations: {cnt_success}\")\n",
    "    #print(f\"Total skipped due to NaNs: {cnt_nan_skipped}\")\n",
    "    #print(f\"Total skipped due to zeros: {cnt_zero_skipped}\")\n",
    "    print(f\"Total skipped rows (NaNs + zeros): {cnt_total_skipped}\")\n",
    "    print(f\"Final check: len(df_flux) == cnt_success + len(nan_files)? {len(df_flux) == cnt_success + cnt_total_skipped}\")\n",
    "\n",
    "    return nan_files\n",
    "\n",
    "def process_star_data_fusion(\n",
    "    model_path, \n",
    "    X, \n",
    "    classes_path, \n",
    "    d_model_spectra=1024, \n",
    "    d_model_gaia=1024, \n",
    "    num_classes=55, \n",
    "    input_dim_spectra=3647, \n",
    "    input_dim_gaia=18, \n",
    "    depth=12, \n",
    "    sigmoid_constant=0.5,\n",
    "    class_to_plot=\"AllStars***lamost\"\n",
    "):\n",
    "    \"\"\"Processes star data using the fused StarClassifierFusion model.\"\"\"\n",
    "\n",
    "    # Load the data\n",
    "    classes = pd.read_pickle(classes_path)\n",
    "\n",
    "    # Load the trained fusion model\n",
    "    model = StarClassifierFusion(\n",
    "        d_model_spectra=d_model_spectra,\n",
    "        d_model_gaia=d_model_gaia,\n",
    "        num_classes=num_classes,\n",
    "        input_dim_spectra=input_dim_spectra,\n",
    "        input_dim_gaia=input_dim_gaia,\n",
    "        n_layers=depth,\n",
    "        use_cross_attention=True,  # Change to False for late fusion\n",
    "        n_cross_attn_heads=8\n",
    "    )\n",
    "\n",
    "    # Load the state dictionary\n",
    "    state_dict = torch.load(model_path)\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "    # Get multi-hot encoded labels\n",
    "    #y = X[classes]\n",
    "\n",
    "    # Define Gaia columns\n",
    "    gaia_columns = [\n",
    "        \"parallax\", \"ra\", \"dec\", \"ra_error\", \"dec_error\", \"parallax_error\", \"pmra\", \"pmdec\",\n",
    "        \"pmra_error\", \"pmdec_error\", \"phot_g_mean_flux\", \"flagnopllx\", \"phot_g_mean_flux_error\",\n",
    "        \"phot_bp_mean_flux\", \"phot_rp_mean_flux\", \"phot_bp_mean_flux_error\", \"phot_rp_mean_flux_error\",\n",
    "        \"flagnoflux\"\n",
    "    ]\n",
    "\n",
    "    # Separate Gaia and Spectra features\n",
    "    X_spectra = X.drop(columns={\"obsid\",\"source_id\", *gaia_columns})\n",
    "    X_gaia = X[gaia_columns]\n",
    "\n",
    "    print(f\"X_spectra shape: {X_spectra.shape}\")\n",
    "    print(f\"X_gaia shape: {X_gaia.shape}\")\n",
    "    #print(f\"y shape: {y.shape}\")\n",
    "\n",
    "    if class_to_plot != \"AllStars***lamost\":\n",
    "        # Filter for a specific class\n",
    "        X_spectra = X_spectra[y[class_to_plot] == 1]\n",
    "        X_gaia = X_gaia[y[class_to_plot] == 1]\n",
    "        #y = y[y[class_to_plot] == 1]\n",
    "\n",
    "        print(f\"X_spectra shape after filtering for {class_to_plot}: {X_spectra.shape}\")\n",
    "        print(f\"X_gaia shape after filtering for {class_to_plot}: {X_gaia.shape}\")\n",
    "       # print(f\"y shape after filtering for {class_to_plot}: {y.shape}\")\n",
    "\n",
    "    # Drop label columns from spectra\n",
    "    #X_spectra.drop(classes, axis=1, inplace=True)\n",
    "\n",
    "    # Convert to tensors\n",
    "    X_spectra = torch.tensor(X_spectra.values, dtype=torch.float32)\n",
    "    X_gaia = torch.tensor(X_gaia.values, dtype=torch.float32)\n",
    "    #y = torch.tensor(y.values, dtype=torch.float32)\n",
    "\n",
    "    # Create DataLoader\n",
    "    class MultiModalDataset(Dataset):\n",
    "        def __init__(self, X_spectra, X_gaia):\n",
    "            self.X_spectra = X_spectra\n",
    "            self.X_gaia = X_gaia\n",
    "            #self.y = y\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.X_spectra)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return self.X_spectra[idx], self.X_gaia[idx]\n",
    "\n",
    "    dataset = MultiModalDataset(X_spectra, X_gaia)\n",
    "    loader = DataLoader(dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "    # Move model to device\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_predicted = []\n",
    "    all_y = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_spc, X_ga in loader:\n",
    "            # Move data to device\n",
    "            X_spc, X_ga = X_spc.to(device), X_ga.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(X_spc, X_ga)\n",
    "            predicted = (torch.sigmoid(outputs) > sigmoid_constant).float()\n",
    "\n",
    "            # Store predictions and labels\n",
    "            all_predicted.append(predicted.cpu().numpy())\n",
    "            #all_y.append(y_batch.cpu().numpy())\n",
    "\n",
    "            # Free GPU memory\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # Concatenate all predictions and labels\n",
    "    #y_cpu = np.concatenate(all_y, axis=0)\n",
    "    predicted_cpu = np.concatenate(all_predicted, axis=0)\n",
    "\n",
    "    return predicted_cpu\n",
    "\n",
    "class StarClassifierFusion(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model_spectra,\n",
    "        d_model_gaia,\n",
    "        input_dim_spectra,\n",
    "        input_dim_gaia,\n",
    "        num_classes=55,\n",
    "        n_layers=6,\n",
    "        use_cross_attention=False,\n",
    "        n_cross_attn_heads=8\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model_spectra (int): embedding dimension for the spectra MAMBA\n",
    "            d_model_gaia (int): embedding dimension for the gaia MAMBA\n",
    "            num_classes (int): multi-label classification\n",
    "            input_dim_spectra (int): # of features for spectra\n",
    "            input_dim_gaia (int): # of features for gaia\n",
    "            n_layers (int): depth for each MAMBA\n",
    "            use_cross_attention (bool): whether to use cross-attention\n",
    "            n_cross_attn_heads (int): number of heads for cross-attention\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # --- MAMBA for spectra ---\n",
    "        config_spectra = MambaConfig(\n",
    "            d_model=d_model_spectra,\n",
    "            d_state=64,\n",
    "            d_conv=4,\n",
    "            n_layers=n_layers,\n",
    "        )\n",
    "        self.mamba_spectra = Mamba(config_spectra)\n",
    "        self.input_proj_spectra = nn.Linear(input_dim_spectra, d_model_spectra)\n",
    "\n",
    "        # --- MAMBA for gaia ---\n",
    "        config_gaia = MambaConfig(\n",
    "            d_model=d_model_gaia,\n",
    "            d_state=64,\n",
    "            d_conv=4,\n",
    "            n_layers=n_layers\n",
    "        )\n",
    "        self.mamba_gaia = Mamba(config_gaia)\n",
    "        self.input_proj_gaia = nn.Linear(input_dim_gaia, d_model_gaia)\n",
    "\n",
    "        # --- Cross Attention (Optional) ---\n",
    "        self.use_cross_attention = use_cross_attention\n",
    "        if use_cross_attention:\n",
    "            # We'll do cross-attn in both directions or just one‚Äîhere is an example with 2 blocks\n",
    "            self.cross_attn_block_spectra = CrossAttentionBlock(d_model_spectra, n_heads=n_cross_attn_heads)\n",
    "            self.cross_attn_block_gaia = CrossAttentionBlock(d_model_gaia, n_heads=n_cross_attn_heads)\n",
    "\n",
    "        # --- Final Classifier ---\n",
    "        # If you do late fusion by concatenation, the dimension is d_model_spectra + d_model_gaia\n",
    "        # If you do average fusion, it is max(d_model_spectra, d_model_gaia) (or keep them separate).\n",
    "        fusion_dim = d_model_spectra + d_model_gaia\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(fusion_dim),\n",
    "            nn.Linear(fusion_dim, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x_spectra, x_gaia):\n",
    "        \"\"\"\n",
    "        x_spectra : (batch_size, input_dim_spectra) or potentially (batch_size, seq_len_spectra, input_dim_spectra)\n",
    "        x_gaia    : (batch_size, input_dim_gaia) or (batch_size, seq_len_gaia, input_dim_gaia)\n",
    "        \"\"\"\n",
    "        # For MAMBA, we expect shape: (B, seq_len, d_model). \n",
    "        # If your input is just (B, d_in), we turn it into (B, 1, d_in).\n",
    "        \n",
    "        # --- Project to d_model and add sequence dimension (seq_len=1) ---\n",
    "        x_spectra = self.input_proj_spectra(x_spectra)  # (B, d_model_spectra)\n",
    "        x_spectra = x_spectra.unsqueeze(1)             # (B, 1, d_model_spectra)\n",
    "\n",
    "        x_gaia = self.input_proj_gaia(x_gaia)          # (B, d_model_gaia)\n",
    "        x_gaia = x_gaia.unsqueeze(1)                   # (B, 1, d_model_gaia)\n",
    "\n",
    "        # --- MAMBA encoding (each modality separately) ---\n",
    "        x_spectra = self.mamba_spectra(x_spectra)  # (B, 1, d_model_spectra)\n",
    "        x_gaia = self.mamba_gaia(x_gaia)          # (B, 1, d_model_gaia)\n",
    "\n",
    "        # Optionally, use cross-attention to fuse the representations\n",
    "        if self.use_cross_attention:\n",
    "            # Cross-attention from spectra -> gaia\n",
    "            x_spectra_fused = self.cross_attn_block_spectra(x_spectra, x_gaia)\n",
    "            # Cross-attention from gaia -> spectra\n",
    "            x_gaia_fused = self.cross_attn_block_gaia(x_gaia, x_spectra)\n",
    "            \n",
    "            # Update x_spectra and x_gaia\n",
    "            x_spectra = x_spectra_fused\n",
    "            x_gaia = x_gaia_fused\n",
    "        \n",
    "        # --- Pool across sequence dimension (since our seq_len=1, just squeeze) ---\n",
    "        x_spectra = x_spectra.mean(dim=1)  # (B, d_model_spectra)\n",
    "        x_gaia = x_gaia.mean(dim=1)        # (B, d_model_gaia)\n",
    "\n",
    "        # --- Late Fusion by Concatenation ---\n",
    "        x_fused = torch.cat([x_spectra, x_gaia], dim=-1)  # (B, d_model_spectra + d_model_gaia)\n",
    "\n",
    "        # --- Final classification ---\n",
    "        logits = self.classifier(x_fused)  # (B, num_classes)\n",
    "        return logits\n",
    "class CrossAttentionBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple cross-attention block with a feed-forward sub-layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads=8):\n",
    "        super().__init__()\n",
    "        self.cross_attn = nn.MultiheadAttention(\n",
    "            embed_dim=d_model, \n",
    "            num_heads=n_heads, \n",
    "            batch_first=True\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * d_model, d_model)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x_q, x_kv):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x_q  : (batch_size, seq_len_q, d_model)\n",
    "            x_kv : (batch_size, seq_len_kv, d_model)\n",
    "        \"\"\"\n",
    "        # Cross-attention\n",
    "        attn_output, _ = self.cross_attn(query=x_q, key=x_kv, value=x_kv)\n",
    "        x = self.norm1(x_q + attn_output)\n",
    "\n",
    "        # Feed forward\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = self.norm2(x + ffn_out)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "def download_lamost_spectra(obsid_list, save_folder=\"lamost_spectra_uniques\", num_workers=50):\n",
    "    \"\"\"\n",
    "    Downloads LAMOST spectra by obsid in parallel, skipping already downloaded files.\n",
    "    \n",
    "    :param obsid_list: List of obsids to download\n",
    "    :param save_folder: Folder where spectra will be saved\n",
    "    :param num_workers: Number of parallel download threads\n",
    "    \"\"\"\n",
    "    if not os.path.exists(save_folder):\n",
    "        os.makedirs(save_folder)\n",
    "    \n",
    "    # Check which files are already downloaded\n",
    "    existing_files = set(os.listdir(save_folder))\n",
    "    obsid_list = [obsid for obsid in obsid_list if str(obsid) not in existing_files]\n",
    "    \n",
    "    if not obsid_list:\n",
    "        print(\"‚úÖ All spectra are already downloaded.\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"üìÇ {len(obsid_list)} new spectra will be downloaded.\")\n",
    "    \n",
    "    # Create a requests Session with Retry to handle transient errors\n",
    "    retries = Retry(total=5, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])\n",
    "    session = requests.Session()\n",
    "    session.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
    "\n",
    "    # Use ThreadPoolExecutor to download in parallel\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        future_to_obsid = {\n",
    "            executor.submit(download_one_spectrum, obsid, session, save_folder): obsid \n",
    "            for obsid in obsid_list\n",
    "        }\n",
    "\n",
    "        # Wrap with tqdm for progress bar\n",
    "        for future in tqdm(as_completed(future_to_obsid), total=len(future_to_obsid), desc=\"Downloading Spectra\"):\n",
    "            obsid = future_to_obsid[future]\n",
    "            try:\n",
    "                obsid, success, error_msg = future.result()\n",
    "                results.append((obsid, success, error_msg))\n",
    "            except Exception as e:\n",
    "                results.append((obsid, False, str(e)))\n",
    "\n",
    "    # Print any failures\n",
    "    failures = [r for r in results if not r[1]]\n",
    "    if failures:\n",
    "        print(f\"‚ùå Failed to download {len(failures)} spectra:\")\n",
    "        for (obsid, _, err) in failures[:10]:  # show first 10 errors\n",
    "            print(f\"  obsid={obsid} => Error: {err}\")\n",
    "\n",
    "    # Return list of successfully downloaded obsids for reference\n",
    "    downloaded_obsids = [r[0] for r in results if r[1]]\n",
    "    return downloaded_obsids\n",
    "\n",
    "def download_one_spectrum(obsid, session, save_folder):\n",
    "    \"\"\"\n",
    "    Helper function to download one spectrum file given an obsid.\n",
    "    Uses the same session to get the file and saves it locally.\n",
    "    \"\"\"\n",
    "    url = f\"https://www.lamost.org/dr7/v2.0/spectrum/fits/{obsid}\"\n",
    "    local_path = os.path.join(save_folder, str(obsid))\n",
    "    \n",
    "    try:\n",
    "        resp = session.get(url, timeout=30)\n",
    "        resp.raise_for_status()\n",
    "        with open(local_path, \"wb\") as f:\n",
    "            f.write(resp.content)\n",
    "        return obsid, True, None\n",
    "    except Exception as e:\n",
    "        return obsid, False, str(e)\n",
    "    \n",
    "def precision_per_label(true_labels, predicted_labels):\n",
    "    \"\"\"\n",
    "    Calculates the precision for each class.\n",
    "    \"\"\"\n",
    "    precisions = []\n",
    "    for i in range(true_labels.shape[1]):\n",
    "        true_positives = np.sum((true_labels[:, i] == 1) & (predicted_labels[:, i] == 1))\n",
    "        false_positives = np.sum((true_labels[:, i] == 0) & (predicted_labels[:, i] == 1))\n",
    "\n",
    "        if true_positives + false_positives == 0:\n",
    "            precision = 0  # Avoid division by zero\n",
    "        else:\n",
    "            precision = true_positives / (true_positives + false_positives)\n",
    "\n",
    "        precisions.append(precision)\n",
    "    return precisions\n",
    "\n",
    "def f1_per_label(true_labels, predicted_labels):\n",
    "    \"\"\"Calculate the F1 score for each class.\"\"\"\n",
    "    f1_scores = []\n",
    "    for i in range(true_labels.shape[1]):\n",
    "        f1 = f1_score(true_labels[:, i], predicted_labels[:, i])\n",
    "        f1_scores.append(f1)\n",
    "    return f1_scores\n",
    "\n",
    "def recall_per_label(true_labels, predicted_labels):\n",
    "    \"\"\"Calculate the Recall score for each class.\"\"\"\n",
    "    recalls = []\n",
    "    for i in range(true_labels.shape[1]):\n",
    "        recall = recall_score(true_labels[:, i], predicted_labels[:, i])\n",
    "        recalls.append(recall)\n",
    "    return recalls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example usage with eclipsing binaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "‚úÖ Retrieved 2184477 eclipsing binary sources from Gaia DR3.\n"
     ]
    }
   ],
   "source": [
    "# Define ADQL query to fetch source IDs of eclipsing binaries\n",
    "query = \"\"\"\n",
    "SELECT source_id\n",
    "FROM gaiadr3.vari_eclipsing_binary\n",
    "\"\"\"\n",
    "\n",
    "# Run the query asynchronously\n",
    "job = Gaia.launch_job_async(query)\n",
    "results = job.get_results()\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "gaia_ids = results.to_pandas()\n",
    "\n",
    "# Convert to list\n",
    "gaia_ids = gaia_ids['source_id'].values.tolist()\n",
    "\n",
    "gaia_ids_small = gaia_ids[:1000]\n",
    "\n",
    "print(f\"‚úÖ Retrieved {len(gaia_ids)} eclipsing binary sources from Gaia DR3.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making lamost catalogue minimal (only needs to be done once)\n",
    "#lamost_catalogue = pd.read_csv(\"lamost/dr9_v2.0_LRS_catalogue.csv\")  # Load LAMOST catalog before passing it\n",
    "#lamost_catalogue = lamost_catalogue[[\"obsid\", \"ra\", \"dec\"]]\n",
    "#lamost_catalogue.to_csv(\"lamost/minimal.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the LAMOST catalog to cross-match with Gaia as csv\n",
    "lamost_catalogue = pd.read_csv(\"lamost_ra_dec.csv\")  # Load LAMOST catalog (Just obsid and Ra, Dec)\n",
    "label_cols = pd.read_pickle(\"list_of_classes.pkl\")\n",
    "\n",
    "# Example usage:\n",
    "model_path = \"model_fusion_mamba_v2.pth\"\n",
    "gaia_transformers = \"gaia_preprocessing.pkl\"\n",
    "\n",
    "df_predictions, gaia_lamost_merged = predict_star_labels(gaia_ids, model_path, lamost_catalogue, gaia_transformers)\n",
    "\n",
    "# Save the predictions to a npy file\n",
    "np.save(\"y_predictions.npy\", df_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Metrics and List of Incorrectly Classified IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Incorrectly Classified Gaia IDs:\n",
      "                 source_id\n",
      "1          544743587681792\n",
      "13        4326135874205184\n",
      "16        5147329325746432\n",
      "29        8080070434651136\n",
      "30        8081895796121216\n",
      "...                    ...\n",
      "30882  6897102854672378112\n",
      "30885  6897563996720348160\n",
      "30889  6898075097828930304\n",
      "30894  6898906706576365952\n",
      "30902  6910854854621600896\n",
      "\n",
      "[5993 rows x 1 columns]\n",
      "\n",
      "üìä Performance Metrics:\n",
      "   Class  Precision    Recall  F1 Score\n",
      "1     **        1.0  0.858076  0.923618\n",
      "54   EB*        1.0  0.856491  0.922699\n"
     ]
    }
   ],
   "source": [
    "# Load the predictions and class labels\n",
    "y_pred = np.load(\"y_predictions_small.npy\")\n",
    "y_pred = np.array(df_predictions.iloc[:, :-1], dtype=int)\n",
    "classes = pd.read_pickle(\"Pickles/Updated_list_of_Classes.pkl\")\n",
    "\n",
    "# Generate the expected y_true for eclipsing binaries\n",
    "y_true = np.zeros_like(y_pred)\n",
    "y_true[:, -1] = 1  # \"EB*\" column (last column)\n",
    "y_true[:, 1] = 1   # \"**\" column (second column)\n",
    "\n",
    "# Compute precision, recall, and F1-score for each class\n",
    "precision = precision_score(y_true, y_pred, average=None, zero_division=0)\n",
    "recall = recall_score(y_true, y_pred, average=None, zero_division=0)\n",
    "f1 = f1_score(y_true, y_pred, average=None, zero_division=0)\n",
    "#recall = recall_per_label(y_true, y_pred, average=None, zero_division=0)\n",
    "#f1 = f1_per_label(y_true, y_pred, average=None, zero_division=0)\n",
    "\n",
    "# Create a DataFrame to store metrics per class\n",
    "metrics_df = pd.DataFrame({\n",
    "    \"Class\": classes,\n",
    "    \"Precision\": precision,\n",
    "    \"Recall\": recall,\n",
    "    \"F1 Score\": f1\n",
    "})\n",
    "\n",
    "# Identify incorrectly classified samples (False Positives and False Negatives)\n",
    "incorrect_predictions = (y_pred != y_true).any(axis=1)\n",
    "incorrect_gaia_ids = df_predictions.loc[incorrect_predictions, \"source_id\"]\n",
    "\n",
    "# Display incorrectly classified Gaia IDs\n",
    "print(\"\\nüîç Incorrectly Classified Gaia IDs:\")\n",
    "print(pd.DataFrame({\"source_id\": incorrect_gaia_ids}))\n",
    "\n",
    "# Display the performance metrics for the non-zero classes\n",
    "print(\"\\nüìä Performance Metrics:\")\n",
    "metrics_df = metrics_df[metrics_df[\"Precision\"] > 0]\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the Incorrectly classified Spectrum and Gaia Data Version 1. Plots all incorectly classified Spectra and Gaia\n",
    "## Check v2 below for large catalogues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spectrum_with_gaia(source_id, gaia_lamost_merged, spectra_folder=\"lamost_spectra_uniques\"):\n",
    "    \"\"\"\n",
    "    Plots the LAMOST spectrum from FITS files and displays Gaia parameters below it.\n",
    "    \n",
    "    :param source_id: Gaia Source ID of the incorrectly classified source\n",
    "    :param gaia_lamost_merged: DataFrame containing Gaia and LAMOST cross-matched data\n",
    "    :param spectra_folder: Path to the folder containing LAMOST FITS spectra\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if 'obsid' not in gaia_lamost_merged.columns:\n",
    "            print(f\"‚ö†Ô∏è 'obsid' column not found in gaia_lamost_merged.\")\n",
    "            return\n",
    "        \n",
    "        match = gaia_lamost_merged.loc[gaia_lamost_merged['source_id'] == source_id]\n",
    "        if match.empty:\n",
    "            print(f\"‚ö†Ô∏è No LAMOST match found for source_id {source_id}.\")\n",
    "            return\n",
    "        \n",
    "        obsid = int(match.iloc[0]['obsid'])\n",
    "        print(f\"Found match: Source ID {source_id} -> ObsID {obsid}\")\n",
    "        \n",
    "        fits_path = f\"{spectra_folder}/{int(obsid)}\"\n",
    "        \n",
    "        # Load FITS data\n",
    "        with fits.open(fits_path) as hdul:\n",
    "            data = hdul[0].data\n",
    "            if data is None or data.shape[0] < 3:\n",
    "                print(f\"‚ö†Ô∏è Skipping {obsid}: Data not found or incorrect format.\")\n",
    "                return\n",
    "            \n",
    "            flux = data[0]\n",
    "            wavelength = data[2]\n",
    "            \n",
    "            fig, ax = plt.subplots(2, 1, figsize=(10, 10), gridspec_kw={'height_ratios': [3, 3]})\n",
    "            \n",
    "            # Plot Spectrum\n",
    "            ax[0].plot(wavelength, flux, color='blue', alpha=0.7, lw=1)\n",
    "            ax[0].set_xlabel(\"Wavelength (√Ö)\")\n",
    "            ax[0].set_ylabel(\"Flux\")\n",
    "            ax[0].set_title(f\"LAMOST Spectrum for Source ID: {source_id} (ObsID: {obsid})\")\n",
    "            ax[0].grid()\n",
    "            \n",
    "            # Show Gaia Data in a Horizontal Bar Plot\n",
    "            gaia_info = match.iloc[[0]].drop(columns=[\"source_id\", \"obsid\"], errors='ignore')\n",
    "            \n",
    "            # Identify potential data quality issues\n",
    "            issues = []\n",
    "            for col in gaia_info.columns:\n",
    "                value = gaia_info[col].values[0]\n",
    "                if col.endswith(\"_error\") and value > 1:\n",
    "                    issues.append(f\"Large error in {col}\")\n",
    "                if \"_flux\" in col and value < -1:\n",
    "                    issues.append(f\"Dim object in {col}\")\n",
    "\n",
    "            issue_text = \"; \".join(issues) if issues else \"No significant data issues\"\n",
    "\n",
    "            # Drop flux_ columns from bar plot for clarity\n",
    "            gaia_info = gaia_info.loc[:, ~gaia_info.columns.str.startswith(\"flux_\")]\n",
    "            if not gaia_info.empty:\n",
    "                gaia_data = gaia_info.to_dict(orient='records')[0]\n",
    "                labels = list(gaia_data.keys())\n",
    "                values = list(gaia_data.values())\n",
    "                \n",
    "                ax[1].bar(labels, values, color='skyblue')\n",
    "                ax[1].tick_params(\"x\", labelrotation=90)\n",
    "                ax[1].set_xlabel(issue_text)  # Display identified issues below the bar plot\n",
    "                ax[1].set_title(\"Gaia Parameters\")\n",
    "            else:\n",
    "                ax[1].text(0.5, 0.5, \"No Gaia Data Available\", ha='center', va='center', fontsize=12)\n",
    "                ax[1].axis(\"off\")\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {fits_path}: {e}\")\n",
    "\n",
    "gaia_lamost_merged['obsid'] = gaia_lamost_merged['obsid'].astype(int)\n",
    "gaia_lamost_merged['source_id'] = gaia_lamost_merged['source_id'].astype(int)\n",
    "\n",
    "# Loop through incorrectly classified sources and plot all spectra with labels if gaia data is bad\n",
    "#for source_id in incorrect_gaia_ids.astype(int):\n",
    "#    plot_spectrum_with_gaia(source_id, gaia_lamost_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Spectra and Gaia version 2 (Optimized for big queries, only plots incorrectly classified data that is \"good\" on gaia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gaia_lamost_merged' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 93\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfits_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# Ensure proper data types\u001b[39;00m\n\u001b[1;32m---> 93\u001b[0m gaia_lamost_merged[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobsid\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mgaia_lamost_merged\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobsid\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m     94\u001b[0m gaia_lamost_merged[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m gaia_lamost_merged[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# Loop through incorrectly classified sources and plot spectra only if valid\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m#for source_id in incorrect_gaia_ids.astype(int):\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m#    plot_spectrum_with_gaiav2(source_id, gaia_lamost_merged)\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'gaia_lamost_merged' is not defined"
     ]
    }
   ],
   "source": [
    "def plot_spectrum_with_gaiav2(source_id, gaia_lamost_merged, spectra_folder=\"lamost_spectra_uniques\"):\n",
    "    \"\"\"\n",
    "    Plots the LAMOST spectrum from FITS files and displays Gaia parameters below it,\n",
    "    only if there are no significant data issues.\n",
    "    \n",
    "    :param source_id: Gaia Source ID of the incorrectly classified source\n",
    "    :param gaia_lamost_merged: DataFrame containing Gaia and LAMOST cross-matched data\n",
    "    :param spectra_folder: Path to the folder containing LAMOST FITS spectra\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if 'obsid' not in gaia_lamost_merged.columns:\n",
    "            print(f\"‚ö†Ô∏è 'obsid' column not found in gaia_lamost_merged.\")\n",
    "            return\n",
    "        \n",
    "        match = gaia_lamost_merged.loc[gaia_lamost_merged['source_id'] == source_id]\n",
    "        if match.empty:\n",
    "            print(f\"‚ö†Ô∏è No LAMOST match found for source_id {source_id}.\")\n",
    "            return\n",
    "        \n",
    "        obsid = int(match.iloc[0]['obsid'])\n",
    "        fits_path = f\"{spectra_folder}/{obsid}\"\n",
    "        \n",
    "        # --- Step 1: Check for Gaia Data Issues BEFORE loading FITS ---\n",
    "        gaia_info = match.iloc[[0]].drop(columns=[\"source_id\", \"obsid\"], errors='ignore')\n",
    "        issues = []\n",
    "\n",
    "        for col in gaia_info.columns:\n",
    "            value = gaia_info[col].values[0]\n",
    "            if col.endswith(\"_error\") and value > 1:\n",
    "                issues.append(f\"Large error in {col}\")\n",
    "            if \"_flux\" in col and value < -1:\n",
    "                issues.append(f\"Dim object in {col}\")\n",
    "\n",
    "        if issues:\n",
    "            print(f\"‚ö†Ô∏è Skipping {obsid}: {', '.join(issues)}\")\n",
    "            return\n",
    "        \n",
    "        # --- Step 2: Check FITS File Exists and is Readable ---\n",
    "        try:\n",
    "            with fits.open(fits_path, memmap=True) as hdul:\n",
    "                data = hdul[0].data\n",
    "        except FileNotFoundError:\n",
    "            print(f\"‚ö†Ô∏è FITS file not found: {fits_path}\")\n",
    "            return\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error reading FITS file {obsid}: {e}\")\n",
    "            return\n",
    "\n",
    "        # --- Step 3: Validate FITS Data Format ---\n",
    "        if data is None or data.shape[0] < 3:\n",
    "            print(f\"‚ö†Ô∏è Skipping {obsid}: Data not found or incorrect format.\")\n",
    "            return\n",
    "        \n",
    "        flux = data[0]\n",
    "        wavelength = data[2]\n",
    "        \n",
    "        # --- Step 4: Check for Spectral Data Issues ---\n",
    "        if np.isnan(flux).any() or np.isnan(wavelength).any():\n",
    "            print(f\"‚ö†Ô∏è Skipping {obsid}: Spectrum contains NaN values.\")\n",
    "            return\n",
    "        if np.max(flux) > 1e6 or np.min(flux) < -1e6:\n",
    "            print(f\"‚ö†Ô∏è Skipping {obsid}: Flux values are extreme.\")\n",
    "            return\n",
    "\n",
    "        # --- Step 5: Plot Spectrum ---\n",
    "        fig, ax = plt.subplots(2, 1, figsize=(10, 10), gridspec_kw={'height_ratios': [3, 3]})\n",
    "        ax[0].plot(wavelength, flux, color='blue', alpha=0.7, lw=1)\n",
    "        ax[0].set_xlabel(\"Wavelength (√Ö)\")\n",
    "        ax[0].set_ylabel(\"Flux\")\n",
    "        ax[0].set_title(f\"LAMOST Spectrum for Source ID: {source_id} (ObsID: {obsid})\")\n",
    "        ax[0].grid()\n",
    "        \n",
    "        # --- Step 6: Show Gaia Data in a Horizontal Bar Plot ---\n",
    "        gaia_info = gaia_info.loc[:, ~gaia_info.columns.str.startswith(\"flux_\")]\n",
    "        if not gaia_info.empty:\n",
    "            gaia_data = gaia_info.to_dict(orient='records')[0]\n",
    "            labels = list(gaia_data.keys())\n",
    "            values = list(gaia_data.values())\n",
    "            ax[1].bar(labels, values, color='skyblue')\n",
    "            ax[1].tick_params(\"x\", labelrotation=90)\n",
    "            ax[1].set_title(\"Gaia Parameters\")\n",
    "        else:\n",
    "            ax[1].text(0.5, 0.5, \"No Gaia Data Available\", ha='center', va='center', fontsize=12)\n",
    "            ax[1].axis(\"off\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {fits_path}: {e}\")\n",
    "\n",
    "# Ensure proper data types\n",
    "gaia_lamost_merged['obsid'] = gaia_lamost_merged['obsid'].astype(int)\n",
    "gaia_lamost_merged['source_id'] = gaia_lamost_merged['source_id'].astype(int)\n",
    "\n",
    "# Loop through incorrectly classified sources and plot spectra only if valid\n",
    "#for source_id in incorrect_gaia_ids.astype(int):\n",
    "#    plot_spectrum_with_gaiav2(source_id, gaia_lamost_merged)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
