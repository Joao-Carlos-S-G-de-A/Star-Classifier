{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "596dc203",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import io\n",
    "import gzip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from astropy.io import fits\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "import random\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from astropy import units as u\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astroquery.gaia import Gaia\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.preprocessing import MinMaxScaler, PowerTransformer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from requests.exceptions import RequestException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3261e90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_star_labels(gaia_ids, model_path, lamost_catalogue, gaia_transformer_path):\n",
    "    \"\"\"\n",
    "    Given a list of Gaia DR3 IDs, this function:\n",
    "    1) Queries Gaia for star parameters.\n",
    "    2) Cross-matches with LAMOST spectra.\n",
    "    3) Downloads and processes LAMOST spectra.\n",
    "    4) Normalizes both Gaia and LAMOST data.\n",
    "    5) Applies a trained StarClassifierFusion model to predict labels.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with Gaia IDs and predicted multi-label classifications.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n🚀 Step 1: Querying Gaia data...\")\n",
    "    print(\"🔗 Gaia IDs:\", len(gaia_ids))\n",
    "    df_gaia = query_gaia_data(gaia_ids)\n",
    "    if df_gaia.empty:\n",
    "        print(\"⚠️ No Gaia data found. Exiting.\")\n",
    "        return None\n",
    "    print(\"🔗 Gaia data:\", df_gaia.shape)\n",
    "\n",
    "    print(\"\\n🔄 Step 2: Cross-matching with LAMOST catalog...\")\n",
    "    \n",
    "    df_matched = crossmatch_lamost(df_gaia, lamost_catalogue)\n",
    "    if df_matched.empty:\n",
    "        print(\"⚠️ No LAMOST matches found. Exiting.\")\n",
    "        return None\n",
    "\n",
    "    print(\"\\n📥 Step 3: Downloading LAMOST spectra (if needed)...\")\n",
    "    obsids = df_matched[\"obsid\"].unique()\n",
    "    spectra_folder = \"lamost_spectra_uniques\"\n",
    "    download_lamost_spectra(obsids, save_folder=spectra_folder, num_workers=500)\n",
    "\n",
    "    print(\"\\n🔧 Step 4: Converting from FITS LAMOST spectra...\")\n",
    "    #process_lamost_fits_files(folder_path=spectra_folder, output_file=\"Pickles/lamost_data.csv\")\n",
    "    process_lamost_fits_files(folder_path=\"lamost_spectra_uniques\", \n",
    "                          output_file=\"Pickles/lamost_data.csv\", \n",
    "                          matched_obsids=obsids)\n",
    "\n",
    "    print(\"\\n📊 Step 5: Extracting and saving flux & frequency values...\")\n",
    "    extract_flux_frequency_from_csv(csv_path=\"Pickles/lamost_data.csv\")\n",
    "\n",
    "    print(\"\\n📊 Step 6: Interpolating and normalizing LAMOST spectra...\")\n",
    "    nan_files = interpolate_spectrum(\"Pickles/flux_values.pkl\", \"Pickles/freq_values.pkl\", \"Pickles/lamost_data_interpolated.pkl\")\n",
    "    spectrum_interpolated = pd.read_pickle(\"Pickles/lamost_data_interpolated.pkl\")\n",
    "    spectrum_normalized = normalize_lamost_spectra(spectrum_interpolated)\n",
    "\n",
    "    if spectrum_normalized.empty:\n",
    "        print(\"⚠️ No processed LAMOST spectra found. Exiting.\")\n",
    "        return None\n",
    "\n",
    "    print(\"\\n📊 Step 7: Normalizing Gaia data...\")\n",
    "    with open(gaia_transformer_path, \"rb\") as f:\n",
    "        gaia_transformers = pickle.load(f)   # Dict of {col_name: fitted PowerTransformer}\n",
    "    gaia_normalized = apply_gaia_transforms(df_gaia, gaia_transformers)\n",
    "\n",
    "    print(\"\\n🔗 Step 8: Merging Gaia and LAMOST data...\")\n",
    "    gaia_lamost_match = df_matched[[\"source_id\", \"obsid\"]]\n",
    "    spectrum_normalized[\"obsid\"] = spectrum_normalized[\"obsid\"].astype(int)\n",
    "    gaia_lamost_match[\"obsid\"] = gaia_lamost_match[\"obsid\"].astype(int)\n",
    "\n",
    "    # Identify and remove all obsid values that appear more than once\n",
    "    obsid_counts = gaia_lamost_match[\"obsid\"].value_counts()\n",
    "    unique_obsids = obsid_counts[obsid_counts == 1].index  # Keep only obsid values that appear once\n",
    "\n",
    "    # Filter dataset to keep only unique obsid values\n",
    "    gaia_lamost_match = gaia_lamost_match[gaia_lamost_match[\"obsid\"].isin(unique_obsids)]\n",
    "\n",
    "    # Now, map the cleaned obsid-to-source_id mapping\n",
    "    spectrum_normalized[\"source_id\"] = spectrum_normalized[\"obsid\"].astype(int).map(\n",
    "        gaia_lamost_match.set_index(\"obsid\")[\"source_id\"]\n",
    "    )\n",
    "\n",
    "    # Merge Gaia and LAMOST data\n",
    "    gaia_lamost_merged = pd.merge(gaia_normalized, spectrum_normalized, on=\"source_id\", how=\"inner\")\n",
    "\n",
    "    if gaia_lamost_merged.empty:\n",
    "        print(\"⚠️ No valid data after merging. Exiting.\")\n",
    "        return None\n",
    "    \n",
    "    print(\"\\n🤖 Step 9: Predicting labels using the trained model...\")\n",
    "    predictions = process_star_data_fusion(model_path, gaia_lamost_merged, \"Pickles/Updated_List_of_Classes_ubuntu.pkl\", sigmoid_constant=0.5)\n",
    "\n",
    "    print(\"\\n💾 Step 10: Saving predictions...\")\n",
    "    df_predictions = pd.DataFrame(predictions, columns=pd.read_pickle(\"Pickles/Updated_List_of_Classes_ubuntu.pkl\"))\n",
    "    df_predictions[\"source_id\"] = gaia_lamost_merged[\"source_id\"].values\n",
    "\n",
    "    return df_predictions, gaia_lamost_merged\n",
    "\n",
    "def process_lamost_fits_files(folder_path=\"lamost_spectra_uniques\", \n",
    "                             output_file=\"Pickles/lamost_data.csv\", \n",
    "                             batch_size=10000, \n",
    "                             matched_obsids=None):\n",
    "    \"\"\"\n",
    "    Processes LAMOST FITS spectra by extracting both flux and wavelength data.\n",
    "    Handles both regular and gzipped FITS files with efficient error handling.\n",
    "    \"\"\"\n",
    "    print(\"\\n📂 Processing LAMOST FITS files...\")\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "    \n",
    "    # Define column headers\n",
    "    columns = [f'col_{i}' for i in range(3748)] + ['file_name', 'row']\n",
    "    \n",
    "    # Initialize the CSV file with headers\n",
    "    with open(output_file, 'w') as f:\n",
    "        pd.DataFrame(columns=columns).to_csv(f, index=False)\n",
    "    \n",
    "    # Get list of FITS files in the folder\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"⚠️ Folder {folder_path} does not exist. Creating it.\")\n",
    "        os.makedirs(folder_path)\n",
    "        return 0\n",
    "    \n",
    "    all_files = os.listdir(folder_path)\n",
    "    \n",
    "    # Filter files based on matched_obsids\n",
    "    if matched_obsids is not None:\n",
    "        matched_obsids = set(str(obsid) for obsid in matched_obsids)\n",
    "        all_files = [f for f in all_files if str(f).split('.')[0] in matched_obsids]\n",
    "    \n",
    "    total_files = len(all_files)\n",
    "    if total_files == 0:\n",
    "        print(\"⚠️ No matching FITS files found for processing.\")\n",
    "        return 0\n",
    "    \n",
    "    batch_list = []\n",
    "    processed_count = 0\n",
    "    error_count = 0\n",
    "    \n",
    "    # Sample a few files first to understand their structure\n",
    "    sample_files = all_files[:min(3, len(all_files))]\n",
    "    print(\"Examining sample files to determine FITS structure...\")\n",
    "    field_names = set()\n",
    "    \n",
    "    for filename in sample_files:\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        try:\n",
    "            hdul = open_fits_file(file_path)\n",
    "            if hdul:\n",
    "                with hdul:\n",
    "                    for hdu in hdul:\n",
    "                        if hasattr(hdu, 'data') and hdu.data is not None:\n",
    "                            if isinstance(hdu.data, fits.fitsrec.FITS_rec) and len(hdu.data) > 0:\n",
    "                                print(f\"Found FITS_rec in {filename}, names: {hdu.data.dtype.names}\")\n",
    "                                field_names.update(hdu.data.dtype.names)\n",
    "        except Exception as e:\n",
    "            print(f\"Error examining {filename}: {str(e)}\")\n",
    "    \n",
    "    if field_names:\n",
    "        print(f\"Found field names across sample files: {field_names}\")\n",
    "    \n",
    "    # Process all files\n",
    "    with tqdm(total=total_files, desc='Processing FITS files') as pbar:\n",
    "        for filename in all_files:\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            \n",
    "            try:\n",
    "                # Skip very small files\n",
    "                if os.path.getsize(file_path) < 100:\n",
    "                    print(f\"⚠️ Skipping {filename}: File too small\")\n",
    "                    error_count += 1\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "                \n",
    "                hdul = open_fits_file(file_path)\n",
    "                if not hdul:\n",
    "                    error_count += 1\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "                \n",
    "                with hdul:\n",
    "                    # Extract both flux and wavelength data\n",
    "                    flux_data, wave_data = extract_flux_and_wavelength(hdul, filename)\n",
    "                    \n",
    "                    # Process flux data if available\n",
    "                    if flux_data is not None and len(flux_data) > 0:\n",
    "                        # Ensure correct length\n",
    "                        if len(flux_data) < 3748:\n",
    "                            flux_data = np.pad(flux_data, (0, 3748-len(flux_data)), 'constant')\n",
    "                        elif len(flux_data) > 3748:\n",
    "                            flux_data = flux_data[:3748]\n",
    "                        \n",
    "                        # Create flux dictionary\n",
    "                        flux_dict = {\n",
    "                            f'col_{j}': float(value) if not np.isnan(value) else 0.0 \n",
    "                            for j, value in enumerate(flux_data)\n",
    "                        }\n",
    "                        flux_dict['file_name'] = filename\n",
    "                        flux_dict['row'] = 0  # Row 0 for flux data\n",
    "                        batch_list.append(flux_dict)\n",
    "                    \n",
    "                    # Process wavelength data if available\n",
    "                    if wave_data is not None and len(wave_data) > 0:\n",
    "                        # Ensure correct length\n",
    "                        if len(wave_data) < 3748:\n",
    "                            wave_data = np.pad(wave_data, (0, 3748-len(wave_data)), 'constant')\n",
    "                        elif len(wave_data) > 3748:\n",
    "                            wave_data = wave_data[:3748]\n",
    "                        \n",
    "                        # Create wavelength dictionary\n",
    "                        wave_dict = {\n",
    "                            f'col_{j}': float(value) if not np.isnan(value) else 0.0 \n",
    "                            for j, value in enumerate(wave_data)\n",
    "                        }\n",
    "                        wave_dict['file_name'] = filename\n",
    "                        wave_dict['row'] = 2  # Row 2 for wavelength data\n",
    "                        batch_list.append(wave_dict)\n",
    "                    \n",
    "                    # If we got both flux and wavelength, count as processed\n",
    "                    if (flux_data is not None and len(flux_data) > 0) or (wave_data is not None and len(wave_data) > 0):\n",
    "                        processed_count += 1\n",
    "                    else:\n",
    "                        error_count += 1\n",
    "                        print(f\"⚠️ Could not extract data from {filename}\")\n",
    "                \n",
    "                # Write batch to CSV when it gets large enough\n",
    "                if len(batch_list) >= batch_size:\n",
    "                    pd.DataFrame(batch_list).to_csv(output_file, mode='a', header=False, index=False)\n",
    "                    batch_list.clear()\n",
    "            \n",
    "            except Exception as e:\n",
    "                error_count += 1\n",
    "                print(f\"⚠️ Error processing {filename}: {str(e)}\")\n",
    "            \n",
    "            pbar.update(1)\n",
    "        \n",
    "        # Write any remaining data\n",
    "        if batch_list:\n",
    "            pd.DataFrame(batch_list).to_csv(output_file, mode='a', header=False, index=False)\n",
    "    \n",
    "    print(f\"✅ Successfully processed {processed_count} files\")\n",
    "    print(f\"⚠️ Encountered errors in {error_count} files\")\n",
    "    \n",
    "    return processed_count\n",
    "\n",
    "\n",
    "def open_fits_file(file_path):\n",
    "    \"\"\"\n",
    "    Opens a FITS file, handling both regular and gzipped formats.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if the file is gzipped\n",
    "        with open(file_path, 'rb') as f:\n",
    "            file_start = f.read(2)\n",
    "            f.seek(0)  # Reset file pointer\n",
    "            \n",
    "            if file_start == b'\\x1f\\x8b':  # gzip magic number\n",
    "                # Handle gzipped file\n",
    "                with gzip.GzipFile(fileobj=f) as gz_f:\n",
    "                    file_content = gz_f.read()\n",
    "                return fits.open(io.BytesIO(file_content), ignore_missing_simple=True)\n",
    "            else:\n",
    "                # Handle regular file\n",
    "                return fits.open(file_path, ignore_missing_simple=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error opening file {os.path.basename(file_path)}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def extract_flux_and_wavelength(hdul, filename):\n",
    "    \"\"\"\n",
    "    Extracts both flux and wavelength data from a FITS file.\n",
    "    \n",
    "    Args:\n",
    "        hdul (HDUList): The FITS file HDUList\n",
    "        filename (str): Filename for error reporting\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (flux_data, wavelength_data) - Both are arrays or None if extraction fails\n",
    "    \"\"\"\n",
    "    flux_data = None\n",
    "    wave_data = None\n",
    "    \n",
    "    try:\n",
    "        # Try each HDU until we find data\n",
    "        for hdu in hdul:\n",
    "            if not hasattr(hdu, 'data') or hdu.data is None:\n",
    "                continue\n",
    "                \n",
    "            # Handle FITS_rec (record array) format\n",
    "            if isinstance(hdu.data, fits.fitsrec.FITS_rec):\n",
    "                if len(hdu.data) == 0:\n",
    "                    continue\n",
    "                    \n",
    "                # Get field names\n",
    "                field_names = hdu.data.dtype.names\n",
    "                \n",
    "                # Look for flux and wavelength fields\n",
    "                flux_field = None\n",
    "                wave_field = None\n",
    "                \n",
    "                # Search for field names (case-insensitive)\n",
    "                for field in field_names:\n",
    "                    field_lower = field.lower()\n",
    "                    if 'flux' in field_lower or 'spectrum' in field_lower or 'spec' in field_lower:\n",
    "                        flux_field = field\n",
    "                    elif 'wave' in field_lower or 'lambda' in field_lower or 'wavelength' in field_lower:\n",
    "                        wave_field = field\n",
    "                \n",
    "                # Extract flux data\n",
    "                if flux_field and flux_data is None:\n",
    "                    try:\n",
    "                        flux_array = hdu.data[0][flux_field]\n",
    "                        if isinstance(flux_array, np.ndarray):\n",
    "                            flux_data = flux_array\n",
    "                        else:\n",
    "                            flux_data = np.array([flux_array])\n",
    "                        #print(f\"Found flux data in {filename}, field: {flux_field}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error extracting flux from field {flux_field}: {str(e)}\")\n",
    "                \n",
    "                # Extract wavelength data\n",
    "                if wave_field and wave_data is None:\n",
    "                    try:\n",
    "                        wave_array = hdu.data[0][wave_field]\n",
    "                        if isinstance(wave_array, np.ndarray):\n",
    "                            wave_data = wave_array\n",
    "                        else:\n",
    "                            wave_data = np.array([wave_array])\n",
    "                        #print(f\"Found wavelength data in {filename}, field: {wave_field}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error extracting wavelength from field {wave_field}: {str(e)}\")\n",
    "            \n",
    "            # Handle standard array data\n",
    "            elif isinstance(hdu.data, np.ndarray):\n",
    "                # In standard LAMOST FITS, HDU 0 often contains the flux, HDU 2 contains wavelength\n",
    "                hdu_index = hdul.index(hdu)\n",
    "                \n",
    "                if flux_data is None and hdu.data.size > 0:\n",
    "                    # Take the first row if there are multiple\n",
    "                    if hdu.data.ndim > 1:\n",
    "                        flux_data = hdu.data[0]\n",
    "                    else:\n",
    "                        flux_data = hdu.data\n",
    "                    print(f\"Found flux data in {filename}, HDU: {hdu_index}\")\n",
    "                \n",
    "                # Try to find wavelength data in a different HDU if not already found\n",
    "                if wave_data is None and hdu_index > 0:\n",
    "                    # Check if this might be wavelength data\n",
    "                    if hdu.data.size > 0:\n",
    "                        # Take the first row if there are multiple\n",
    "                        if hdu.data.ndim > 1:\n",
    "                            wave_data = hdu.data[0]\n",
    "                        else:\n",
    "                            wave_data = hdu.data\n",
    "                        print(f\"Found potential wavelength data in {filename}, HDU: {hdu_index}\")\n",
    "            \n",
    "            # If we have both flux and wavelength, we can stop\n",
    "            if flux_data is not None and wave_data is not None:\n",
    "                break\n",
    "        \n",
    "        # If we found data, convert to the right format\n",
    "        if flux_data is not None and not isinstance(flux_data, np.ndarray):\n",
    "            flux_data = np.array(flux_data)\n",
    "        if wave_data is not None and not isinstance(wave_data, np.ndarray):\n",
    "            wave_data = np.array(wave_data)\n",
    "        \n",
    "        return flux_data, wave_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting data from {filename}: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def extract_flux_frequency_from_csv(csv_path=\"Pickles/lamost_data.csv\", \n",
    "                                   flux_pickle=\"Pickles/flux_values.pkl\", \n",
    "                                   freq_pickle=\"Pickles/freq_values.pkl\", \n",
    "                                   chunk_size=10000):\n",
    "    \"\"\"\n",
    "    Extracts flux and frequency (wavelength) data from a CSV file and saves them as separate pickle files.\n",
    "    \"\"\"\n",
    "    print(\"\\n📊 Extracting flux and frequency values...\")\n",
    "\n",
    "    # Initialize empty dataframes\n",
    "    flux_values = pd.DataFrame()\n",
    "    freq_values = pd.DataFrame()\n",
    "\n",
    "    try:\n",
    "        # Count total rows for progress tracking\n",
    "        total_rows = sum(1 for _ in open(csv_path)) - 1  # Subtract header row\n",
    "        \n",
    "        # Process the CSV in chunks\n",
    "        for chunk in tqdm(pd.read_csv(csv_path, chunksize=chunk_size), \n",
    "                         total=total_rows // chunk_size + 1):\n",
    "            # If 'row' column is present, use it to filter rows\n",
    "            if 'row' in chunk.columns:\n",
    "                flux_chunk = chunk[chunk['row'] == 0].drop(columns=['row'])\n",
    "                wave_chunk = chunk[chunk['row'] == 2].drop(columns=['row'])\n",
    "            else:\n",
    "                # For old format data\n",
    "                flux_mask = chunk.index % 3 == 0  # First row for each file\n",
    "                flux_chunk = chunk[flux_mask]\n",
    "                \n",
    "                wave_mask = chunk.index % 3 == 2  # Third row for each file\n",
    "                wave_chunk = chunk[wave_mask]\n",
    "            \n",
    "            # Concatenate with existing data\n",
    "            flux_values = pd.concat([flux_values, flux_chunk])\n",
    "            freq_values = pd.concat([freq_values, wave_chunk])\n",
    "        \n",
    "        # Ensure filenames match between flux and frequency\n",
    "        if not freq_values.empty and not flux_values.empty:\n",
    "            common_files = set(flux_values['file_name']).intersection(set(freq_values['file_name']))\n",
    "            flux_values = flux_values[flux_values['file_name'].isin(common_files)]\n",
    "            freq_values = freq_values[freq_values['file_name'].isin(common_files)]\n",
    "        \n",
    "        print(f\"✅ Flux values shape: {flux_values.shape}, Frequency values shape: {freq_values.shape}\")\n",
    "        \n",
    "        # If no wavelength data was found, generate synthetic wavelength data based on LAMOST standard wavelength range\n",
    "        if freq_values.empty and not flux_values.empty:\n",
    "            print(\"⚠️ No wavelength data found, generating synthetic wavelength range\")\n",
    "            # Create synthetic wavelength values (3800-9000 Å is typical LAMOST range)\n",
    "            wavelength_range = np.linspace(3800, 9000, 3748)\n",
    "            \n",
    "            # Create wavelength entries for each file in flux_values\n",
    "            wavelength_rows = []\n",
    "            for filename in flux_values['file_name'].unique():\n",
    "                wave_dict = {f'col_{j}': float(value) for j, value in enumerate(wavelength_range)}\n",
    "                wave_dict['file_name'] = filename\n",
    "                wavelength_rows.append(wave_dict)\n",
    "            \n",
    "            freq_values = pd.DataFrame(wavelength_rows)\n",
    "            print(f\"✅ Created synthetic wavelength data with shape: {freq_values.shape}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error processing CSV: {str(e)}\")\n",
    "        return 0, 0\n",
    "    \n",
    "    # Save extracted values\n",
    "    os.makedirs(os.path.dirname(flux_pickle), exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(freq_pickle), exist_ok=True)\n",
    "    \n",
    "    flux_values.to_pickle(flux_pickle)\n",
    "    freq_values.to_pickle(freq_pickle)\n",
    "    \n",
    "    return flux_values.shape[0], freq_values.shape[0]\n",
    "\n",
    "def query_gaia_data(gaia_id_list):\n",
    "    \"\"\"\n",
    "    Given a list of Gaia DR3 source IDs, queries the Gaia archive\n",
    "    for the relevant columns used during training.\n",
    "    Returns a concatenated DataFrame of results.\n",
    "    \"\"\"\n",
    "    # Columns you actually needed\n",
    "    desired_cols = [\n",
    "        \"source_id\", \"ra\", \"ra_error\", \"dec\", \"dec_error\",\n",
    "        \"pmra\", \"pmra_error\", \"pmdec\", \"pmdec_error\",\n",
    "        \"parallax\", \"parallax_error\",\n",
    "        \"phot_g_mean_flux\", \"phot_g_mean_flux_error\",\n",
    "        \"phot_bp_mean_flux\", \"phot_bp_mean_flux_error\",\n",
    "        \"phot_rp_mean_flux\", \"phot_rp_mean_flux_error\"\n",
    "    ]\n",
    "\n",
    "    all_dfs = []\n",
    "    chunks = split_ids_into_chunks(gaia_id_list, chunk_size=30000)\n",
    "    for chunk in chunks:\n",
    "        query = f\"\"\"\n",
    "        SELECT {', '.join(desired_cols)}\n",
    "        FROM gaiadr3.gaia_source\n",
    "        WHERE source_id IN ({chunk})\n",
    "        \"\"\"\n",
    "        job = Gaia.launch_job_async(query)\n",
    "        tbl = job.get_results()\n",
    "        df_tmp = tbl.to_pandas()\n",
    "        all_dfs.append(df_tmp)\n",
    "\n",
    "    # Convert string IDs to integers\n",
    "    if isinstance(gaia_id_list[0], str):\n",
    "        gaia_id_list = [int(x) for x in gaia_id_list]\n",
    "    \n",
    "    # Check for missing IDs\n",
    "    all_ids = pd.concat(all_dfs)[\"source_id\"].values\n",
    "    missing_ids = set(gaia_id_list) - set(all_ids)\n",
    "    if missing_ids:\n",
    "        print(f\"Warning: {len(missing_ids)} IDs not found in Gaia DR3.\")\n",
    "        print(f\"Missing IDs: {missing_ids}\")\n",
    "\n",
    "    if not all_dfs:\n",
    "        return pd.DataFrame(columns=desired_cols)\n",
    "    else:\n",
    "        return pd.concat(all_dfs, ignore_index=True)\n",
    "    \n",
    "def query_gaia_data(gaia_id_list, desired_cols=None, chunk_size=30000, MAX_RETRIES=5, RETRY_DELAY_SECONDS=5):\n",
    "    \"\"\"\n",
    "    Given a list of Gaia DR3 source IDs, queries the Gaia archive\n",
    "    for the relevant columns. Includes a retry mechanism for transient errors.\n",
    "    Returns a concatenated DataFrame of results.\n",
    "    \"\"\"\n",
    "    if desired_cols is None:\n",
    "        desired_cols = [\n",
    "            \"source_id\", \"ra\", \"ra_error\", \"dec\", \"dec_error\",\n",
    "            \"pmra\", \"pmra_error\", \"pmdec\", \"pmdec_error\",\n",
    "            \"parallax\", \"parallax_error\",\n",
    "            \"phot_g_mean_flux\", \"phot_g_mean_flux_error\",\n",
    "            \"phot_bp_mean_flux\", \"phot_bp_mean_flux_error\",\n",
    "            \"phot_rp_mean_flux\", \"phot_rp_mean_flux_error\"\n",
    "        ]\n",
    "\n",
    "    # Convert input IDs to strings for consistency\n",
    "    gaia_id_list_str = [str(x) for x in gaia_id_list]\n",
    "    num_total_ids = len(gaia_id_list_str)\n",
    "\n",
    "    chunks_sql = split_ids_into_chunks(gaia_id_list_str, chunk_size=chunk_size)\n",
    "    num_chunks = len(chunks_sql)\n",
    "    print(f\"Gaia query split into {num_chunks} chunks of up to {chunk_size} IDs.\")\n",
    "\n",
    "    all_dfs = []\n",
    "    failed_chunks = 0\n",
    "\n",
    "    for i, chunk_sql in enumerate(chunks_sql):\n",
    "        print(f\"\\nProcessing Chunk {i+1}/{num_chunks}...\")\n",
    "        df_tmp = None # Initialize chunk result to None\n",
    "\n",
    "        for attempt in range(MAX_RETRIES):\n",
    "            try:\n",
    "                print(f\"  Attempt {attempt + 1}/{MAX_RETRIES}: Launching Gaia job...\")\n",
    "                query = f\"\"\"\n",
    "                SELECT {', '.join(desired_cols)}\n",
    "                FROM gaiadr3.gaia_source\n",
    "                WHERE source_id IN ({chunk_sql})\n",
    "                \"\"\"\n",
    "                # Set a timeout for the job launch/get_results (optional, tune as needed)\n",
    "                # Gaia.TIMEOUT = 60 # Example: 60 seconds timeout for HTTP requests\n",
    "                # Gaia.POLL_INTERVAL = 10 # Example: Check job status every 10s\n",
    "\n",
    "                job = Gaia.launch_job_async(query)\n",
    "                print(f\"  Job launched (ID: {job.jobid}). Waiting for results...\")\n",
    "                tbl = job.get_results() # This can also time out or fail\n",
    "                print(f\"  Results received for chunk {i+1}.\")\n",
    "                df_tmp = tbl.to_pandas()\n",
    "                print(f\"  Retrieved {len(df_tmp)} records for this chunk.\")\n",
    "                # Success! Break the retry loop for this chunk\n",
    "                break\n",
    "\n",
    "            # --- Catch Specific Exceptions ---\n",
    "            # Catch the error you observed\n",
    "            except ConnectionResetError as e:\n",
    "                print(f\"  Attempt {attempt + 1} failed: ConnectionResetError - {e}\")\n",
    "            # Catch common network/request errors from astroquery/requests\n",
    "            except RequestException as e:\n",
    "                print(f\"  Attempt {attempt + 1} failed: RequestException - {e}\")\n",
    "            # Catch potential socket timeouts\n",
    "            #except SocketTimeoutError as e:\n",
    "            #     print(f\"  Attempt {attempt + 1} failed: SocketTimeoutError - {e}\")\n",
    "            # Catch potential errors during job execution reported by Gaia TAP service\n",
    "            except Exception as e:\n",
    "                 # Check if it's a TAP service error indicating job failure\n",
    "                 if \"job execution failed\" in str(e).lower() or \"error executing query\" in str(e).lower():\n",
    "                     print(f\"  Attempt {attempt + 1} failed: Gaia TAP execution error - {e}\")\n",
    "                     # Often retrying these won't help if the query is bad, but sometimes it's transient\n",
    "                 else:\n",
    "                     print(f\"  Attempt {attempt + 1} failed with unexpected error: {type(e).__name__} - {e}\")\n",
    "                 # Decide if you want to retry unexpected errors or just fail the chunk\n",
    "                 # For now, we'll retry them too.\n",
    "\n",
    "            # --- Retry Logic ---\n",
    "            if attempt < MAX_RETRIES - 1:\n",
    "                print(f\"  Retrying in {RETRY_DELAY_SECONDS} seconds...\")\n",
    "                time.sleep(RETRY_DELAY_SECONDS)\n",
    "            else:\n",
    "                print(f\"  Max retries reached for chunk {i+1}. Skipping this chunk.\")\n",
    "                failed_chunks += 1\n",
    "\n",
    "        # End of retry loop for one chunk\n",
    "\n",
    "        # Append the result if successful\n",
    "        if df_tmp is not None and not df_tmp.empty:\n",
    "            all_dfs.append(df_tmp)\n",
    "\n",
    "        # Optional short sleep between chunks to be nice to the server\n",
    "        # Even if a chunk failed, wait before starting the next one\n",
    "        time.sleep(2)\n",
    "\n",
    "    # --- Consolidate Results ---\n",
    "    print(\"\\nConsolidating results...\")\n",
    "    if failed_chunks > 0:\n",
    "        print(f\"Warning: Failed to retrieve data for {failed_chunks} out of {num_chunks} chunks after retries.\")\n",
    "\n",
    "    if not all_dfs:\n",
    "         print(\"No data was successfully retrieved from Gaia.\")\n",
    "         return pd.DataFrame(columns=desired_cols) # Return empty DataFrame\n",
    "\n",
    "    final_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    print(f\"Consolidated DataFrame shape: {final_df.shape}\")\n",
    "\n",
    "    # --- Final Checks (same as before) ---\n",
    "    if gaia_id_list and isinstance(gaia_id_list[0], int):\n",
    "         try:\n",
    "             final_df['source_id'] = final_df['source_id'].astype(int)\n",
    "             input_ids = set(gaia_id_list)\n",
    "         except ValueError:\n",
    "             print(\"Warning: Could not convert all retrieved source_ids to int.\")\n",
    "             input_ids = set(gaia_id_list_str) # Compare as strings\n",
    "    else:\n",
    "         input_ids = set(gaia_id_list_str) # Compare as strings\n",
    "\n",
    "    retrieved_ids_final = set(final_df['source_id'].astype(str).tolist())\n",
    "    missing_ids = input_ids - retrieved_ids_final\n",
    "    if missing_ids:\n",
    "        # This count includes IDs that might have been in failed chunks\n",
    "        print(f\"Warning: {len(missing_ids)} out of {num_total_ids} requested IDs were not found in the final results (includes skipped chunks).\")\n",
    "    else:\n",
    "        print(\"All requested IDs that exist in Gaia DR3 and were in successful chunks seem to be retrieved.\")\n",
    "\n",
    "    return final_df\n",
    "    \n",
    "     \n",
    "def split_ids_into_chunks(gaia_id_list, chunk_size=50000):\n",
    "    \"\"\"\n",
    "    Takes a Python list of Gaia IDs (strings or ints),\n",
    "    returns a list of comma-joined strings, each containing up to `chunk_size` IDs.\n",
    "    \"\"\"\n",
    "    # Convert everything to string for the SQL query\n",
    "    gaia_id_list = [str(x) for x in gaia_id_list]\n",
    "    chunks = []\n",
    "    for i in range(0, len(gaia_id_list), chunk_size):\n",
    "        chunk = \", \".join(gaia_id_list[i:i+chunk_size])\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def crossmatch_lamost(gaia_df, lamost_df, match_radius=3*u.arcsec):\n",
    "    \"\"\"\n",
    "    Cross-matches Gaia sources with a local LAMOST catalogue.\n",
    "    Returns a merged DataFrame of matched objects.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure RA/Dec are numeric\n",
    "    gaia_df['ra'] = pd.to_numeric(gaia_df['ra'], errors='coerce')\n",
    "    gaia_df['dec'] = pd.to_numeric(gaia_df['dec'], errors='coerce')\n",
    "    lamost_df['ra'] = pd.to_numeric(lamost_df['ra'], errors='coerce')\n",
    "    lamost_df['dec'] = pd.to_numeric(lamost_df['dec'], errors='coerce')\n",
    "\n",
    "    # Drop NaN values\n",
    "    gaia_df = gaia_df.dropna(subset=['ra', 'dec'])\n",
    "    lamost_df = lamost_df.dropna(subset=['ra', 'dec'])\n",
    "\n",
    "    print(f\"After NaN removal: Gaia={gaia_df.shape}, LAMOST={lamost_df.shape}\")\n",
    "\n",
    "    # Check if LAMOST coordinates are in arcseconds (convert if necessary)\n",
    "    if lamost_df['ra'].max() > 360:  # RA should not exceed 360 degrees\n",
    "        print(\"⚠️ LAMOST RA/Dec seem to be in arcseconds. Converting to degrees.\")\n",
    "        lamost_df['ra'] /= 3600\n",
    "        lamost_df['dec'] /= 3600\n",
    "\n",
    "    # Convert to SkyCoord objects (ensuring same frame)\n",
    "    gaia_coords = SkyCoord(ra=gaia_df['ra'].values*u.deg,\n",
    "                           dec=gaia_df['dec'].values*u.deg,\n",
    "                           frame='icrs')\n",
    "\n",
    "    lamost_coords = SkyCoord(ra=lamost_df['ra'].values*u.deg,\n",
    "                             dec=lamost_df['dec'].values*u.deg,\n",
    "                             frame='icrs')\n",
    "\n",
    "    # Perform crossmatch\n",
    "    idx, d2d, _ = gaia_coords.match_to_catalog_sky(lamost_coords)\n",
    "\n",
    "    # Apply matching radius filter\n",
    "    matches = d2d < match_radius\n",
    "    #print(f\"Match distances (arcsec): {d2d.to(u.arcsec).value[matches]}\")\n",
    "\n",
    "    if matches.sum() == 0:\n",
    "        print(\"⚠️ No matches found! Try increasing `match_radius`.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Extract matched rows correctly\n",
    "    gaia_matched = gaia_df.iloc[matches].copy().reset_index(drop=True)\n",
    "    lamost_matched = lamost_df.iloc[idx[matches]].copy().reset_index(drop=True)\n",
    "\n",
    "    print(f\"Matched Gaia Objects: {gaia_matched.shape}\")\n",
    "    print(f\"Matched LAMOST Objects: {lamost_matched.shape}\")\n",
    "\n",
    "    # Merge matches into final DataFrame\n",
    "    final = pd.concat([gaia_matched, lamost_matched], axis=1)\n",
    "\n",
    "    return final\n",
    "\n",
    "\n",
    "\n",
    "def download_lamost_spectra(obsid_list, save_folder=\"lamost_spectra_uniques\", num_workers=10):\n",
    "    \"\"\"\n",
    "    Downloads LAMOST spectra by obsid in parallel with robust error handling\n",
    "    and rate limiting to avoid overwhelming the server.\n",
    "    \n",
    "    Args:\n",
    "        obsid_list (list): List of obsids to download\n",
    "        save_folder (str): Folder where spectra will be saved\n",
    "        num_workers (int): Number of parallel download threads\n",
    "        \n",
    "    Returns:\n",
    "        list: List of successfully downloaded obsids\n",
    "    \"\"\"\n",
    "    if not os.path.exists(save_folder):\n",
    "        os.makedirs(save_folder)\n",
    "    \n",
    "    # Check which files are already downloaded\n",
    "    existing_files = set(os.listdir(save_folder))\n",
    "    obsid_list = [obsid for obsid in obsid_list if str(obsid) not in existing_files]\n",
    "    \n",
    "    if not obsid_list:\n",
    "        print(\"✅ All spectra are already downloaded.\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"📂 {len(obsid_list)} new spectra will be downloaded.\")\n",
    "    \n",
    "    # Create a requests Session with conservative retry settings\n",
    "    retries = Retry(\n",
    "        total=5, \n",
    "        backoff_factor=2,  # Exponential backoff\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "        respect_retry_after_header=True\n",
    "    )\n",
    "    session = requests.Session()\n",
    "    session.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
    "    \n",
    "    # Split the obsid_list into smaller chunks to download in batches\n",
    "    chunk_size = 100  # Download in smaller batches\n",
    "    obsid_chunks = [obsid_list[i:i + chunk_size] for i in range(0, len(obsid_list), chunk_size)]\n",
    "    \n",
    "    downloaded_obsids = []\n",
    "    \n",
    "    for chunk_idx, chunk in enumerate(obsid_chunks):\n",
    "        print(f\"Processing batch {chunk_idx+1}/{len(obsid_chunks)} ({len(chunk)} files)\")\n",
    "        \n",
    "        # Use ThreadPoolExecutor to download in parallel\n",
    "        results = []\n",
    "        with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "            future_to_obsid = {\n",
    "                executor.submit(download_one_spectrum, obsid, session, save_folder): obsid \n",
    "                for obsid in chunk\n",
    "            }\n",
    "\n",
    "            # Wrap with tqdm for progress bar\n",
    "            for future in tqdm(as_completed(future_to_obsid), total=len(future_to_obsid), desc=\"Downloading Spectra\"):\n",
    "                obsid = future_to_obsid[future]\n",
    "                try:\n",
    "                    obsid, success, error_msg = future.result()\n",
    "                    results.append((obsid, success, error_msg))\n",
    "                except Exception as e:\n",
    "                    results.append((obsid, False, str(e)))\n",
    "\n",
    "        # Print any failures\n",
    "        failures = [r for r in results if not r[1]]\n",
    "        if failures:\n",
    "            print(f\"❌ Failed to download {len(failures)} spectra in this batch.\")\n",
    "        \n",
    "        # Add successful downloads to the list\n",
    "        batch_downloaded = [r[0] for r in results if r[1]]\n",
    "        downloaded_obsids.extend(batch_downloaded)\n",
    "        \n",
    "        # Rest between batches to avoid overwhelming the server\n",
    "        if chunk_idx < len(obsid_chunks) - 1:\n",
    "            print(f\"Sleeping between batches to avoid server overload...\")\n",
    "            time.sleep(2)  # Sleep 2 seconds between batches\n",
    "    \n",
    "    print(f\"✅ Successfully downloaded {len(downloaded_obsids)} out of {len(obsid_list)} spectra.\")\n",
    "    return downloaded_obsids\n",
    "\n",
    "\n",
    "def interpolate_spectrum(fluxes_loc, frequencies_loc, output_dir, limit=10, edge_limit=20):\n",
    "    \"\"\"Interpolates the flux values to fill in missing data points.\"\"\"\n",
    "    # Load the data from the pickle file    \n",
    "    df_freq = pd.read_pickle(frequencies_loc).reset_index(drop=True)      \n",
    "    df_flux = pd.read_pickle(fluxes_loc).reset_index(drop=True)  # Reset index for zero-based iteration\n",
    "\n",
    "    # Initialize an empty list to store the results before concatenating into a DataFrame\n",
    "    results_list = []\n",
    "\n",
    "    # Initialize lists to store problematic file_names\n",
    "    nan_files = []  \n",
    "\n",
    "    # Count the number of successful interpolations\n",
    "    cnt_success = 0\n",
    "\n",
    "    # Debugging counters\n",
    "    cnt_total_skipped = 0\n",
    "    cnt_nan_skipped = 0\n",
    "    cnt_zero_skipped = 0\n",
    "\n",
    "    # Overwrite the output file at the beginning\n",
    "    if os.path.exists(output_dir):\n",
    "        os.remove(output_dir)\n",
    "\n",
    "    # Loop through each row in the DataFrame (each row is a spectrum) with tqdm for progress bar\n",
    "    for index, row in tqdm(df_flux.iterrows(), total=len(df_flux), desc='Interpolating spectra'):\n",
    "\n",
    "        # Extract the fluxes (assuming they start at column 0 and continue to the last column)\n",
    "        fluxes = row[:-2].values  # Exclude the last columns (file_name, label)\n",
    "\n",
    "        # Extract the frequencies\n",
    "        frequencies = df_freq.iloc[int(index), :-2].values  # Exclude the last columns (file_name, label)\n",
    "\n",
    "        # Count the number of NaN and 0 values in the fluxes and frequencies\n",
    "        fluxes = pd.to_numeric(row[:-2], errors='coerce').values  # Exclude and convert to numeric\n",
    "        frequencies = pd.to_numeric(df_freq.iloc[index, :-2], errors='coerce').values  # Same for frequencies\n",
    "        num_nan = np.isnan(fluxes).sum() + np.isnan(frequencies).sum()  # Count NaN values\n",
    "        num_zero = (fluxes == 0).sum() + (frequencies == 0).sum()  # Count zero values\n",
    "        num_freq_nan = np.isnan(frequencies).sum() + (frequencies == 0).sum()\n",
    "        if num_freq_nan > 0:\n",
    "            print(f\"Number of NaN or zero frequency values: {num_freq_nan}\")\n",
    "        # Special handling for NaN values, counting nans in sequence, except for the first and last 10\n",
    "        if num_nan > limit and index > edge_limit and index < len(fluxes)-edge_limit:\n",
    "            cnt_nan_skipped += 1  # Debug: count NaN-skipped rows\n",
    "            nan_files.append(row['file_name'])\n",
    "            continue\n",
    "        \n",
    "        if num_zero > limit and index > edge_limit and index < len(fluxes)-edge_limit:\n",
    "            cnt_zero_skipped += 1  # Debug: count zero-skipped rows\n",
    "            nan_files.append(row['file_name'])\n",
    "            continue\n",
    "\n",
    "        # Deal with NaN values\n",
    "        fluxes = fluxes[~np.isnan(fluxes)]\n",
    "        frequencies = frequencies[~np.isnan(fluxes)]\n",
    "\n",
    "        # Interpolate to fill in missing values\n",
    "        f = interp1d(frequencies, fluxes, kind='linear', fill_value=\"extrapolate\")\n",
    "        new_frequencies = np.linspace(frequencies.min(), frequencies.max(), len(row[:-2].values))\n",
    "\n",
    "        # Interpolated flux values\n",
    "        interpolated_fluxes = f(new_frequencies)\n",
    "\n",
    "        # Store the interpolated data along with labels and other metadata\n",
    "        # Create a dictionary for the interpolated spectrum\n",
    "        interpolated_data = {f'flux_{i}': value for i, value in enumerate(interpolated_fluxes)}\n",
    "\n",
    "        # Add the original metadata back (e.g., file_name, label, row)\n",
    "        interpolated_data['file_name'] = row['file_name']\n",
    "                \n",
    "        # Append the interpolated data to the results list\n",
    "        results_list.append(interpolated_data)\n",
    "\n",
    "        if index % 2000 == 0:  # Save every 5000 rows\n",
    "            if os.path.exists(output_dir):\n",
    "                existing_df = pd.read_pickle(output_dir)  # Load existing data\n",
    "                new_df = pd.DataFrame(results_list)\n",
    "                # Concatenate existing and new data\n",
    "                combined_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "                combined_df.to_pickle(output_dir)  # Save combined DataFrame\n",
    "            else:\n",
    "                # If the file doesn't exist, create a new DataFrame and save\n",
    "                pd.DataFrame(results_list).to_pickle(output_dir)\n",
    "            cnt_success += len(results_list)  # Increment the count of successful interpolations\n",
    "            results_list = []  # Clear list to free memory\n",
    "\n",
    "    print(f\"Initial number of rows: {len(df_flux)}\")\n",
    "\n",
    "    # After the loop, save any remaining results\n",
    "    if results_list:\n",
    "        if os.path.exists(output_dir):\n",
    "            existing_df = pd.read_pickle(output_dir)\n",
    "            new_df = pd.DataFrame(results_list)\n",
    "            combined_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "            combined_df.to_pickle(output_dir)\n",
    "        else:\n",
    "            pd.DataFrame(results_list).to_pickle(output_dir)\n",
    "        cnt_success += len(results_list)\n",
    "\n",
    "    # Debugging information\n",
    "    cnt_total_skipped = len(nan_files)\n",
    "    print(f\"Total successful interpolations: {cnt_success}\")\n",
    "    #print(f\"Total skipped due to NaNs: {cnt_nan_skipped}\")\n",
    "    #print(f\"Total skipped due to zeros: {cnt_zero_skipped}\")\n",
    "    print(f\"Total skipped rows (NaNs + zeros): {cnt_total_skipped}\")\n",
    "    print(f\"Final check: len(df_flux) == cnt_success + len(nan_files)? {len(df_flux) == cnt_success + cnt_total_skipped}\")\n",
    "\n",
    "    return nan_files\n",
    "\n",
    "\n",
    "def normalize_lamost_spectra(spectra_df):\n",
    "    \"\"\"\n",
    "    Reads LAMOST FITS spectra, applies interpolation, normalization, and transformation.\n",
    "    Returns a DataFrame of final spectral features (one row per spectrum).\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    spectra = spectra_df.iloc[:, 100:-1].values  # Exclude the last column (file_name)\n",
    "\n",
    "    #print(f\"Shape of the spectra array: {spectra.shape}\")\n",
    "\n",
    "    # Normalize the spectra between 0 and 1\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    spectra_normalized = min_max_scaler.fit_transform(spectra.T).T\n",
    "\n",
    "    #print(f\"Shape of the normalized spectra array: {spectra_normalized.shape}\")\n",
    "\n",
    "    # Apply the Yeo-Johnson transformation to the spectra\n",
    "    pt = PowerTransformer(method='yeo-johnson', standardize=True)\n",
    "    spectra_transformed = pt.fit_transform(spectra_normalized.T).T\n",
    "\n",
    "    # Create a new DataFrame with the transformed spectra\n",
    "    df_transformed = pd.DataFrame(spectra_transformed, columns=spectra_df.columns[100:-1]) # Exclude the first 100+3 columns and the last column\n",
    "\n",
    "    #print(f\"Shape of the transformed spectra array: {spectra_transformed.shape}\")\n",
    "\n",
    "    # Add the file_name column back to the DataFrame\n",
    "    #print(f\"Available columns in spectra_df: {spectra_df.columns}\")\n",
    "    df_transformed['obsid'] = spectra_df['file_name']\n",
    "\n",
    "    return df_transformed\n",
    "\n",
    "\n",
    "def apply_gaia_transforms(gaia_df, transformers_dict):\n",
    "    \"\"\"\n",
    "    Applies the same Yeo-Johnson (or other) transformations used in training\n",
    "    to the relevant Gaia columns. \n",
    "    \"\"\"\n",
    "    # Fill the same NaN values or set the same flags as in training\n",
    "    # e.g. if you flagged parallax=NaN => set parallax=0, error=10\n",
    "    # do that here too, to keep consistent with your training pipeline\n",
    "    #\n",
    "    # Example based on your code:\n",
    "    gaia_df['flagnopllx'] = np.where(gaia_df['parallax'].isna(), 1, 0)\n",
    "    gaia_df['parallax']       = gaia_df['parallax'].fillna(0)\n",
    "    gaia_df['parallax_error'] = gaia_df['parallax_error'].fillna(10)\n",
    "    gaia_df['pmra']           = gaia_df['pmra'].fillna(0)\n",
    "    gaia_df['pmra_error']     = gaia_df['pmra_error'].fillna(10)\n",
    "    gaia_df['pmdec']          = gaia_df['pmdec'].fillna(0)\n",
    "    gaia_df['pmdec_error']    = gaia_df['pmdec_error'].fillna(10)\n",
    "\n",
    "    gaia_df['flagnoflux'] = 0\n",
    "    # If G or BP or RP is missing\n",
    "    missing_flux = gaia_df['phot_g_mean_flux'].isna() | gaia_df['phot_bp_mean_flux'].isna() \n",
    "    gaia_df.loc[missing_flux, 'flagnoflux'] = 1\n",
    "\n",
    "    # fill flux with 0 and error with large number\n",
    "    gaia_df['phot_g_mean_flux']       = gaia_df['phot_g_mean_flux'].fillna(0)\n",
    "    gaia_df['phot_g_mean_flux_error'] = gaia_df['phot_g_mean_flux_error'].fillna(50000)\n",
    "    gaia_df['phot_bp_mean_flux']      = gaia_df['phot_bp_mean_flux'].fillna(0)\n",
    "    gaia_df['phot_bp_mean_flux_error']= gaia_df['phot_bp_mean_flux_error'].fillna(50000)\n",
    "    gaia_df['phot_rp_mean_flux']      = gaia_df['phot_rp_mean_flux'].fillna(0)\n",
    "    gaia_df['phot_rp_mean_flux_error']= gaia_df['phot_rp_mean_flux_error'].fillna(50000)\n",
    "\n",
    "    # Drop any rows that are incomplete, if that was your final approach:\n",
    "    gaia_df.dropna(axis=0, inplace=True)\n",
    "    print(f\"Dropped {len(gaia_df) - len(gaia_df.dropna())} rows with NaN values.\")\n",
    "\n",
    "    # Remove source_id and other columns not to be transformed to be added back later\n",
    "    source_id = gaia_df['source_id']\n",
    "    gaia_df = gaia_df.drop(columns=[\"source_id\"])\n",
    "\n",
    "    # Now apply the stored transformations:\n",
    "    for col, transformer in transformers_dict.items():\n",
    "        if col in gaia_df.columns:\n",
    "            #print(f\"Transforming column: {col}\")\n",
    "            gaia_df[col] = transformer.transform(gaia_df[[col]])\n",
    "            #print(f\"Transformed column: {col}\")\n",
    "        else:\n",
    "            # If the column didn't exist, maybe set to 0 or skip?\n",
    "            print(f\"Warning: column {col} not found in new data, skipping transform.\")\n",
    "\n",
    "    # Add back the source_id column\n",
    "    gaia_df['source_id'] = source_id\n",
    "    return gaia_df\n",
    "\n",
    "\n",
    "def process_star_data_fusion(\n",
    "    model_path, \n",
    "    X, \n",
    "    classes_path, \n",
    "    d_model_spectra=2048, \n",
    "    d_model_gaia=2048,\n",
    "    num_classes=55, \n",
    "    input_dim_spectra=3647, \n",
    "    input_dim_gaia=18, \n",
    "    depth=20, \n",
    "    sigmoid_constant=0.5,\n",
    "    class_to_plot=\"AllStars***lamost\"\n",
    "):\n",
    "    \"\"\"Processes star data using the fused StarClassifierFusion model.\"\"\"\n",
    "\n",
    "    # Load the data\n",
    "    classes = pd.read_pickle(classes_path)\n",
    "\n",
    "    # Load the trained fusion model\n",
    "    model = StarClassifierFusion(\n",
    "        d_model_spectra=d_model_spectra,\n",
    "        d_model_gaia=d_model_gaia,\n",
    "        num_classes=num_classes,\n",
    "        input_dim_spectra=input_dim_spectra,\n",
    "        input_dim_gaia=input_dim_gaia,\n",
    "        n_layers=depth,\n",
    "        use_cross_attention=True,  # Change to False for late fusion\n",
    "        n_cross_attn_heads=8\n",
    "    )\n",
    "\n",
    "    # Load the state dictionary\n",
    "    state_dict = torch.load(model_path, weights_only=False)\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "    # Get multi-hot encoded labels\n",
    "    #y = X[classes]\n",
    "\n",
    "    # Define Gaia columns\n",
    "    gaia_columns = [\n",
    "        \"parallax\", \"ra\", \"dec\", \"ra_error\", \"dec_error\", \"parallax_error\", \"pmra\", \"pmdec\",\n",
    "        \"pmra_error\", \"pmdec_error\", \"phot_g_mean_flux\", \"flagnopllx\", \"phot_g_mean_flux_error\",\n",
    "        \"phot_bp_mean_flux\", \"phot_rp_mean_flux\", \"phot_bp_mean_flux_error\", \"phot_rp_mean_flux_error\",\n",
    "        \"flagnoflux\"\n",
    "    ]\n",
    "\n",
    "    # Separate Gaia and Spectra features\n",
    "    X_spectra = X.drop(columns={\"obsid\",\"source_id\", *gaia_columns})\n",
    "    X_gaia = X[gaia_columns]\n",
    "\n",
    "    print(f\"X_spectra shape: {X_spectra.shape}\")\n",
    "    print(f\"X_gaia shape: {X_gaia.shape}\")\n",
    "    #print(f\"y shape: {y.shape}\")\n",
    "\n",
    "    if class_to_plot != \"AllStars***lamost\":\n",
    "        # Filter for a specific class\n",
    "        X_spectra = X_spectra[y[class_to_plot] == 1]\n",
    "        X_gaia = X_gaia[y[class_to_plot] == 1]\n",
    "        #y = y[y[class_to_plot] == 1]\n",
    "\n",
    "        print(f\"X_spectra shape after filtering for {class_to_plot}: {X_spectra.shape}\")\n",
    "        print(f\"X_gaia shape after filtering for {class_to_plot}: {X_gaia.shape}\")\n",
    "       # print(f\"y shape after filtering for {class_to_plot}: {y.shape}\")\n",
    "\n",
    "    # Drop label columns from spectra\n",
    "    #X_spectra.drop(classes, axis=1, inplace=True)\n",
    "\n",
    "    # Convert to tensors\n",
    "    X_spectra = torch.tensor(X_spectra.values, dtype=torch.float32)\n",
    "    X_gaia = torch.tensor(X_gaia.values, dtype=torch.float32)\n",
    "    #y = torch.tensor(y.values, dtype=torch.float32)\n",
    "\n",
    "    # Create DataLoader\n",
    "    class MultiModalDataset(Dataset):\n",
    "        def __init__(self, X_spectra, X_gaia):\n",
    "            self.X_spectra = X_spectra\n",
    "            self.X_gaia = X_gaia\n",
    "            #self.y = y\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.X_spectra)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return self.X_spectra[idx], self.X_gaia[idx]\n",
    "\n",
    "    dataset = MultiModalDataset(X_spectra, X_gaia)\n",
    "    loader = DataLoader(dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "    # Move model to device\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_predicted = []\n",
    "    all_y = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_spc, X_ga in loader:\n",
    "            # Move data to device\n",
    "            X_spc, X_ga = X_spc.to(device), X_ga.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(X_spc, X_ga)\n",
    "            predicted = (torch.sigmoid(outputs) > sigmoid_constant).float()\n",
    "\n",
    "            # Store predictions and labels\n",
    "            all_predicted.append(predicted.cpu().numpy())\n",
    "            #all_y.append(y_batch.cpu().numpy())\n",
    "\n",
    "            # Free GPU memory\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # Concatenate all predictions and labels\n",
    "    #y_cpu = np.concatenate(all_y, axis=0)\n",
    "    predicted_cpu = np.concatenate(all_predicted, axis=0)\n",
    "\n",
    "    return predicted_cpu\n",
    "\n",
    "\n",
    "def custom_precision_score(y_true, y_pred, zero_division=0):\n",
    "    \"\"\"\n",
    "    Calculates a custom precision-like metric: TP / (TP + FN')\n",
    "    where FN' are False Negatives for which the model made at least one\n",
    "    (wrong) prediction for that sample.\n",
    "\n",
    "    Args:\n",
    "        y_true (np.ndarray): Ground truth labels (binary, samples x classes).\n",
    "        y_pred (np.ndarray): Predicted labels (binary, samples x classes).\n",
    "        zero_division (int or float): Value to return if the denominator (TP + FN') is 0.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of custom precision scores for each class.\n",
    "    \"\"\"\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        raise ValueError(\"y_true and y_pred must have the same shape.\")\n",
    "    if y_true.ndim != 2:\n",
    "         raise ValueError(\"y_true and y_pred must be 2D arrays (samples x classes).\")\n",
    "\n",
    "    num_samples, num_classes = y_true.shape\n",
    "    custom_precisions = np.zeros(num_classes, dtype=float)\n",
    "\n",
    "    # Identify samples where the model made *any* prediction\n",
    "    # Shape: (num_samples,) - boolean mask\n",
    "    model_made_a_prediction_mask = np.sum(y_pred, axis=1) > 0\n",
    "\n",
    "    for c in range(num_classes):\n",
    "        # Identify True Positives for class c\n",
    "        # Shape: (num_samples,) - boolean mask\n",
    "        tp_mask = (y_true[:, c] == 1) & (y_pred[:, c] == 1)\n",
    "        tp = np.sum(tp_mask)\n",
    "\n",
    "        # Identify standard False Negatives for class c\n",
    "        # Shape: (num_samples,) - boolean mask\n",
    "        fn_mask = (y_true[:, c] == 1) & (y_pred[:, c] == 0)\n",
    "\n",
    "        # Identify the specific FN' subset:\n",
    "        # These are samples that are FN for class c AND for which the model\n",
    "        # made *some* prediction (could be for any class).\n",
    "        # Shape: (num_samples,) - boolean mask\n",
    "        fn_prime_mask = fn_mask & model_made_a_prediction_mask\n",
    "        fn_prime = np.sum(fn_prime_mask)\n",
    "\n",
    "        # Calculate the custom metric for class c\n",
    "        denominator = tp + fn_prime\n",
    "        if denominator > 0:\n",
    "            custom_precisions[c] = tp / denominator\n",
    "        else:\n",
    "            # Assign the zero_division value if denominator is 0\n",
    "            custom_precisions[c] = float(zero_division)\n",
    "\n",
    "    return custom_precisions\n",
    "\n",
    "def custom_f1_score(y_true, y_pred, zero_division=0):\n",
    "    \"\"\"\n",
    "    Calculates a custom F1 score based on the harmonic mean of\n",
    "    custom_precision_score and standard recall_score.\n",
    "\n",
    "    Args:\n",
    "        y_true (np.ndarray): Ground truth labels (binary, samples x classes).\n",
    "        y_pred (np.ndarray): Predicted labels (binary, samples x classes).\n",
    "        zero_division (int or float): Value to return for F1 when both\n",
    "                                     custom precision and recall are 0.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of custom F1 scores for each class.\n",
    "    \"\"\"\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        raise ValueError(\"y_true and y_pred must have the same shape.\")\n",
    "    if y_true.ndim != 2:\n",
    "         raise ValueError(\"y_true and y_pred must be 2D arrays (samples x classes).\")\n",
    "\n",
    "    # Calculate custom precision per class\n",
    "    p_custom = custom_precision_score(y_true, y_pred, zero_division=zero_division)\n",
    "\n",
    "    # Calculate standard recall per class\n",
    "    # Note: Using the same zero_division behavior for consistency here\n",
    "    r_standard = recall_score(y_true, y_pred, average=None, zero_division=zero_division)\n",
    "\n",
    "    # Calculate custom F1 using vectorization, handling division by zero\n",
    "    denominator = p_custom + r_standard\n",
    "    f1_custom = np.zeros_like(denominator, dtype=float) # Initialize with zeros\n",
    "\n",
    "    # Create a mask for where the denominator is non-zero\n",
    "    valid_mask = denominator > 0\n",
    "\n",
    "    # Calculate F1 only where the denominator is valid\n",
    "    f1_custom[valid_mask] = 2 * (p_custom[valid_mask] * r_standard[valid_mask]) / denominator[valid_mask]\n",
    "\n",
    "    # For cases where denominator is zero, F1 should be zero_division\n",
    "    # (If zero_division is 0, the initial np.zeros_like already handles this)\n",
    "    if zero_division != 0:\n",
    "        zero_mask = ~valid_mask # Where denominator is zero\n",
    "        f1_custom[zero_mask] = float(zero_division) # Assign zero_division value\n",
    "\n",
    "    return f1_custom\n",
    "\n",
    "def exact_match_ratio(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the Exact Match Ratio (Subset Accuracy).\n",
    "\n",
    "    This metric requires the set of predicted labels for a sample to\n",
    "    exactly match the set of true labels for that sample.\n",
    "\n",
    "    Args:\n",
    "        y_true (np.ndarray): Ground truth labels (binary, samples x classes).\n",
    "        y_pred (np.ndarray): Predicted labels (binary, samples x classes).\n",
    "\n",
    "    Returns:\n",
    "        float: The Exact Match Ratio (a single value between 0 and 1).\n",
    "    \"\"\"\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        raise ValueError(\"y_true and y_pred must have the same shape.\")\n",
    "    if y_true.ndim != 2:\n",
    "        raise ValueError(\"y_true and y_pred must be 2D arrays (samples x classes).\")\n",
    "\n",
    "    # Check equality for each sample across all classes\n",
    "    # .all(axis=1) returns True only if all columns match for a given row (sample)\n",
    "    exact_matches_mask = (y_true == y_pred).all(axis=1)\n",
    "\n",
    "    # Calculate the ratio (mean of a boolean array gives the proportion of True values)\n",
    "    ratio = np.mean(exact_matches_mask)\n",
    "\n",
    "    return ratio\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from functools import partial\n",
    "\n",
    "# Import the needed components from your MambaOut implementation\n",
    "from timm.models.layers import DropPath\n",
    "\n",
    "class GatedCNNBlock(nn.Module):\n",
    "    \"\"\"Adaptation of GatedCNNBlock for sequence data with dynamic kernel size adaptation\"\"\"\n",
    "    def __init__(self, dim, d_state=256, d_conv=4, expand=2, drop_path=0.):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim, eps=1e-6)\n",
    "        hidden = int(expand * dim)\n",
    "        self.fc1 = nn.Linear(dim, hidden * 2)\n",
    "        self.act = nn.GELU()\n",
    "        \n",
    "        # Store these for dynamic convolution sizing\n",
    "        self.d_conv = d_conv\n",
    "        self.hidden = hidden\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden, dim)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        \n",
    "        # Use simpler approach for sequence length 1 (common case)\n",
    "        # This avoids dynamic convolution creation\n",
    "        self.use_identity_for_length_1 = True\n",
    "        \n",
    "        # Cache for static convolution with kernel size 1 (for length 1 sequences)\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            in_channels=hidden,\n",
    "            out_channels=hidden, \n",
    "            kernel_size=1,\n",
    "            padding=0,\n",
    "            groups=hidden\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input shape: [B, seq_len, dim]\n",
    "        shortcut = x\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        # Split the channels for gating mechanism\n",
    "        x = self.fc1(x)  # [B, seq_len, hidden*2]\n",
    "        g, c = torch.chunk(x, 2, dim=-1)  # Each: [B, seq_len, hidden]\n",
    "        \n",
    "        # Get sequence length\n",
    "        batch_size, seq_len, channels = c.shape\n",
    "        \n",
    "        # Apply gating mechanism\n",
    "        c_permuted = c.permute(0, 2, 1)  # [B, hidden, seq_len]\n",
    "        \n",
    "        # Special case for sequence length 1 (most common)\n",
    "        if seq_len == 1 and self.use_identity_for_length_1:\n",
    "            # Use the pre-created kernel size 1 conv, which is like identity but keeps channels\n",
    "            c_conv = self.conv1(c_permuted)\n",
    "        else:\n",
    "            # For other sequence lengths, fallback to kernel size 1 to avoid issues\n",
    "            # The conv1 layer is already initialized and on the correct device\n",
    "            c_conv = self.conv1(c_permuted)\n",
    "        \n",
    "        c_final = c_conv.permute(0, 2, 1)  # [B, seq_len, hidden]\n",
    "        \n",
    "        # Gating mechanism\n",
    "        x = self.fc2(self.act(g) * c_final)  # [B, seq_len, dim]\n",
    "        \n",
    "        x = self.drop_path(x)\n",
    "        return x + shortcut\n",
    "\n",
    "class SequenceMambaOut(nn.Module):\n",
    "    \"\"\"Adaptation of MambaOut for sequence data with a single stage\"\"\"\n",
    "    def __init__(self, d_model, d_state=256, d_conv=4, expand=2, depth=1, drop_path=0.):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Create a sequence of GatedCNNBlocks\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[GatedCNNBlock(\n",
    "                dim=d_model,\n",
    "                d_state=d_state,\n",
    "                d_conv=d_conv,\n",
    "                expand=expand,\n",
    "                drop_path=drop_path\n",
    "            ) for _ in range(depth)]\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.blocks(x)\n",
    "\n",
    "class CrossAttentionBlock(nn.Module):\n",
    "    def __init__(self, dim, n_heads=8):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=dim,\n",
    "            num_heads=n_heads,\n",
    "            batch_first=True\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, context):\n",
    "        \"\"\"\n",
    "        x: (B, seq_len_x, dim)\n",
    "        context: (B, seq_len_context, dim)\n",
    "        \"\"\"\n",
    "        x_norm = self.norm(x)\n",
    "        attn_output, _ = self.attention(\n",
    "            query=x_norm,\n",
    "            key=context,\n",
    "            value=context\n",
    "        )\n",
    "        return x + attn_output\n",
    "    \n",
    "def process_star_data_fusion(\n",
    "    model_path, \n",
    "    X, \n",
    "    classes_path, \n",
    "    d_model_spectra=2048, \n",
    "    d_model_gaia=2048,\n",
    "    num_classes=55, \n",
    "    input_dim_spectra=3647, \n",
    "    input_dim_gaia=18, \n",
    "    n_layers=20, \n",
    "    sigmoid_constant=0.5,\n",
    "    classifier_hidden_dim_multiplier=5,\n",
    "    class_to_plot=\"AllStars***lamost\"\n",
    "):\n",
    "    \"\"\"Processes star data using the fused StarClassifierFusion model.\"\"\"\n",
    "\n",
    "    # Load the data\n",
    "    classes = pd.read_pickle(classes_path)\n",
    "\n",
    "    # Load the trained fusion model\n",
    "    model = StarClassifierFusion(\n",
    "        d_model_spectra=d_model_spectra,\n",
    "        d_model_gaia=d_model_gaia,\n",
    "        num_classes=num_classes,\n",
    "        input_dim_spectra=input_dim_spectra,\n",
    "        input_dim_gaia=input_dim_gaia,\n",
    "        n_layers=n_layers,\n",
    "        use_cross_attention=True,  # set to False to compare with late fusion\n",
    "        n_cross_attn_heads=8,\n",
    "        classifier_hidden_dim_multiplier=classifier_hidden_dim_multiplier,\n",
    "        classifier_dropout=0.2,\n",
    "    )\n",
    "\n",
    "    # Load the state dictionary\n",
    "    state_dict = torch.load(model_path, weights_only=False)\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "    # Get multi-hot encoded labels\n",
    "    #y = X[classes]\n",
    "\n",
    "    # Define Gaia columns\n",
    "    gaia_columns = [\n",
    "        \"parallax\", \"ra\", \"dec\", \"ra_error\", \"dec_error\", \"parallax_error\", \"pmra\", \"pmdec\",\n",
    "        \"pmra_error\", \"pmdec_error\", \"phot_g_mean_flux\", \"flagnopllx\", \"phot_g_mean_flux_error\",\n",
    "        \"phot_bp_mean_flux\", \"phot_rp_mean_flux\", \"phot_bp_mean_flux_error\", \"phot_rp_mean_flux_error\",\n",
    "        \"flagnoflux\"\n",
    "    ]\n",
    "\n",
    "    # Separate Gaia and Spectra features\n",
    "    X_spectra = X.drop(columns={\"obsid\",\"source_id\", *gaia_columns})\n",
    "    X_gaia = X[gaia_columns]\n",
    "\n",
    "    print(f\"X_spectra shape: {X_spectra.shape}\")\n",
    "    print(f\"X_gaia shape: {X_gaia.shape}\")\n",
    "    #print(f\"y shape: {y.shape}\")\n",
    "\n",
    "    if class_to_plot != \"AllStars***lamost\":\n",
    "        # Filter for a specific class\n",
    "        X_spectra = X_spectra[y[class_to_plot] == 1]\n",
    "        X_gaia = X_gaia[y[class_to_plot] == 1]\n",
    "        #y = y[y[class_to_plot] == 1]\n",
    "\n",
    "        print(f\"X_spectra shape after filtering for {class_to_plot}: {X_spectra.shape}\")\n",
    "        print(f\"X_gaia shape after filtering for {class_to_plot}: {X_gaia.shape}\")\n",
    "       # print(f\"y shape after filtering for {class_to_plot}: {y.shape}\")\n",
    "\n",
    "    # Drop label columns from spectra\n",
    "    #X_spectra.drop(classes, axis=1, inplace=True)\n",
    "\n",
    "    # Convert to tensors\n",
    "    X_spectra = torch.tensor(X_spectra.values, dtype=torch.float32)\n",
    "    X_gaia = torch.tensor(X_gaia.values, dtype=torch.float32)\n",
    "    #y = torch.tensor(y.values, dtype=torch.float32)\n",
    "\n",
    "    # Create DataLoader\n",
    "    class MultiModalDataset(Dataset):\n",
    "        def __init__(self, X_spectra, X_gaia):\n",
    "            self.X_spectra = X_spectra\n",
    "            self.X_gaia = X_gaia\n",
    "            #self.y = y\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.X_spectra)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return self.X_spectra[idx], self.X_gaia[idx]\n",
    "\n",
    "    dataset = MultiModalDataset(X_spectra, X_gaia)\n",
    "    loader = DataLoader(dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "    # Move model to device\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_predicted = []\n",
    "    all_y = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_spc, X_ga in loader:\n",
    "            # Move data to device\n",
    "            X_spc, X_ga = X_spc.to(device), X_ga.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(X_spc, X_ga)\n",
    "            predicted = (torch.sigmoid(outputs) > sigmoid_constant).float()\n",
    "\n",
    "            # Store predictions and labels\n",
    "            all_predicted.append(predicted.cpu().numpy())\n",
    "            #all_y.append(y_batch.cpu().numpy())\n",
    "\n",
    "            # Free GPU memory\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # Concatenate all predictions and labels\n",
    "    #y_cpu = np.concatenate(all_y, axis=0)\n",
    "    predicted_cpu = np.concatenate(all_predicted, axis=0)\n",
    "\n",
    "    return predicted_cpu\n",
    "\n",
    "class StarClassifierFusion(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model_spectra = 2048,\n",
    "        d_model_gaia = 2048,\n",
    "        num_classes = 55,\n",
    "        input_dim_spectra = 3647,\n",
    "        input_dim_gaia = 18,\n",
    "        n_layers=20,\n",
    "        use_cross_attention=True,\n",
    "        n_cross_attn_heads=8,\n",
    "        d_state=256,\n",
    "        d_conv=4,\n",
    "        expand=2,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model_spectra (int): embedding dimension for the spectra MAMBA\n",
    "            d_model_gaia (int): embedding dimension for the gaia MAMBA\n",
    "            num_classes (int): multi-label classification\n",
    "            input_dim_spectra (int): # of features for spectra\n",
    "            input_dim_gaia (int): # of features for gaia\n",
    "            n_layers (int): depth for each MAMBA\n",
    "            use_cross_attention (bool): whether to use cross-attention\n",
    "            n_cross_attn_heads (int): number of heads for cross-attention\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # --- MambaOut for spectra ---\n",
    "        self.mamba_spectra = nn.Sequential(\n",
    "            *[SequenceMambaOut(\n",
    "                d_model=d_model_spectra,\n",
    "                d_state=d_state,\n",
    "                d_conv=d_conv,\n",
    "                expand=expand,\n",
    "                depth=1,  # Each SequenceMambaOut has depth 1\n",
    "                drop_path=0.1 if i > 0 else 0.0,  # Optional: add some dropout for regularization\n",
    "            ) for i in range(n_layers)]\n",
    "        )\n",
    "        self.input_proj_spectra = nn.Linear(input_dim_spectra, d_model_spectra)\n",
    "\n",
    "        # --- MambaOut for gaia ---\n",
    "        self.mamba_gaia = nn.Sequential(\n",
    "            *[SequenceMambaOut(\n",
    "                d_model=d_model_gaia,\n",
    "                d_state=d_state,\n",
    "                d_conv=d_conv,\n",
    "                expand=expand,\n",
    "                depth=1,  # Each SequenceMambaOut has depth 1\n",
    "                drop_path=0.1 if i > 0 else 0.0,  # Optional: add some dropout for regularization\n",
    "            ) for i in range(n_layers)]\n",
    "        )\n",
    "        self.input_proj_gaia = nn.Linear(input_dim_gaia, d_model_gaia)\n",
    "\n",
    "        # --- Cross Attention (Optional) ---\n",
    "        self.use_cross_attention = use_cross_attention\n",
    "        if use_cross_attention:\n",
    "            self.cross_attn_block_spectra = CrossAttentionBlock(d_model_spectra, n_heads=n_cross_attn_heads)\n",
    "            self.cross_attn_block_gaia = CrossAttentionBlock(d_model_gaia, n_heads=n_cross_attn_heads)\n",
    "\n",
    "        # --- Final Classifier ---\n",
    "        fusion_dim = d_model_spectra + d_model_gaia\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(fusion_dim),\n",
    "            nn.Linear(fusion_dim, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x_spectra, x_gaia):\n",
    "        \"\"\"\n",
    "        x_spectra : (batch_size, input_dim_spectra) or (batch_size, seq_len_spectra, input_dim_spectra)\n",
    "        x_gaia    : (batch_size, input_dim_gaia) or (batch_size, seq_len_gaia, input_dim_gaia)\n",
    "        \"\"\"\n",
    "        # For MambaOut, we expect shape: (B, seq_len, d_model). \n",
    "        # If input is just (B, d_in), we turn it into (B, 1, d_in).\n",
    "        \n",
    "        # --- Project to d_model and add sequence dimension if needed ---\n",
    "        if len(x_spectra.shape) == 2:\n",
    "            x_spectra = self.input_proj_spectra(x_spectra)  # (B, d_model_spectra)\n",
    "            x_spectra = x_spectra.unsqueeze(1)              # (B, 1, d_model_spectra)\n",
    "        else:\n",
    "            x_spectra = self.input_proj_spectra(x_spectra)  # (B, seq_len, d_model_spectra)\n",
    "        \n",
    "        if len(x_gaia.shape) == 2:\n",
    "            x_gaia = self.input_proj_gaia(x_gaia)           # (B, d_model_gaia)\n",
    "            x_gaia = x_gaia.unsqueeze(1)                    # (B, 1, d_model_gaia)\n",
    "        else:\n",
    "            x_gaia = self.input_proj_gaia(x_gaia)           # (B, seq_len, d_model_gaia)\n",
    "\n",
    "        # --- MambaOut encoding (each modality separately) ---\n",
    "        x_spectra = self.mamba_spectra(x_spectra)  # (B, seq_len, d_model_spectra)\n",
    "        x_gaia = self.mamba_gaia(x_gaia)           # (B, seq_len, d_model_gaia)\n",
    "\n",
    "        # Optionally, use cross-attention to fuse the representations\n",
    "        if self.use_cross_attention:\n",
    "            # Cross-attention from spectra -> gaia\n",
    "            x_spectra_fused = self.cross_attn_block_spectra(x_spectra, x_gaia)\n",
    "            # Cross-attention from gaia -> spectra\n",
    "            x_gaia_fused = self.cross_attn_block_gaia(x_gaia, x_spectra)\n",
    "            \n",
    "            # Update x_spectra and x_gaia\n",
    "            x_spectra = x_spectra_fused\n",
    "            x_gaia = x_gaia_fused\n",
    "        \n",
    "        # --- Pool across sequence dimension ---\n",
    "        x_spectra = x_spectra.mean(dim=1)  # (B, d_model_spectra)\n",
    "        x_gaia = x_gaia.mean(dim=1)        # (B, d_model_gaia)\n",
    "\n",
    "        # --- Late Fusion by Concatenation ---\n",
    "        x_fused = torch.cat([x_spectra, x_gaia], dim=-1)  # (B, d_model_spectra + d_model_gaia)\n",
    "\n",
    "        # --- Final classification ---\n",
    "        logits = self.classifier(x_fused)  # (B, num_classes)\n",
    "        return logits\n",
    "    \n",
    "\n",
    "class StarClassifierFusion(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model_spectra,\n",
    "        d_model_gaia,\n",
    "        num_classes,\n",
    "        input_dim_spectra,\n",
    "        input_dim_gaia,\n",
    "        n_layers=20,\n",
    "        use_cross_attention=True,\n",
    "        n_cross_attn_heads=8,\n",
    "        d_state=256,\n",
    "        d_conv=4,\n",
    "        expand=2,\n",
    "        classifier_hidden_dim_multiplier=5, # Multiplier for hidden dim in classifier MLP\n",
    "        classifier_dropout=0.2,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model_spectra (int): embedding dimension for the spectra MAMBA\n",
    "            d_model_gaia (int): embedding dimension for the gaia MAMBA\n",
    "            num_classes (int): multi-label classification\n",
    "            input_dim_spectra (int): # of features for spectra\n",
    "            input_dim_gaia (int): # of features for gaia\n",
    "            n_layers (int): depth for each MAMBA\n",
    "            use_cross_attention (bool): whether to use cross-attention\n",
    "            n_cross_attn_heads (int): number of heads for cross-attention\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # --- MambaOut for spectra ---\n",
    "        self.mamba_spectra = nn.Sequential(\n",
    "            *[SequenceMambaOut(\n",
    "                d_model=d_model_spectra,\n",
    "                d_state=d_state,\n",
    "                d_conv=d_conv,\n",
    "                expand=expand,\n",
    "                depth=1,  # Each SequenceMambaOut has depth 1\n",
    "                drop_path=0.1 if i > 0 else 0.0,  # Optional: add some dropout for regularization\n",
    "            ) for i in range(n_layers)]\n",
    "        )\n",
    "        self.input_proj_spectra = nn.Linear(input_dim_spectra, d_model_spectra)\n",
    "\n",
    "        # --- MambaOut for gaia ---\n",
    "        self.mamba_gaia = nn.Sequential(\n",
    "            *[SequenceMambaOut(\n",
    "                d_model=d_model_gaia,\n",
    "                d_state=d_state,\n",
    "                d_conv=d_conv,\n",
    "                expand=expand,\n",
    "                depth=1,  # Each SequenceMambaOut has depth 1\n",
    "                drop_path=0.1 if i > 0 else 0.0,  # Optional: add some dropout for regularization\n",
    "            ) for i in range(n_layers)]\n",
    "        )\n",
    "        self.input_proj_gaia = nn.Linear(input_dim_gaia, d_model_gaia)\n",
    "\n",
    "        # --- Cross Attention (Optional) ---\n",
    "        self.use_cross_attention = use_cross_attention\n",
    "        if use_cross_attention:\n",
    "            self.cross_attn_block_spectra = CrossAttentionBlock(d_model_spectra, n_heads=n_cross_attn_heads)\n",
    "            self.cross_attn_block_gaia = CrossAttentionBlock(d_model_gaia, n_heads=n_cross_attn_heads)\n",
    "\n",
    "        # --- Final Classifier (Improved MLP) ---\n",
    "        fusion_dim = d_model_spectra + d_model_gaia\n",
    "        classifier_hidden_dim = int(fusion_dim * classifier_hidden_dim_multiplier)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(fusion_dim),\n",
    "            nn.Linear(fusion_dim, classifier_hidden_dim),\n",
    "            nn.GELU(), # Or ReLU, SiLU, etc.\n",
    "            nn.Dropout(classifier_dropout),\n",
    "            nn.Linear(classifier_hidden_dim, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x_spectra, x_gaia):\n",
    "        \"\"\"\n",
    "        x_spectra : (batch_size, input_dim_spectra) or (batch_size, seq_len_spectra, input_dim_spectra)\n",
    "        x_gaia    : (batch_size, input_dim_gaia) or (batch_size, seq_len_gaia, input_dim_gaia)\n",
    "        \"\"\"\n",
    "        # For MambaOut, we expect shape: (B, seq_len, d_model). \n",
    "        # If input is just (B, d_in), we turn it into (B, 1, d_in).\n",
    "        \n",
    "        # --- Project to d_model and add sequence dimension if needed ---\n",
    "        if len(x_spectra.shape) == 2:\n",
    "            x_spectra = self.input_proj_spectra(x_spectra)  # (B, d_model_spectra)\n",
    "            x_spectra = x_spectra.unsqueeze(1)              # (B, 1, d_model_spectra)\n",
    "        else:\n",
    "            x_spectra = self.input_proj_spectra(x_spectra)  # (B, seq_len, d_model_spectra)\n",
    "        \n",
    "        if len(x_gaia.shape) == 2:\n",
    "            x_gaia = self.input_proj_gaia(x_gaia)           # (B, d_model_gaia)\n",
    "            x_gaia = x_gaia.unsqueeze(1)                    # (B, 1, d_model_gaia)\n",
    "        else:\n",
    "            x_gaia = self.input_proj_gaia(x_gaia)           # (B, seq_len, d_model_gaia)\n",
    "\n",
    "        # --- MambaOut encoding (each modality separately) ---\n",
    "        x_spectra = self.mamba_spectra(x_spectra)  # (B, seq_len, d_model_spectra)\n",
    "        x_gaia = self.mamba_gaia(x_gaia)           # (B, seq_len, d_model_gaia)\n",
    "\n",
    "        # Optionally, use cross-attention to fuse the representations\n",
    "        if self.use_cross_attention:\n",
    "            # Cross-attention from spectra -> gaia\n",
    "            x_spectra_fused = self.cross_attn_block_spectra(x_spectra, x_gaia)\n",
    "            # Cross-attention from gaia -> spectra\n",
    "            x_gaia_fused = self.cross_attn_block_gaia(x_gaia, x_spectra)\n",
    "            \n",
    "            # Update x_spectra and x_gaia\n",
    "            x_spectra = x_spectra_fused\n",
    "            x_gaia = x_gaia_fused\n",
    "        \n",
    "        # --- Pool across sequence dimension ---\n",
    "        x_spectra = x_spectra.mean(dim=1)  # (B, d_model_spectra)\n",
    "        x_gaia = x_gaia.mean(dim=1)        # (B, d_model_gaia)\n",
    "\n",
    "        # --- Late Fusion by Concatenation ---\n",
    "        x_fused = torch.cat([x_spectra, x_gaia], dim=-1)  # (B, d_model_spectra + d_model_gaia)\n",
    "\n",
    "        # --- Final classification ---\n",
    "        logits = self.classifier(x_fused)  # (B, num_classes)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a1f87e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "✅ Retrieved 2184477 eclipsing binary sources from Gaia DR3.\n"
     ]
    }
   ],
   "source": [
    "# Define ADQL query to fetch source IDs of eclipsing binaries\n",
    "query = \"\"\"\n",
    "SELECT source_id\n",
    "FROM gaiadr3.vari_eclipsing_binary\n",
    "\"\"\"\n",
    "\n",
    "# Run the query asynchronously\n",
    "job = Gaia.launch_job_async(query)\n",
    "results = job.get_results()\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "gaia_ids = results.to_pandas()\n",
    "\n",
    "# Convert to list\n",
    "gaia_ids = gaia_ids['source_id'].values.tolist()\n",
    "\n",
    "gaia_ids_small = gaia_ids[:1000]\n",
    "\n",
    "print(f\"✅ Retrieved {len(gaia_ids)} eclipsing binary sources from Gaia DR3.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0c48e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Step 1: Querying Gaia data...\n",
      "🔗 Gaia IDs: 2184477\n",
      "Gaia query split into 73 chunks of up to 30000 IDs.\n",
      "\n",
      "Processing Chunk 1/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746194630281O). Waiting for results...\n",
      "  Results received for chunk 1.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 2/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746194642178O). Waiting for results...\n",
      "  Results received for chunk 2.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 3/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746194650894O). Waiting for results...\n",
      "  Results received for chunk 3.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 4/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746194660981O). Waiting for results...\n",
      "  Results received for chunk 4.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 5/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746194670871O). Waiting for results...\n",
      "  Results received for chunk 5.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 6/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746194681603O). Waiting for results...\n",
      "  Results received for chunk 6.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 7/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746194694248O). Waiting for results...\n",
      "  Results received for chunk 7.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 8/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746194705189O). Waiting for results...\n",
      "  Results received for chunk 8.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 9/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746194716021O). Waiting for results...\n",
      "  Results received for chunk 9.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 10/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746194726569O). Waiting for results...\n",
      "  Results received for chunk 10.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 11/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746194737090O). Waiting for results...\n",
      "  Results received for chunk 11.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 12/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746194746803O). Waiting for results...\n",
      "  Results received for chunk 12.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 13/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746194756939O). Waiting for results...\n",
      "  Results received for chunk 13.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 14/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746194767375O). Waiting for results...\n",
      "  Results received for chunk 14.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 15/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746194785517O). Waiting for results...\n",
      "  Results received for chunk 15.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 16/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746194794476O). Waiting for results...\n",
      "  Results received for chunk 16.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 17/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746194806117O). Waiting for results...\n",
      "  Results received for chunk 17.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 18/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746194817352O). Waiting for results...\n",
      "  Results received for chunk 18.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 19/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746194827677O). Waiting for results...\n",
      "  Results received for chunk 19.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 20/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746194837551O). Waiting for results...\n",
      "  Results received for chunk 20.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 21/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746194847768O). Waiting for results...\n",
      "  Results received for chunk 21.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 22/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746194862010O). Waiting for results...\n",
      "  Results received for chunk 22.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 23/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746194873624O). Waiting for results...\n",
      "  Results received for chunk 23.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 24/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746194884440O). Waiting for results...\n",
      "  Results received for chunk 24.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 25/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746194899029O). Waiting for results...\n",
      "  Results received for chunk 25.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 26/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746194910037O). Waiting for results...\n",
      "  Results received for chunk 26.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 27/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746194921583O). Waiting for results...\n",
      "  Results received for chunk 27.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 28/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746194932671O). Waiting for results...\n",
      "  Results received for chunk 28.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 29/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746194946361O). Waiting for results...\n",
      "  Results received for chunk 29.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 30/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746194962384O). Waiting for results...\n",
      "  Results received for chunk 30.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 31/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746194972888O). Waiting for results...\n",
      "  Results received for chunk 31.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 32/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746194983062O). Waiting for results...\n",
      "  Results received for chunk 32.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 33/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746194994409O). Waiting for results...\n",
      "  Results received for chunk 33.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 34/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746195004770O). Waiting for results...\n",
      "  Results received for chunk 34.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 35/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746195014599O). Waiting for results...\n",
      "  Results received for chunk 35.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 36/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746195025321O). Waiting for results...\n",
      "  Results received for chunk 36.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 37/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746195043381O). Waiting for results...\n",
      "  Results received for chunk 37.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 38/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746195058102O). Waiting for results...\n",
      "  Results received for chunk 38.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 39/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746195070424O). Waiting for results...\n",
      "  Results received for chunk 39.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 40/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746195081080O). Waiting for results...\n",
      "  Results received for chunk 40.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 41/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746195091701O). Waiting for results...\n",
      "  Results received for chunk 41.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 42/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746195103288O). Waiting for results...\n",
      "  Results received for chunk 42.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 43/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746195115215O). Waiting for results...\n",
      "  Results received for chunk 43.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 44/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746195127881O). Waiting for results...\n",
      "  Results received for chunk 44.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 45/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746195140460O). Waiting for results...\n",
      "  Results received for chunk 45.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 46/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746195150050O). Waiting for results...\n",
      "  Results received for chunk 46.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 47/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746195162301O). Waiting for results...\n",
      "  Results received for chunk 47.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 48/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746195170433O). Waiting for results...\n",
      "  Results received for chunk 48.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 49/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746195181282O). Waiting for results...\n",
      "  Results received for chunk 49.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 50/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746195192062O). Waiting for results...\n",
      "  Results received for chunk 50.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 51/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746195202685O). Waiting for results...\n",
      "  Results received for chunk 51.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 52/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746195216054O). Waiting for results...\n",
      "  Results received for chunk 52.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 53/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746195227216O). Waiting for results...\n",
      "  Results received for chunk 53.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 54/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746195239700O). Waiting for results...\n",
      "  Results received for chunk 54.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 55/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "  Attempt 1 failed with unexpected error: gaierror - [Errno -3] Temporary failure in name resolution\n",
      "  Retrying in 5 seconds...\n",
      "  Attempt 2/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746195284917O). Waiting for results...\n",
      "  Results received for chunk 55.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 56/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "  Attempt 1 failed with unexpected error: gaierror - [Errno -3] Temporary failure in name resolution\n",
      "  Retrying in 5 seconds...\n",
      "  Attempt 2/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746195318765O). Waiting for results...\n",
      "  Results received for chunk 56.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 57/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746195334557O). Waiting for results...\n",
      "  Results received for chunk 57.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 58/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746195354119O). Waiting for results...\n",
      "  Results received for chunk 58.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 59/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746195380652O). Waiting for results...\n",
      "  Results received for chunk 59.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 60/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746195395940O). Waiting for results...\n",
      "  Results received for chunk 60.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 61/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746195412638O). Waiting for results...\n",
      "  Results received for chunk 61.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 62/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746195426412O). Waiting for results...\n",
      "  Results received for chunk 62.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 63/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746195443455O). Waiting for results...\n",
      "  Results received for chunk 63.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 64/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746195461384O). Waiting for results...\n",
      "  Results received for chunk 64.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 65/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746195472105O). Waiting for results...\n",
      "  Results received for chunk 65.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 66/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746195481955O). Waiting for results...\n",
      "  Results received for chunk 66.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 67/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746195499478O). Waiting for results...\n",
      "  Results received for chunk 67.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 68/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746195511595O). Waiting for results...\n",
      "  Results received for chunk 68.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 69/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746195523789O). Waiting for results...\n",
      "  Results received for chunk 69.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 70/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746195534842O). Waiting for results...\n",
      "  Results received for chunk 70.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 71/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746195548169O). Waiting for results...\n",
      "  Results received for chunk 71.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 72/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746195564920O). Waiting for results...\n",
      "  Results received for chunk 72.\n",
      "  Retrieved 30000 records for this chunk.\n",
      "\n",
      "Processing Chunk 73/73...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746195577537O). Waiting for results...\n",
      "  Results received for chunk 73.\n",
      "  Retrieved 24477 records for this chunk.\n",
      "\n",
      "Consolidating results...\n",
      "Consolidated DataFrame shape: (2184477, 17)\n",
      "Warning: 2184477 out of 2184477 requested IDs were not found in the final results (includes skipped chunks).\n",
      "🔗 Gaia data: (2184477, 17)\n",
      "\n",
      "🔄 Step 2: Cross-matching with LAMOST catalog...\n",
      "After NaN removal: Gaia=(2184477, 17), LAMOST=(10809336, 3)\n",
      "Matched Gaia Objects: (34442, 17)\n",
      "Matched LAMOST Objects: (34442, 3)\n",
      "\n",
      "📥 Step 3: Downloading LAMOST spectra (if needed)...\n",
      "✅ All spectra are already downloaded.\n",
      "\n",
      "🔧 Step 4: Converting from FITS LAMOST spectra...\n",
      "\n",
      "📂 Processing LAMOST FITS files...\n",
      "Examining sample files to determine FITS structure...\n",
      "Found FITS_rec in 228615165, names: ('FLUX', 'IVAR', 'WAVELENGTH', 'ANDMASK', 'ORMASK', 'NORMALIZATION')\n",
      "Found FITS_rec in 248108121, names: ('FLUX', 'IVAR', 'WAVELENGTH', 'ANDMASK', 'ORMASK', 'NORMALIZATION')\n",
      "Found FITS_rec in 397605222, names: ('FLUX', 'IVAR', 'WAVELENGTH', 'ANDMASK', 'ORMASK', 'NORMALIZATION')\n",
      "Found field names across sample files: {'ANDMASK', 'NORMALIZATION', 'IVAR', 'ORMASK', 'WAVELENGTH', 'FLUX'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing FITS files: 100%|██████████| 34435/34435 [05:35<00:00, 102.61it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 34435 files\n",
      "⚠️ Encountered errors in 0 files\n",
      "\n",
      "📊 Step 5: Extracting and saving flux & frequency values...\n",
      "\n",
      "📊 Extracting flux and frequency values...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:14<00:00,  2.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flux values shape: (34435, 3749), Frequency values shape: (34435, 3749)\n",
      "\n",
      "📊 Step 6: Interpolating and normalizing LAMOST spectra...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Interpolating spectra: 100%|██████████| 34435/34435 [01:06<00:00, 516.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial number of rows: 34435\n",
      "Total successful interpolations: 34257\n",
      "Total skipped rows (NaNs + zeros): 178\n",
      "Final check: len(df_flux) == cnt_success + len(nan_files)? True\n",
      "\n",
      "📊 Step 7: Normalizing Gaia data...\n",
      "Dropped 0 rows with NaN values.\n",
      "\n",
      "🔗 Step 8: Merging Gaia and LAMOST data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24511/1102485407.py:60: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  gaia_lamost_match[\"obsid\"] = gaia_lamost_match[\"obsid\"].astype(int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🤖 Step 9: Predicting labels using the trained model...\n",
      "X_spectra shape: (34253, 3647)\n",
      "X_gaia shape: (34253, 18)\n",
      "\n",
      "💾 Step 10: Saving predictions...\n"
     ]
    }
   ],
   "source": [
    "# Load the LAMOST catalog to cross-match with Gaia as csv\n",
    "lamost_catalogue = pd.read_csv(\"lamost/minimal.csv\")  # Load LAMOST catalog (Just obsid and Ra, Dec)\n",
    "label_cols = pd.read_pickle(\"Pickles/Updated_List_of_Classes_ubuntu.pkl\")\n",
    "\n",
    "# Example usage:\n",
    "model_path = \"Models/model_fusion_mambaoutv3.pth\"\n",
    "gaia_transformers = \"Pickles/gaia_normalization.pkl\"\n",
    "\n",
    "df_predictions, gaia_lamost_merged = predict_star_labels(gaia_ids, model_path, lamost_catalogue, gaia_transformers)\n",
    "\n",
    "# Save the predictions to a npy file\n",
    "np.save(\"y_predictions_ecl.npy\", df_predictions)\n",
    "\n",
    "# Save the merged DataFrame to a CSV file\n",
    "gaia_lamost_merged.to_csv(\"gaia_lamost_merged_ecl.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4a7689d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_precision_score(y_true, y_pred, zero_division=0):\n",
    "    \"\"\"\n",
    "    Calculates a custom precision-like metric: TP / (TP + FN')\n",
    "    where FN' are False Negatives for which the model made at least one\n",
    "    (wrong) prediction for that sample.\n",
    "\n",
    "    Args:\n",
    "        y_true (np.ndarray): Ground truth labels (binary, samples x classes).\n",
    "        y_pred (np.ndarray): Predicted labels (binary, samples x classes).\n",
    "        zero_division (int or float): Value to return if the denominator (TP + FN') is 0.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of custom precision scores for each class.\n",
    "    \"\"\"\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        raise ValueError(\"y_true and y_pred must have the same shape.\")\n",
    "    if y_true.ndim != 2:\n",
    "         raise ValueError(\"y_true and y_pred must be 2D arrays (samples x classes).\")\n",
    "\n",
    "    num_samples, num_classes = y_true.shape\n",
    "    custom_precisions = np.zeros(num_classes, dtype=float)\n",
    "\n",
    "    # Identify samples where the model made *any* prediction\n",
    "    # Shape: (num_samples,) - boolean mask\n",
    "    model_made_a_prediction_mask = np.sum(y_pred, axis=1) > 0\n",
    "\n",
    "    for c in range(num_classes):\n",
    "        # Identify True Positives for class c\n",
    "        # Shape: (num_samples,) - boolean mask\n",
    "        tp_mask = (y_true[:, c] == 1) & (y_pred[:, c] == 1)\n",
    "        tp = np.sum(tp_mask)\n",
    "\n",
    "        # Identify standard False Negatives for class c\n",
    "        # Shape: (num_samples,) - boolean mask\n",
    "        fn_mask = (y_true[:, c] == 1) & (y_pred[:, c] == 0)\n",
    "\n",
    "        # Identify the specific FN' subset:\n",
    "        # These are samples that are FN for class c AND for which the model\n",
    "        # made *some* prediction (could be for any class).\n",
    "        # Shape: (num_samples,) - boolean mask\n",
    "        fn_prime_mask = fn_mask & model_made_a_prediction_mask\n",
    "        fn_prime = np.sum(fn_prime_mask)\n",
    "\n",
    "        # Calculate the custom metric for class c\n",
    "        denominator = tp + fn_prime\n",
    "        if denominator > 0:\n",
    "            custom_precisions[c] = tp / denominator\n",
    "        else:\n",
    "            # Assign the zero_division value if denominator is 0\n",
    "            custom_precisions[c] = float(zero_division)\n",
    "\n",
    "    return custom_precisions\n",
    "\n",
    "def custom_f1_score(y_true, y_pred, zero_division=0):\n",
    "    \"\"\"\n",
    "    Calculates a custom F1 score based on the harmonic mean of\n",
    "    custom_precision_score and standard recall_score.\n",
    "\n",
    "    Args:\n",
    "        y_true (np.ndarray): Ground truth labels (binary, samples x classes).\n",
    "        y_pred (np.ndarray): Predicted labels (binary, samples x classes).\n",
    "        zero_division (int or float): Value to return for F1 when both\n",
    "                                     custom precision and recall are 0.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of custom F1 scores for each class.\n",
    "    \"\"\"\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        raise ValueError(\"y_true and y_pred must have the same shape.\")\n",
    "    if y_true.ndim != 2:\n",
    "         raise ValueError(\"y_true and y_pred must be 2D arrays (samples x classes).\")\n",
    "\n",
    "    # Calculate custom precision per class\n",
    "    p_custom = custom_precision_score(y_true, y_pred, zero_division=zero_division)\n",
    "\n",
    "    # Calculate standard recall per class\n",
    "    # Note: Using the same zero_division behavior for consistency here\n",
    "    r_standard = recall_score(y_true, y_pred, average=None, zero_division=zero_division)\n",
    "\n",
    "    # Calculate custom F1 using vectorization, handling division by zero\n",
    "    denominator = p_custom + r_standard\n",
    "    f1_custom = np.zeros_like(denominator, dtype=float) # Initialize with zeros\n",
    "\n",
    "    # Create a mask for where the denominator is non-zero\n",
    "    valid_mask = denominator > 0\n",
    "\n",
    "    # Calculate F1 only where the denominator is valid\n",
    "    f1_custom[valid_mask] = 2 * (p_custom[valid_mask] * r_standard[valid_mask]) / denominator[valid_mask]\n",
    "\n",
    "    # For cases where denominator is zero, F1 should be zero_division\n",
    "    # (If zero_division is 0, the initial np.zeros_like already handles this)\n",
    "    if zero_division != 0:\n",
    "        zero_mask = ~valid_mask # Where denominator is zero\n",
    "        f1_custom[zero_mask] = float(zero_division) # Assign zero_division value\n",
    "\n",
    "    return f1_custom\n",
    "\n",
    "def exact_match_ratio(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the Exact Match Ratio (Subset Accuracy).\n",
    "\n",
    "    This metric requires the set of predicted labels for a sample to\n",
    "    exactly match the set of true labels for that sample.\n",
    "\n",
    "    Args:\n",
    "        y_true (np.ndarray): Ground truth labels (binary, samples x classes).\n",
    "        y_pred (np.ndarray): Predicted labels (binary, samples x classes).\n",
    "\n",
    "    Returns:\n",
    "        float: The Exact Match Ratio (a single value between 0 and 1).\n",
    "    \"\"\"\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        raise ValueError(\"y_true and y_pred must have the same shape.\")\n",
    "    if y_true.ndim != 2:\n",
    "        raise ValueError(\"y_true and y_pred must be 2D arrays (samples x classes).\")\n",
    "\n",
    "    # Check equality for each sample across all classes\n",
    "    # .all(axis=1) returns True only if all columns match for a given row (sample)\n",
    "    exact_matches_mask = (y_true == y_pred).all(axis=1)\n",
    "\n",
    "    # Calculate the ratio (mean of a boolean array gives the proportion of True values)\n",
    "    ratio = np.mean(exact_matches_mask)\n",
    "\n",
    "    return ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bc6bcfcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Incorrectly Classified Gaia IDs:\n",
      "                 source_id\n",
      "1          544743587681792\n",
      "4         1179432379804928\n",
      "10        2367390269205248\n",
      "12        2590827352388096\n",
      "14        3715047927383424\n",
      "...                    ...\n",
      "34246  6911669219076603136\n",
      "34247  6911722369297224704\n",
      "34248  6912724166123684608\n",
      "34251  6916019432537074048\n",
      "34252  6916024384634026240\n",
      "\n",
      "[15871 rows x 1 columns]\n",
      "\n",
      "📊 Performance Metrics:\n",
      "   Class  Precision  Custom Precision    Recall  F1 Score  Custom F1 Score  \\\n",
      "1     **        1.0          0.820649  0.677138  0.807492         0.742018   \n",
      "54   EB*        1.0          0.758978  0.626252  0.770178         0.686256   \n",
      "\n",
      "    Exact Match Ratio  \n",
      "1            0.536654  \n",
      "54           0.536654  \n"
     ]
    }
   ],
   "source": [
    "# Load the predictions and class labels\n",
    "y_pred = np.load(\"y_predictions_ecl.npy\")\n",
    "y_pred = np.array(df_predictions.iloc[:, :-1], dtype=int)\n",
    "classes = pd.read_pickle(\"Pickles/Updated_List_of_Classes_ubuntu.pkl\")\n",
    "\n",
    "# Generate the expected y_true for eclipsing binaries\n",
    "y_true = np.zeros_like(y_pred)\n",
    "y_true[:, -1] = 1  # \"EB*\" column (last column)\n",
    "y_true[:, 1] = 1   # \"**\" column (second column)\n",
    "\n",
    "# Compute precision, recall, and F1-score for each class\n",
    "precision = precision_score(y_true, y_pred, average=None, zero_division=0)\n",
    "custom_precision = custom_precision_score(y_true, y_pred, zero_division=0)\n",
    "custom_f1 = custom_f1_score(y_true, y_pred, zero_division=0)\n",
    "recall = recall_score(y_true, y_pred, average=None, zero_division=0)\n",
    "f1 = f1_score(y_true, y_pred, average=None, zero_division=0)\n",
    "exact_match = exact_match_ratio(y_true, y_pred)\n",
    "\n",
    "# Create a DataFrame to store metrics per class\n",
    "metrics_df = pd.DataFrame({\n",
    "    \"Class\": classes,\n",
    "    \"Precision\": precision,\n",
    "    \"Custom Precision\": custom_precision,\n",
    "    \"Recall\": recall,\n",
    "    \"F1 Score\": f1,\n",
    "    \"Custom F1 Score\": custom_f1,\n",
    "    \"Exact Match Ratio\": exact_match\n",
    "})\n",
    "\n",
    "# Identify Correctly Classified Samples (True Positives)\n",
    "correct_predictions = (y_pred == y_true).all(axis=1)\n",
    "correct_gaia_ids = df_predictions.loc[correct_predictions, \"source_id\"]\n",
    "\n",
    "# Identify incorrectly classified samples (False Positives and False Negatives)\n",
    "incorrect_predictions = (y_pred != y_true).any(axis=1)\n",
    "incorrect_gaia_ids = df_predictions.loc[incorrect_predictions, \"source_id\"]\n",
    "\n",
    "# Display incorrectly classified Gaia IDs\n",
    "print(\"\\n🔍 Incorrectly Classified Gaia IDs:\")\n",
    "print(pd.DataFrame({\"source_id\": incorrect_gaia_ids}))\n",
    "\n",
    "# Display the performance metrics for the non-zero classes\n",
    "print(\"\\n📊 Performance Metrics:\")\n",
    "metrics_df = metrics_df[metrics_df[\"Precision\"] > 0]\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8b73a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_spectra shape: (34253, 3647)\n",
      "X_gaia shape: (34253, 18)\n"
     ]
    }
   ],
   "source": [
    "recall_predictions = process_star_data_fusion(model_path, gaia_lamost_merged, \"Pickles/Updated_List_of_Classes_ubuntu.pkl\", sigmoid_constant=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "efd2bcad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred shape: (34253, 55)\n",
      "\n",
      "🔍 Incorrectly Classified Gaia IDs:\n",
      "                 source_id\n",
      "0          138577120584320\n",
      "1          544743587681792\n",
      "2          608515262200832\n",
      "4         1179432379804928\n",
      "10        2367390269205248\n",
      "...                    ...\n",
      "34245  6911578200129779200\n",
      "34248  6912724166123684608\n",
      "34249  6912964203255804544\n",
      "34251  6916019432537074048\n",
      "34252  6916024384634026240\n",
      "\n",
      "[15552 rows x 1 columns]\n",
      "\n",
      "📊 Performance Metrics:\n",
      "   Class  Precision  Custom Precision    Recall  F1 Score  Custom F1 Score  \\\n",
      "1     **        1.0          0.839757  0.826935   0.90527         0.833297   \n",
      "54   EB*        1.0          0.856567  0.843488   0.91510         0.849977   \n",
      "\n",
      "    Exact Match Ratio  \n",
      "1            0.545967  \n",
      "54           0.545967  \n"
     ]
    }
   ],
   "source": [
    "# Load the predictions and class labels\n",
    "y_pred = recall_predictions\n",
    "print(f\"y_pred shape: {y_pred.shape}\")\n",
    "classes = pd.read_pickle(\"Pickles/Updated_List_of_Classes_ubuntu.pkl\")\n",
    "\n",
    "# Generate the expected y_true for eclipsing binaries\n",
    "y_true = np.zeros_like(y_pred)\n",
    "y_true[:, -1] = 1  # \"EB*\" column (last column)\n",
    "y_true[:, 1] = 1   # \"**\" column (second column)\n",
    "\n",
    "# Compute precision, recall, and F1-score for each class\n",
    "precision = precision_score(y_true, y_pred, average=None, zero_division=0)\n",
    "custom_precision = custom_precision_score(y_true, y_pred, zero_division=0)\n",
    "recall = recall_score(y_true, y_pred, average=None, zero_division=0)\n",
    "f1 = f1_score(y_true, y_pred, average=None, zero_division=0)\n",
    "custom_f1 = custom_f1_score(y_true, y_pred, zero_division=0)\n",
    "exact_match = exact_match_ratio(y_true, y_pred)\n",
    "\n",
    "# Create a DataFrame to store metrics per class\n",
    "metrics_df = pd.DataFrame({\n",
    "    \"Class\": classes,\n",
    "    \"Precision\": precision,\n",
    "    \"Custom Precision\": custom_precision,\n",
    "    \"Recall\": recall,\n",
    "    \"F1 Score\": f1,\n",
    "    \"Custom F1 Score\": custom_f1,\n",
    "    \"Exact Match Ratio\": exact_match\n",
    "})\n",
    "\n",
    "# Identify Correctly Classified Samples (True Positives)\n",
    "correct_predictions = (y_pred == y_true).all(axis=1)\n",
    "correct_gaia_ids = df_predictions.loc[correct_predictions, \"source_id\"]\n",
    "\n",
    "# Identify incorrectly classified samples (False Positives and False Negatives)\n",
    "incorrect_predictions = (y_pred != y_true).any(axis=1)\n",
    "incorrect_gaia_ids = df_predictions.loc[incorrect_predictions, \"source_id\"]\n",
    "\n",
    "# Display incorrectly classified Gaia IDs\n",
    "print(\"\\n🔍 Incorrectly Classified Gaia IDs:\")\n",
    "print(pd.DataFrame({\"source_id\": incorrect_gaia_ids}))\n",
    "\n",
    "# Display the performance metrics for the non-zero classes\n",
    "print(\"\\n📊 Performance Metrics:\")\n",
    "metrics_df = metrics_df[metrics_df[\"Precision\"] > 0]\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c064e13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_spectra shape: (34253, 3647)\n",
      "X_gaia shape: (34253, 18)\n",
      "y_pred shape: (34253, 55)\n",
      "\n",
      "🔍 Incorrectly Classified Gaia IDs:\n",
      "                 source_id\n",
      "0          138577120584320\n",
      "1          544743587681792\n",
      "4         1179432379804928\n",
      "14        3715047927383424\n",
      "15        4326135874205184\n",
      "...                    ...\n",
      "34243  6911428735267723904\n",
      "34246  6911669219076603136\n",
      "34248  6912724166123684608\n",
      "34251  6916019432537074048\n",
      "34252  6916024384634026240\n",
      "\n",
      "[13986 rows x 1 columns]\n",
      "\n",
      "📊 Performance Metrics:\n",
      "   Class  Precision  Custom Precision    Recall  F1 Score  Custom F1 Score  \\\n",
      "1     **        1.0          0.819891  0.743439  0.852842         0.779795   \n",
      "54   EB*        1.0          0.793232  0.719265  0.836713         0.754440   \n",
      "\n",
      "    Exact Match Ratio  \n",
      "1            0.591685  \n",
      "54           0.591685  \n"
     ]
    }
   ],
   "source": [
    "recall_predictions03 = process_star_data_fusion(model_path, gaia_lamost_merged, \"Pickles/Updated_List_of_Classes_ubuntu.pkl\", sigmoid_constant=0.3)\n",
    "\n",
    "# Load the predictions and class labels\n",
    "y_pred = recall_predictions03\n",
    "print(f\"y_pred shape: {y_pred.shape}\")\n",
    "classes = pd.read_pickle(\"Pickles/Updated_List_of_Classes_ubuntu.pkl\")\n",
    "\n",
    "# Generate the expected y_true for eclipsing binaries\n",
    "y_true = np.zeros_like(y_pred)\n",
    "y_true[:, -1] = 1  # \"EB*\" column (last column)\n",
    "y_true[:, 1] = 1   # \"**\" column (second column)\n",
    "\n",
    "# Compute precision, recall, and F1-score for each class\n",
    "precision = precision_score(y_true, y_pred, average=None, zero_division=0)\n",
    "custom_precision = custom_precision_score(y_true, y_pred, zero_division=0)\n",
    "recall = recall_score(y_true, y_pred, average=None, zero_division=0)\n",
    "f1 = f1_score(y_true, y_pred, average=None, zero_division=0)\n",
    "custom_f1 = custom_f1_score(y_true, y_pred, zero_division=0)\n",
    "exact_match = exact_match_ratio(y_true, y_pred)\n",
    "\n",
    "# Create a DataFrame to store metrics per class\n",
    "metrics_df = pd.DataFrame({\n",
    "    \"Class\": classes,\n",
    "    \"Precision\": precision,\n",
    "    \"Custom Precision\": custom_precision,\n",
    "    \"Recall\": recall,\n",
    "    \"F1 Score\": f1,\n",
    "    \"Custom F1 Score\": custom_f1,\n",
    "    \"Exact Match Ratio\": exact_match\n",
    "})\n",
    "\n",
    "# Identify Correctly Classified Samples (True Positives)\n",
    "correct_predictions = (y_pred == y_true).all(axis=1)\n",
    "correct_gaia_ids = df_predictions.loc[correct_predictions, \"source_id\"]\n",
    "\n",
    "# Identify incorrectly classified samples (False Positives and False Negatives)\n",
    "incorrect_predictions = (y_pred != y_true).any(axis=1)\n",
    "incorrect_gaia_ids = df_predictions.loc[incorrect_predictions, \"source_id\"]\n",
    "\n",
    "# Display incorrectly classified Gaia IDs\n",
    "print(\"\\n🔍 Incorrectly Classified Gaia IDs:\")\n",
    "print(pd.DataFrame({\"source_id\": incorrect_gaia_ids}))\n",
    "\n",
    "# Display the performance metrics for the non-zero classes\n",
    "print(\"\\n📊 Performance Metrics:\")\n",
    "metrics_df = metrics_df[metrics_df[\"Precision\"] > 0]\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e44c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_spectra shape: (34253, 3647)\n",
      "X_gaia shape: (34253, 18)\n"
     ]
    }
   ],
   "source": [
    "precision_predictions = process_star_data_fusion(model_path, gaia_lamost_merged, \"Pickles/Updated_List_of_Classes_ubuntu.pkl\", sigmoid_constant=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "88151624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred shape: (34253, 55)\n",
      "\n",
      "🔍 Incorrectly Classified Gaia IDs:\n",
      "                 source_id\n",
      "1          544743587681792\n",
      "2          608515262200832\n",
      "4         1179432379804928\n",
      "10        2367390269205248\n",
      "12        2590827352388096\n",
      "...                    ...\n",
      "34246  6911669219076603136\n",
      "34247  6911722369297224704\n",
      "34248  6912724166123684608\n",
      "34251  6916019432537074048\n",
      "34252  6916024384634026240\n",
      "\n",
      "[17312 rows x 1 columns]\n",
      "\n",
      "📊 Performance Metrics:\n",
      "   Class  Precision  Custom Precision    Recall  F1 Score  Custom F1 Score  \\\n",
      "1     **        1.0          0.821548  0.640032  0.780511         0.719518   \n",
      "54   EB*        1.0          0.743751  0.579424  0.733715         0.651383   \n",
      "\n",
      "    Exact Match Ratio  \n",
      "1            0.494584  \n",
      "54           0.494584  \n"
     ]
    }
   ],
   "source": [
    "# Load the predictions and class labels\n",
    "y_pred = precision_predictions\n",
    "print(f\"y_pred shape: {y_pred.shape}\")\n",
    "classes = pd.read_pickle(\"Pickles/Updated_List_of_Classes_ubuntu.pkl\")\n",
    "\n",
    "# Generate the expected y_true for eclipsing binaries\n",
    "y_true = np.zeros_like(y_pred)\n",
    "y_true[:, -1] = 1  # \"EB*\" column (last column)\n",
    "y_true[:, 1] = 1   # \"**\" column (second column)\n",
    "\n",
    "# Compute precision, recall, and F1-score for each class\n",
    "precision = precision_score(y_true, y_pred, average=None, zero_division=0)\n",
    "custom_precision = custom_precision_score(y_true, y_pred, zero_division=0)\n",
    "recall = recall_score(y_true, y_pred, average=None, zero_division=0)\n",
    "f1 = f1_score(y_true, y_pred, average=None, zero_division=0)\n",
    "custom_f1 = custom_f1_score(y_true, y_pred, zero_division=0)\n",
    "exact_match = exact_match_ratio(y_true, y_pred)\n",
    "\n",
    "# Create a DataFrame to store metrics per class\n",
    "metrics_df = pd.DataFrame({\n",
    "    \"Class\": classes,\n",
    "    \"Precision\": precision,\n",
    "    \"Custom Precision\": custom_precision,\n",
    "    \"Recall\": recall,\n",
    "    \"F1 Score\": f1,\n",
    "    \"Custom F1 Score\": custom_f1,\n",
    "    \"Exact Match Ratio\": exact_match\n",
    "})\n",
    "\n",
    "# Identify Correctly Classified Samples (True Positives)\n",
    "correct_predictions = (y_pred == y_true).all(axis=1)\n",
    "correct_gaia_ids = df_predictions.loc[correct_predictions, \"source_id\"]\n",
    "\n",
    "# Identify incorrectly classified samples (False Positives and False Negatives)\n",
    "incorrect_predictions = (y_pred != y_true).any(axis=1)\n",
    "incorrect_gaia_ids = df_predictions.loc[incorrect_predictions, \"source_id\"]\n",
    "\n",
    "# Display incorrectly classified Gaia IDs\n",
    "print(\"\\n🔍 Incorrectly Classified Gaia IDs:\")\n",
    "print(pd.DataFrame({\"source_id\": incorrect_gaia_ids}))\n",
    "\n",
    "# Display the performance metrics for the non-zero classes\n",
    "print(\"\\n📊 Performance Metrics:\")\n",
    "metrics_df = metrics_df[metrics_df[\"Precision\"] > 0]\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99885c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the predictions and class labels\n",
    "y_pred = np.load(\"y_predictions_ecl.npy\")\n",
    "classes = pd.read_pickle(\"Pickles/Updated_List_of_Classes_ubuntu.pkl\")\n",
    "\n",
    "# Remove the last column (source_id) from y_pred\n",
    "y_pred = y_pred[:, :-1]\n",
    "\n",
    "# Number of times a Class was predicted by the model\n",
    "predicted_classes = np.sum(y_pred, axis=0)\n",
    "\n",
    "# Histogram of predicted classes\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(predicted_classes, bins=range(len(classes)), align='left', rwidth=0.8)\n",
    "plt.xticks(range(len(classes)), classes, rotation=90)\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Histogram of Predicted Classes')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a5a3cd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_one_spectrum(obsid, session, save_folder):\n",
    "    \"\"\"\n",
    "    Helper function to download one spectrum file given an obsid.\n",
    "    Uses the same session to get the file and saves it locally.\n",
    "    \n",
    "    Args:\n",
    "        obsid: LAMOST observation ID\n",
    "        session: Requests session to use\n",
    "        save_folder: Folder to save the spectrum file\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (obsid, success, error_message)\n",
    "    \"\"\"\n",
    "    url = f\"https://www.lamost.org/dr10/v2.0/spectrum/fits/{obsid}\"\n",
    "    local_path = os.path.join(save_folder, str(obsid))\n",
    "    \n",
    "    # If already downloaded, skip\n",
    "    if os.path.exists(local_path):\n",
    "        return obsid, True, None\n",
    "    \n",
    "    # Add a small random delay to prevent hammering the server\n",
    "    time.sleep(random.uniform(0.1, 0.5))\n",
    "    \n",
    "    try:\n",
    "        resp = session.get(url, timeout=30)\n",
    "        resp.raise_for_status()\n",
    "        \n",
    "        # Save the raw content - we'll handle format detection during processing\n",
    "        with open(local_path, \"wb\") as f:\n",
    "            f.write(resp.content)\n",
    "            \n",
    "        return obsid, True, None\n",
    "    except Exception as e:\n",
    "        return obsid, False, str(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9f5084",
   "metadata": {},
   "source": [
    "# Doing the triple plot for ECB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb04284",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "373dec63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RS*', '**', 'El*', 'Y*O', 's*b', 'cC*', 'HB*', 'dS*', 'Or*', 'LP*', 'BS*', 'Ae*', 'WV*', 'HS*', 'Ev*', 'AB*', 'sg*', 's*r', 'Ce*', 'gD*', 'OH*', 'HXB', 'Pu*', 'RV*', 'Sy*', 'V*', 'TT*', 'SN*', 'Be*', 'SB*', 'Em*', 'Er*', 'PM*', 'HV*', 'pA*', 'C*', 'BY*', 'Ro*', 'XB*', 'Ma*', 'Pe*', 'CV*', 'bC*', 'RR*', 'Mi*', 'SX*', 'RG*', 'LM*', 'WD*', 'S*', 'MS*', 'Ir*', 'a2*', 'PN', 'EB*']\n",
      "EB_index: 54, bin_index: 1\n",
      "29377\n",
      "\n",
      "--- Identifying Specific Misclassifications (FN') ---\n",
      "\n",
      "🔍 Found 9482 samples meeting the FN' criteria for CV* or **:\n",
      "         source_id\n",
      "0     1.179432e+15\n",
      "1     2.367390e+15\n",
      "2     2.590827e+15\n",
      "3     3.715048e+15\n",
      "4     4.326136e+15\n",
      "...            ...\n",
      "9477  6.911429e+18\n",
      "9478  6.911578e+18\n",
      "9479  6.911722e+18\n",
      "9480  6.916019e+18\n",
      "9481  6.916024e+18\n",
      "\n",
      "[9482 rows x 1 columns]\n",
      "\n",
      "🔍 Labels given by the model to FN' samples:\n",
      "      RS*  **  El*  Y*O  s*b  cC*  HB*  dS*  Or*  LP*  ...  SX*  RG*  LM*  \\\n",
      "0       0   0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
      "1       0   1    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
      "2       0   0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
      "3       0   0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
      "4       0   0    0    0    0    0    1    0    0    0  ...    0    0    0   \n",
      "...   ...  ..  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
      "9477    0   1    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
      "9478    0   0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
      "9479    0   0    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
      "9480    0   0    0    0    0    0    1    0    0    0  ...    0    0    0   \n",
      "9481    0   1    0    0    0    0    0    0    0    0  ...    0    0    0   \n",
      "\n",
      "      WD*  S*  MS*  Ir*  a2*  PN  EB*  \n",
      "0       0   0    1    0    0   0    0  \n",
      "1       0   0    0    0    0   0    0  \n",
      "2       0   0    0    0    0   0    1  \n",
      "3       0   0    0    0    0   0    1  \n",
      "4       0   0    0    0    0   0    0  \n",
      "...   ...  ..  ...  ...  ...  ..  ...  \n",
      "9477    0   0    0    0    0   0    0  \n",
      "9478    0   0    0    0    0   0    1  \n",
      "9479    0   0    0    0    0   0    1  \n",
      "9480    0   0    0    0    0   0    1  \n",
      "9481    0   0    0    0    0   0    0  \n",
      "\n",
      "[9482 rows x 55 columns]\n",
      "\n",
      "🔍 Incorrectly Classified Gaia IDs:\n",
      "          source_id\n",
      "0      5.447436e+14\n",
      "1      1.179432e+15\n",
      "2      2.367390e+15\n",
      "3      2.590827e+15\n",
      "4      3.715048e+15\n",
      "...             ...\n",
      "15866  6.911669e+18\n",
      "15867  6.911722e+18\n",
      "15868  6.912724e+18\n",
      "15869  6.916019e+18\n",
      "15870  6.916024e+18\n",
      "\n",
      "[15871 rows x 1 columns]\n",
      "\n",
      "📊 Performance Metrics:\n",
      "   Class  Precision    Recall  F1 Score  Custom F1 Score  Custom Precision  \\\n",
      "1     **        1.0  0.677138  0.807492         0.742018          0.820649   \n",
      "54   EB*        1.0  0.626252  0.770178         0.686256          0.758978   \n",
      "\n",
      "    Exact Match Ratio  Support  \n",
      "1            0.536654    34253  \n",
      "54           0.536654    34253  \n",
      "Exact match accuracy: 0.5366537237614224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "y_pred_ = np.load(\"y_predictions_ecl.npy\")\n",
    "y_pred = np.array(y_pred_[:, :-1], dtype=int) # last column is source_id probably\n",
    "classes = pd.read_pickle(\"Pickles/Updated_List_of_Classes_ubuntu.pkl\")\n",
    "\n",
    "print(classes)\n",
    "\n",
    "EB_index = classes.index(\"EB*\")\n",
    "bin_index = classes.index(\"**\")\n",
    "\n",
    "print(f\"EB_index: {EB_index}, bin_index: {bin_index}\")  \n",
    "\n",
    "# Generate the expected y_true for eclipsing binaries\n",
    "y_true = np.zeros_like(y_pred)\n",
    "y_true[:, EB_index] = 1  # \"EB*\" column\n",
    "y_true[:, bin_index] = 1   # \"**\" column (second column)\n",
    "\n",
    "# Count the number of misclassified samples\n",
    "print(np.sum(y_true!=y_pred))\n",
    "\n",
    "# Compute precision, recall, and F1-score for each class\n",
    "precision = precision_score(y_true, y_pred, average=None, zero_division=0)\n",
    "recall = recall_score(y_true, y_pred, average=None, zero_division=0)\n",
    "f1 = f1_score(y_true, y_pred, average=None, zero_division=0)\n",
    "custom_f1 = custom_f1_score(y_true, y_pred, zero_division=0)\n",
    "custom_precision = custom_precision_score(y_true, y_pred, zero_division=0)\n",
    "exact_match = exact_match_ratio(y_true, y_pred)\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "_, _, _, support = score(y_true, y_pred)\n",
    "\n",
    "# Create a DataFrame to store metrics per class\n",
    "metrics_df = pd.DataFrame({\n",
    "    \"Class\": classes,\n",
    "    \"Precision\": precision,\n",
    "    \"Recall\": recall,\n",
    "    \"F1 Score\": f1,\n",
    "    \"Custom F1 Score\": custom_f1,\n",
    "    \"Custom Precision\": custom_precision,\n",
    "    \"Exact Match Ratio\": exact_match,\n",
    "    \"Support\": support\n",
    "})\n",
    "\n",
    "# Identify Correctly Classified Samples (True Positives)\n",
    "correct_predictions = (y_pred == y_true).all(axis=1)\n",
    "correct_gaia_ids = y_pred_[correct_predictions, -1]\n",
    "\n",
    "# Identify incorrectly classified samples (False Positives and False Negatives)\n",
    "incorrect_predictions = (y_pred != y_true).any(axis=1)\n",
    "incorrect_gaia_ids = y_pred_[incorrect_predictions, -1]\n",
    "\n",
    "# Not precise classification\n",
    "print(\"\\n--- Identifying Specific Misclassifications (FN') ---\")\n",
    "\n",
    "# 1. Find samples where the model predicted *at least one* label (any label)\n",
    "model_predicted_something_mask = np.sum(y_pred, axis=1) > 0\n",
    "\n",
    "# 2. Find samples where the model missed the target classes\n",
    "missed_cv_mask = (y_true[:, EB_index] == 1) & (y_pred[:, EB_index] == 0)\n",
    "missed_star_mask = (y_true[:, bin_index] == 1) & (y_pred[:, bin_index] == 0)\n",
    "\n",
    "# 3. Find FN' samples for each target class\n",
    "# FN' = Missed the target class AND predicted something else\n",
    "fn_prime_cv_mask = missed_cv_mask & model_predicted_something_mask\n",
    "fn_prime_star_mask = missed_star_mask & model_predicted_something_mask\n",
    "\n",
    "# 4. Combine: Find samples that are FN' for *either* CV* OR **\n",
    "overall_fn_prime_mask = fn_prime_cv_mask | fn_prime_star_mask\n",
    "\n",
    "# 5. Get the corresponding Gaia IDs\n",
    "# Ensure df_predictions index aligns with y_pred rows\n",
    "# Using .iloc requires the integer indices based on the mask\n",
    "fn_prime_indices = np.where(overall_fn_prime_mask)[0]\n",
    "fn_prime_gaia_ids_ecl = y_pred_[fn_prime_indices, -1] # Assumes last col is source_id\n",
    "\n",
    "print(f\"\\n🔍 Found {len(fn_prime_gaia_ids_ecl)} samples meeting the FN' criteria for CV* or **:\")\n",
    "print(pd.DataFrame({\"source_id\": fn_prime_gaia_ids_ecl}))\n",
    "\n",
    "# State the labels given by the model to the FN' samples\n",
    "print(\"\\n🔍 Labels given by the model to FN' samples:\")\n",
    "fn_prime_labels = y_pred[overall_fn_prime_mask]\n",
    "print_fn =pd.DataFrame(fn_prime_labels, columns=classes)\n",
    "print(pd.DataFrame(fn_prime_labels, columns=classes))\n",
    "\n",
    "# Display incorrectly classified Gaia IDs\n",
    "print(\"\\n🔍 Incorrectly Classified Gaia IDs:\")\n",
    "print(pd.DataFrame({\"source_id\": incorrect_gaia_ids}))\n",
    "\n",
    "# Display the performance metrics for the non-zero classes\n",
    "print(\"\\n📊 Performance Metrics:\")\n",
    "metrics_df = metrics_df[metrics_df[\"Precision\"] > 0]\n",
    "print(metrics_df)\n",
    "exact_match = np.mean((y_pred == y_true).all(axis=1))\n",
    "print(\"Exact match accuracy:\", exact_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b3ac86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "Total stars returned by the query: 1000000\n"
     ]
    }
   ],
   "source": [
    "# Background: Random Stars in Milky Way for CMD\n",
    "\n",
    "# Define the ADQL query to select stars with parallax > 0.03 mas (roughly within 30kpc, inside milky way)\n",
    "query = \"\"\"\n",
    "SELECT TOP 1000000 source_id, ra, dec, parallax, phot_bp_mean_mag, phot_rp_mean_mag, phot_g_mean_mag, parallax_error\n",
    "FROM gaiadr3.gaia_source\n",
    "WHERE parallax > 0.03 \n",
    "\"\"\"\n",
    "\n",
    "# Launch an asynchronous job (this will return more than 2000 rows if available)\n",
    "job = Gaia.launch_job_async(query)\n",
    "gaia_table = job.get_results()\n",
    "\n",
    "# Convert to a pandas DataFrame\n",
    "df_all = gaia_table.to_pandas()\n",
    "print(f\"Total stars returned by the query: {len(df_all)}\")\n",
    "\n",
    "# Define the ADQL query to fetch detailed information for the Correctly Classified Gaia IDs\n",
    "query = \"\"\"\n",
    "SELECT source_id, ra, dec, parallax, phot_bp_mean_mag, phot_rp_mean_mag, phot_g_mean_mag, parallax_error\n",
    "FROM gaiadr3.gaia_source\n",
    "WHERE source_id IN ({})\n",
    "\"\"\"\n",
    "\n",
    "# Join the source IDs into a single string\n",
    "source_ids_str = \",\".join([str(id) for id in correct_gaia_ids])\n",
    "full_query = query.format(source_ids_str)\n",
    "\n",
    "# Run the query asynchronously\n",
    "job = Gaia.launch_job_async(full_query)\n",
    "results = job.get_results()\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "correct_df = results.to_pandas()\n",
    "\n",
    "print(f\"✅ Retrieved detailed information for {len(correct_df)} correctly classified Gaia IDs.\")\n",
    "\n",
    "# Define the ADQL query to fetch detailed information for the incorrectly Classified Gaia IDs\n",
    "query = \"\"\"\n",
    "SELECT source_id, ra, dec, parallax, phot_bp_mean_mag, phot_rp_mean_mag, phot_g_mean_mag, parallax_error\n",
    "FROM gaiadr3.gaia_source\n",
    "WHERE source_id IN ({})\n",
    "\"\"\"\n",
    "\n",
    "# Join the source IDs into a single string\n",
    "source_ids_str = \",\".join([str(id) for id in incorrect_gaia_ids])\n",
    "full_query = query.format(source_ids_str)\n",
    "\n",
    "# Run the query asynchronously\n",
    "job = Gaia.launch_job_async(full_query)\n",
    "results = job.get_results()\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "incorrect_df = results.to_pandas()\n",
    "\n",
    "print(f\"✅ Retrieved detailed information for {len(incorrect_df)} incorrectly classified Gaia IDs.\")\n",
    "# Define the ADQL query to fetch detailed information for the Correctly Classified Gaia IDs\n",
    "query = \"\"\"\n",
    "SELECT source_id, ra, dec, parallax, phot_bp_mean_mag, phot_rp_mean_mag, phot_g_mean_mag, parallax_error\n",
    "FROM gaiadr3.gaia_source\n",
    "WHERE source_id IN ({})\n",
    "\"\"\"\n",
    "\n",
    "# Join the source IDs into a single string\n",
    "source_ids_str = \",\".join([str(id) for id in correct_gaia_ids])\n",
    "full_query = query.format(source_ids_str)\n",
    "\n",
    "# Run the query asynchronously\n",
    "job = Gaia.launch_job_async(full_query)\n",
    "results = job.get_results()\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "correct_df = results.to_pandas()\n",
    "\n",
    "print(f\"✅ Retrieved detailed information for {len(correct_df)} correctly classified Gaia IDs.\")\n",
    "\n",
    "# Define the ADQL query to fetch detailed information for the incorrectly Classified Gaia IDs\n",
    "query = \"\"\"\n",
    "SELECT source_id, ra, dec, parallax, phot_bp_mean_mag, phot_rp_mean_mag, phot_g_mean_mag, parallax_error\n",
    "FROM gaiadr3.gaia_source\n",
    "WHERE source_id IN ({})\n",
    "\"\"\"\n",
    "\n",
    "# Join the source IDs into a single string\n",
    "source_ids_str = \",\".join([str(id) for id in incorrect_gaia_ids])\n",
    "full_query = query.format(source_ids_str)\n",
    "\n",
    "# Run the query asynchronously\n",
    "job = Gaia.launch_job_async(full_query)\n",
    "results = job.get_results()\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "incorrect_df = results.to_pandas()\n",
    "\n",
    "print(f\"✅ Retrieved detailed information for {len(incorrect_df)} incorrectly classified Gaia IDs.\")\n",
    "\n",
    "# --- Example: Prepare your Gaia DataFrame ---\n",
    "# Clean out the bad parallax values (negative or zero)\n",
    "print(f\"Total stars before cleaning: {len(df_all)}\")\n",
    "df_all = df_all[df_all['parallax'] > 0].copy()\n",
    "\n",
    "# Remove stars with large parallax errors (e.g., > 10% of the parallax value)\n",
    "df_all = df_all[df_all['parallax_error'] < 0.1 * df_all['parallax']].copy()\n",
    "\n",
    "print(f\"Total stars after cleaning: {len(df_all)}\")\n",
    "\n",
    "# Combine the eclisping binary IDs with the Gaia DataFrame concatenated\n",
    "df_sample = pd.concat([df_all, incorrect_df, correct_df], axis=0)\n",
    "print(f\"Combined DataFrame shape: {df_sample.shape}\")\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_sample.to_csv(\"gaia_sample_ecl.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390af134",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fn_prime_gaia_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 269\u001b[0m\n\u001b[1;32m    266\u001b[0m n_\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m# Loop through incorrectly classified sources and plot all spectra with labels if Gaia data is problematic.\u001b[39;00m\n\u001b[0;32m--> 269\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m source_id \u001b[38;5;129;01min\u001b[39;00m \u001b[43mfn_prime_gaia_ids\u001b[49m\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m    270\u001b[0m     plot_spectrum_with_gaia_and_cmd(source_id, gaia_lamost_merged, save_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImages_and_Plots/CMD_Spectra_Gaia_CV_take2.png\u001b[39m\u001b[38;5;124m\"\u001b[39m, df_sample\u001b[38;5;241m=\u001b[39mdf_sample, correct_df\u001b[38;5;241m=\u001b[39mcorrect_df, incorrect_df\u001b[38;5;241m=\u001b[39mincorrect_df, n\u001b[38;5;241m=\u001b[39mn_)\n\u001b[1;32m    271\u001b[0m     n_\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fn_prime_gaia_ids' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.io import fits\n",
    "import gzip\n",
    "import io\n",
    "import os\n",
    "\n",
    "def add_vertical_line_between_groups(ax, labels):\n",
    "    \"\"\"\n",
    "    Draws a vertical dashed line on the provided axis between the error and flux groups.\n",
    "    \n",
    "    :param ax: The matplotlib axes object where the bar chart is plotted.\n",
    "    :param labels: List of labels for the bars, ordered such that error columns come first and flux columns second.\n",
    "    \"\"\"\n",
    "    # Count the number of error bars (assumes errors come first)\n",
    "    num_error = sum(1 for label in labels if label.endswith(\"_error\"))\n",
    "    if num_error and num_error < len(labels):\n",
    "        # Vertical line placed between the last error and the first flux bar\n",
    "        separation_index = num_error - 0.5\n",
    "        ax.axvline(x=separation_index, color='black', linestyle='--', linewidth=5)\n",
    "\n",
    "\n",
    "def open_fits_file(file_path):\n",
    "    \"\"\"\n",
    "    Opens a FITS file, handling both regular and gzipped formats.\n",
    "    \n",
    "    :param file_path: Path to the FITS file\n",
    "    :return: FITS HDU list or None if there was an error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if the file is gzipped\n",
    "        with open(file_path, 'rb') as f:\n",
    "            file_start = f.read(2)\n",
    "            f.seek(0)  # Reset file pointer\n",
    "            if file_start == b'\\x1f\\x8b':  # gzip magic number\n",
    "                # Handle gzipped file\n",
    "                with gzip.GzipFile(fileobj=f) as gz_f:\n",
    "                    file_content = gz_f.read()\n",
    "                print(f\"Opening gzipped file: {file_path}\")\n",
    "                return fits.open(io.BytesIO(file_content), ignore_missing_simple=True)\n",
    "            else:\n",
    "                # Handle regular file\n",
    "                print(f\"Opening regular file: {file_path}\")\n",
    "                return fits.open(file_path, ignore_missing_simple=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error opening file {os.path.basename(file_path)}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def plot_spectrum_with_gaia_and_cmd(source_id, gaia_lamost_merged, df_sample, correct_df, incorrect_df, n,\n",
    "                                    spectra_folder=\"lamost_spectra_uniques\", save_path=None):\n",
    "    \"\"\"\n",
    "    Plots the LAMOST spectrum, the Gaia parameters with issues, and a Color–Magnitude Diagram (CMD) \n",
    "    in a single figure with three subplots.\n",
    "    \n",
    "    :param source_id: Gaia Source ID of the incorrectly classified source.\n",
    "    :param gaia_lamost_merged: DataFrame containing Gaia and LAMOST cross-matched data.\n",
    "    :param df_sample: DataFrame containing Gaia photometric and parallax data for the CMD.\n",
    "    :param correct_df: DataFrame containing correctly classified Gaia IDs.\n",
    "    :param incorrect_df: DataFrame containing incorrectly classified Gaia IDs.\n",
    "    :param spectra_folder: Path to the folder containing LAMOST FITS spectra.\n",
    "    :param save_path: If provided, the complete figure is saved to this path.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if 'obsid' not in gaia_lamost_merged.columns:\n",
    "            print(\"⚠️ 'obsid' column not found in gaia_lamost_merged.\")\n",
    "            return\n",
    "\n",
    "        match = gaia_lamost_merged.loc[gaia_lamost_merged['source_id'] == source_id]\n",
    "        if match.empty:\n",
    "            print(f\"⚠️ No LAMOST match found for source_id {source_id}.\")\n",
    "            return\n",
    "\n",
    "        obsid = int(match.iloc[0]['obsid'])\n",
    "        print(f\"Found match: Source ID {source_id} -> ObsID {obsid}\")\n",
    "\n",
    "        fits_path = f\"{spectra_folder}/{int(obsid)}\"\n",
    "        \n",
    "        # Use the open_fits_file function to handle both regular and gzipped FITS files\n",
    "        hdul = open_fits_file(fits_path)\n",
    "        \n",
    "        if hdul is None:\n",
    "            print(f\"⚠️ Failed to open FITS file for ObsID {obsid}.\")\n",
    "            return\n",
    "            \n",
    "        # Process the FITS data\n",
    "        try:\n",
    "            # After opening the FITS file, add debugging:\n",
    "            print(f\"FITS file structure for ObsID {obsid}:\")\n",
    "            for i, hdu in enumerate(hdul):\n",
    "                print(f\"  HDU {i}: {hdu.__class__.__name__}, shape={getattr(hdu.data, 'shape', 'No data')}\")\n",
    "            \n",
    "            # LAMOST DR5 and later uses BinTableHDU in the first extension\n",
    "            if len(hdul) > 1 and isinstance(hdul[1], fits.BinTableHDU):\n",
    "                print(\"Using data from BinTableHDU (extension 1)\")\n",
    "                table_data = hdul[1].data\n",
    "                \n",
    "                # Debug table column names\n",
    "                print(f\"  BinTable columns: {table_data.names}\")\n",
    "                \n",
    "                # For LAMOST spectra, typical column names are 'FLUX', 'WAVELENGTH', 'LOGLAM', etc.\n",
    "                # Use appropriate column names based on what's available\n",
    "                if 'FLUX' in table_data.names and 'WAVELENGTH' in table_data.names:\n",
    "                    flux = table_data['FLUX'][0]  # First row\n",
    "                    wavelength = table_data['WAVELENGTH'][0]\n",
    "                    print(f\"  Using FLUX and WAVELENGTH columns\")\n",
    "                elif 'FLUX' in table_data.names and 'LOGLAM' in table_data.names:\n",
    "                    flux = table_data['FLUX'][0]  # First row\n",
    "                    # Convert log wavelength to linear wavelength\n",
    "                    log_wavelength = table_data['LOGLAM'][0]\n",
    "                    wavelength = 10**log_wavelength\n",
    "                    print(f\"  Using FLUX and LOGLAM (converted) columns\")\n",
    "                # Add more conditions for different column naming conventions\n",
    "                else:\n",
    "                    # If column names don't match known formats, try first two columns\n",
    "                    # (often wavelength is first, flux is second)\n",
    "                    print(f\"  Unknown column format, using first two columns\")\n",
    "                    wavelength = table_data[table_data.names[0]][0]\n",
    "                    flux = table_data[table_data.names[1]][0]\n",
    "            # Fallback to original method with primary HDU\n",
    "            elif hdul[0].data is not None and len(hdul[0].data.shape) >= 1:\n",
    "                print(\"Using data from PrimaryHDU\")\n",
    "                data = hdul[0].data\n",
    "                if data.shape[0] < 3:\n",
    "                    print(f\"⚠️ Skipping {obsid}: Primary HDU data has insufficient dimensions: {data.shape}\")\n",
    "                    return\n",
    "                flux = data[0]\n",
    "                wavelength = data[2]\n",
    "            else:\n",
    "                print(f\"⚠️ Skipping {obsid}: No usable data found in FITS file.\")\n",
    "                return\n",
    "                \n",
    "            # Check that we have valid data before proceeding\n",
    "            if flux is None or wavelength is None or len(flux) == 0 or len(wavelength) == 0:\n",
    "                print(f\"⚠️ Skipping {obsid}: Empty flux or wavelength arrays\")\n",
    "                return\n",
    "                \n",
    "            print(f\"  Data loaded successfully. Wavelength range: {min(wavelength):.2f}-{max(wavelength):.2f} Å\")\n",
    "            print(f\"  Flux range: {min(flux):.2e}-{max(flux):.2e}\")\n",
    "            \n",
    "            # Create a figure with three subplots using GridSpec.\n",
    "            # Top row: two subplots (spectrum and Gaia issues), Bottom row: CMD spanning full width.\n",
    "            fig = plt.figure(figsize=(24, 12))\n",
    "            gs = fig.add_gridspec(1, 3, height_ratios=[1])\n",
    "            ax1 = fig.add_subplot(gs[0, 1])\n",
    "            ax2 = fig.add_subplot(gs[0, 2])\n",
    "            ax3 = fig.add_subplot(gs[0, 0])\n",
    "            plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "            # --- Subplot 1: LAMOST Spectrum ---\n",
    "            ax1.plot(wavelength, flux, color='blue', alpha=0.7, lw=1, zorder=10)\n",
    "            ax1.set_xlabel(\"Wavelength (Å)\")\n",
    "            ax1.set_ylabel(\"Flux\")\n",
    "            ax1.set_title(f\"LAMOST Spectrum\")\n",
    "            ax1.grid(zorder=0)\n",
    "\n",
    "            # --- Subplot 2: Gaia Parameters with Issues ---\n",
    "            ax2.grid(zorder=0)\n",
    "            gaia_info = match.iloc[[0]].drop(columns=[\"source_id\", \"obsid\"], errors='ignore')\n",
    "            issues_dict = {}\n",
    "            issues_text_list = []\n",
    "            for col in gaia_info.columns:\n",
    "                value = gaia_info[col].values[0]\n",
    "                if col.endswith(\"_error\") and value > 1:\n",
    "                    issues_dict[col] = value\n",
    "                    issues_text_list.append(f\"Large error in {col}\")\n",
    "                elif col.endswith(\"_flux\") and value < -1:\n",
    "                    issues_dict[col] = value\n",
    "                    issues_text_list.append(f\"Dim object in {col}\")\n",
    "\n",
    "            if issues_dict:\n",
    "                # Order the labels: errors first, then fluxes.\n",
    "                error_labels = [l for l in issues_dict.keys() if l.endswith(\"_error\")]\n",
    "                flux_labels = [l for l in issues_dict.keys() if l.endswith(\"_flux\")]\n",
    "                ordered_labels = error_labels + flux_labels\n",
    "                ordered_values = [issues_dict[l] for l in ordered_labels]\n",
    "\n",
    "                ax2.bar(ordered_labels, ordered_values, color='skyblue', zorder=3)\n",
    "                ax2.tick_params(\"x\", labelrotation=45)\n",
    "                ax2.set_title(\"Gaia Parameters with Issues\")\n",
    "                ax2.set_ylabel(\"Standard Deviations from Mean\")\n",
    "\n",
    "                # Add a vertical dashed line between error and flux groups.\n",
    "                add_vertical_line_between_groups(ax2, ordered_labels)\n",
    "            else:\n",
    "                ax2.text(0.5, 0.5, \"No significant data issues\", ha='center', va='center', fontsize=12)\n",
    "                ax2.axis(\"off\")\n",
    "\n",
    "            # --- Subplot 3: Color–Magnitude Diagram (CMD) ---\n",
    "            # Compute additional columns for CMD.\n",
    "            df_sample['color'] = df_sample['phot_bp_mean_mag'] - df_sample['phot_rp_mean_mag']\n",
    "            df_sample['distance_pc'] = 1000 / df_sample['parallax']\n",
    "            df_sample['abs_mag'] = df_sample['phot_g_mean_mag'] - 5 * np.log10(df_sample['distance_pc'] / 10)\n",
    "            df_sample['is_correct'] = df_sample['source_id'].isin(correct_df['source_id'])\n",
    "            df_sample['is_incorrect'] = df_sample['source_id'].isin(incorrect_df['source_id'])\n",
    "\n",
    "            # Plot background stars (those not flagged as correct or incorrect)\n",
    "            mask_background = ~(df_sample['is_correct'] | df_sample['is_incorrect'])\n",
    "            ax3.scatter(df_sample.loc[mask_background, 'color'], \n",
    "                        df_sample.loc[mask_background, 'abs_mag'],\n",
    "                        s=3, color='gray', alpha=0.6, label='Nearby Stars')\n",
    "\n",
    "            # Plot the incorrect in red.\n",
    "            ax3.scatter(df_sample[df_sample['is_incorrect']]['color'],\n",
    "                        df_sample[df_sample['is_incorrect']]['abs_mag'],\n",
    "                        s=100, color='red', label='Incorrectly Classified', alpha=1, \n",
    "                        edgecolor='black', marker='H')\n",
    "            \n",
    "            # Plot the correct in green.\n",
    "            #ax3.scatter(df_sample[df_sample['is_correct']]['color'],\n",
    "            #            df_sample[df_sample['is_correct']]['abs_mag'],\n",
    "            #            s=100, color='green', label='Correctly Classified', alpha=1, \n",
    "            #            edgecolor='black', marker='x')\n",
    "            \n",
    "            # Plot the target source in blue. FLUX IS NOT THE SAME AS MAGNITUDE, data for both exist in the Gaia table.\n",
    "            target_color = df_sample.loc[df_sample['source_id'] == source_id, 'color'].values[0]\n",
    "            target_abs_mag = df_sample.loc[df_sample['source_id'] == source_id, 'abs_mag'].values[0]\n",
    "            ax3.scatter(target_color, target_abs_mag, s=200, color='blue', label='Target Source', alpha=1, edgecolor='black', marker='o')\n",
    "            #target_abs_mag = match['phot_g_mean_flux'].values[0] - 5 * np.log10((1/match['parallax'].values[0] )/ 10)\n",
    "            #ax3.scatter(target_color, target_abs_mag, s=200, color='blue', label='Target Source', alpha=1, edgecolor='black', marker='o')\n",
    "\n",
    "\n",
    "            # In a CMD, brighter (lower) magnitudes are at the top.\n",
    "            ax3.invert_yaxis()\n",
    "            ax3.set_xlim(-0.5, 3.5)\n",
    "            ax3.set_ylim(14, 0.5)\n",
    "            ax3.set_xlabel('Colour (BP - RP)')\n",
    "            ax3.set_ylabel('Absolute G Magnitude')\n",
    "            ax3.set_title('Colour–Magnitude Diagram (CMD)')\n",
    "            ax3.legend(loc='lower right')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            if save_path:\n",
    "                save_path= save_path.replace(\".png\", f\"_{n}.png\")\n",
    "                plt.savefig(save_path)\n",
    "            plt.show()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing FITS data for source_id {source_id}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            \n",
    "        finally:\n",
    "            # Ensure proper cleanup of FITS file\n",
    "            if hdul is not None:\n",
    "                try:\n",
    "                    hdul.close()\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not close FITS file: {e}\")\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in overall processing for source_id {source_id}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "gaia_lamost_merged = pd.read_csv(\"gaia_lamost_merged_ecl.csv\")                                                                                                                                                                                                                                           \n",
    "\n",
    "\n",
    "#gaia_lamost_merged = pd.DataFrame(gaia_lamost_merged, columns=[\"source_id\", \"obsid\", \"ra\", \"dec\", \"parallax\", \"phot_bp_mean_mag\", \"phot_rp_mean_mag\", \"phot_g_mean_mag\", \"parallax_error\", \"phot_bp_mean_flux\", \"phot_rp_mean_flux\", \"phot_g_mean_flux\", \"parallax_error_flux\", \"phot_bp_mean_mag_error\", \"phot_rp_mean_mag_error\", \"phot_g_mean_mag_error\"])\n",
    "\n",
    "\n",
    "# Example type conversions (ensure these columns are in the correct type)\n",
    "gaia_lamost_merged['obsid'] = gaia_lamost_merged['obsid'].astype(int)\n",
    "gaia_lamost_merged['source_id'] = gaia_lamost_merged['source_id'].astype(int)\n",
    "\n",
    "# Initialize a counter for the save path\n",
    "n_=1\n",
    "\n",
    "# Loop through incorrectly classified sources and plot all spectra with labels if Gaia data is problematic.\n",
    "for source_id in fn_prime_gaia_ids_ecl.astype(int):\n",
    "    plot_spectrum_with_gaia_and_cmd(source_id, gaia_lamost_merged, save_path=f\"Images_and_Plots/CMD_Spectra_Gaia_CV_take2.png\", df_sample=df_sample, correct_df=correct_df, incorrect_df=incorrect_df, n=n_)\n",
    "    n_+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a44e5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2748510",
   "metadata": {},
   "source": [
    "# Cataclysmic Binaries MambaOut WAS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a52c54f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1593140224924964864', '4471866725361723520', '2698490156365025536', '5171137394568701184', '6226943645600487552', '2163612727665972096', '2307289214897332480', '2104562321825510400', '1800384942558699008', '1013298268207936128', '1332378466733219456', '1563999425873420800', '2465053942183130240', '3876618514794039040', '3445477328117272576', '4714563374364671872', '4406459119386466176', '3681313024562519552', '2754909740118313344', '5099482805904892288', '1920126431748251776', '2477023401857408640', '1809844934461976832', '2923643719394227328', '2818311909906928384', '1030279027003254784', '1612331959869359872', '1203263915795342336', '5294908873052262016', '3859020040917830400', '4306244746253355776', '3688359000015020800', '2096274276193099136', '1558322303741820928', '2488974302977323008', '5745881603063095680', '2234727353044624128', '4545086473126911360', '2155490364688727168', '2355217815809560192', '1796893134146598144', '1374430388449392000', '6557154200328277120', '1759156585115470976', '6750469849393418752', '1679210927155357952', '6153373329716524800', '1620622591076494208', '3800596876396315648', '1289860214647954816', '4390013139853618560', '6159094290578416896', '3443331944709730176', '3614573802627605632', '4775330422798414208', '1088338463629882624', '6164939367406725632', '5463274408550748160', '1776946133496845440', '1368447331503747968', '4433651721269127552', '3071240270519385856', '4318147616785473408', '5092046675621447552', '2507796391561705728', '3212803625248377088', '5097770801875122432', '5544760551021856000', '6611621395028216192', '3749056714792756352', '3699606286708406912', '6690782895038162304', '3750072904055666176', '5634831718933218944', '6492236727426610304', '2895137398676678144', '3210749947983249408', '4822693466712614272', '1992580429800923520', '3971777500966832384', '5397102019219938944', '6414928454268212352', '4019704731785215360', '4458774810347862528', '1756386438585507712', '4846684780367607296', '3519860496622195072', '6749618724313541760', '1571768231438756864', '1658224789234521600', '6896767366186700416', '4565045873343899136', '5242787486412627072', '1565395908719728384', '5984221987022142464', '1949139691749510272', '1966073476490400384', '1913454217232996736', '6318149711371454464', '3741979368899159552', '4564460623922361472', '5041907811522399488', '1595085299649674240', '2257709040148720640', '4318508939464901760', '1328757431346007680', '4953766320874344704', '5759752908615824128', '1214663721071751296', '2041088653728555264', '6530347209195388032', '3776795812613678976', '3968038512892514048', '5814026207016102656', '5665509162794290944', '3958581402928688768', '2218642047686814208', '6048007439673413760', '4659256736897181312', '5496736223875627648', '2261657107228063232', '1099223590090176896', '1696309737421796352', '1037411284055821568', '3408324422192886528', '4654758944077464192', '1101817445394854400', '6185040879503491584', '1638288405744536704', '3386289728137230976', '6447773336193467520', '1014962001098647680', '2125911337244477184', '5719598950133755392', '1425200578380530048', '6849890031536423552', '3549823111895468672', '4482863941883637632', '3251069657846812800', '6096905573613586944', '1249384550225817856', '4919598030674335360', '4040721003309923840', '2262055164796424576', '3650020648757787008', '6347097928386163072', '6108446661838778496', '6794414550313352448', '2096934223687181696', '5652773897568243072', '1084309161535850368', '4409581010854168064', '2080764530817013376', '4653893040002306432', '5670829935783719936', '2132186284460008832', '5551154524667855104', '1163811724898541440', '5069444152526439040', '2126055476346353792', '6175970630128957568', '3408997392029195904', '4701058897674276736', '3071338844311737088', '6782793537041886464', '1296877675817415680', '2438469709529193856', '6039131391540808832', '3799290858445023488', '5628258258606112768', '3469884768265773824', '1091051096255456384', '1440078211950524544', '2652196593667289856', '3618331314896034048', '1640767568241611136', '6453536224527716224', '4461280391188698368', '4200218019655998720', '2069105580089321472', '5380310174561335296', '5367717429234054912', '3258480812896290560', '2866189598275905152', '4581360216423882624', '4476137370261520000', '4503256687122329088', '5524430207364715520', '5067753236787919232', '4697621824327141248', '5800369654124923008', '4706297108508032128', '3244090159897565952', '3996419759863758592', '5287103543082731264', '3084580919978268672', '3302990624137350528', '6002900490531164032', '5956624073383922688', '1040150781699159680', '4744804750196738560', '4794480174438869632', '1353646217070728704', '2796463449923353856', '6659603532710863232', '1202680251217734400', '4357536624383506560', '4918835764173746432', '2056003803844470528', '3551116004427251328', '2606580678024447104', '3048347059745859072', '1204588041329337600', '1347851790791563136', '1729491319015998208', '1759321791033449472', '6610409767574182912', '5948618632304703360', '3393671398233326976', '2194206707428148480', '2037590591849681152', '4176399299256927488', '6911950900211768704', '1846629263455015680', '5168129581727355008', '5084805635638179584', '2116887920191163776', '4635645888625306496', '1707635020719834880', '2654255639643078528', '6184077776037155072', '2147082017019347968', '1777232423131443840', '4459922356886052992', '5278321915508032256', '2151537856610343040', '1370642265950491776', '2701376099511002240', '6231616192056505984', '4798833587650467200', '1796202327310776704', '3568974066128043520', '5417357870965826816', '3133113805608088064', '5517905804146718592', '6684490840966796928', '1263484034505616256', '4484363984990385664', '5399305608023994624', '2235124349761327232', '2933992632075460352', '1580508421164776064', '1313538545446290688', '2443219535337313280', '2058291887543939968', '2856493142666940544', '5590218008142683520', '3742038776886730368', '2244648731797186688', '6137656700054685056', '1223491257550872192', '6914558112863956224', '1334848936218459136', '6396170751536790016', '6427690176495592960', '6401799112903799168', '2126637221076540544', '3158071830502039424', '4625018043590839424', '1326482511724121344', '1212352655004983808', '4883986502614113664', '6415545035475037312', '1767199173369655424', '4695093944014478080', '5698588863483876096', '3035322730338241792', '2123837555230207744', '2040949943465818880', '2096120477706881536', '2203373473312011008', '5893562752219619200', '5332776117932015104', '4413231423818141568', '4731746232846281344', '2106069275529926272', '3095470964175509632', '1108037726271701248', '1766838396116677248', '3161562367602419328', '2871939764914510976', '4526200711655400064', '3828660708104028032', '5452961607957113216', '2679569936368095488', '1985702920142544896', '6397189827017123072', '3146975005100167296', '4951395327129718528', '2868074058126513280', '5390485501841902592', '5389717630410364160', '4313192495850348544', '2494281851762928512', '4193346037615507968', '6760253239457454592', '2725143108496515840', '5662614457916388224', '4622567476987430400', '3040914949554883328', '2136026049644724864', '4764871219656633600', '4112610784274894208', '2037573102742604544', '2885173972200577920', '4289610788917017600', '1808517927003421056', '1066429869812792576', '6602422361059249792', '2162954949132840192', '2177784092519048704', '2337436792938619392', '5667667675918226176', '3036063148338139776', '4707482485122305536', '5477422099543150592', '2006109065688505472', '1842553511290318464', '5091939988633728640', '1910583495451438720', '2807841432470056448', '2017742684676480896', '2113091615775375232', '2158980729992107776', '4597436725689986688', '3653323684766790656', '3297832265335277824', '2517357336654841984', '1996248233085177472', '2648391016419660032', '3044263340416525568', '6027098061438434944', '1762594276937620992', '2291308397504739968', '5365035381081290496', '4222299752184927616', '4474002634076551680', '1112772429499375488', '5348039989335089408', '1794142701512576256', '4760105592664097280', '1925516856226108416', '6463668670854493952', '3391618889196357248', '4129079750474756480', '6422675166520651264', '3646410775989868800', '2979418405216259072', '1028258463932870656', '3141492569543716096', '2047921053359656704', '5365680794401390336', '6912759105682151424', '5366164442080271232', '2824150286583562496', '1209876314302933632', '5084901254493219968', '4537992286672097280', '1856408770857725184', '2136300034200904192', '1925596502097065344', '2684755714241156736', '2017736641707652608', '1815021160316471808', '3436435910858051200', '1780220380340013568', '3022568258011201024', '4192167185053426560', '2675351827511262720', '6057269485137752448', '4480616299611285888', '5891405647833287296', '1872069664728668288', '2083145484587589632', '3081121787677797504', '2126572929707190144', '1074721664954807040', '3221553165121825536', '2107209847044730112', '2256734632327338496', '1844515452349537536', '2562420855079002624', '2026498267562886016', '3197024400735832832', '2066230155323606784', '1963247113132725120', '5429765829625368832', '6769204324939596672', '2176555392574688768', '6179636986011888512', '3221858928137002880', '1768858645653683200', '4228442478834563712', '5489659560883611264', '2427474150870397056', '1616427198302006144', '3119646304291839872', '4676468869876208128', '2163877198882886656', '2838503311371673472', '1894166339444439808', '6637401025693171200', '2044839156635614976', '2034071055160353792', '3599772726850899712', '3099139244207211520', '4611319954072608384', '3854288674880283264', '5307787074808526592', '2262915708740112000', '3748782322217725696', '3222072856163838976', '2116226254706461568', '2287476977438712576', '2105585421693855744', '4339398736975240192', '5916893594339415936', '1114243404258933376', '1559987685901122304', '3831382647922429952', '6106172669928936448', '3350560685476251648', '4207860450105410688', '2764037271473043584', '2616360833953395456', '5219652387372513792', '3084722963136349440', '3446266197646225536', '4506083222318688000', '1382549392763335936', '2193827788233092096', '5935487817742197120', '1008926403817205888', '2258357545848389888', '5431690421650045568', '1813953083546374144', '2006086487034715904', '3164551664840579200', '2947045278211416960', '2565601982736199168', '3345033985983457920', '3358158723141670656', '2901783160488793728', '2014349389931360768', '6051805187189158656', '5702723680027513344', '5553468275089335040', '2329317895999827840', '3345789487910734464', '3886153548349816576', '3040161784089380480', '5660433576602424448', '3446676070669830528', '4804695423438691200', '3769067109159364608', '4304999622372178048', '1109608206832496512', '2097903507608061440', '1222867765738898560', '3843535378146405888', '2913032092154691200', '2128235773544310400', '1300273349974628864', '4766218190119756928', '5710870820677041664', '1703849402186302592', '1014839031890550912', '3586245912034740736', '3201141899983079296', '2175710417888871040', '1972957892448494592', '5737067406753752960', '3235497858779604608', '2326333512204996992', '5199087877799046272', '2060961982804342784', '1123169888190445568', '2032525446708555520', '1868798617634638592', '4556859562598924032', '2171623670612602496', '4501373322484989952', '4138663368701798272', '1970340955998528512', '3114391428987853312', '3384185396023552896', '5453215457704376064', '5329177523764942848', '2754217223886986240', '3233813338246568448']\n",
      "500\n",
      "{'parallax': PowerTransformer(), 'ra': PowerTransformer(), 'dec': PowerTransformer(), 'ra_error': PowerTransformer(), 'dec_error': PowerTransformer(), 'parallax_error': PowerTransformer(), 'pmra': PowerTransformer(), 'pmdec': PowerTransformer(), 'pmra_error': PowerTransformer(), 'pmdec_error': PowerTransformer(), 'phot_g_mean_flux': PowerTransformer(), 'flagnopllx': PowerTransformer(), 'phot_g_mean_flux_error': PowerTransformer(), 'phot_bp_mean_flux': PowerTransformer(), 'phot_rp_mean_flux': PowerTransformer(), 'phot_bp_mean_flux_error': PowerTransformer(), 'phot_rp_mean_flux_error': PowerTransformer(), 'flagnoflux': PowerTransformer()}\n",
      "18\n",
      "\n",
      "🚀 Step 1: Querying Gaia data...\n",
      "🔗 Gaia IDs: 500\n",
      "Gaia query split into 1 chunks of up to 30000 IDs.\n",
      "\n",
      "Processing Chunk 1/1...\n",
      "  Attempt 1/5: Launching Gaia job...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "  Job launched (ID: 1746266832456O). Waiting for results...\n",
      "  Results received for chunk 1.\n",
      "  Retrieved 500 records for this chunk.\n",
      "\n",
      "Consolidating results...\n",
      "Consolidated DataFrame shape: (500, 17)\n",
      "All requested IDs that exist in Gaia DR3 and were in successful chunks seem to be retrieved.\n",
      "🔗 Gaia data: (500, 17)\n",
      "\n",
      "🔄 Step 2: Cross-matching with LAMOST catalog...\n",
      "After NaN removal: Gaia=(500, 17), LAMOST=(10809336, 3)\n",
      "Matched Gaia Objects: (65, 17)\n",
      "Matched LAMOST Objects: (65, 3)\n",
      "\n",
      "📥 Step 3: Downloading LAMOST spectra (if needed)...\n",
      "✅ All spectra are already downloaded.\n",
      "\n",
      "🔧 Step 4: Converting from FITS LAMOST spectra...\n",
      "\n",
      "📂 Processing LAMOST FITS files...\n",
      "Examining sample files to determine FITS structure...\n",
      "Opening gzipped file: lamost_spectra_uniques/446308146\n",
      "Found FITS_rec in 446308146, names: ('FLUX', 'IVAR', 'WAVELENGTH', 'ANDMASK', 'ORMASK', 'NORMALIZATION')\n",
      "Opening gzipped file: lamost_spectra_uniques/660601090\n",
      "Found FITS_rec in 660601090, names: ('FLUX', 'IVAR', 'WAVELENGTH', 'ANDMASK', 'ORMASK', 'NORMALIZATION')\n",
      "Opening gzipped file: lamost_spectra_uniques/457204145\n",
      "Found FITS_rec in 457204145, names: ('FLUX', 'IVAR', 'WAVELENGTH', 'ANDMASK', 'ORMASK', 'NORMALIZATION')\n",
      "Found field names across sample files: {'ANDMASK', 'NORMALIZATION', 'ORMASK', 'WAVELENGTH', 'FLUX', 'IVAR'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing FITS files:  28%|██▊       | 18/65 [00:00<00:00, 172.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening gzipped file: lamost_spectra_uniques/446308146\n",
      "Opening gzipped file: lamost_spectra_uniques/660601090\n",
      "Opening gzipped file: lamost_spectra_uniques/457204145\n",
      "Opening gzipped file: lamost_spectra_uniques/797001189\n",
      "Opening gzipped file: lamost_spectra_uniques/814503145\n",
      "Opening gzipped file: lamost_spectra_uniques/631913192\n",
      "Opening gzipped file: lamost_spectra_uniques/3210010\n",
      "Opening gzipped file: lamost_spectra_uniques/384707148\n",
      "Opening gzipped file: lamost_spectra_uniques/565605203\n",
      "Opening gzipped file: lamost_spectra_uniques/377711035\n",
      "Opening gzipped file: lamost_spectra_uniques/266807242\n",
      "Opening gzipped file: lamost_spectra_uniques/577507136\n",
      "Opening gzipped file: lamost_spectra_uniques/250808080\n",
      "Opening gzipped file: lamost_spectra_uniques/679513244\n",
      "Opening gzipped file: lamost_spectra_uniques/315401163\n",
      "Opening gzipped file: lamost_spectra_uniques/866605170\n",
      "Opening gzipped file: lamost_spectra_uniques/641310250\n",
      "Opening gzipped file: lamost_spectra_uniques/616216075\n",
      "Opening gzipped file: lamost_spectra_uniques/82609065\n",
      "Opening gzipped file: lamost_spectra_uniques/255312155\n",
      "Opening gzipped file: lamost_spectra_uniques/709406147\n",
      "Opening gzipped file: lamost_spectra_uniques/397906239\n",
      "Opening gzipped file: lamost_spectra_uniques/615106099\n",
      "Opening gzipped file: lamost_spectra_uniques/587903144\n",
      "Opening gzipped file: lamost_spectra_uniques/384207162\n",
      "Opening gzipped file: lamost_spectra_uniques/400112059\n",
      "Opening gzipped file: lamost_spectra_uniques/369401250\n",
      "Opening gzipped file: lamost_spectra_uniques/55907056\n",
      "Opening gzipped file: lamost_spectra_uniques/74415176\n",
      "Opening gzipped file: lamost_spectra_uniques/471011180\n",
      "Opening gzipped file: lamost_spectra_uniques/462012204\n",
      "Opening gzipped file: lamost_spectra_uniques/451112097\n",
      "Opening gzipped file: lamost_spectra_uniques/566413139\n",
      "Opening gzipped file: lamost_spectra_uniques/469011118\n",
      "Opening gzipped file: lamost_spectra_uniques/104606213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing FITS files:  82%|████████▏ | 53/65 [00:00<00:00, 168.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening gzipped file: lamost_spectra_uniques/247615160\n",
      "Opening gzipped file: lamost_spectra_uniques/235613172\n",
      "Opening gzipped file: lamost_spectra_uniques/372410008\n",
      "Opening gzipped file: lamost_spectra_uniques/195807009\n",
      "Opening gzipped file: lamost_spectra_uniques/409102003\n",
      "Opening gzipped file: lamost_spectra_uniques/155803169\n",
      "Opening gzipped file: lamost_spectra_uniques/453806027\n",
      "Opening gzipped file: lamost_spectra_uniques/28908076\n",
      "Opening gzipped file: lamost_spectra_uniques/807204250\n",
      "Opening gzipped file: lamost_spectra_uniques/601205115\n",
      "Opening gzipped file: lamost_spectra_uniques/565809243\n",
      "Opening gzipped file: lamost_spectra_uniques/4315066\n",
      "Opening gzipped file: lamost_spectra_uniques/241503093\n",
      "Opening gzipped file: lamost_spectra_uniques/164609001\n",
      "Opening gzipped file: lamost_spectra_uniques/564905074\n",
      "Opening gzipped file: lamost_spectra_uniques/471703215\n",
      "Opening gzipped file: lamost_spectra_uniques/757314103\n",
      "Opening gzipped file: lamost_spectra_uniques/417206083\n",
      "Opening gzipped file: lamost_spectra_uniques/504410172\n",
      "Opening gzipped file: lamost_spectra_uniques/823606181\n",
      "Opening gzipped file: lamost_spectra_uniques/384616147\n",
      "Opening gzipped file: lamost_spectra_uniques/566811085\n",
      "Opening gzipped file: lamost_spectra_uniques/354201006\n",
      "Opening gzipped file: lamost_spectra_uniques/779715034\n",
      "Opening gzipped file: lamost_spectra_uniques/15508077\n",
      "Opening gzipped file: lamost_spectra_uniques/354403106\n",
      "Opening gzipped file: lamost_spectra_uniques/386706029\n",
      "Opening gzipped file: lamost_spectra_uniques/616215107\n",
      "Opening gzipped file: lamost_spectra_uniques/571307062\n",
      "Opening gzipped file: lamost_spectra_uniques/587413212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing FITS files: 100%|██████████| 65/65 [00:00<00:00, 98.87it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully processed 65 files\n",
      "⚠️ Encountered errors in 0 files\n",
      "\n",
      "📊 Step 5: Extracting and saving flux & frequency values...\n",
      "\n",
      "📊 Extracting flux and frequency values...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 23.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Flux values shape: (65, 3749), Frequency values shape: (65, 3749)\n",
      "\n",
      "📊 Step 6: Interpolating and normalizing LAMOST spectra...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Interpolating spectra: 100%|██████████| 65/65 [00:00<00:00, 683.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial number of rows: 65\n",
      "Total successful interpolations: 64\n",
      "Total skipped rows (NaNs + zeros): 1\n",
      "Final check: len(df_flux) == cnt_success + len(nan_files)? True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5644/3309002737.py:60: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  gaia_lamost_match[\"obsid\"] = gaia_lamost_match[\"obsid\"].astype(int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Step 7: Normalizing Gaia data...\n",
      "Dropped 0 rows with NaN values.\n",
      "\n",
      "🔗 Step 8: Merging Gaia and LAMOST data...\n",
      "\n",
      "🤖 Step 9: Predicting labels using the trained model...\n",
      "X_spectra shape: (64, 3647)\n",
      "X_gaia shape: (64, 18)\n",
      "\n",
      "💾 Step 10: Saving predictions...\n"
     ]
    }
   ],
   "source": [
    "cat_gaia_ids = []\n",
    "with open('Pickles/Cataclysmic Bin Catalogue Abrahams et al.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        # Check if the line starts with a digit (to avoid header lines)\n",
    "        if line and line[0].isdigit():\n",
    "            cat_gaia_ids.append(line[:19].strip())\n",
    "print(cat_gaia_ids)\n",
    "print(len(cat_gaia_ids))\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('Pickles/gaia_normalization.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "print(data)\n",
    "print(len(data))\n",
    "\n",
    "# Load the LAMOST catalog to cross-match with Gaia as csv\n",
    "lamost_catalogue = pd.read_csv(\"lamost/minimal.csv\")  # Load LAMOST catalog (Just obsid and Ra, Dec)\n",
    "label_cols = pd.read_pickle(\"Pickles/Updated_List_of_Classes_ubuntu.pkl\")\n",
    "\n",
    "# Example usage:\n",
    "model_path = \"Models/model_fusion_mambaoutv3.pth\"\n",
    "gaia_transformers = \"Pickles/gaia_normalization.pkl\"\n",
    "\n",
    "df_predictions, gaia_lamost_merged = predict_star_labels(cat_gaia_ids, model_path, lamost_catalogue, gaia_transformers)\n",
    "\n",
    "# Save the predictions to a npy file\n",
    "np.save(\"y_predictions_bin_out.npy\", df_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "29ad8e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RS*', '**', 'El*', 'Y*O', 's*b', 'cC*', 'HB*', 'dS*', 'Or*', 'LP*', 'BS*', 'Ae*', 'WV*', 'HS*', 'Ev*', 'AB*', 'sg*', 's*r', 'Ce*', 'gD*', 'OH*', 'HXB', 'Pu*', 'RV*', 'Sy*', 'V*', 'TT*', 'SN*', 'Be*', 'SB*', 'Em*', 'Er*', 'PM*', 'HV*', 'pA*', 'C*', 'BY*', 'Ro*', 'XB*', 'Ma*', 'Pe*', 'CV*', 'bC*', 'RR*', 'Mi*', 'SX*', 'RG*', 'LM*', 'WD*', 'S*', 'MS*', 'Ir*', 'a2*', 'PN', 'EB*']\n",
      "41 1\n",
      "14\n",
      "\n",
      "--- Identifying Specific Misclassifications (FN') ---\n",
      "\n",
      "🔍 Found 5 samples meeting the FN' criteria for CV* or **:\n",
      "      source_id\n",
      "0  1.558322e+18\n",
      "1  1.612332e+18\n",
      "2  2.116888e+18\n",
      "3  3.408324e+18\n",
      "4  3.446676e+18\n",
      "\n",
      "🔍 Labels given by the model to FN' samples:\n",
      "   RS*  **  El*  Y*O  s*b  cC*  HB*  dS*  Or*  LP*  ...  SX*  RG*  LM*  WD*  \\\n",
      "0    0   0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
      "1    0   0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
      "2    0   0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
      "3    0   1    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
      "4    0   1    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
      "\n",
      "   S*  MS*  Ir*  a2*  PN  EB*  \n",
      "0   0    0    0    0   0    0  \n",
      "1   0    0    0    0   0    0  \n",
      "2   0    0    0    0   0    0  \n",
      "3   0    0    0    0   0    0  \n",
      "4   0    0    0    0   0    1  \n",
      "\n",
      "[5 rows x 55 columns]\n",
      "\n",
      "🔍 Incorrectly Classified Gaia IDs:\n",
      "      source_id\n",
      "0  1.558322e+18\n",
      "1  1.612332e+18\n",
      "2  2.116888e+18\n",
      "3  2.698490e+18\n",
      "4  3.350561e+18\n",
      "5  3.408324e+18\n",
      "6  3.446676e+18\n",
      "7  3.681313e+18\n",
      "\n",
      "📊 Performance Metrics:\n",
      "   Class  Precision   Recall  F1 Score  Custom F1 Score  Custom Precision  \\\n",
      "1     **        1.0  0.90625   0.95082            0.928           0.95082   \n",
      "41   CV*        1.0  0.90625   0.95082            0.928           0.95082   \n",
      "\n",
      "    Exact Match Ratio  Support  \n",
      "1               0.875       64  \n",
      "41              0.875       64  \n",
      "Exact match accuracy: 0.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "y_pred_ = np.load(\"y_predictions_bin_out.npy\")\n",
    "y_pred = np.array(y_pred_[:, :-1], dtype=int) # last column is source_id probably\n",
    "classes = pd.read_pickle(\"Pickles/Updated_List_of_Classes_ubuntu.pkl\")\n",
    "\n",
    "print(classes)\n",
    "\n",
    "cv_index = classes.index(\"CV*\")\n",
    "star_index = classes.index(\"**\")\n",
    "\n",
    "print(cv_index, star_index)\n",
    "\n",
    "# Generate the expected y_true for eclipsing binaries\n",
    "y_true = np.zeros_like(y_pred)\n",
    "y_true[:, -14] = 1  # \"CV*\" column\n",
    "y_true[:, 1] = 1   # \"**\" column (second column)\n",
    "\n",
    "# Count the number of misclassified samples\n",
    "print(np.sum(y_true!=y_pred))\n",
    "\n",
    "# Compute precision, recall, and F1-score for each class\n",
    "precision = precision_score(y_true, y_pred, average=None, zero_division=0)\n",
    "recall = recall_score(y_true, y_pred, average=None, zero_division=0)\n",
    "f1 = f1_score(y_true, y_pred, average=None, zero_division=0)\n",
    "custom_f1 = custom_f1_score(y_true, y_pred, zero_division=0)\n",
    "custom_precision = custom_precision_score(y_true, y_pred, zero_division=0)\n",
    "exact_match = exact_match_ratio(y_true, y_pred)\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "_, _, _, support = score(y_true, y_pred)\n",
    "\n",
    "# Create a DataFrame to store metrics per class\n",
    "metrics_df = pd.DataFrame({\n",
    "    \"Class\": classes,\n",
    "    \"Precision\": precision,\n",
    "    \"Recall\": recall,\n",
    "    \"F1 Score\": f1,\n",
    "    \"Custom F1 Score\": custom_f1,\n",
    "    \"Custom Precision\": custom_precision,\n",
    "    \"Exact Match Ratio\": exact_match,\n",
    "    \"Support\": support\n",
    "})\n",
    "\n",
    "# Identify Correctly Classified Samples (True Positives)\n",
    "correct_predictions = (y_pred == y_true).all(axis=1)\n",
    "correct_gaia_ids = y_pred_[correct_predictions, -1]\n",
    "\n",
    "# Identify incorrectly classified samples (False Positives and False Negatives)\n",
    "incorrect_predictions = (y_pred != y_true).any(axis=1)\n",
    "incorrect_gaia_ids = y_pred_[incorrect_predictions, -1]\n",
    "\n",
    "# Not precise classification\n",
    "print(\"\\n--- Identifying Specific Misclassifications (FN') ---\")\n",
    "\n",
    "# 1. Find samples where the model predicted *at least one* label (any label)\n",
    "model_predicted_something_mask = np.sum(y_pred, axis=1) > 0\n",
    "\n",
    "# 2. Find samples where the model missed the target classes\n",
    "missed_cv_mask = (y_true[:, cv_index] == 1) & (y_pred[:, cv_index] == 0)\n",
    "missed_star_mask = (y_true[:, star_index] == 1) & (y_pred[:, star_index] == 0)\n",
    "\n",
    "# 3. Find FN' samples for each target class\n",
    "# FN' = Missed the target class AND predicted something else\n",
    "fn_prime_cv_mask = missed_cv_mask & model_predicted_something_mask\n",
    "fn_prime_star_mask = missed_star_mask & model_predicted_something_mask\n",
    "\n",
    "# 4. Combine: Find samples that are FN' for *either* CV* OR **\n",
    "overall_fn_prime_mask = fn_prime_cv_mask | fn_prime_star_mask\n",
    "\n",
    "# 5. Get the corresponding Gaia IDs\n",
    "# Ensure df_predictions index aligns with y_pred rows\n",
    "# Using .iloc requires the integer indices based on the mask\n",
    "fn_prime_indices = np.where(overall_fn_prime_mask)[0]\n",
    "fn_prime_gaia_ids = y_pred_[fn_prime_indices, -1] # Assumes last col is source_id\n",
    "\n",
    "print(f\"\\n🔍 Found {len(fn_prime_gaia_ids)} samples meeting the FN' criteria for CV* or **:\")\n",
    "print(pd.DataFrame({\"source_id\": fn_prime_gaia_ids}))\n",
    "\n",
    "# State the labels given by the model to the FN' samples\n",
    "print(\"\\n🔍 Labels given by the model to FN' samples:\")\n",
    "fn_prime_labels = y_pred[overall_fn_prime_mask]\n",
    "print_fn =pd.DataFrame(fn_prime_labels, columns=classes)\n",
    "print(pd.DataFrame(fn_prime_labels, columns=classes))\n",
    "\n",
    "# Display incorrectly classified Gaia IDs\n",
    "print(\"\\n🔍 Incorrectly Classified Gaia IDs:\")\n",
    "print(pd.DataFrame({\"source_id\": incorrect_gaia_ids}))\n",
    "\n",
    "# Display the performance metrics for the non-zero classes\n",
    "print(\"\\n📊 Performance Metrics:\")\n",
    "metrics_df = metrics_df[metrics_df[\"Precision\"] > 0]\n",
    "print(metrics_df)\n",
    "exact_match = np.mean((y_pred == y_true).all(axis=1))\n",
    "print(\"Exact match accuracy:\", exact_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17337353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For plotting\n",
    "#incorrect_gaia_ids = fn_prime_gaia_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ac632a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found match: Source ID 1558322303741820928 -> ObsID 566413139\n",
      "⚠️ Skipping 566413139: Data not found or incorrect format.\n",
      "Found match: Source ID 1612331959869359872 -> ObsID 814503145\n",
      "⚠️ Skipping 814503145: Data not found or incorrect format.\n",
      "⚠️ No LAMOST match found for source_id 2116887920191163904.\n",
      "⚠️ No LAMOST match found for source_id 3408324422192886784.\n",
      "⚠️ No LAMOST match found for source_id 3446676070669830656.\n"
     ]
    }
   ],
   "source": [
    "def add_vertical_line_between_groups(ax, labels):\n",
    "    \"\"\"\n",
    "    Draws a vertical dashed line on the provided axis between the error and flux groups.\n",
    "    \n",
    "    :param ax: The matplotlib axes object where the bar chart is plotted.\n",
    "    :param labels: List of labels for the bars, ordered such that error columns come first and flux columns second.\n",
    "    \"\"\"\n",
    "    # Count the number of error bars (assumes errors come first)\n",
    "    num_error = sum(1 for label in labels if label.endswith(\"_error\"))\n",
    "    if num_error and num_error < len(labels):\n",
    "        # Vertical line placed between the last error and the first flux bar\n",
    "        separation_index = num_error - 0.5\n",
    "        ax.axvline(x=separation_index, color='black', linestyle='--', linewidth=5)\n",
    "\n",
    "def plot_spectrum_with_gaia(source_id, gaia_lamost_merged, spectra_folder=\"lamost_spectra_uniques\", save_path=None):\n",
    "    \"\"\"\n",
    "    Plots the LAMOST spectrum from FITS files and displays only Gaia parameters that have issues.\n",
    "    \n",
    "    :param source_id: Gaia Source ID of the incorrectly classified source\n",
    "    :param gaia_lamost_merged: DataFrame containing Gaia and LAMOST cross-matched data\n",
    "    :param spectra_folder: Path to the folder containing LAMOST FITS spectra\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if 'obsid' not in gaia_lamost_merged.columns:\n",
    "            print(f\"⚠️ 'obsid' column not found in gaia_lamost_merged.\")\n",
    "            return\n",
    "        \n",
    "        match = gaia_lamost_merged.loc[gaia_lamost_merged['source_id'] == source_id]\n",
    "        if match.empty:\n",
    "            print(f\"⚠️ No LAMOST match found for source_id {source_id}.\")\n",
    "            return\n",
    "        \n",
    "        obsid = int(match.iloc[0]['obsid'])\n",
    "        print(f\"Found match: Source ID {source_id} -> ObsID {obsid}\")\n",
    "        \n",
    "        fits_path = f\"{spectra_folder}/{int(obsid)}\"\n",
    "        \n",
    "        # Load FITS data\n",
    "        with fits.open(fits_path) as hdul:\n",
    "            data = hdul[0].data\n",
    "            if data is None or data.shape[0] < 3:\n",
    "                print(f\"⚠️ Skipping {obsid}: Data not found or incorrect format.\")\n",
    "                return\n",
    "            \n",
    "            flux = data[0]\n",
    "            wavelength = data[2]\n",
    "            \n",
    "            fig, ax = plt.subplots(2, 1, figsize=(9, 12), gridspec_kw={'height_ratios': [3, 4]})\n",
    "            plt.rcParams.update({'font.size': 16})\n",
    "            \n",
    "            # Plot the LAMOST spectrum\n",
    "            ax[0].plot(wavelength, flux, color='blue', alpha=0.7, lw=1, zorder=10)\n",
    "            ax[0].set_xlabel(\"Wavelength (Å)\")\n",
    "            ax[0].set_ylabel(\"Flux\")\n",
    "            ax[0].set_title(f\"LAMOST Spectrum for Gaia ID: {source_id} (LAMOST ID: {obsid})\")\n",
    "            ax[0].grid(zorder=0)\n",
    "            ax[1].grid(zorder=0)\n",
    "\n",
    "            # Get Gaia info (dropping unneeded columns)\n",
    "            gaia_info = match.iloc[[0]].drop(columns=[\"source_id\", \"obsid\"], errors='ignore')\n",
    "            \n",
    "            # Build a dictionary for only the columns with issues.\n",
    "            issues_dict = {}\n",
    "            issues_text_list = []\n",
    "            for col in gaia_info.columns:\n",
    "                value = gaia_info[col].values[0]\n",
    "                if col.endswith(\"_error\") and value > 1:\n",
    "                    issues_dict[col] = value\n",
    "                    issues_text_list.append(f\"Large error in {col}\")\n",
    "                elif col.endswith(\"_flux\") and value < -1:\n",
    "                    issues_dict[col] = value\n",
    "                    issues_text_list.append(f\"Dim object in {col}\")\n",
    "            \n",
    "            issue_text = \"; \".join(issues_text_list) if issues_text_list else \"No significant data issues\"\n",
    "            # Plot only the problematic Gaia parameters if any exist.\n",
    "            if issues_dict:\n",
    "                # Order the labels: errors first, then fluxes.\n",
    "                error_labels = [l for l in issues_dict.keys() if l.endswith(\"_error\")]\n",
    "                flux_labels = [l for l in issues_dict.keys() if l.endswith(\"_flux\")]\n",
    "                ordered_labels = error_labels + flux_labels\n",
    "                ordered_values = [issues_dict[l] for l in ordered_labels]\n",
    "                \n",
    "                ax[1].bar(ordered_labels, ordered_values, color='skyblue', zorder=3)\n",
    "                ax[1].tick_params(\"x\", labelrotation=90)\n",
    "                ax[1].set_title(\"Gaia Parameters with Issues\")\n",
    "                ax[1].set_ylabel(\"Standard Deviations from Mean\")\n",
    "                \n",
    "                # Add a vertical dashed line between error and flux groups.\n",
    "                add_vertical_line_between_groups(ax[1], ordered_labels)\n",
    "            else:\n",
    "                ax[1].text(0.5, 0.5, \"No significant data issues\", ha='center', va='center', fontsize=12)\n",
    "                ax[1].axis(\"off\")\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            if save_path:\n",
    "                plt.savefig(save_path)\n",
    "            plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {fits_path}: {e}\")\n",
    "\n",
    "# Example type conversions (ensure these columns are in the correct type)\n",
    "gaia_lamost_merged['obsid'] = gaia_lamost_merged['obsid'].astype(int)\n",
    "gaia_lamost_merged['source_id'] = gaia_lamost_merged['source_id'].astype(int)\n",
    "\n",
    "# Loop through incorrectly classified sources and plot all spectra with labels if Gaia data is problematic.\n",
    "for source_id in incorrect_gaia_ids.astype(int):\n",
    "    plot_spectrum_with_gaia(source_id, gaia_lamost_merged, save_path=f\"Images_and_Plots/{source_id}_spectrum.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ed9c4f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "Total stars returned by the query: 1000000\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "✅ Retrieved detailed information for 1 correctly classified Gaia IDs.\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "✅ Retrieved detailed information for 0 incorrectly classified Gaia IDs.\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "✅ Retrieved detailed information for 1 correctly classified Gaia IDs.\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "✅ Retrieved detailed information for 0 incorrectly classified Gaia IDs.\n",
      "Total stars before cleaning: 1000000\n",
      "Total stars after cleaning: 39158\n",
      "Combined DataFrame shape: (39159, 8)\n"
     ]
    }
   ],
   "source": [
    "# Background: Random Stars in Milky Way for CMD\n",
    "\n",
    "# Define the ADQL query to select stars with parallax > 0.03 mas (roughly within 30kpc, inside milky way)\n",
    "query = \"\"\"\n",
    "SELECT TOP 1000000 source_id, ra, dec, parallax, phot_bp_mean_mag, phot_rp_mean_mag, phot_g_mean_mag, parallax_error\n",
    "FROM gaiadr3.gaia_source\n",
    "WHERE parallax > 0.03 \n",
    "\"\"\"\n",
    "\n",
    "# Launch an asynchronous job (this will return more than 2000 rows if available)\n",
    "job = Gaia.launch_job_async(query)\n",
    "gaia_table = job.get_results()\n",
    "\n",
    "# Convert to a pandas DataFrame\n",
    "df_all = gaia_table.to_pandas()\n",
    "print(f\"Total stars returned by the query: {len(df_all)}\")\n",
    "\n",
    "# Define the ADQL query to fetch detailed information for the Correctly Classified Gaia IDs\n",
    "query = \"\"\"\n",
    "SELECT source_id, ra, dec, parallax, phot_bp_mean_mag, phot_rp_mean_mag, phot_g_mean_mag, parallax_error\n",
    "FROM gaiadr3.gaia_source\n",
    "WHERE source_id IN ({})\n",
    "\"\"\"\n",
    "\n",
    "# Join the source IDs into a single string\n",
    "source_ids_str = \",\".join([str(id) for id in correct_gaia_ids])\n",
    "full_query = query.format(source_ids_str)\n",
    "\n",
    "# Run the query asynchronously\n",
    "job = Gaia.launch_job_async(full_query)\n",
    "results = job.get_results()\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "correct_df = results.to_pandas()\n",
    "\n",
    "print(f\"✅ Retrieved detailed information for {len(correct_df)} correctly classified Gaia IDs.\")\n",
    "\n",
    "# Define the ADQL query to fetch detailed information for the incorrectly Classified Gaia IDs\n",
    "query = \"\"\"\n",
    "SELECT source_id, ra, dec, parallax, phot_bp_mean_mag, phot_rp_mean_mag, phot_g_mean_mag, parallax_error\n",
    "FROM gaiadr3.gaia_source\n",
    "WHERE source_id IN ({})\n",
    "\"\"\"\n",
    "\n",
    "# Join the source IDs into a single string\n",
    "source_ids_str = \",\".join([str(id) for id in incorrect_gaia_ids])\n",
    "full_query = query.format(source_ids_str)\n",
    "\n",
    "# Run the query asynchronously\n",
    "job = Gaia.launch_job_async(full_query)\n",
    "results = job.get_results()\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "incorrect_df = results.to_pandas()\n",
    "\n",
    "print(f\"✅ Retrieved detailed information for {len(incorrect_df)} incorrectly classified Gaia IDs.\")\n",
    "# Define the ADQL query to fetch detailed information for the Correctly Classified Gaia IDs\n",
    "query = \"\"\"\n",
    "SELECT source_id, ra, dec, parallax, phot_bp_mean_mag, phot_rp_mean_mag, phot_g_mean_mag, parallax_error\n",
    "FROM gaiadr3.gaia_source\n",
    "WHERE source_id IN ({})\n",
    "\"\"\"\n",
    "\n",
    "# Join the source IDs into a single string\n",
    "source_ids_str = \",\".join([str(id) for id in correct_gaia_ids])\n",
    "full_query = query.format(source_ids_str)\n",
    "\n",
    "# Run the query asynchronously\n",
    "job = Gaia.launch_job_async(full_query)\n",
    "results = job.get_results()\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "correct_df = results.to_pandas()\n",
    "\n",
    "print(f\"✅ Retrieved detailed information for {len(correct_df)} correctly classified Gaia IDs.\")\n",
    "\n",
    "# Define the ADQL query to fetch detailed information for the incorrectly Classified Gaia IDs\n",
    "query = \"\"\"\n",
    "SELECT source_id, ra, dec, parallax, phot_bp_mean_mag, phot_rp_mean_mag, phot_g_mean_mag, parallax_error\n",
    "FROM gaiadr3.gaia_source\n",
    "WHERE source_id IN ({})\n",
    "\"\"\"\n",
    "\n",
    "# Join the source IDs into a single string\n",
    "source_ids_str = \",\".join([str(id) for id in incorrect_gaia_ids])\n",
    "full_query = query.format(source_ids_str)\n",
    "\n",
    "# Run the query asynchronously\n",
    "job = Gaia.launch_job_async(full_query)\n",
    "results = job.get_results()\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "incorrect_df = results.to_pandas()\n",
    "\n",
    "print(f\"✅ Retrieved detailed information for {len(incorrect_df)} incorrectly classified Gaia IDs.\")\n",
    "\n",
    "# --- Example: Prepare your Gaia DataFrame ---\n",
    "# Clean out the bad parallax values (negative or zero)\n",
    "print(f\"Total stars before cleaning: {len(df_all)}\")\n",
    "df_all = df_all[df_all['parallax'] > 0].copy()\n",
    "\n",
    "# Remove stars with large parallax errors (e.g., > 10% of the parallax value)\n",
    "df_all = df_all[df_all['parallax_error'] < 0.1 * df_all['parallax']].copy()\n",
    "\n",
    "print(f\"Total stars after cleaning: {len(df_all)}\")\n",
    "\n",
    "# Combine the eclisping binary IDs with the Gaia DataFrame concatenated\n",
    "df_sample = pd.concat([df_all, incorrect_df, correct_df], axis=0)\n",
    "print(f\"Combined DataFrame shape: {df_sample.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fce39267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save df_sample to a CSV file\n",
    "df_sample.to_csv(\"Pickles/gaia_sample_background.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7d95759",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'gaia_lamost_merged.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 255\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtraceback\u001b[39;00m\n\u001b[1;32m    253\u001b[0m         traceback\u001b[38;5;241m.\u001b[39mprint_exc()\n\u001b[0;32m--> 255\u001b[0m gaia_lamost_merged \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgaia_lamost_merged.npy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m                                                                                                                                                                                                                                                           \n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m#gaia_lamost_merged = pd.DataFrame(gaia_lamost_merged, columns=[\"source_id\", \"obsid\", \"ra\", \"dec\", \"parallax\", \"phot_bp_mean_mag\", \"phot_rp_mean_mag\", \"phot_g_mean_mag\", \"parallax_error\", \"phot_bp_mean_flux\", \"phot_rp_mean_flux\", \"phot_g_mean_flux\", \"parallax_error_flux\", \"phot_bp_mean_mag_error\", \"phot_rp_mean_mag_error\", \"phot_g_mean_mag_error\"])\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \n\u001b[1;32m    260\u001b[0m \n\u001b[1;32m    261\u001b[0m \u001b[38;5;66;03m# Example type conversions (ensure these columns are in the correct type)\u001b[39;00m\n\u001b[1;32m    262\u001b[0m gaia_lamost_merged[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobsid\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m gaia_lamost_merged[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobsid\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/numpy/lib/_npyio_impl.py:451\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    449\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 451\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    452\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'gaia_lamost_merged.npy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.io import fits\n",
    "import gzip\n",
    "import io\n",
    "import os\n",
    "\n",
    "def add_vertical_line_between_groups(ax, labels):\n",
    "    \"\"\"\n",
    "    Draws a vertical dashed line on the provided axis between the error and flux groups.\n",
    "    \n",
    "    :param ax: The matplotlib axes object where the bar chart is plotted.\n",
    "    :param labels: List of labels for the bars, ordered such that error columns come first and flux columns second.\n",
    "    \"\"\"\n",
    "    # Count the number of error bars (assumes errors come first)\n",
    "    num_error = sum(1 for label in labels if label.endswith(\"_error\"))\n",
    "    if num_error and num_error < len(labels):\n",
    "        # Vertical line placed between the last error and the first flux bar\n",
    "        separation_index = num_error - 0.5\n",
    "        ax.axvline(x=separation_index, color='black', linestyle='--', linewidth=5)\n",
    "\n",
    "\n",
    "def open_fits_file(file_path):\n",
    "    \"\"\"\n",
    "    Opens a FITS file, handling both regular and gzipped formats.\n",
    "    \n",
    "    :param file_path: Path to the FITS file\n",
    "    :return: FITS HDU list or None if there was an error\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if the file is gzipped\n",
    "        with open(file_path, 'rb') as f:\n",
    "            file_start = f.read(2)\n",
    "            f.seek(0)  # Reset file pointer\n",
    "            if file_start == b'\\x1f\\x8b':  # gzip magic number\n",
    "                # Handle gzipped file\n",
    "                with gzip.GzipFile(fileobj=f) as gz_f:\n",
    "                    file_content = gz_f.read()\n",
    "                print(f\"Opening gzipped file: {file_path}\")\n",
    "                return fits.open(io.BytesIO(file_content), ignore_missing_simple=True)\n",
    "            else:\n",
    "                # Handle regular file\n",
    "                print(f\"Opening regular file: {file_path}\")\n",
    "                return fits.open(file_path, ignore_missing_simple=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error opening file {os.path.basename(file_path)}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def plot_spectrum_with_gaia_and_cmd(source_id, gaia_lamost_merged, df_sample, correct_df, incorrect_df, n,\n",
    "                                    spectra_folder=\"lamost_spectra_uniques\", save_path=None):\n",
    "    \"\"\"\n",
    "    Plots the LAMOST spectrum, the Gaia parameters with issues, and a Color–Magnitude Diagram (CMD) \n",
    "    in a single figure with three subplots.\n",
    "    \n",
    "    :param source_id: Gaia Source ID of the incorrectly classified source.\n",
    "    :param gaia_lamost_merged: DataFrame containing Gaia and LAMOST cross-matched data.\n",
    "    :param df_sample: DataFrame containing Gaia photometric and parallax data for the CMD.\n",
    "    :param correct_df: DataFrame containing correctly classified Gaia IDs.\n",
    "    :param incorrect_df: DataFrame containing incorrectly classified Gaia IDs.\n",
    "    :param spectra_folder: Path to the folder containing LAMOST FITS spectra.\n",
    "    :param save_path: If provided, the complete figure is saved to this path.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if 'obsid' not in gaia_lamost_merged.columns:\n",
    "            print(\"⚠️ 'obsid' column not found in gaia_lamost_merged.\")\n",
    "            return\n",
    "\n",
    "        match = gaia_lamost_merged.loc[gaia_lamost_merged['source_id'] == source_id]\n",
    "        if match.empty:\n",
    "            print(f\"⚠️ No LAMOST match found for source_id {source_id}.\")\n",
    "            return\n",
    "\n",
    "        obsid = int(match.iloc[0]['obsid'])\n",
    "        print(f\"Found match: Source ID {source_id} -> ObsID {obsid}\")\n",
    "\n",
    "        fits_path = f\"{spectra_folder}/{int(obsid)}\"\n",
    "        \n",
    "        # Use the open_fits_file function to handle both regular and gzipped FITS files\n",
    "        hdul = open_fits_file(fits_path)\n",
    "        \n",
    "        if hdul is None:\n",
    "            print(f\"⚠️ Failed to open FITS file for ObsID {obsid}.\")\n",
    "            return\n",
    "            \n",
    "        # Process the FITS data\n",
    "        try:\n",
    "            # After opening the FITS file, add debugging:\n",
    "            print(f\"FITS file structure for ObsID {obsid}:\")\n",
    "            for i, hdu in enumerate(hdul):\n",
    "                print(f\"  HDU {i}: {hdu.__class__.__name__}, shape={getattr(hdu.data, 'shape', 'No data')}\")\n",
    "            \n",
    "            # LAMOST DR5 and later uses BinTableHDU in the first extension\n",
    "            if len(hdul) > 1 and isinstance(hdul[1], fits.BinTableHDU):\n",
    "                print(\"Using data from BinTableHDU (extension 1)\")\n",
    "                table_data = hdul[1].data\n",
    "                \n",
    "                # Debug table column names\n",
    "                print(f\"  BinTable columns: {table_data.names}\")\n",
    "                \n",
    "                # For LAMOST spectra, typical column names are 'FLUX', 'WAVELENGTH', 'LOGLAM', etc.\n",
    "                # Use appropriate column names based on what's available\n",
    "                if 'FLUX' in table_data.names and 'WAVELENGTH' in table_data.names:\n",
    "                    flux = table_data['FLUX'][0]  # First row\n",
    "                    wavelength = table_data['WAVELENGTH'][0]\n",
    "                    print(f\"  Using FLUX and WAVELENGTH columns\")\n",
    "                elif 'FLUX' in table_data.names and 'LOGLAM' in table_data.names:\n",
    "                    flux = table_data['FLUX'][0]  # First row\n",
    "                    # Convert log wavelength to linear wavelength\n",
    "                    log_wavelength = table_data['LOGLAM'][0]\n",
    "                    wavelength = 10**log_wavelength\n",
    "                    print(f\"  Using FLUX and LOGLAM (converted) columns\")\n",
    "                # Add more conditions for different column naming conventions\n",
    "                else:\n",
    "                    # If column names don't match known formats, try first two columns\n",
    "                    # (often wavelength is first, flux is second)\n",
    "                    print(f\"  Unknown column format, using first two columns\")\n",
    "                    wavelength = table_data[table_data.names[0]][0]\n",
    "                    flux = table_data[table_data.names[1]][0]\n",
    "            # Fallback to original method with primary HDU\n",
    "            elif hdul[0].data is not None and len(hdul[0].data.shape) >= 1:\n",
    "                print(\"Using data from PrimaryHDU\")\n",
    "                data = hdul[0].data\n",
    "                if data.shape[0] < 3:\n",
    "                    print(f\"⚠️ Skipping {obsid}: Primary HDU data has insufficient dimensions: {data.shape}\")\n",
    "                    return\n",
    "                flux = data[0]\n",
    "                wavelength = data[2]\n",
    "            else:\n",
    "                print(f\"⚠️ Skipping {obsid}: No usable data found in FITS file.\")\n",
    "                return\n",
    "                \n",
    "            # Check that we have valid data before proceeding\n",
    "            if flux is None or wavelength is None or len(flux) == 0 or len(wavelength) == 0:\n",
    "                print(f\"⚠️ Skipping {obsid}: Empty flux or wavelength arrays\")\n",
    "                return\n",
    "                \n",
    "            print(f\"  Data loaded successfully. Wavelength range: {min(wavelength):.2f}-{max(wavelength):.2f} Å\")\n",
    "            print(f\"  Flux range: {min(flux):.2e}-{max(flux):.2e}\")\n",
    "            \n",
    "            # Create a figure with three subplots using GridSpec.\n",
    "            # Top row: two subplots (spectrum and Gaia issues), Bottom row: CMD spanning full width.\n",
    "            fig = plt.figure(figsize=(24, 12))\n",
    "            gs = fig.add_gridspec(1, 3, height_ratios=[1])\n",
    "            ax1 = fig.add_subplot(gs[0, 1])\n",
    "            ax2 = fig.add_subplot(gs[0, 2])\n",
    "            ax3 = fig.add_subplot(gs[0, 0])\n",
    "            plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "            # --- Subplot 1: LAMOST Spectrum ---\n",
    "            ax1.plot(wavelength, flux, color='blue', alpha=0.7, lw=1, zorder=10)\n",
    "            ax1.set_xlabel(\"Wavelength (Å)\")\n",
    "            ax1.set_ylabel(\"Flux\")\n",
    "            ax1.set_title(f\"LAMOST Spectrum\")\n",
    "            ax1.grid(zorder=0)\n",
    "\n",
    "            # --- Subplot 2: Gaia Parameters with Issues ---\n",
    "            ax2.grid(zorder=0)\n",
    "            gaia_info = match.iloc[[0]].drop(columns=[\"source_id\", \"obsid\"], errors='ignore')\n",
    "            issues_dict = {}\n",
    "            issues_text_list = []\n",
    "            for col in gaia_info.columns:\n",
    "                value = gaia_info[col].values[0]\n",
    "                if col.endswith(\"_error\") and value > 1:\n",
    "                    issues_dict[col] = value\n",
    "                    issues_text_list.append(f\"Large error in {col}\")\n",
    "                elif col.endswith(\"_flux\") and value < -1:\n",
    "                    issues_dict[col] = value\n",
    "                    issues_text_list.append(f\"Dim object in {col}\")\n",
    "\n",
    "            if issues_dict:\n",
    "                # Order the labels: errors first, then fluxes.\n",
    "                error_labels = [l for l in issues_dict.keys() if l.endswith(\"_error\")]\n",
    "                flux_labels = [l for l in issues_dict.keys() if l.endswith(\"_flux\")]\n",
    "                ordered_labels = error_labels + flux_labels\n",
    "                ordered_values = [issues_dict[l] for l in ordered_labels]\n",
    "\n",
    "                ax2.bar(ordered_labels, ordered_values, color='skyblue', zorder=3)\n",
    "                ax2.tick_params(\"x\", labelrotation=45)\n",
    "                ax2.set_title(\"Gaia Parameters with Issues\")\n",
    "                ax2.set_ylabel(\"Standard Deviations from Mean\")\n",
    "\n",
    "                # Add a vertical dashed line between error and flux groups.\n",
    "                add_vertical_line_between_groups(ax2, ordered_labels)\n",
    "            else:\n",
    "                ax2.text(0.5, 0.5, \"No significant data issues\", ha='center', va='center', fontsize=12)\n",
    "                ax2.axis(\"off\")\n",
    "\n",
    "            # --- Subplot 3: Color–Magnitude Diagram (CMD) ---\n",
    "            # Compute additional columns for CMD.\n",
    "            df_sample['color'] = df_sample['phot_bp_mean_mag'] - df_sample['phot_rp_mean_mag']\n",
    "            df_sample['distance_pc'] = 1000 / df_sample['parallax']\n",
    "            df_sample['abs_mag'] = df_sample['phot_g_mean_mag'] - 5 * np.log10(df_sample['distance_pc'] / 10)\n",
    "            df_sample['is_correct'] = df_sample['source_id'].isin(correct_df['source_id'])\n",
    "            df_sample['is_incorrect'] = df_sample['source_id'].isin(incorrect_df['source_id'])\n",
    "\n",
    "            # Plot background stars (those not flagged as correct or incorrect)\n",
    "            mask_background = ~(df_sample['is_correct'] | df_sample['is_incorrect'])\n",
    "            ax3.scatter(df_sample.loc[mask_background, 'color'], \n",
    "                        df_sample.loc[mask_background, 'abs_mag'],\n",
    "                        s=3, color='gray', alpha=0.6, label='Nearby Stars')\n",
    "\n",
    "            # Plot the incorrect in red.\n",
    "            ax3.scatter(df_sample[df_sample['is_incorrect']]['color'],\n",
    "                        df_sample[df_sample['is_incorrect']]['abs_mag'],\n",
    "                        s=100, color='red', label='Incorrectly Classified', alpha=1, \n",
    "                        edgecolor='black', marker='H')\n",
    "            \n",
    "            # Plot the correct in green.\n",
    "            #ax3.scatter(df_sample[df_sample['is_correct']]['color'],\n",
    "            #            df_sample[df_sample['is_correct']]['abs_mag'],\n",
    "            #            s=100, color='green', label='Correctly Classified', alpha=1, \n",
    "            #            edgecolor='black', marker='x')\n",
    "            \n",
    "            # Plot the target source in blue. FLUX IS NOT THE SAME AS MAGNITUDE, data for both exist in the Gaia table.\n",
    "            target_color = df_sample.loc[df_sample['source_id'] == source_id, 'color'].values[0]\n",
    "            target_abs_mag = df_sample.loc[df_sample['source_id'] == source_id, 'abs_mag'].values[0]\n",
    "            ax3.scatter(target_color, target_abs_mag, s=200, color='blue', label='Target Source', alpha=1, edgecolor='black', marker='o')\n",
    "            #target_abs_mag = match['phot_g_mean_flux'].values[0] - 5 * np.log10((1/match['parallax'].values[0] )/ 10)\n",
    "            #ax3.scatter(target_color, target_abs_mag, s=200, color='blue', label='Target Source', alpha=1, edgecolor='black', marker='o')\n",
    "\n",
    "\n",
    "            # In a CMD, brighter (lower) magnitudes are at the top.\n",
    "            ax3.invert_yaxis()\n",
    "            ax3.set_xlim(-0.5, 3.5)\n",
    "            ax3.set_ylim(14, 0.5)\n",
    "            ax3.set_xlabel('Colour (BP - RP)')\n",
    "            ax3.set_ylabel('Absolute G Magnitude')\n",
    "            ax3.set_title('Colour–Magnitude Diagram (CMD)')\n",
    "            ax3.legend(loc='lower right')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            if save_path:\n",
    "                save_path= save_path.replace(\".png\", f\"_{n}.png\")\n",
    "                plt.savefig(save_path)\n",
    "            plt.show()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing FITS data for source_id {source_id}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            \n",
    "        finally:\n",
    "            # Ensure proper cleanup of FITS file\n",
    "            if hdul is not None:\n",
    "                try:\n",
    "                    hdul.close()\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not close FITS file: {e}\")\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in overall processing for source_id {source_id}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "gaia_lamost_merged = np.load(\"gaia_lamost_merged.npy\", allow_pickle=True)                                                                                                                                                                                                                                                           \n",
    "\n",
    "\n",
    "#gaia_lamost_merged = pd.DataFrame(gaia_lamost_merged, columns=[\"source_id\", \"obsid\", \"ra\", \"dec\", \"parallax\", \"phot_bp_mean_mag\", \"phot_rp_mean_mag\", \"phot_g_mean_mag\", \"parallax_error\", \"phot_bp_mean_flux\", \"phot_rp_mean_flux\", \"phot_g_mean_flux\", \"parallax_error_flux\", \"phot_bp_mean_mag_error\", \"phot_rp_mean_mag_error\", \"phot_g_mean_mag_error\"])\n",
    "\n",
    "\n",
    "# Example type conversions (ensure these columns are in the correct type)\n",
    "gaia_lamost_merged['obsid'] = gaia_lamost_merged['obsid'].astype(int)\n",
    "gaia_lamost_merged['source_id'] = gaia_lamost_merged['source_id'].astype(int)\n",
    "\n",
    "# Initialize a counter for the save path\n",
    "n_=1\n",
    "\n",
    "# Loop through incorrectly classified sources and plot all spectra with labels if Gaia data is problematic.\n",
    "for source_id in fn_prime_gaia_ids.astype(int):\n",
    "    plot_spectrum_with_gaia_and_cmd(source_id, gaia_lamost_merged, save_path=f\"Images_and_Plots/CMD_Spectra_Gaia_CV_take2.png\", df_sample=df_sample, correct_df=correct_df, incorrect_df=incorrect_df, n=n_)\n",
    "    n_+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e36fc2c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in gaia_lamost_merged:\n"
     ]
    }
   ],
   "source": [
    "print(\"Columns in gaia_lamost_merged:\")\n",
    "col = np.array(gaia_lamost_merged.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ad1008",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
