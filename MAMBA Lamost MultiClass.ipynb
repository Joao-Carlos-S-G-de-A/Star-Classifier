{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import wandb\n",
    "import torch\n",
    "import mambapy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import nn, optim\n",
    "import sklearn.metrics as metrics\n",
    "from mambapy.mamba import Mamba, MambaConfig\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, hamming_loss, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BalancedMultiLabelDataset(Dataset):\n",
    "    def __init__(self, X, y, limit_per_label=201):\n",
    "        \"\"\"\n",
    "        Multilabel version of the BalancedDataset.\n",
    "        \n",
    "        Args:\n",
    "        - X (array-like): Input features.\n",
    "        - y (array-like): Multi-hot encoded labels (2D array, each row is a multi-hot vector).\n",
    "        - limit_per_label (int): Target number of samples per label.\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.limit_per_label = limit_per_label\n",
    "        self.num_classes = y.shape[1]  # Number of possible classes\n",
    "        self.indices = self.balance_classes()\n",
    "\n",
    "    def balance_classes(self):\n",
    "        indices = []\n",
    "        class_counts = torch.sum(self.y, axis=0)  # Total occurrences of each class\n",
    "        for cls in range(self.num_classes):\n",
    "            cls_indices = np.where(self.y[:, cls] == 1)[0]  # Indices where this label is active\n",
    "            if len(cls_indices) < self.limit_per_label:  # Upsample minority classes\n",
    "                if len(cls_indices) == 0:\n",
    "                    print(f\"No samples found for class {cls}. Skipping.\")\n",
    "                    continue  # Skip this class if there are no samples for it\n",
    "                extra_indices = np.random.choice(cls_indices, self.limit_per_label - len(cls_indices), replace=True)\n",
    "                cls_indices = np.concatenate([cls_indices, extra_indices])\n",
    "            elif len(cls_indices) > self.limit_per_label:  # Downsample majority classes\n",
    "                cls_indices = np.random.choice(cls_indices, self.limit_per_label, replace=False)\n",
    "            indices.extend(cls_indices)\n",
    "        indices = np.unique(indices)  # Remove duplicate indices\n",
    "        np.random.shuffle(indices)\n",
    "        return indices\n",
    "\n",
    "    def re_sample(self):\n",
    "        \"\"\"Rebalance the dataset if needed, for example, after changes to the dataset.\"\"\"\n",
    "        self.indices = self.balance_classes()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        index = self.indices[idx]\n",
    "        return self.X[index], self.y[index]\n",
    "    \n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    metrics = {\n",
    "        \"micro_f1\": f1_score(y_true, y_pred, average='micro'),\n",
    "        \"macro_f1\": f1_score(y_true, y_pred, average='macro'),\n",
    "        \"weighted_f1\": f1_score(y_true, y_pred, average='weighted'),\n",
    "        \"micro_precision\": precision_score(y_true, y_pred, average='micro'),\n",
    "        \"macro_precision\": precision_score(y_true, y_pred, average='macro'),\n",
    "        \"weighted_precision\": precision_score(y_true, y_pred, average='weighted'),\n",
    "        \"micro_recall\": recall_score(y_true, y_pred, average='micro'),\n",
    "        \"macro_recall\": recall_score(y_true, y_pred, average='macro'),\n",
    "        \"weighted_recall\": recall_score(y_true, y_pred, average='weighted'),\n",
    "        \"hamming_loss\": hamming_loss(y_true, y_pred)\n",
    "    }\n",
    "    \n",
    "    # Check if there are at least two classes present in y_true\n",
    "    #if len(np.unique(y_true)) > 1:\n",
    "        #metrics[\"roc_auc\"] = roc_auc_score(y_true, y_pred, average='macro', multi_class='ovr')\n",
    "    #else:\n",
    "       # metrics[\"roc_auc\"] = None  # or you can set it to a default value or message\n",
    "    \n",
    "    return metrics\n",
    "    \n",
    "def calculate_class_weights(y):\n",
    "    if y.ndim > 1:  # Check if y is 2D (multi-hot encoded)\n",
    "        class_counts = np.sum(y, axis=0)  # Count how many times each class appears\n",
    "    else:\n",
    "        class_counts = np.bincount(y)  # For a 1D array, use bincount \n",
    "    print(\"Class counts:\", class_counts)       \n",
    "    total_samples = y.shape[0] if y.ndim > 1 else len(y)\n",
    "    class_weights = np.where(class_counts > 0, total_samples / (len(class_counts) * class_counts), 0)\n",
    "    return class_weights\n",
    "\n",
    "    \n",
    "def train_model_mamba(\n",
    "    model, train_loader, val_loader, test_loader, \n",
    "    num_epochs=500, lr=1e-4, max_patience=20, device='cuda'\n",
    "):\n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Define optimizer, scheduler, and loss function\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=int(max_patience / 5), verbose=True\n",
    "    )\n",
    "    all_labels = []\n",
    "\n",
    "    for _, y_batch in train_loader:\n",
    "        all_labels.extend(y_batch.cpu().numpy())    \n",
    "        print(\"Shape of train_loader:\", train_loader.dataset)\n",
    "        print(\"Shape of val_loader:\", len(val_loader))\n",
    "        print(\"Shape of test_loader:\", len(test_loader))\n",
    "        print(\"Shape of all_labels:\", len(all_labels))\n",
    "        \n",
    "    class_weights = calculate_class_weights(np.array(all_labels))\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "    print(\"Class weights:\", class_weights)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights)\n",
    "    best_val_loss = float('inf')\n",
    "    patience = max_patience\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Resample training and validation data\n",
    "        train_loader.dataset.re_sample()\n",
    "        val_loader.dataset.balance_classes()\n",
    "\n",
    "        # Class weights\n",
    "        all_labels = []\n",
    "        for _, y_batch in train_loader:\n",
    "            all_labels.extend(y_batch.cpu().numpy())\n",
    "        class_weights = calculate_class_weights(np.array(all_labels))\n",
    "        class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights)\n",
    "\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss, train_accuracy = 0.0, 0.0\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * X_batch.size(0)\n",
    "            # Convert outputs to binary predictions\n",
    "            predicted = (outputs > 0.5).float()\n",
    "\n",
    "            # Calculate accuracy for each batch\n",
    "            correct = (predicted == y_batch).float()\n",
    "            train_accuracy += correct.mean(dim=1).mean().item()  # Mean across classes and samples\n",
    "\n",
    "            \n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss, val_accuracy = 0.0, 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_val, y_val in val_loader:\n",
    "                X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "                outputs = model(X_val)\n",
    "                loss = criterion(outputs, y_val)\n",
    "\n",
    "                val_loss += loss.item() * X_val.size(0)\n",
    "                predicted = (outputs > 0.5).float()\n",
    "                correct = (predicted == y_val).float()\n",
    "                val_accuracy += correct.mean(dim=1).mean().item()\n",
    "\n",
    "        # Test phase\n",
    "        test_loss, test_accuracy = 0.0, 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_test, y_test in test_loader:\n",
    "                X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "                outputs = model(X_test)\n",
    "                loss = criterion(outputs, y_test)\n",
    "\n",
    "                test_loss += loss.item() * X_test.size(0)\n",
    "                predicted = (outputs > 0.5).float()\n",
    "                correct = (predicted == y_test).float()\n",
    "                test_accuracy += correct.mean(dim=1).mean().item()\n",
    "\n",
    "        # Test phase and metric collection\n",
    "        # Inside your test phase\n",
    "        y_true, y_pred = [], []\n",
    "        with torch.no_grad():\n",
    "            for X_test, y_test in test_loader:\n",
    "                X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "                outputs = model(X_test)\n",
    "                predicted = (outputs > 0.5).float()\n",
    "                y_true.extend(y_test.cpu().numpy())\n",
    "                y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "        metrics = calculate_metrics(np.array(y_true), np.array(y_pred))\n",
    "        wandb.log(metrics)\n",
    "\n",
    "        # Update scheduler\n",
    "        scheduler.step(val_loss / len(val_loader.dataset))\n",
    "\n",
    "        # Log metrics to WandB\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": train_loss / len(train_loader.dataset),\n",
    "            \"val_loss\": val_loss / len(val_loader.dataset),\n",
    "            \"train_accuracy\": train_accuracy / len(train_loader),\n",
    "            \"val_accuracy\": val_accuracy / len(val_loader),\n",
    "            \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "            \"test_loss\": test_loss / len(test_loader.dataset),\n",
    "            \"test_accuracy\": test_accuracy / len(test_loader),\n",
    "            #\"confusion_matrix\": wandb.plot.confusion_matrix(\n",
    "            #   probs=None, y_true=y_true, preds=y_pred, class_names=np.unique(y_true)\n",
    "            #), remove for now as it is not multilabel\n",
    "            #\"classification_report\": classification_report(\n",
    "            #    y_true, y_pred, target_names=[str(i) for i in range(len(np.unique(y_true)))]\n",
    "            #)\n",
    "        })\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience = max_patience\n",
    "            best_model = model.state_dict()\n",
    "        else:\n",
    "            patience -= 1\n",
    "            if patience <= 0:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    # Load the best model weights\n",
    "    model.load_state_dict(best_model)\n",
    "    return model\n",
    "\n",
    "class StarClassifierMAMBA(nn.Module):\n",
    "    def __init__(self, d_model, num_classes, d_state=64, d_conv=4, input_dim=17, n_layers=6):\n",
    "        super(StarClassifierMAMBA, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # MAMBA layer initialization\n",
    "        config = MambaConfig(\n",
    "            d_model=d_model,\n",
    "            d_state=d_state,\n",
    "            d_conv=d_conv,\n",
    "            n_layers=n_layers\n",
    "\n",
    "        )\n",
    "        self.mamba_layer = Mamba(config)\n",
    "\n",
    "        # Input projection to match the MAMBA layer dimension\n",
    "        self.input_projection = nn.Linear(input_dim, d_model)\n",
    "\n",
    "        # Fully connected classifier head with sigmoid activation for multi-label classification\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, num_classes)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_projection(x)  # Ensure the input has the correct dimension\n",
    "        #x = x.unsqueeze(1)  # Adds a sequence dimension (L=1).\n",
    "        x = self.mamba_layer(x)\n",
    "        x = x.mean(dim=1)  # Pooling operation for classification\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open the data and prepare datasets for Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No samples found for class 51. Skipping.\n",
      "No samples found for class 11. Skipping.\n",
      "No samples found for class 20. Skipping.\n",
      "No samples found for class 21. Skipping.\n",
      "No samples found for class 24. Skipping.\n",
      "No samples found for class 27. Skipping.\n",
      "No samples found for class 34. Skipping.\n",
      "No samples found for class 38. Skipping.\n",
      "No samples found for class 53. Skipping.\n",
      "Train dataset shape: torch.Size([87134, 1, 3647])\n",
      "Validation dataset shape: torch.Size([21784, 1, 3647])\n",
      "Test dataset shape: torch.Size([27237, 1, 3647])\n",
      "Train labels shape: torch.Size([87134, 55])\n",
      "Validation labels shape: torch.Size([21784, 55])\n",
      "Test labels shape: torch.Size([27237, 55])\n"
     ]
    }
   ],
   "source": [
    "# If X exists, delete it\n",
    "if 'X' in locals():   \n",
    "    del X, y\n",
    "gc.collect()\n",
    "\n",
    "batch_size = 1024\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and preprocess your data (example from original script)\n",
    "    X = pd.read_pickle(\"Pickles/train_data_transformed.pkl\")\n",
    "    classes = pd.read_pickle(\"Pickles/Updated_list_of_Classes.pkl\")\n",
    "\n",
    "    # Get labels and set them as y, drop them from X\n",
    "    y = X[classes]\n",
    "\n",
    "    # Drop gaia data\n",
    "    X.drop([\"parallax\", \"ra\", \"dec\", \"ra_error\", \"dec_error\", \"parallax_error\", \"pmra\", \"pmdec\", \"pmra_error\", \"pmdec_error\", \n",
    "            \"phot_g_mean_flux\", \"flagnopllx\", \"phot_g_mean_flux_error\", \"phot_bp_mean_flux\", \"phot_rp_mean_flux\", \n",
    "            \"phot_bp_mean_flux_error\", \"phot_rp_mean_flux_error\", \"obsid\", \"flagnoflux\", \"otype\"], axis=1, inplace=True)\n",
    "    \n",
    "    # Drop labels\n",
    "    X.drop(classes, axis=1, inplace=True)\n",
    "    \n",
    "    # Read test data\n",
    "    X_test = pd.read_pickle(\"Pickles/test_data_transformed.pkl\")\n",
    "\n",
    "    # Get labels and set them as y, drop them from X\n",
    "    y_test = X_test[classes]\n",
    "\n",
    "    # Drop gaia data\n",
    "    X_test.drop([\"parallax\", \"ra\", \"dec\", \"ra_error\", \"dec_error\", \"parallax_error\", \"pmra\", \"pmdec\", \"pmra_error\", \"pmdec_error\", \n",
    "                \"phot_g_mean_flux\", \"flagnopllx\", \"phot_g_mean_flux_error\", \"phot_bp_mean_flux\", \"phot_rp_mean_flux\", \n",
    "                \"phot_bp_mean_flux_error\", \"phot_rp_mean_flux_error\", \"obsid\", \"flagnoflux\", \"otype\"], axis=1, inplace=True)\n",
    "    \n",
    "    # Drop labels\n",
    "    X_test.drop(classes, axis=1, inplace=True)\n",
    "    \n",
    "    # Split validation data\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Clear memory\n",
    "    del X, y\n",
    "    gc.collect()\n",
    "\n",
    "    # Convert to torch tensors and create datasets\n",
    "    X_train = torch.tensor(X_train.values, dtype=torch.float32).unsqueeze(1)  # Convert DataFrame to numpy array\n",
    "    X_val = torch.tensor(X_val.values, dtype=torch.float32).unsqueeze(1)      # Convert DataFrame to numpy array    \n",
    "    X_test = torch.tensor(X_test.values, dtype=torch.float32).unsqueeze(1)    # Convert DataFrame to numpy array\n",
    "    y_train = torch.tensor(y_train.values, dtype=torch.float32)  # Convert DataFrame to numpy array and float32\n",
    "    y_val = torch.tensor(y_val.values, dtype=torch.float32)      # Convert DataFrame to numpy array and float32\n",
    "    y_test = torch.tensor(y_test.values, dtype=torch.float32)    # Convert DataFrame to numpy array and float32\n",
    "\n",
    "    train_dataset = BalancedMultiLabelDataset(X_train, y_train)\n",
    "    val_dataset = BalancedMultiLabelDataset(X_val, y_val)\n",
    "    test_dataset = BalancedMultiLabelDataset(X_test, y_test)\n",
    "\n",
    "\n",
    "    #train_dataset = BalancedDataset(X_train, y_train)\n",
    "    #val_dataset = BalancedValidationDataset(X_val, y_val)\n",
    "    #test_dataset = BalancedValidationDataset(X_test, y_test, limit_per_label=10000)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Print the shapes of the datasets\n",
    "    print(f\"Train dataset shape: {X_train.shape}\")\n",
    "    print(f\"Validation dataset shape: {X_val.shape}\")\n",
    "    print(f\"Test dataset shape: {X_test.shape}\")\n",
    "    print(f\"Train labels shape: {y_train.shape}\")\n",
    "    print(f\"Validation labels shape: {y_val.shape}\")\n",
    "    print(f\"Test labels shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:lkk4f6jk) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██</td></tr><tr><td>hamming_loss</td><td>█▄▃▂▂▂▂▂▂▂▂▂▂▃▂▂▁▂▂▂▁</td></tr><tr><td>learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>macro_f1</td><td>▂▁▁▁▁▁▁▁▁▁▂▂▃▃▄▄█▅▅▅▇</td></tr><tr><td>macro_precision</td><td>▄▂▁▂▂▂▁▁▁▂▃▃▃▃▅▃▇▇█▆█</td></tr><tr><td>macro_recall</td><td>▂▁▁▁▁▁▁▁▁▁▁▂▃▄▃▄█▅▅▅▇</td></tr><tr><td>micro_f1</td><td>▂▁▁▁▁▁▁▁▁▁▁▁▂▂▃▃█▄▄▄█</td></tr><tr><td>micro_precision</td><td>▁▁▁▂▁▂▁▁▁▅▅█▇▄▇█▇█▇██</td></tr><tr><td>micro_recall</td><td>▂▁▁▁▁▁▁▁▁▁▁▁▂▂▃▃█▄▄▄█</td></tr><tr><td>test_accuracy</td><td>▁▅▆▇▇▇▇▇▇▇▇▇▇▆▇▇▇▇▇▇█</td></tr><tr><td>test_loss</td><td>█▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_accuracy</td><td>▁████████████████████</td></tr><tr><td>train_loss</td><td>█▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▅▇▇▇▇▇▇▇▇▇▇▇▇█▇█████</td></tr><tr><td>val_loss</td><td>█▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>weighted_f1</td><td>▃▁▁▁▁▁▁▁▁▁▁▂▂▂▄▃▇▄▄▄█</td></tr><tr><td>weighted_precision</td><td>▆▂▁▆▆▆▁▁▁▁▁▁▁▁▂▁▇▇█▇█</td></tr><tr><td>weighted_recall</td><td>▂▁▁▁▁▁▁▁▁▁▁▁▂▂▃▃█▄▄▄█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>20</td></tr><tr><td>hamming_loss</td><td>0.03378</td></tr><tr><td>learning_rate</td><td>0.0002</td></tr><tr><td>macro_f1</td><td>0.04944</td></tr><tr><td>macro_precision</td><td>0.11201</td></tr><tr><td>macro_recall</td><td>0.03598</td></tr><tr><td>micro_f1</td><td>0.03206</td></tr><tr><td>micro_precision</td><td>0.81319</td></tr><tr><td>micro_recall</td><td>0.01635</td></tr><tr><td>test_accuracy</td><td>0.96621</td></tr><tr><td>test_loss</td><td>0.06399</td></tr><tr><td>train_accuracy</td><td>0.96581</td></tr><tr><td>train_loss</td><td>0.058</td></tr><tr><td>val_accuracy</td><td>0.96619</td></tr><tr><td>val_loss</td><td>0.05019</td></tr><tr><td>weighted_f1</td><td>0.02441</td></tr><tr><td>weighted_precision</td><td>0.22145</td></tr><tr><td>weighted_recall</td><td>0.01635</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">hardy-microwave-61</strong> at: <a href='https://wandb.ai/joaoc-university-of-southampton/ALLSTARS%2A%2A%2Alamost-mamba-test/runs/lkk4f6jk' target=\"_blank\">https://wandb.ai/joaoc-university-of-southampton/ALLSTARS%2A%2A%2Alamost-mamba-test/runs/lkk4f6jk</a><br/> View project at: <a href='https://wandb.ai/joaoc-university-of-southampton/ALLSTARS%2A%2A%2Alamost-mamba-test' target=\"_blank\">https://wandb.ai/joaoc-university-of-southampton/ALLSTARS%2A%2A%2Alamost-mamba-test</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241213_165326-lkk4f6jk\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:lkk4f6jk). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\jcwin\\OneDrive - University of Southampton\\_Southampton\\2024-25\\Star-Classifier\\wandb\\run-20241213_165546-12weojm9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/joaoc-university-of-southampton/ALLSTARS%2A%2A%2Alamost-mamba-test/runs/12weojm9' target=\"_blank\">wise-firebrand-62</a></strong> to <a href='https://wandb.ai/joaoc-university-of-southampton/ALLSTARS%2A%2A%2Alamost-mamba-test' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/joaoc-university-of-southampton/ALLSTARS%2A%2A%2Alamost-mamba-test' target=\"_blank\">https://wandb.ai/joaoc-university-of-southampton/ALLSTARS%2A%2A%2Alamost-mamba-test</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/joaoc-university-of-southampton/ALLSTARS%2A%2A%2Alamost-mamba-test/runs/12weojm9' target=\"_blank\">https://wandb.ai/joaoc-university-of-southampton/ALLSTARS%2A%2A%2Alamost-mamba-test/runs/12weojm9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StarClassifierMAMBA(\n",
      "  (mamba_layer): Mamba(\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x ResidualBlock(\n",
      "        (mixer): MambaBlock(\n",
      "          (in_proj): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "          (conv1d): Conv1d(2048, 2048, kernel_size=(4,), stride=(1,), padding=(3,), groups=2048)\n",
      "          (x_proj): Linear(in_features=2048, out_features=192, bias=False)\n",
      "          (dt_proj): Linear(in_features=64, out_features=2048, bias=True)\n",
      "          (out_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "        )\n",
      "        (norm): RMSNorm()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (input_projection): Linear(in_features=3647, out_features=1024, bias=True)\n",
      "  (classifier): Sequential(\n",
      "    (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): Linear(in_features=1024, out_features=55, bias=True)\n",
      "  )\n",
      ")\n",
      "mamba_layer.layers.0.mixer.A_log 131072\n",
      "mamba_layer.layers.0.mixer.D 2048\n",
      "mamba_layer.layers.0.mixer.in_proj.weight 4194304\n",
      "mamba_layer.layers.0.mixer.conv1d.weight 8192\n",
      "mamba_layer.layers.0.mixer.conv1d.bias 2048\n",
      "mamba_layer.layers.0.mixer.x_proj.weight 393216\n",
      "mamba_layer.layers.0.mixer.dt_proj.weight 131072\n",
      "mamba_layer.layers.0.mixer.dt_proj.bias 2048\n",
      "mamba_layer.layers.0.mixer.out_proj.weight 2097152\n",
      "mamba_layer.layers.0.norm.weight 1024\n",
      "mamba_layer.layers.1.mixer.A_log 131072\n",
      "mamba_layer.layers.1.mixer.D 2048\n",
      "mamba_layer.layers.1.mixer.in_proj.weight 4194304\n",
      "mamba_layer.layers.1.mixer.conv1d.weight 8192\n",
      "mamba_layer.layers.1.mixer.conv1d.bias 2048\n",
      "mamba_layer.layers.1.mixer.x_proj.weight 393216\n",
      "mamba_layer.layers.1.mixer.dt_proj.weight 131072\n",
      "mamba_layer.layers.1.mixer.dt_proj.bias 2048\n",
      "mamba_layer.layers.1.mixer.out_proj.weight 2097152\n",
      "mamba_layer.layers.1.norm.weight 1024\n",
      "mamba_layer.layers.2.mixer.A_log 131072\n",
      "mamba_layer.layers.2.mixer.D 2048\n",
      "mamba_layer.layers.2.mixer.in_proj.weight 4194304\n",
      "mamba_layer.layers.2.mixer.conv1d.weight 8192\n",
      "mamba_layer.layers.2.mixer.conv1d.bias 2048\n",
      "mamba_layer.layers.2.mixer.x_proj.weight 393216\n",
      "mamba_layer.layers.2.mixer.dt_proj.weight 131072\n",
      "mamba_layer.layers.2.mixer.dt_proj.bias 2048\n",
      "mamba_layer.layers.2.mixer.out_proj.weight 2097152\n",
      "mamba_layer.layers.2.norm.weight 1024\n",
      "mamba_layer.layers.3.mixer.A_log 131072\n",
      "mamba_layer.layers.3.mixer.D 2048\n",
      "mamba_layer.layers.3.mixer.in_proj.weight 4194304\n",
      "mamba_layer.layers.3.mixer.conv1d.weight 8192\n",
      "mamba_layer.layers.3.mixer.conv1d.bias 2048\n",
      "mamba_layer.layers.3.mixer.x_proj.weight 393216\n",
      "mamba_layer.layers.3.mixer.dt_proj.weight 131072\n",
      "mamba_layer.layers.3.mixer.dt_proj.bias 2048\n",
      "mamba_layer.layers.3.mixer.out_proj.weight 2097152\n",
      "mamba_layer.layers.3.norm.weight 1024\n",
      "mamba_layer.layers.4.mixer.A_log 131072\n",
      "mamba_layer.layers.4.mixer.D 2048\n",
      "mamba_layer.layers.4.mixer.in_proj.weight 4194304\n",
      "mamba_layer.layers.4.mixer.conv1d.weight 8192\n",
      "mamba_layer.layers.4.mixer.conv1d.bias 2048\n",
      "mamba_layer.layers.4.mixer.x_proj.weight 393216\n",
      "mamba_layer.layers.4.mixer.dt_proj.weight 131072\n",
      "mamba_layer.layers.4.mixer.dt_proj.bias 2048\n",
      "mamba_layer.layers.4.mixer.out_proj.weight 2097152\n",
      "mamba_layer.layers.4.norm.weight 1024\n",
      "mamba_layer.layers.5.mixer.A_log 131072\n",
      "mamba_layer.layers.5.mixer.D 2048\n",
      "mamba_layer.layers.5.mixer.in_proj.weight 4194304\n",
      "mamba_layer.layers.5.mixer.conv1d.weight 8192\n",
      "mamba_layer.layers.5.mixer.conv1d.bias 2048\n",
      "mamba_layer.layers.5.mixer.x_proj.weight 393216\n",
      "mamba_layer.layers.5.mixer.dt_proj.weight 131072\n",
      "mamba_layer.layers.5.mixer.dt_proj.bias 2048\n",
      "mamba_layer.layers.5.mixer.out_proj.weight 2097152\n",
      "mamba_layer.layers.5.norm.weight 1024\n",
      "input_projection.weight 3734528\n",
      "input_projection.bias 1024\n",
      "classifier.0.weight 1024\n",
      "classifier.0.bias 1024\n",
      "classifier.1.weight 56320\n",
      "classifier.1.bias 55\n",
      "Total number of parameters: 45567031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jcwin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_loader: <__main__.BalancedMultiLabelDataset object at 0x0000018CD8DC7F90>\n",
      "Shape of val_loader: 5\n",
      "Shape of test_loader: 5\n",
      "Shape of all_labels: 1024\n",
      "Shape of train_loader: <__main__.BalancedMultiLabelDataset object at 0x0000018CD8DC7F90>\n",
      "Shape of val_loader: 5\n",
      "Shape of test_loader: 5\n",
      "Shape of all_labels: 2048\n",
      "Shape of train_loader: <__main__.BalancedMultiLabelDataset object at 0x0000018CD8DC7F90>\n",
      "Shape of val_loader: 5\n",
      "Shape of test_loader: 5\n",
      "Shape of all_labels: 3072\n",
      "Shape of train_loader: <__main__.BalancedMultiLabelDataset object at 0x0000018CD8DC7F90>\n",
      "Shape of val_loader: 5\n",
      "Shape of test_loader: 5\n",
      "Shape of all_labels: 4096\n",
      "Shape of train_loader: <__main__.BalancedMultiLabelDataset object at 0x0000018CD8DC7F90>\n",
      "Shape of val_loader: 5\n",
      "Shape of test_loader: 5\n",
      "Shape of all_labels: 5120\n",
      "Shape of train_loader: <__main__.BalancedMultiLabelDataset object at 0x0000018CD8DC7F90>\n",
      "Shape of val_loader: 5\n",
      "Shape of test_loader: 5\n",
      "Shape of all_labels: 6012\n",
      "Class counts: [2.040e+02 1.122e+03 2.200e+01 3.300e+02 1.900e+01 7.700e+01 4.580e+02\n",
      " 3.250e+02 1.170e+02 2.220e+02 2.140e+02 2.000e+00 2.500e+01 2.050e+02\n",
      " 2.009e+03 2.580e+02 3.400e+01 1.100e+01 9.200e+01 2.220e+02 3.000e+00\n",
      " 2.000e+00 2.190e+02 6.000e+00 2.000e+00 7.940e+02 9.700e+01 1.000e+00\n",
      " 3.200e+01 2.880e+02 2.010e+02 2.200e+02 2.010e+02 1.120e+02 6.000e+00\n",
      " 2.020e+02 2.030e+02 3.550e+02 7.000e+00 3.600e+01 2.110e+02 9.200e+01\n",
      " 1.000e+00 2.860e+02 2.350e+02 7.000e+00 2.810e+02 2.010e+02 2.170e+02\n",
      " 3.000e+01 7.940e+02 0.000e+00 1.000e+01 4.000e+00 2.970e+02]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jcwin\\AppData\\Local\\Temp\\ipykernel_16424\\1141872216.py:86: RuntimeWarning: divide by zero encountered in divide\n",
      "  class_weights = np.where(class_counts > 0, total_samples / (len(class_counts) * class_counts), 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: tensor([5.3583e-01, 9.7423e-02, 4.9686e+00, 3.3124e-01, 5.7531e+00, 1.4196e+00,\n",
      "        2.3867e-01, 3.3634e-01, 9.3427e-01, 4.9238e-01, 5.1079e-01, 5.4655e+01,\n",
      "        4.3724e+00, 5.3322e-01, 5.4410e-02, 4.2368e-01, 3.2150e+00, 9.9372e+00,\n",
      "        1.1881e+00, 4.9238e-01, 3.6436e+01, 5.4655e+01, 4.9913e-01, 1.8218e+01,\n",
      "        5.4655e+01, 1.3767e-01, 1.1269e+00, 1.0931e+02, 3.4159e+00, 3.7955e-01,\n",
      "        5.4383e-01, 4.9686e-01, 5.4383e-01, 9.7597e-01, 1.8218e+01, 5.4113e-01,\n",
      "        5.3847e-01, 3.0791e-01, 1.5616e+01, 3.0364e+00, 5.1805e-01, 1.1881e+00,\n",
      "        1.0931e+02, 3.8220e-01, 4.6515e-01, 1.5616e+01, 3.8900e-01, 5.4383e-01,\n",
      "        5.0373e-01, 3.6436e+00, 1.3767e-01, 0.0000e+00, 1.0931e+01, 2.7327e+01,\n",
      "        3.6804e-01], device='cuda:0')\n",
      "No samples found for class 51. Skipping.\n",
      "No samples found for class 11. Skipping.\n",
      "No samples found for class 20. Skipping.\n",
      "No samples found for class 21. Skipping.\n",
      "No samples found for class 24. Skipping.\n",
      "No samples found for class 27. Skipping.\n",
      "No samples found for class 34. Skipping.\n",
      "No samples found for class 38. Skipping.\n",
      "No samples found for class 53. Skipping.\n",
      "Class counts: [2.070e+02 1.121e+03 2.200e+01 3.310e+02 1.900e+01 7.700e+01 4.590e+02\n",
      " 3.160e+02 1.170e+02 2.220e+02 2.090e+02 2.000e+00 2.500e+01 2.040e+02\n",
      " 2.014e+03 2.550e+02 3.400e+01 1.100e+01 9.200e+01 2.190e+02 3.000e+00\n",
      " 2.000e+00 2.230e+02 6.000e+00 2.000e+00 7.920e+02 9.700e+01 1.000e+00\n",
      " 3.200e+01 2.820e+02 2.010e+02 2.200e+02 2.010e+02 1.120e+02 6.000e+00\n",
      " 2.020e+02 2.020e+02 3.490e+02 7.000e+00 3.600e+01 2.090e+02 9.200e+01\n",
      " 1.000e+00 2.970e+02 2.310e+02 7.000e+00 2.920e+02 2.010e+02 2.140e+02\n",
      " 3.000e+01 7.780e+02 0.000e+00 1.000e+01 4.000e+00 3.010e+02]\n",
      "No samples found for class 51. Skipping.\n",
      "No samples found for class 11. Skipping.\n",
      "No samples found for class 20. Skipping.\n",
      "No samples found for class 21. Skipping.\n",
      "No samples found for class 24. Skipping.\n",
      "No samples found for class 27. Skipping.\n",
      "No samples found for class 34. Skipping.\n",
      "No samples found for class 38. Skipping.\n",
      "No samples found for class 53. Skipping.\n",
      "Class counts: [2.050e+02 1.119e+03 2.200e+01 3.260e+02 1.900e+01 7.700e+01 4.660e+02\n",
      " 3.110e+02 1.170e+02 2.160e+02 2.070e+02 2.000e+00 2.500e+01 2.020e+02\n",
      " 2.010e+03 2.560e+02 3.400e+01 1.100e+01 9.200e+01 2.200e+02 3.000e+00\n",
      " 2.000e+00 2.190e+02 6.000e+00 2.000e+00 7.880e+02 9.700e+01 1.000e+00\n",
      " 3.200e+01 2.730e+02 2.010e+02 2.210e+02 2.010e+02 1.120e+02 6.000e+00\n",
      " 2.020e+02 2.060e+02 3.480e+02 7.000e+00 3.600e+01 2.110e+02 9.200e+01\n",
      " 1.000e+00 2.840e+02 2.360e+02 7.000e+00 2.830e+02 2.010e+02 2.190e+02\n",
      " 3.000e+01 7.730e+02 0.000e+00 1.000e+01 4.000e+00 3.060e+02]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jcwin\\AppData\\Local\\Temp\\ipykernel_16424\\1141872216.py:86: RuntimeWarning: divide by zero encountered in divide\n",
      "  class_weights = np.where(class_counts > 0, total_samples / (len(class_counts) * class_counts), 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No samples found for class 51. Skipping.\n",
      "No samples found for class 11. Skipping.\n",
      "No samples found for class 20. Skipping.\n",
      "No samples found for class 21. Skipping.\n",
      "No samples found for class 24. Skipping.\n",
      "No samples found for class 27. Skipping.\n",
      "No samples found for class 34. Skipping.\n",
      "No samples found for class 38. Skipping.\n",
      "No samples found for class 53. Skipping.\n",
      "Class counts: [2.100e+02 1.120e+03 2.200e+01 3.320e+02 1.900e+01 7.700e+01 4.670e+02\n",
      " 3.160e+02 1.170e+02 2.180e+02 2.090e+02 2.000e+00 2.500e+01 2.030e+02\n",
      " 2.014e+03 2.570e+02 3.400e+01 1.100e+01 9.200e+01 2.200e+02 3.000e+00\n",
      " 2.000e+00 2.130e+02 6.000e+00 2.000e+00 7.940e+02 9.700e+01 1.000e+00\n",
      " 3.200e+01 2.930e+02 2.010e+02 2.240e+02 2.010e+02 1.120e+02 6.000e+00\n",
      " 2.020e+02 2.020e+02 3.570e+02 7.000e+00 3.600e+01 2.110e+02 9.200e+01\n",
      " 1.000e+00 2.820e+02 2.330e+02 7.000e+00 2.880e+02 2.010e+02 2.130e+02\n",
      " 3.000e+01 7.810e+02 0.000e+00 1.000e+01 4.000e+00 2.850e+02]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jcwin\\AppData\\Local\\Temp\\ipykernel_16424\\1141872216.py:86: RuntimeWarning: divide by zero encountered in divide\n",
      "  class_weights = np.where(class_counts > 0, total_samples / (len(class_counts) * class_counts), 0)\n",
      "C:\\Users\\jcwin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\jcwin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▅█</td></tr><tr><td>hamming_loss</td><td>█▃▁</td></tr><tr><td>learning_rate</td><td>▁▁▁</td></tr><tr><td>macro_f1</td><td>▁▁█</td></tr><tr><td>macro_precision</td><td>█▆▁</td></tr><tr><td>macro_recall</td><td>█▆▁</td></tr><tr><td>micro_f1</td><td>█▃▁</td></tr><tr><td>micro_precision</td><td>▁▄█</td></tr><tr><td>micro_recall</td><td>█▃▁</td></tr><tr><td>test_accuracy</td><td>▁▆█</td></tr><tr><td>test_loss</td><td>█▂▁</td></tr><tr><td>train_accuracy</td><td>▁██</td></tr><tr><td>train_loss</td><td>█▂▁</td></tr><tr><td>val_accuracy</td><td>▁▇█</td></tr><tr><td>val_loss</td><td>█▂▁</td></tr><tr><td>weighted_f1</td><td>█▃▁</td></tr><tr><td>weighted_precision</td><td>█▅▁</td></tr><tr><td>weighted_recall</td><td>█▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>2</td></tr><tr><td>hamming_loss</td><td>0.03431</td></tr><tr><td>learning_rate</td><td>0.0002</td></tr><tr><td>macro_f1</td><td>0.01214</td></tr><tr><td>macro_precision</td><td>0.03636</td></tr><tr><td>macro_recall</td><td>0.0091</td></tr><tr><td>micro_f1</td><td>0.00044</td></tr><tr><td>micro_precision</td><td>0.06897</td></tr><tr><td>micro_recall</td><td>0.00022</td></tr><tr><td>test_accuracy</td><td>0.96569</td></tr><tr><td>test_loss</td><td>0.07345</td></tr><tr><td>train_accuracy</td><td>0.96439</td></tr><tr><td>train_loss</td><td>0.08876</td></tr><tr><td>val_accuracy</td><td>0.96569</td></tr><tr><td>val_loss</td><td>0.06408</td></tr><tr><td>weighted_f1</td><td>0.00037</td></tr><tr><td>weighted_precision</td><td>0.17401</td></tr><tr><td>weighted_recall</td><td>0.00022</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">wise-firebrand-62</strong> at: <a href='https://wandb.ai/joaoc-university-of-southampton/ALLSTARS%2A%2A%2Alamost-mamba-test/runs/12weojm9' target=\"_blank\">https://wandb.ai/joaoc-university-of-southampton/ALLSTARS%2A%2A%2Alamost-mamba-test/runs/12weojm9</a><br/> View project at: <a href='https://wandb.ai/joaoc-university-of-southampton/ALLSTARS%2A%2A%2Alamost-mamba-test' target=\"_blank\">https://wandb.ai/joaoc-university-of-southampton/ALLSTARS%2A%2A%2Alamost-mamba-test</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241213_165546-12weojm9\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Same, but now with class weights\n",
    "# Define the model with your parameters\n",
    "d_model = 1024 # Embedding dimension\n",
    "num_classes = 55  # Star classification categories\n",
    "input_dim = 3647 # Number of spectra points\n",
    "\n",
    "# Define the training parameters\n",
    "num_epochs = 3\n",
    "lr = 2e-4\n",
    "patience = 100   \n",
    "depth = 6\n",
    "\n",
    "# Define the config dictionary object\n",
    "config = {\"num_classes\": num_classes, \"batch_size\": batch_size, \"lr\": lr, \"patience\": patience, \"num_epochs\": num_epochs, \"d_model\": d_model, \"depth\": depth}\n",
    "\n",
    "# Initialize WandB project\n",
    "wandb.init(project=\"ALLSTARS***lamost-mamba-test\", entity=\"joaoc-university-of-southampton\", config=config)\n",
    "# Initialize and train the model\n",
    "# Train the model using your `train_model_vit` or an adjusted training loop\n",
    "model_mamba = StarClassifierMAMBA(d_model=d_model, num_classes=num_classes, input_dim=input_dim, n_layers=depth)\n",
    "print(model_mamba)\n",
    "# print number of parameters per layer\n",
    "for name, param in model_mamba.named_parameters():\n",
    "    print(name, param.numel())\n",
    "print(\"Total number of parameters:\", sum(p.numel() for p in model_mamba.parameters() if p.requires_grad))\n",
    "\n",
    "# Move the model to device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model_mamba = model_mamba.to(device)\n",
    "\n",
    "# Train the model using your `train_model_vit` or an adjusted training loop\n",
    "trained_model = train_model_mamba(\n",
    "    model=model_mamba,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    num_epochs=num_epochs,\n",
    "    lr=lr,\n",
    "    max_patience=patience,\n",
    "    device=device\n",
    ")\n",
    "# Save the model and finish WandB session\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(trained_model.state_dict(), \"mamba_star_classifier_devout_feather_60.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[4598   11]\n",
      "  [ 186   15]]\n",
      "\n",
      " [[3823   22]\n",
      "  [ 732  233]]\n",
      "\n",
      " [[4804    0]\n",
      "  [   6    0]]\n",
      "\n",
      " [[4633   15]\n",
      "  [ 114   48]]\n",
      "\n",
      " [[4804    0]\n",
      "  [   4    2]]\n",
      "\n",
      " [[4786    0]\n",
      "  [  14   10]]\n",
      "\n",
      " [[4347   31]\n",
      "  [ 229  203]]\n",
      "\n",
      " [[4511   26]\n",
      "  [ 183   90]]\n",
      "\n",
      " [[4767    7]\n",
      "  [  27    9]]\n",
      "\n",
      " [[4571   21]\n",
      "  [  83  135]]\n",
      "\n",
      " [[4716    6]\n",
      "  [  67   21]]\n",
      "\n",
      " [[4808    0]\n",
      "  [   2    0]]\n",
      "\n",
      " [[4802    0]\n",
      "  [   8    0]]\n",
      "\n",
      " [[4594   12]\n",
      "  [  52  152]]\n",
      "\n",
      " [[3218   24]\n",
      "  [ 735  833]]\n",
      "\n",
      " [[4717    6]\n",
      "  [  38   49]]\n",
      "\n",
      " [[4800    0]\n",
      "  [   5    5]]\n",
      "\n",
      " [[4808    0]\n",
      "  [   1    1]]\n",
      "\n",
      " [[4781    0]\n",
      "  [  18   11]]\n",
      "\n",
      " [[4680   10]\n",
      "  [  63   57]]\n",
      "\n",
      " [[4809    0]\n",
      "  [   1    0]]\n",
      "\n",
      " [[4809    0]\n",
      "  [   1    0]]\n",
      "\n",
      " [[4577   26]\n",
      "  [ 183   24]]\n",
      "\n",
      " [[4807    1]\n",
      "  [   2    0]]\n",
      "\n",
      " [[4808    0]\n",
      "  [   2    0]]\n",
      "\n",
      " [[4012   35]\n",
      "  [ 603  160]]\n",
      "\n",
      " [[4775    7]\n",
      "  [  26    2]]\n",
      "\n",
      " [[4809    0]\n",
      "  [   1    0]]\n",
      "\n",
      " [[4793    5]\n",
      "  [  10    2]]\n",
      "\n",
      " [[4537    0]\n",
      "  [ 273    0]]\n",
      "\n",
      " [[4605    4]\n",
      "  [ 192    9]]\n",
      "\n",
      " [[4582   15]\n",
      "  [ 194   19]]\n",
      "\n",
      " [[4604    5]\n",
      "  [ 188   13]]\n",
      "\n",
      " [[4772    2]\n",
      "  [  36    0]]\n",
      "\n",
      " [[4809    0]\n",
      "  [   1    0]]\n",
      "\n",
      " [[4725    3]\n",
      "  [  31   51]]\n",
      "\n",
      " [[4666    6]\n",
      "  [ 124   14]]\n",
      "\n",
      " [[4441   27]\n",
      "  [ 258   84]]\n",
      "\n",
      " [[4808    0]\n",
      "  [   2    0]]\n",
      "\n",
      " [[4798    0]\n",
      "  [   6    6]]\n",
      "\n",
      " [[4570   35]\n",
      "  [  98  107]]\n",
      "\n",
      " [[4781    1]\n",
      "  [   6   22]]\n",
      "\n",
      " [[4808    0]\n",
      "  [   2    0]]\n",
      "\n",
      " [[4525   27]\n",
      "  [  80  178]]\n",
      "\n",
      " [[4729    4]\n",
      "  [  29   48]]\n",
      "\n",
      " [[4808    0]\n",
      "  [   2    0]]\n",
      "\n",
      " [[4520   14]\n",
      "  [ 234   42]]\n",
      "\n",
      " [[4586   23]\n",
      "  [  90  111]]\n",
      "\n",
      " [[4569   24]\n",
      "  [  34  183]]\n",
      "\n",
      " [[4798    3]\n",
      "  [   7    2]]\n",
      "\n",
      " [[4291   24]\n",
      "  [ 349  146]]\n",
      "\n",
      " [[4809    0]\n",
      "  [   1    0]]\n",
      "\n",
      " [[4806    0]\n",
      "  [   4    0]]\n",
      "\n",
      " [[4808    0]\n",
      "  [   2    0]]\n",
      "\n",
      " [[4450   52]\n",
      "  [ 179  129]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jcwin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\jcwin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         RS*       0.58      0.07      0.13       201\n",
      "          **       0.91      0.24      0.38       965\n",
      "         El*       0.00      0.00      0.00         6\n",
      "         Y*O       0.76      0.30      0.43       162\n",
      "         s*b       1.00      0.33      0.50         6\n",
      "         cC*       1.00      0.42      0.59        24\n",
      "         HB*       0.87      0.47      0.61       432\n",
      "         dS*       0.78      0.33      0.46       273\n",
      "         Or*       0.56      0.25      0.35        36\n",
      "         LP*       0.87      0.62      0.72       218\n",
      "         BS*       0.78      0.24      0.37        88\n",
      "         Ae*       0.00      0.00      0.00         2\n",
      "         WV*       0.00      0.00      0.00         8\n",
      "         HS*       0.93      0.75      0.83       204\n",
      "         Ev*       0.97      0.53      0.69      1568\n",
      "         AB*       0.89      0.56      0.69        87\n",
      "         sg*       1.00      0.50      0.67        10\n",
      "         s*r       1.00      0.50      0.67         2\n",
      "         Ce*       1.00      0.38      0.55        29\n",
      "         gD*       0.85      0.47      0.61       120\n",
      "         OH*       0.00      0.00      0.00         1\n",
      "         HXB       0.00      0.00      0.00         1\n",
      "         Pu*       0.48      0.12      0.19       207\n",
      "         RV*       0.00      0.00      0.00         2\n",
      "         Sy*       0.00      0.00      0.00         2\n",
      "          V*       0.82      0.21      0.33       763\n",
      "         TT*       0.22      0.07      0.11        28\n",
      "         SN*       0.00      0.00      0.00         1\n",
      "         Be*       0.29      0.17      0.21        12\n",
      "         SB*       0.00      0.00      0.00       273\n",
      "         Em*       0.69      0.04      0.08       201\n",
      "         Er*       0.56      0.09      0.15       213\n",
      "         PM*       0.72      0.06      0.12       201\n",
      "         HV*       0.00      0.00      0.00        36\n",
      "         pA*       0.00      0.00      0.00         1\n",
      "          C*       0.94      0.62      0.75        82\n",
      "         BY*       0.70      0.10      0.18       138\n",
      "         Ro*       0.76      0.25      0.37       342\n",
      "         XB*       0.00      0.00      0.00         2\n",
      "         Ma*       1.00      0.50      0.67        12\n",
      "         Pe*       0.75      0.52      0.62       205\n",
      "         CV*       0.96      0.79      0.86        28\n",
      "         bC*       0.00      0.00      0.00         2\n",
      "         RR*       0.87      0.69      0.77       258\n",
      "         Mi*       0.92      0.62      0.74        77\n",
      "         SX*       0.00      0.00      0.00         2\n",
      "         RG*       0.75      0.15      0.25       276\n",
      "         LM*       0.83      0.55      0.66       201\n",
      "         WD*       0.88      0.84      0.86       217\n",
      "          S*       0.40      0.22      0.29         9\n",
      "         MS*       0.86      0.29      0.44       495\n",
      "         Ir*       0.00      0.00      0.00         1\n",
      "         a2*       0.00      0.00      0.00         4\n",
      "          PN       0.00      0.00      0.00         2\n",
      "         EB*       0.71      0.42      0.53       308\n",
      "\n",
      "   micro avg       0.86      0.36      0.50      9044\n",
      "   macro avg       0.54      0.26      0.33      9044\n",
      "weighted avg       0.80      0.36      0.47      9044\n",
      " samples avg       0.39      0.34      0.35      9044\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "# Confusion matrix and classification report\n",
    "y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for X_test, y_test in test_loader:\n",
    "        X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "        outputs = trained_model(X_test)\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        y_true.extend(y_test.cpu().numpy())\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "cm = metrics.multilabel_confusion_matrix(y_true, y_pred)\n",
    "print(cm)\n",
    "print(metrics.classification_report(y_true, y_pred, target_names=classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear vram and cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Model for Em*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jcwin\\AppData\\Local\\Temp\\ipykernel_16424\\3321981113.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\"mamba_star_classifier_devout_feather_60.pth\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(108918, 3702)\n",
      "(108918, 55)\n",
      "(9269, 3702)\n",
      "(9269, 55)\n",
      "No samples found for class 0. Skipping.\n",
      "No samples found for class 1. Skipping.\n",
      "No samples found for class 2. Skipping.\n",
      "No samples found for class 3. Skipping.\n",
      "No samples found for class 4. Skipping.\n",
      "No samples found for class 5. Skipping.\n",
      "No samples found for class 6. Skipping.\n",
      "No samples found for class 7. Skipping.\n",
      "No samples found for class 8. Skipping.\n",
      "No samples found for class 9. Skipping.\n",
      "No samples found for class 10. Skipping.\n",
      "No samples found for class 11. Skipping.\n",
      "No samples found for class 12. Skipping.\n",
      "No samples found for class 13. Skipping.\n",
      "No samples found for class 14. Skipping.\n",
      "No samples found for class 15. Skipping.\n",
      "No samples found for class 16. Skipping.\n",
      "No samples found for class 17. Skipping.\n",
      "No samples found for class 18. Skipping.\n",
      "No samples found for class 19. Skipping.\n",
      "No samples found for class 20. Skipping.\n",
      "No samples found for class 21. Skipping.\n",
      "No samples found for class 22. Skipping.\n",
      "No samples found for class 23. Skipping.\n",
      "No samples found for class 24. Skipping.\n",
      "No samples found for class 25. Skipping.\n",
      "No samples found for class 26. Skipping.\n",
      "No samples found for class 27. Skipping.\n",
      "No samples found for class 28. Skipping.\n",
      "No samples found for class 29. Skipping.\n",
      "No samples found for class 31. Skipping.\n",
      "No samples found for class 32. Skipping.\n",
      "No samples found for class 33. Skipping.\n",
      "No samples found for class 34. Skipping.\n",
      "No samples found for class 35. Skipping.\n",
      "No samples found for class 36. Skipping.\n",
      "No samples found for class 37. Skipping.\n",
      "No samples found for class 38. Skipping.\n",
      "No samples found for class 39. Skipping.\n",
      "No samples found for class 40. Skipping.\n",
      "No samples found for class 41. Skipping.\n",
      "No samples found for class 42. Skipping.\n",
      "No samples found for class 43. Skipping.\n",
      "No samples found for class 44. Skipping.\n",
      "No samples found for class 45. Skipping.\n",
      "No samples found for class 46. Skipping.\n",
      "No samples found for class 47. Skipping.\n",
      "No samples found for class 48. Skipping.\n",
      "No samples found for class 49. Skipping.\n",
      "No samples found for class 50. Skipping.\n",
      "No samples found for class 51. Skipping.\n",
      "No samples found for class 52. Skipping.\n",
      "No samples found for class 53. Skipping.\n",
      "No samples found for class 54. Skipping.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 9.05 GiB. GPU 0 has a total capacity of 16.00 GiB of which 0 bytes is free. Of the allocated memory 22.06 GiB is allocated by PyTorch, and 721.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 61\u001b[0m\n\u001b[0;32m     59\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 61\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m     predicted \u001b[38;5;241m=\u001b[39m (outputs \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Move tensors to CPU before converting to NumPy arrays\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[9], line 30\u001b[0m, in \u001b[0;36mStarClassifierMAMBA.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     28\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_projection(x)  \u001b[38;5;66;03m# Ensure the input has the correct dimension\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m#x = x.unsqueeze(1)  # Adds a sequence dimension (L=1).\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmamba_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Pooling operation for classification\u001b[39;00m\n\u001b[0;32m     32\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(x)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\mambapy\\mamba.py:83\u001b[0m, in \u001b[0;36mMamba.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;66;03m# x : (B, L, D)\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \n\u001b[0;32m     80\u001b[0m     \u001b[38;5;66;03m# y : (B, L, D)\u001b[39;00m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m---> 83\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\mambapy\\mamba.py:111\u001b[0m, in \u001b[0;36mResidualBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;66;03m# x : (B, L, D)\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \n\u001b[0;32m    109\u001b[0m     \u001b[38;5;66;03m# output : (B, L, D)\u001b[39;00m\n\u001b[1;32m--> 111\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmixer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m x\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\mambapy\\mamba.py:222\u001b[0m, in \u001b[0;36mMambaBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    219\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m) \u001b[38;5;66;03m# (B, L, ED)\u001b[39;00m\n\u001b[0;32m    221\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msilu(x)\n\u001b[1;32m--> 222\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cuda:\n\u001b[0;32m    225\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_proj(y) \u001b[38;5;66;03m# (B, L, D)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\mambapy\\mamba.py:268\u001b[0m, in \u001b[0;36mMambaBlock.ssm\u001b[1;34m(self, x, z)\u001b[0m\n\u001b[0;32m    265\u001b[0m delta \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftplus(delta \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdt_proj\u001b[38;5;241m.\u001b[39mbias)\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpscan:\n\u001b[1;32m--> 268\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselective_scan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    270\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselective_scan_seq(x, delta, A, B, C, D)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\mambapy\\mamba.py:284\u001b[0m, in \u001b[0;36mMambaBlock.selective_scan\u001b[1;34m(self, x, delta, A, B, C, D)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselective_scan\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, delta, A, B, C, D):\n\u001b[0;32m    275\u001b[0m     \u001b[38;5;66;03m# x : (B, L, ED)\u001b[39;00m\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;66;03m# Δ : (B, L, ED)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    281\u001b[0m \n\u001b[0;32m    282\u001b[0m     \u001b[38;5;66;03m# y : (B, L, ED)\u001b[39;00m\n\u001b[1;32m--> 284\u001b[0m     deltaA \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(\u001b[43mdelta\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m) \u001b[38;5;66;03m# (B, L, ED, N)\u001b[39;00m\n\u001b[0;32m    285\u001b[0m     deltaB \u001b[38;5;241m=\u001b[39m delta\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m B\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m) \u001b[38;5;66;03m# (B, L, ED, N)\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     BX \u001b[38;5;241m=\u001b[39m deltaB \u001b[38;5;241m*\u001b[39m (x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;66;03m# (B, L, ED, N)\u001b[39;00m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 9.05 GiB. GPU 0 has a total capacity of 16.00 GiB of which 0 bytes is free. Of the allocated memory 22.06 GiB is allocated by PyTorch, and 721.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Same, but now with class weights\n",
    "d_model = 2048 # Embedding dimension\n",
    "num_classes = 55  # Star classification categories\n",
    "input_dim = 3647 # Number of spectra points\n",
    "\n",
    "# Define the training parameters\n",
    "num_epochs = 3000\n",
    "lr = 2e-5\n",
    "patience = 100   \n",
    "depth = 6\n",
    "\n",
    "# Load the data\n",
    "X = pd.read_pickle(\"Pickles/train_data_transformed.pkl\")\n",
    "classes = pd.read_pickle(\"Pickles/Updated_list_of_Classes.pkl\")\n",
    "\n",
    "# Load the trained model\n",
    "trained_model = StarClassifierMAMBA(d_model=d_model, num_classes=num_classes, input_dim=input_dim, n_layers=depth)\n",
    "\n",
    "# Load the state dictionary\n",
    "state_dict = torch.load(\"mamba_star_classifier_devout_feather_60.pth\")\n",
    "#model.load_state_dict(state_dict)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "#trained_model.eval()\n",
    "\n",
    "# Get the spectral data for all stars with the Em* label multihot encoded\n",
    "y = X[classes]\n",
    "\n",
    "# Drop gaia data\n",
    "X.drop([\"parallax\", \"ra\", \"dec\", \"ra_error\", \"dec_error\", \"parallax_error\", \"pmra\", \"pmdec\", \"pmra_error\", \"pmdec_error\", \n",
    "        \"phot_g_mean_flux\", \"flagnopllx\", \"phot_g_mean_flux_error\", \"phot_bp_mean_flux\", \"phot_rp_mean_flux\", \n",
    "        \"phot_bp_mean_flux_error\", \"phot_rp_mean_flux_error\", \"obsid\", \"flagnoflux\", \"otype\"], axis=1, inplace=True)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "# Get the spectral data for all stars with a value of 1 in the Em* label\n",
    "X = X[y[\"Em*\"] == 1]\n",
    "y = y[y[\"Em*\"] == 1]\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "# Drop labels\n",
    "X.drop(classes, axis=1, inplace=True)\n",
    "\n",
    "# Do batches of 32\n",
    "X = X.values\n",
    "y = y.values\n",
    "X = torch.tensor(X, dtype=torch.float32).unsqueeze(1)\n",
    "y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# Do batches of 32\n",
    "dataset = BalancedMultiLabelDataset(X, y)\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Pass the data through the trained model\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = trained_model.to(device)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X.to(device))\n",
    "    predicted = (outputs > 0.5).float()\n",
    "\n",
    "# Move tensors to CPU before converting to NumPy arrays\n",
    "y_cpu = y.cpu()\n",
    "predicted_cpu = predicted.cpu()\n",
    "\n",
    "# Confusion matrix and classification report\n",
    "cm = metrics.multilabel_confusion_matrix(y_cpu.numpy(), predicted_cpu.numpy())\n",
    "print(cm)\n",
    "print(metrics.classification_report(y_cpu.numpy(), predicted_cpu.numpy(), target_names=classes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
