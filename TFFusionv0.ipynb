{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from astropy.io import fits\n",
    "from astropy import units as u\n",
    "from matplotlib import pyplot as plt\n",
    "from astropy.visualization import quantity_support\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "import gc\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import random\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_convnet(input_shape, num_classes, \n",
    "                   num_filters=[128, 128, 128, 128, 128, 128, 128, 128], \n",
    "                   kernel_size=(9,),\n",
    "                   dense_units1=256, \n",
    "                   dense_units2=128,\n",
    "                   dense_units3=64,\n",
    "                   dropout_rate=0.2,\n",
    "                   padding='same'):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    \n",
    "    # First convolutional layer\n",
    "    model.add(tf.keras.layers.Conv1D(filters=num_filters[0], kernel_size=kernel_size, \n",
    "                                     activation='relu', input_shape=input_shape, padding=padding))\n",
    "    model.add(tf.keras.layers.MaxPooling1D(pool_size=2))\n",
    "    \n",
    "    # Additional convolutional layers\n",
    "    for filters in num_filters[1:]:\n",
    "        model.add(tf.keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, \n",
    "                                         activation='relu', padding=padding))\n",
    "        model.add(tf.keras.layers.MaxPooling1D(pool_size=2))\n",
    "        model.add(tf.keras.layers.Dropout(rate=dropout_rate))\n",
    "    \n",
    "    # Flatten the output and add dense layers\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(units=dense_units1, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(rate=dropout_rate))\n",
    "\n",
    "    # Adding another dense layer\n",
    "    if dense_units2:\n",
    "        model.add(tf.keras.layers.Dense(units=dense_units2, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dropout(rate=dropout_rate))\n",
    "\n",
    "    # Adding another dense layer\n",
    "    if dense_units3:\n",
    "        model.add(tf.keras.layers.Dense(units=dense_units3, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dropout(rate=dropout_rate))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(tf.keras.layers.Dense(units=num_classes, activation='softmax'))\n",
    "\n",
    "    # Optimizer and loss function\n",
    "    optimizer_ = tf.keras.optimizers.AdamW(learning_rate=1e-4) \n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizer_, \n",
    "                  loss='sparse_categorical_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_spectra(spectra):\n",
    "    \"\"\"Normalize spectra by dividing by the mean and applying the natural logarithm.\"\"\"\n",
    "    mean_value = np.mean(spectra)\n",
    "    std_value = np.std(spectra)\n",
    "    min_value = np.min(spectra)\n",
    "    if std_value == 0:\n",
    "        print(\"Warning: Standard deviation is zero, cannot normalize spectra.\")\n",
    "        return spectra  # Avoid division by zero\n",
    "    normalized_spectra = ((spectra - min_value + 0.01) / (mean_value - min_value + 0.01)) - 1  # Avoid negative values\n",
    "    return normalized_spectra\n",
    "\n",
    "def load_single_spectrum_npy(file_path):\n",
    "    \"\"\"Load a single spectrum from a .npy file.\"\"\"\n",
    "    try:\n",
    "        spectrum = np.load(file_path)\n",
    "        spectrum = normalize_spectra(spectrum)\n",
    "        return spectrum\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return None\n",
    "    \n",
    "def load_all_spectra_parallel_npy(file_list, max_workers=512):\n",
    "    \"\"\"Load spectra from .npy files in parallel using ThreadPoolExecutor.\"\"\"\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        results = list(tqdm(executor.map(load_single_spectrum_npy, file_list), \n",
    "                            total=len(file_list), desc=\"Loading spectra from .npy\"))\n",
    "    # Filter out None results\n",
    "    spectra_data = [spectrum for spectrum in results if spectrum is not None]\n",
    "    return np.array(spectra_data)\n",
    "\n",
    "def load_validation_dataset_npy(limit_per_label=2000):\n",
    "    \"\"\"Load the validation dataset once and keep it in memory.\"\"\"\n",
    "    val_dataset = generate_datasets_from_preseparated_npy(limit_per_dir=limit_per_label)[1]\n",
    "    return val_dataset \n",
    "\n",
    "def removenan(train_spectra, train_labels, val_spectra, val_labels):\n",
    "    \"\"\"\n",
    "    Removes rows with NaN values from training and validation spectra,\n",
    "    converts the cleaned data to TensorFlow datasets.\n",
    "    \"\"\"\n",
    "    # Convert lists to NumPy arrays and remove the first 10 spectra\n",
    "    print(train_spectra.shape)\n",
    "    train_spectraa = np.array(train_spectra[10:, :])\n",
    "    train_labelsa = np.array(train_labels[10:])\n",
    "\n",
    "    val_spectraa = np.array(val_spectra[10:, :])\n",
    "    val_labelsa = np.array(val_labels[10:])\n",
    "\n",
    "    # Remove rows with any NaN values from training data\n",
    "    mask_train = ~np.isnan(train_spectraa).any(axis=1)\n",
    "    train_spectranan = train_spectraa[mask_train]\n",
    "    train_labelsnan = train_labelsa[mask_train]\n",
    "\n",
    "    # Remove rows with any NaN values from validation data\n",
    "    mask_val = ~np.isnan(val_spectraa).any(axis=1)\n",
    "    val_spectranan = val_spectraa[mask_val]\n",
    "    val_labelsnan = val_labelsa[mask_val]\n",
    "\n",
    "    # Cleanup unused variables and force garbage collection\n",
    "    del val_spectraa, val_labelsa, mask_val, train_spectraa, train_labelsa, mask_train\n",
    "    gc.collect()\n",
    "\n",
    "    # Create TensorFlow datasets\n",
    "    train_dataset = create_tf_dataset(train_spectranan, train_labelsnan)\n",
    "    val_dataset = create_tf_dataset(val_spectranan, val_labelsnan)\n",
    "\n",
    "    # Additional cleanup\n",
    "    del train_spectranan, train_labelsnan\n",
    "    gc.collect()\n",
    "\n",
    "    return train_dataset, val_dataset, val_spectranan, val_labelsnan\n",
    "\n",
    "def create_tf_dataset(spectra, labels, batch_size=32, shuffle=True):\n",
    "    \"\"\"\n",
    "    Converts NumPy arrays of spectra and labels into a TensorFlow dataset.\n",
    "    \"\"\"\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((spectra, labels))\n",
    "\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(labels))\n",
    "\n",
    "    dataset = dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_file_list_from_npy_directories(base_dirs, limit_per_dir=10000):\n",
    "    \"\"\"\n",
    "    Generates a list of .npy files and their associated labels from pre-separated directories.\n",
    "    Assumes that each base directory contains subdirectories labeled as:\n",
    "    \"gal_spectra\", \"star_spectra\", \"agn_spectra\", and \"bin_spectra\".\n",
    "    \"\"\"\n",
    "    spectra_dirs = {\n",
    "        \"gal_spectra\": 0,  # Label 0 for galaxies\n",
    "        \"star_spectra\": 1,  # Label 1 for stars\n",
    "        \"agn_spectra\": 2,   # Label 2 for AGNs\n",
    "        \"bin_spectra\": 3    # Label 3 for binary stars\n",
    "    }\n",
    "\n",
    "    file_list = []\n",
    "    labels = []\n",
    "\n",
    "    print(\"Gathering .npy files from pre-separated directories...\")\n",
    "    for dir_name, label in spectra_dirs.items():\n",
    "        for base_dir in base_dirs:\n",
    "            dir_path = os.path.join(base_dir, dir_name)\n",
    "            dir_files = []\n",
    "\n",
    "            # Collect all .npy files in the directory\n",
    "            for root, dirs, files in os.walk(dir_path):\n",
    "                for file in files:\n",
    "                    if file.endswith('.npy'):  # Only consider .npy files\n",
    "                        file_path = os.path.join(root, file)\n",
    "                        dir_files.append(file_path)\n",
    "\n",
    "            # Randomly select files up to the limit\n",
    "            if len(dir_files) > limit_per_dir:\n",
    "                selected_files = random.sample(dir_files, limit_per_dir)\n",
    "            else:\n",
    "                selected_files = dir_files\n",
    "\n",
    "            # Append selected files and their labels\n",
    "            file_list.extend(selected_files)\n",
    "            labels.extend([label] * len(selected_files))\n",
    "\n",
    "    print(f\"Total .npy spectra files collected: {len(file_list)}\")\n",
    "    return file_list, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_datasets_from_preseparated_npy(limit_per_dir=10000):\n",
    "    \"\"\"\n",
    "    Generates training and validation datasets from pre-separated directories containing .npy files.\n",
    "    \"\"\"\n",
    "    train_base_dir = os.path.join(os.getcwd(), \"training_npy\")\n",
    "    val_base_dir = os.path.join(os.getcwd(), \"validation_npy\")\n",
    "\n",
    "    # Load file paths and labels from the respective directories\n",
    "    train_files, train_labels = generate_file_list_from_npy_directories([train_base_dir], limit_per_dir)\n",
    "    val_files, val_labels = generate_file_list_from_npy_directories([val_base_dir], limit_per_dir)\n",
    "\n",
    "    # Load spectra data in parallel from .npy files\n",
    "    train_spectra = load_all_spectra_parallel_npy(train_files)\n",
    "    val_spectra = load_all_spectra_parallel_npy(val_files)\n",
    "\n",
    "    # Create TensorFlow datasets (or apply further processing if needed)\n",
    "    train_dataset, val_dataset, val_spectranan, val_labelsnan = removenan(train_spectra, train_labels, val_spectra, val_labels)\n",
    "    \n",
    "    return train_dataset, val_dataset, val_spectranan, val_labelsnan\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_convnet_npy(model, val_dataset, limit_per_label=2000, epochs=1, batch_size=32, patience=5):\n",
    "    # Define early stopping callback\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n",
    "    \n",
    "    # Load only the training dataset\n",
    "    train_dataset = generate_datasets_from_preseparated_npy(limit_per_dir=limit_per_label)[0]\n",
    "    \n",
    "    # Fit the model using the pre-loaded validation dataset\n",
    "    history = model.fit(train_dataset,\n",
    "                        validation_data=val_dataset,\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        callbacks=[early_stopping])\n",
    "    \n",
    "    return history\n",
    "\n",
    "def train_convnet_many_times_npy(model, val_dataset, epochs_per_run=1, batch_size=32, num_runs=10, limit_per_label=2000):\n",
    "    histories = []\n",
    "    for i in range(num_runs):\n",
    "        print(f\"Training run {i+1}/{num_runs}...\")\n",
    "        history = train_convnet_npy(model, val_dataset, limit_per_label=limit_per_label, epochs=epochs_per_run, batch_size=batch_size)\n",
    "        histories.append(history)\n",
    "    \n",
    "    return histories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering .npy files from pre-separated directories...\n",
      "Total .npy spectra files collected: 7203\n",
      "Train files: 7203, Train labels: 7203\n",
      "Error loading training_npy/gal_spectra/500010175.npy: cannot reshape array of size 18432 into shape (5,3820)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading spectra from .npy: 100%|██████████| 7203/7203 [00:00<00:00, 360498.44it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (7202, 5) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 69\u001b[0m\n\u001b[1;32m     66\u001b[0m train_files, train_labels \u001b[38;5;241m=\u001b[39m generate_file_list_from_directories([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_npy/\u001b[39m\u001b[38;5;124m\"\u001b[39m], limit_per_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain files: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_files)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train labels: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_labels)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 69\u001b[0m train_spectra \u001b[38;5;241m=\u001b[39m \u001b[43mload_all_spectra_parallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_files\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded train spectra: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_spectra\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 64\u001b[0m, in \u001b[0;36mload_all_spectra_parallel\u001b[0;34m(file_list, max_workers)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Filter out None results (in case of loading errors)\u001b[39;00m\n\u001b[1;32m     62\u001b[0m spectra_data \u001b[38;5;241m=\u001b[39m [spectrum \u001b[38;5;28;01mfor\u001b[39;00m spectrum \u001b[38;5;129;01min\u001b[39;00m spectra_data \u001b[38;5;28;01mif\u001b[39;00m spectrum \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[0;32m---> 64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspectra_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (7202, 5) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "\n",
    "## take 2\n",
    "\n",
    "def generate_file_list_from_directories(base_dirs, limit_per_dir=10000):\n",
    "    \"\"\"\n",
    "    Generates a list of .npy files and their corresponding labels from pre-separated directories.\n",
    "    \"\"\"\n",
    "    spectra_dirs = {\n",
    "        \"gal_spectra\": 0,  # Label 0 for galaxies\n",
    "        \"star_spectra\": 1,  # Label 1 for stars\n",
    "        \"agn_spectra\": 2,   # Label 2 for AGNs\n",
    "        \"bin_spectra\": 3    # Label 3 for binary stars\n",
    "    }\n",
    "\n",
    "    file_list = []\n",
    "    labels = []\n",
    "\n",
    "    print(\"Gathering .npy files from pre-separated directories...\")\n",
    "    for dir_name, label in spectra_dirs.items():\n",
    "        for base_dir in base_dirs:\n",
    "            dir_path = os.path.join(base_dir, dir_name)\n",
    "            dir_files = []\n",
    "\n",
    "            # Collect all .npy files in the directory\n",
    "            for root, dirs, files in os.walk(dir_path):\n",
    "                npy_files = [file for file in files if file.endswith('.npy')]\n",
    "                for npy_file in npy_files:\n",
    "                    file_path = os.path.join(root, npy_file)\n",
    "                    dir_files.append(file_path)\n",
    "\n",
    "            # Randomly select files up to the limit\n",
    "            if len(dir_files) > limit_per_dir:\n",
    "                selected_files = random.sample(dir_files, limit_per_dir)\n",
    "            else:\n",
    "                selected_files = dir_files\n",
    "\n",
    "            # Append selected files and their labels\n",
    "            file_list.extend(selected_files)\n",
    "            labels.extend([label] * len(selected_files))\n",
    "\n",
    "    print(f\"Total .npy spectra files collected: {len(file_list)}\")\n",
    "    return file_list, labels\n",
    "\n",
    "\n",
    "\n",
    "def load_all_spectra_parallel(file_list, max_workers=512):\n",
    "    \"\"\"\n",
    "    Loads spectra from .npy files in parallel using ThreadPoolExecutor.\n",
    "    \"\"\"\n",
    "    def load_single_npy(file_path):\n",
    "        try:\n",
    "            return np.load(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "    # Load spectra in parallel using ThreadPoolExecutor\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        spectra_data = list(tqdm(executor.map(load_single_npy, file_list),\n",
    "                                 total=len(file_list), desc=\"Loading spectra from .npy\"))\n",
    "\n",
    "    # Filter out None results (in case of loading errors)\n",
    "    spectra_data = [spectrum for spectrum in spectra_data if spectrum is not None]\n",
    "    \n",
    "    return np.array(spectra_data)\n",
    "\n",
    "train_files, train_labels = generate_file_list_from_directories([\"training_npy/\"], limit_per_dir=10000)\n",
    "print(f\"Train files: {len(train_files)}, Train labels: {len(train_labels)}\")\n",
    "\n",
    "train_spectra = load_all_spectra_parallel(train_files)\n",
    "print(f\"Loaded train spectra: {train_spectra.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking path: /home/jcwind/Star Classifier/Star-Classifier/training_npy\n"
     ]
    }
   ],
   "source": [
    "print(f\"Checking path: {os.path.abspath('training_npy/')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering .npy files from pre-separated directories...\n",
      "Total .npy spectra files collected: 1\n",
      "Gathering .npy files from pre-separated directories...\n",
      "Total .npy spectra files collected: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading spectra from .npy: 100%|██████████| 1/1 [00:00<00:00, 7825.19it/s]\n",
      "Loading spectra from .npy: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5, 3904)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m len_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3748\u001b[39m\n\u001b[1;32m      2\u001b[0m len_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3748\u001b[39m\n\u001b[0;32m----> 3\u001b[0m file_list, labels \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_datasets_from_preseparated_npy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlimit_per_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m filters_20\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m128\u001b[39m]\n\u001b[1;32m      5\u001b[0m model_20 \u001b[38;5;241m=\u001b[39m create_convnet(input_shape\u001b[38;5;241m=\u001b[39m(len_\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m1\u001b[39m), num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(labels)), num_filters\u001b[38;5;241m=\u001b[39mfilters_20, kernel_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m9\u001b[39m,))\n",
      "Cell \u001b[0;32mIn[5], line 17\u001b[0m, in \u001b[0;36mgenerate_datasets_from_preseparated_npy\u001b[0;34m(limit_per_dir)\u001b[0m\n\u001b[1;32m     14\u001b[0m val_spectra \u001b[38;5;241m=\u001b[39m load_all_spectra_parallel_npy(val_files)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Create TensorFlow datasets (or apply further processing if needed)\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m train_dataset, val_dataset, val_spectranan, val_labelsnan \u001b[38;5;241m=\u001b[39m \u001b[43mremovenan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_spectra\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_spectra\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m train_dataset, val_dataset, val_spectranan, val_labelsnan\n",
      "Cell \u001b[0;32mIn[8], line 46\u001b[0m, in \u001b[0;36mremovenan\u001b[0;34m(train_spectra, train_labels, val_spectra, val_labels)\u001b[0m\n\u001b[1;32m     43\u001b[0m train_spectraa \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(train_spectra[\u001b[38;5;241m10\u001b[39m:, :])\n\u001b[1;32m     44\u001b[0m train_labelsa \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(train_labels[\u001b[38;5;241m10\u001b[39m:])\n\u001b[0;32m---> 46\u001b[0m val_spectraa \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[43mval_spectra\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     47\u001b[0m val_labelsa \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(val_labels[\u001b[38;5;241m10\u001b[39m:])\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Remove rows with any NaN values from training data\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": [
    "len_ = 3748\n",
    "file_list, labels = generate_datasets_from_preseparated_npy(limit_per_dir=1)\n",
    "filters_20=[1024, 1024, 1024, 512, 512, 512, 256, 256, 256, 128]\n",
    "model_20 = create_convnet(input_shape=(len_-10, 1), num_classes=len(set(labels)), num_filters=filters_20, kernel_size=(9,))\n",
    "model_20.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
