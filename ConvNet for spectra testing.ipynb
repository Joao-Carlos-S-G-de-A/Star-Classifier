{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "quantity_support()  # for getting units on the axes below  \n",
    "\n",
    "f = fits.open('gal_spectra/110033')  \n",
    "# The spectrum is in the second HDU of this file.\n",
    "specdata = f[0].data \n",
    "specdata = specdata[0]  # The spectrum is in the first row of the data array.\n",
    "f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from astropy.io import fits\n",
    "from astropy import units as u\n",
    "from matplotlib import pyplot as plt\n",
    "from astropy.visualization import quantity_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the crossmatched data\n",
    "gal_lamost_data = pd.read_pickle(\"gal_lamost_data.pkl\")\n",
    "obsid_list = gal_lamost_data['obsid'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chatgpt code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total spectra files collected: 165948\n"
     ]
    }
   ],
   "source": [
    "# Define the directories containing your spectra\n",
    "spectra_dirs = {\n",
    "    \"gal_spectra\": 0,  # Label 0 for galaxies\n",
    "    \"star_spectra\": 1,  # Label 1 for stars\n",
    "    \"agn_spectra\": 2,   # Label 2 for AGNs\n",
    "    \"bin_spectra\": 3    # Label 3 for binary stars\n",
    "}\n",
    "\n",
    "file_list = []\n",
    "labels = []\n",
    "\n",
    "# Iterate over the directories and assign labels based on the directory name\n",
    "for dir_name, label in spectra_dirs.items():\n",
    "    dir_path = os.path.join(os.getcwd(), dir_name)\n",
    "    for root, dirs, files in os.walk(dir_path):\n",
    "        for file in files:\n",
    "            #if file.endswith(\".fits\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "            file_list.append(file_path)\n",
    "            labels.append(label)\n",
    "\n",
    "print(f\"Total spectra files collected: {len(file_list)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determining the minimum number of rows across spectra...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating min rows:  31%|███       | 50631/165948 [02:10<04:57, 387.19file/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 33\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(spectra_data)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Simulate the file list (replace with actual file paths)\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m#file_list = [\"path_to_spectra1.fits\", \"path_to_spectra2.fits\"]  # Add all your actual file paths here\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Load the truncated spectra with progress bars\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m spectra_data \u001b[38;5;241m=\u001b[39m \u001b[43mload_spectra\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpectra data shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspectra_data\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# (num_files, min_rows)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 13\u001b[0m, in \u001b[0;36mload_spectra\u001b[1;34m(file_list)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDetermining the minimum number of rows across spectra...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_path \u001b[38;5;129;01min\u001b[39;00m tqdm(file_list, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalculating min rows\u001b[39m\u001b[38;5;124m\"\u001b[39m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m---> 13\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mfits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m hdul:\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;66;03m# Access the primary HDU (index 0) and get the first row of data\u001b[39;00m\n\u001b[0;32m     15\u001b[0m         spectra \u001b[38;5;241m=\u001b[39m hdul[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# First row of the primary HDU\u001b[39;00m\n\u001b[0;32m     16\u001b[0m         min_rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(min_rows, \u001b[38;5;28mlen\u001b[39m(spectra))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\astropy\\io\\fits\\hdu\\hdulist.py:223\u001b[0m, in \u001b[0;36mfitsopen\u001b[1;34m(name, mode, memmap, save_backup, cache, lazy_load_hdus, ignore_missing_simple, use_fsspec, fsspec_kwargs, decompress_in_memory, **kwargs)\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m name:\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmpty filename: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 223\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mHDUList\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromfile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemmap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_backup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlazy_load_hdus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_missing_simple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_fsspec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_fsspec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfsspec_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfsspec_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecompress_in_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecompress_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\astropy\\io\\fits\\hdu\\hdulist.py:487\u001b[0m, in \u001b[0;36mHDUList.fromfile\u001b[1;34m(cls, fileobj, mode, memmap, save_backup, cache, lazy_load_hdus, ignore_missing_simple, **kwargs)\u001b[0m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfromfile\u001b[39m(\n\u001b[0;32m    470\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    479\u001b[0m ):\n\u001b[0;32m    480\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    481\u001b[0m \u001b[38;5;124;03m    Creates an `HDUList` instance from a file-like object.\u001b[39;00m\n\u001b[0;32m    482\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;124;03m    documentation for details of the parameters accepted by this method).\u001b[39;00m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 487\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_readfrom\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfileobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfileobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmemmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemmap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_backup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_backup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_missing_simple\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_missing_simple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlazy_load_hdus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlazy_load_hdus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\astropy\\io\\fits\\hdu\\hdulist.py:1169\u001b[0m, in \u001b[0;36mHDUList._readfrom\u001b[1;34m(cls, fileobj, data, mode, memmap, cache, lazy_load_hdus, ignore_missing_simple, use_fsspec, fsspec_kwargs, decompress_in_memory, **kwargs)\u001b[0m\n\u001b[0;32m   1166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fileobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fileobj, _File):\n\u001b[0;32m   1168\u001b[0m         \u001b[38;5;66;03m# instantiate a FITS file object (ffo)\u001b[39;00m\n\u001b[1;32m-> 1169\u001b[0m         fileobj \u001b[38;5;241m=\u001b[39m \u001b[43m_File\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1170\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfileobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1171\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1172\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmemmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemmap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1173\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1174\u001b[0m \u001b[43m            \u001b[49m\u001b[43muse_fsspec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_fsspec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1175\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfsspec_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfsspec_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1176\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdecompress_in_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecompress_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1177\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1178\u001b[0m     \u001b[38;5;66;03m# The Astropy mode is determined by the _File initializer if the\u001b[39;00m\n\u001b[0;32m   1179\u001b[0m     \u001b[38;5;66;03m# supplied mode was None\u001b[39;00m\n\u001b[0;32m   1180\u001b[0m     mode \u001b[38;5;241m=\u001b[39m fileobj\u001b[38;5;241m.\u001b[39mmode\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\astropy\\io\\fits\\file.py:218\u001b[0m, in \u001b[0;36m_File.__init__\u001b[1;34m(self, fileobj, mode, memmap, overwrite, cache, use_fsspec, fsspec_kwargs, decompress_in_memory)\u001b[0m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_open_fileobj(fileobj, mode, overwrite)\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fileobj, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m)):\n\u001b[1;32m--> 218\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open_filename\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfileobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_open_filelike(fileobj, mode, overwrite)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\astropy\\io\\fits\\file.py:650\u001b[0m, in \u001b[0;36m_File._open_filename\u001b[1;34m(self, filename, mode, overwrite)\u001b[0m\n\u001b[0;32m    646\u001b[0m     magic \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    648\u001b[0m ext \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m--> 650\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_read_compressed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmagic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mext\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    651\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, IO_FITS_MODES[mode])\n\u001b[0;32m    652\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose_on_error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\astropy\\io\\fits\\file.py:540\u001b[0m, in \u001b[0;36m_File._try_read_compressed\u001b[1;34m(self, obj_or_name, magic, mode, ext)\u001b[0m\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    539\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfileobj\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m obj_or_name\n\u001b[1;32m--> 540\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m \u001b[43mgzip\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGzipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    541\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgzip\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (is_ostream \u001b[38;5;129;01mand\u001b[39;00m ext \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m magic\u001b[38;5;241m.\u001b[39mstartswith(PKZIP_MAGIC):\n\u001b[0;32m    543\u001b[0m     \u001b[38;5;66;03m# Handle zip files\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\gzip.py:174\u001b[0m, in \u001b[0;36mGzipFile.__init__\u001b[1;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[0m\n\u001b[0;32m    172\u001b[0m     mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fileobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 174\u001b[0m     fileobj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmyfileobj \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, mode \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    176\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(fileobj, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from astropy.io import fits\n",
    "from tqdm import tqdm  # Progress bar\n",
    "\n",
    "def load_spectra(file_list):\n",
    "    spectra_data = []\n",
    "    min_rows = np.inf  # Initialize as infinite to find the minimum number of rows\n",
    "\n",
    "    # First pass: determine the minimum number of rows across all spectra\n",
    "    print(\"Determining the minimum number of rows across spectra...\")\n",
    "    for file_path in tqdm(file_list, desc=\"Calculating min rows\", unit=\"file\"):\n",
    "        with fits.open(file_path) as hdul:\n",
    "            # Access the primary HDU (index 0) and get the first row of data\n",
    "            spectra = hdul[0].data[0]  # First row of the primary HDU\n",
    "            min_rows = min(min_rows, len(spectra))\n",
    "\n",
    "    # Second pass: load and truncate the spectra to the minimum number of rows\n",
    "    print(f\"Loading and truncating spectra to {min_rows} rows...\")\n",
    "    for file_path in tqdm(file_list, desc=\"Loading spectra\", unit=\"file\"):\n",
    "        with fits.open(file_path) as hdul:\n",
    "            # Access the first row of the primary HDU and truncate\n",
    "            spectra = hdul[0].data[0][:min_rows]\n",
    "            spectra_data.append(spectra)\n",
    "\n",
    "    # Convert the list of spectra to a NumPy array for easier processing\n",
    "    return np.array(spectra_data)\n",
    "\n",
    "# Simulate the file list (replace with actual file paths)\n",
    "#file_list = [\"path_to_spectra1.fits\", \"path_to_spectra2.fits\"]  # Add all your actual file paths here\n",
    "\n",
    "# Load the truncated spectra with progress bars\n",
    "spectra_data = load_spectra(file_list)\n",
    "print(f\"Spectra data shape: {spectra_data.shape}\")  # (num_files, min_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering FITS files...\n",
      "Total spectra files collected: 0\n",
      "Determining minimum number of rows across spectra...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding min rows: 0file [00:00, ?file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading spectra (truncated to inf rows)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading spectra: 0file [00:00, ?file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded 0 spectra with shape: (0,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from astropy.io import fits\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function to load spectra efficiently from FITS files\n",
    "def load_spectra(file_list):\n",
    "    spectra_data = []\n",
    "    min_rows = 5000  # Initialize as infinite to find the minimum number of rows\n",
    "\n",
    "    # First pass: determine the minimum number of rows across all spectra\n",
    "    print(\"Determining minimum number of rows across spectra...\")\n",
    "    for file_path in tqdm(file_list, desc=\"Finding min rows\", unit=\"file\"):\n",
    "        try:\n",
    "            with fits.open(file_path) as hdul:\n",
    "                # Access the primary HDU (index 0) and the first row of data\n",
    "                spectra = hdul[0].data[0]\n",
    "                min_rows = min(min_rows, len(spectra))\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path}: {e}\")\n",
    "\n",
    "    # Second pass: load and truncate spectra to the minimum number of rows\n",
    "    print(f\"\\nLoading spectra (truncated to {min_rows} rows)...\")\n",
    "    for file_path in tqdm(file_list, desc=\"Loading spectra\", unit=\"file\"):\n",
    "        try:\n",
    "            with fits.open(file_path) as hdul:\n",
    "                # Access the first row of the primary HDU and truncate\n",
    "                spectra = hdul[0].data[0][:min_rows]\n",
    "                spectra_data.append(spectra)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {e}\")\n",
    "    \n",
    "    # Convert the list of spectra to a NumPy array for easier processing\n",
    "    spectra_data = np.array(spectra_data)\n",
    "    return spectra_data\n",
    "\n",
    "# Example usage: Generate a file_list and load the spectra\n",
    "def generate_file_list():\n",
    "    # Define the directories containing your spectra\n",
    "    spectra_dirs = {\n",
    "        \"gal_spectra\": 0,  # Label 0 for galaxies\n",
    "        \"star_spectra\": 1,  # Label 1 for stars\n",
    "        \"agn_spectra\": 2,   # Label 2 for AGNs\n",
    "        \"bin_spectra\": 3    # Label 3 for binary stars\n",
    "    }\n",
    "\n",
    "    file_list = []\n",
    "    labels = []\n",
    "\n",
    "    # Iterate over the directories and assign labels based on the directory name\n",
    "    print(\"Gathering FITS files...\")\n",
    "    for dir_name, label in spectra_dirs.items():\n",
    "        dir_path = os.path.join(os.getcwd(), dir_name)\n",
    "        for root, dirs, files in os.walk(dir_path):\n",
    "            for file in files:\n",
    "                #if file.endswith(\".fits\"):\n",
    "                file_path = os.path.join(root, file)\n",
    "                file_list.append(file_path)\n",
    "                labels.append(label)\n",
    "\n",
    "    print(f\"Total spectra files collected: {len(file_list)}\")\n",
    "    return file_list, labels\n",
    "\n",
    "# Load the spectra and monitor progress\n",
    "file_list, labels = generate_file_list()\n",
    "spectra_data = load_spectra(file_list)\n",
    "\n",
    "print(f\"\\nLoaded {len(spectra_data)} spectra with shape: {spectra_data.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(spectra_data)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Load the truncated spectra\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m spectra_data \u001b[38;5;241m=\u001b[39m \u001b[43mload_spectra\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpectra data shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspectra_data\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# (num_files, min_rows)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[14], line 10\u001b[0m, in \u001b[0;36mload_spectra\u001b[1;34m(file_list)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_path \u001b[38;5;129;01min\u001b[39;00m file_list:\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m fits\u001b[38;5;241m.\u001b[39mopen(file_path) \u001b[38;5;28;01mas\u001b[39;00m hdul:\n\u001b[1;32m---> 10\u001b[0m         spectra \u001b[38;5;241m=\u001b[39m \u001b[43mhdul\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Assuming 'flux' column contains the spectra\u001b[39;00m\n\u001b[0;32m     11\u001b[0m         min_rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(min_rows, \u001b[38;5;28mlen\u001b[39m(spectra))\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Second pass: load and truncate the spectra to the minimum number of rows\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\astropy\\io\\fits\\hdu\\hdulist.py:396\u001b[0m, in \u001b[0;36mHDUList.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    393\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    394\u001b[0m     \u001b[38;5;66;03m# Raise a more helpful IndexError if the file was not fully read.\u001b[39;00m\n\u001b[0;32m    395\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_all:\n\u001b[1;32m--> 396\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    398\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\n\u001b[0;32m    399\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHDU not found, possibly because the index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    400\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis out of range, or because the file was \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    401\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed before all HDUs were read\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    402\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\astropy\\io\\fits\\hdu\\hdulist.py:390\u001b[0m, in \u001b[0;36mHDUList.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;66;03m# Originally this used recursion, but hypothetically an HDU with\u001b[39;00m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;66;03m# a very large number of HDUs could blow the stack, so use a loop\u001b[39;00m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;66;03m# instead\u001b[39;00m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_while_unread_hdus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_positive_index_of\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    394\u001b[0m     \u001b[38;5;66;03m# Raise a more helpful IndexError if the file was not fully read.\u001b[39;00m\n\u001b[0;32m    395\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_all:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\astropy\\io\\fits\\hdu\\hdulist.py:1270\u001b[0m, in \u001b[0;36mHDUList._try_while_unread_hdus\u001b[1;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1268\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m   1269\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1270\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1271\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1272\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_next_hdu():\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def load_spectra(file_list):\n",
    "    spectra_data = []\n",
    "    min_rows = np.inf  # Initialize as infinite to find the minimum number of rows\n",
    "\n",
    "    # First pass: determine the minimum number of rows across all spectra\n",
    "    for file_path in file_list:\n",
    "        with fits.open(file_path) as hdul:\n",
    "            spectra = hdul[1].data[0]  # Assuming 'flux' column contains the spectra\n",
    "            min_rows = min(min_rows, len(spectra))\n",
    "\n",
    "    # Second pass: load and truncate the spectra to the minimum number of rows\n",
    "    for file_path in file_list:\n",
    "        with fits.open(file_path) as hdul:\n",
    "            spectra = hdul[1].data[0][:min_rows]  # Truncate to the min number of rows\n",
    "            spectra_data.append(spectra)\n",
    "    \n",
    "    # Convert the list of spectra to a NumPy array for easier processing\n",
    "    return np.array(spectra_data)\n",
    "\n",
    "# Load the truncated spectra\n",
    "spectra_data = load_spectra(file_list)\n",
    "print(f\"Spectra data shape: {spectra_data.shape}\")  # (num_files, min_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset for Spectra\n",
    "class SpectraDataset(Dataset):\n",
    "    def __init__(self, file_list, labels, transform=None):\n",
    "        self.file_list = file_list\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_list[idx]\n",
    "        # Assuming each file is a FITS file containing the spectra\n",
    "        with fits.open(file_path) as hdul:\n",
    "            spectra_data = hdul[1].data['flux']  # Assuming 'flux' is the field containing the spectra\n",
    "        \n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Convert spectra to torch tensor\n",
    "        spectra_tensor = torch.tensor(spectra_data, dtype=torch.float32)\n",
    "        \n",
    "        if self.transform:\n",
    "            spectra_tensor = self.transform(spectra_tensor)\n",
    "        \n",
    "        return spectra_tensor, label\n",
    "\n",
    "# Assume file_list contains paths to your downloaded FITS files and labels contains the corresponding labels\n",
    "file_list = [\"path_to_spectrum1.fits\", \"path_to_spectrum2.fits\", ...]\n",
    "labels = [0, 1, 2, 3]  # Corresponding to stars, binary stars, non-active galaxies, AGNs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SpectraCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpectraCNN, self).__init__()\n",
    "        \n",
    "        # Define the layers\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(64 * 256, 128)  # Assuming input spectra length of 256\n",
    "        self.fc2 = nn.Linear(128, 4)  # 4 output classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool1d(x, kernel_size=2)\n",
    "        \n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool1d(x, kernel_size=2)\n",
    "        \n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.max_pool1d(x, kernel_size=2)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "\n",
    "# Create Dataset and DataLoader\n",
    "dataset = SpectraDataset(file_list, labels)\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = SpectraCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for spectra, label in train_loader:\n",
    "        # Reshape the input for 1D CNN\n",
    "        spectra = spectra.unsqueeze(1)  # Add channel dimension (batch_size, 1, spectra_length)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(spectra)\n",
    "        loss = criterion(outputs, label)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for spectra, labels in test_loader:\n",
    "            spectra = spectra.unsqueeze(1)\n",
    "            outputs = model(spectra)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Accuracy: {100 * correct / total:.2f}%')\n",
    "\n",
    "# Assuming you have a test_loader for evaluation\n",
    "# evaluate_model(model, test_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
