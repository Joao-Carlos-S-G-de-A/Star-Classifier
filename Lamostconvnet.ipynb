{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-14 18:16:01.776798: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-14 18:16:01.855124: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-14 18:16:01.877125: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-14 18:16:02.061933: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-14 18:16:03.426420: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import gc\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom BalancedDataGenerator class for training\n",
    "class BalancedDataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, X, y, batch_size=32, limit_per_label=1600):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.limit_per_label = limit_per_label\n",
    "        self.classes = np.unique(y)\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.indices) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        X_batch = self.X[indices]\n",
    "        y_batch = self.y[indices]\n",
    "        return X_batch, y_batch\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indices = []\n",
    "        for cls in self.classes:\n",
    "            cls_indices = np.where(self.y == cls)[0]\n",
    "            if len(cls_indices) > self.limit_per_label:\n",
    "                cls_indices = np.random.choice(cls_indices, self.limit_per_label, replace=False)\n",
    "            self.indices.extend(cls_indices)\n",
    "        np.random.shuffle(self.indices)\n",
    "\n",
    "# Custom BalancedDataGenerator class for validation (400 per class)\n",
    "class BalancedValidationGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, X, y, batch_size=32, limit_per_label=400):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.limit_per_label = limit_per_label\n",
    "        self.classes = np.unique(y)\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.indices) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        X_batch = self.X[indices]\n",
    "        y_batch = self.y[indices]\n",
    "        return X_batch, y_batch\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indices = []\n",
    "        for cls in self.classes:\n",
    "            cls_indices = np.where(self.y == cls)[0]\n",
    "            if len(cls_indices) > self.limit_per_label:\n",
    "                cls_indices = np.random.choice(cls_indices, self.limit_per_label, replace=False)\n",
    "            self.indices.extend(cls_indices)\n",
    "        np.random.shuffle(self.indices)\n",
    "\n",
    "# Create the Conv1D model\n",
    "def create_convnet(input_shape, num_classes, \n",
    "                   num_filters=[128, 128, 128, 128, 128, 128, 128, 128], \n",
    "                   kernel_size=9,\n",
    "                   dense_units=[256, 256, 256, 128, 128, 128, 64, 64, 64, 32, 32, 32, \n",
    "                                16, 16, 16, 8, 8, 8],\n",
    "                   dropout_rate=0.2,\n",
    "                   padding='same'):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    \n",
    "    # First convolutional layer\n",
    "    model.add(tf.keras.layers.Conv1D(filters=num_filters[0], kernel_size=kernel_size, \n",
    "                                     activation='relu', input_shape=input_shape, padding=padding))\n",
    "    model.add(tf.keras.layers.MaxPooling1D(pool_size=2))\n",
    "    \n",
    "    # Additional convolutional layers\n",
    "    for filters in num_filters[1:]:\n",
    "        model.add(tf.keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, \n",
    "                                         activation='relu', padding=padding))\n",
    "        model.add(tf.keras.layers.MaxPooling1D(pool_size=2))\n",
    "        model.add(tf.keras.layers.Dropout(rate=dropout_rate))\n",
    "    \n",
    "    # Flatten the output and add dense layers\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "    for units in dense_units:\n",
    "        model.add(tf.keras.layers.Dense(units=units, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dropout(rate=dropout_rate))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(tf.keras.layers.Dense(units=num_classes, activation='softmax'))\n",
    "\n",
    "    # Optimizer and loss function\n",
    "    optimizer_ = tf.keras.optimizers.AdamW(learning_rate=1e-4) \n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizer_, \n",
    "                  loss='sparse_categorical_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Function to train the model\n",
    "def train_convnet(model, train_dataset, val_dataset, limit_per_label=1600, epochs=1, batch_size=32, patience=5):\n",
    "    # Define early stopping callback\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n",
    "\n",
    "    # Fit the model\n",
    "    history = model.fit(train_dataset,\n",
    "                        validation_data=val_dataset,\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        callbacks=[early_stopping])\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Function to train the model multiple times\n",
    "def train_convnet_many_times(model, train_dataset, val_dataset, epochs_per_run=1, batch_size=32, num_runs=10, limit_per_label=1600):\n",
    "    histories = []\n",
    "    for i in range(num_runs):\n",
    "        print(f\"Training run {i+1}/{num_runs}...\")\n",
    "        history = train_convnet(model, train_dataset, val_dataset, limit_per_label=limit_per_label, epochs=epochs_per_run, batch_size=batch_size)\n",
    "        histories.append(history)\n",
    "    \n",
    "    return histories\n",
    "\n",
    "# Function to print confusion matrix and classification report\n",
    "def print_confusion_matrix(convnet_model, val_spectra, val_labels):   \n",
    "    val_predictions = convnet_model.predict(val_spectra)\n",
    "    predicted_labels = np.argmax(val_predictions, axis=1)\n",
    "    true_labels = np.array(val_labels)\n",
    "\n",
    "    conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(true_labels, predicted_labels))\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Star', 'Binary Star','Galaxy', 'AGN'], yticklabels=['Star', 'Binary Star','Galaxy', 'AGN'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jcwind/.local/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1728926176.138275  217035 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728926176.319108  217035 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728926176.319266  217035 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728926176.322108  217035 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728926176.322211  217035 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728926176.322265  217035 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728926176.496219  217035 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1728926176.496441  217035 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-14 18:16:16.496470: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2112] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1728926176.496630  217035 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-14 18:16:16.497442: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13717 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training run 1/100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jcwind/.local/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1728926181.379788  217834 service.cc:146] XLA service 0x7f9db0005450 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1728926181.379842  217834 service.cc:154]   StreamExecutor device (0): NVIDIA GeForce RTX 3080 Laptop GPU, Compute Capability 8.6\n",
      "2024-10-14 18:16:21.515551: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-10-14 18:16:22.099893: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8907\n",
      "I0000 00:00:1728926230.134289  217834 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 414ms/step - accuracy: 0.3617 - loss: 8.7085 - val_accuracy: 0.3865 - val_loss: 1.2942\n",
      "Training run 2/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 374ms/step - accuracy: 0.4184 - loss: 1.3168 - val_accuracy: 0.3820 - val_loss: 1.2735\n",
      "Training run 3/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 375ms/step - accuracy: 0.4130 - loss: 1.2563 - val_accuracy: 0.4318 - val_loss: 1.1753\n",
      "Training run 4/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 376ms/step - accuracy: 0.4635 - loss: 1.1558 - val_accuracy: 0.4452 - val_loss: 1.0869\n",
      "Training run 5/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 374ms/step - accuracy: 0.4923 - loss: 1.1351 - val_accuracy: 0.5695 - val_loss: 1.0590\n",
      "Training run 6/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 377ms/step - accuracy: 0.5045 - loss: 1.1417 - val_accuracy: 0.5402 - val_loss: 1.0509\n",
      "Training run 7/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 371ms/step - accuracy: 0.5502 - loss: 1.0683 - val_accuracy: 0.5497 - val_loss: 1.0327\n",
      "Training run 8/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 365ms/step - accuracy: 0.5525 - loss: 1.0246 - val_accuracy: 0.5370 - val_loss: 1.0135\n",
      "Training run 9/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 368ms/step - accuracy: 0.5649 - loss: 1.0052 - val_accuracy: 0.5695 - val_loss: 0.9282\n",
      "Training run 10/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 367ms/step - accuracy: 0.5960 - loss: 0.9613 - val_accuracy: 0.6078 - val_loss: 0.8714\n",
      "Training run 11/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 369ms/step - accuracy: 0.5994 - loss: 0.9231 - val_accuracy: 0.5651 - val_loss: 1.0351\n",
      "Training run 12/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 369ms/step - accuracy: 0.5833 - loss: 0.9275 - val_accuracy: 0.6014 - val_loss: 0.9912\n",
      "Training run 13/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 370ms/step - accuracy: 0.6096 - loss: 0.8747 - val_accuracy: 0.6346 - val_loss: 0.7466\n",
      "Training run 14/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 369ms/step - accuracy: 0.6032 - loss: 0.8923 - val_accuracy: 0.5746 - val_loss: 0.9515\n",
      "Training run 15/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 371ms/step - accuracy: 0.6117 - loss: 0.8783 - val_accuracy: 0.6173 - val_loss: 0.8629\n",
      "Training run 16/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 370ms/step - accuracy: 0.6158 - loss: 1.0674 - val_accuracy: 0.5230 - val_loss: 1.0621\n",
      "Training run 17/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 365ms/step - accuracy: 0.5739 - loss: 0.9824 - val_accuracy: 0.5906 - val_loss: 0.8961\n",
      "Training run 18/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 367ms/step - accuracy: 0.6010 - loss: 0.8967 - val_accuracy: 0.6282 - val_loss: 0.8285\n",
      "Training run 19/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 369ms/step - accuracy: 0.6240 - loss: 0.8820 - val_accuracy: 0.6046 - val_loss: 0.9204\n",
      "Training run 20/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 371ms/step - accuracy: 0.6091 - loss: 0.9058 - val_accuracy: 0.5912 - val_loss: 0.8829\n",
      "Training run 21/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 369ms/step - accuracy: 0.6317 - loss: 0.8332 - val_accuracy: 0.6365 - val_loss: 0.8062\n",
      "Training run 22/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 368ms/step - accuracy: 0.6223 - loss: 0.8262 - val_accuracy: 0.6486 - val_loss: 0.7935\n",
      "Training run 23/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 366ms/step - accuracy: 0.6272 - loss: 0.7932 - val_accuracy: 0.6473 - val_loss: 0.7566\n",
      "Training run 24/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 364ms/step - accuracy: 0.6482 - loss: 0.7636 - val_accuracy: 0.6422 - val_loss: 0.7930\n",
      "Training run 25/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 362ms/step - accuracy: 0.6520 - loss: 0.7510 - val_accuracy: 0.6633 - val_loss: 0.7155\n",
      "Training run 26/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 365ms/step - accuracy: 0.6703 - loss: 0.7322 - val_accuracy: 0.6397 - val_loss: 0.7442\n",
      "Training run 27/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 365ms/step - accuracy: 0.6650 - loss: 0.6925 - val_accuracy: 0.6971 - val_loss: 0.6304\n",
      "Training run 28/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 361ms/step - accuracy: 0.6882 - loss: 0.6733 - val_accuracy: 0.6843 - val_loss: 0.6627\n",
      "Training run 29/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 363ms/step - accuracy: 0.6680 - loss: 0.8272 - val_accuracy: 0.5765 - val_loss: 1.0839\n",
      "Training run 30/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 362ms/step - accuracy: 0.5527 - loss: 1.3575 - val_accuracy: 0.5753 - val_loss: 1.0759\n",
      "Training run 31/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 354ms/step - accuracy: 0.6048 - loss: 0.8840 - val_accuracy: 0.6480 - val_loss: 0.7926\n",
      "Training run 32/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 354ms/step - accuracy: 0.6446 - loss: 0.7674 - val_accuracy: 0.6218 - val_loss: 0.8409\n",
      "Training run 33/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 356ms/step - accuracy: 0.6570 - loss: 0.7537 - val_accuracy: 0.6441 - val_loss: 0.7575\n",
      "Training run 34/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 354ms/step - accuracy: 0.6713 - loss: 0.7010 - val_accuracy: 0.6875 - val_loss: 0.6798\n",
      "Training run 35/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 356ms/step - accuracy: 0.6782 - loss: 0.7023 - val_accuracy: 0.6614 - val_loss: 0.7348\n",
      "Training run 36/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 353ms/step - accuracy: 0.6568 - loss: 0.8023 - val_accuracy: 0.6901 - val_loss: 0.6340\n",
      "Training run 37/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 354ms/step - accuracy: 0.6930 - loss: 0.6486 - val_accuracy: 0.6990 - val_loss: 0.6300\n",
      "Training run 38/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 354ms/step - accuracy: 0.6997 - loss: 0.6405 - val_accuracy: 0.7226 - val_loss: 0.6265\n",
      "Training run 39/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 353ms/step - accuracy: 0.7036 - loss: 0.7315 - val_accuracy: 0.5957 - val_loss: 0.8208\n",
      "Training run 40/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 353ms/step - accuracy: 0.5839 - loss: 0.9564 - val_accuracy: 0.6441 - val_loss: 0.7992\n",
      "Training run 41/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 356ms/step - accuracy: 0.6639 - loss: 0.7183 - val_accuracy: 0.6320 - val_loss: 0.7809\n",
      "Training run 42/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 352ms/step - accuracy: 0.6404 - loss: 0.8629 - val_accuracy: 0.6097 - val_loss: 0.8416\n",
      "Training run 43/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 355ms/step - accuracy: 0.6160 - loss: 0.8319 - val_accuracy: 0.6486 - val_loss: 0.7272\n",
      "Training run 44/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 352ms/step - accuracy: 0.6583 - loss: 0.7698 - val_accuracy: 0.6869 - val_loss: 0.6497\n",
      "Training run 45/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 352ms/step - accuracy: 0.6887 - loss: 0.6661 - val_accuracy: 0.7034 - val_loss: 0.5967\n",
      "Training run 46/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 352ms/step - accuracy: 0.6856 - loss: 0.6712 - val_accuracy: 0.7143 - val_loss: 0.5543\n",
      "Training run 47/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 350ms/step - accuracy: 0.7094 - loss: 0.6095 - val_accuracy: 0.7175 - val_loss: 0.5781\n",
      "Training run 48/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 353ms/step - accuracy: 0.7057 - loss: 0.6069 - val_accuracy: 0.7302 - val_loss: 0.5526\n",
      "Training run 49/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 351ms/step - accuracy: 0.7218 - loss: 0.5821 - val_accuracy: 0.7455 - val_loss: 0.5347\n",
      "Training run 50/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 353ms/step - accuracy: 0.7261 - loss: 0.5701 - val_accuracy: 0.7398 - val_loss: 0.5217\n",
      "Training run 51/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 352ms/step - accuracy: 0.7408 - loss: 0.5253 - val_accuracy: 0.7436 - val_loss: 0.5665\n",
      "Training run 52/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 352ms/step - accuracy: 0.7395 - loss: 0.5311 - val_accuracy: 0.7341 - val_loss: 0.5364\n",
      "Training run 53/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 351ms/step - accuracy: 0.7405 - loss: 0.5450 - val_accuracy: 0.7526 - val_loss: 0.4850\n",
      "Training run 54/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 350ms/step - accuracy: 0.7406 - loss: 0.5358 - val_accuracy: 0.6958 - val_loss: 0.5892\n",
      "Training run 55/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 353ms/step - accuracy: 0.6668 - loss: 0.7433 - val_accuracy: 0.6990 - val_loss: 0.6232\n",
      "Training run 56/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 353ms/step - accuracy: 0.6965 - loss: 0.6456 - val_accuracy: 0.7290 - val_loss: 0.5498\n",
      "Training run 57/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 355ms/step - accuracy: 0.7226 - loss: 0.5903 - val_accuracy: 0.7283 - val_loss: 0.5460\n",
      "Training run 58/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 353ms/step - accuracy: 0.7489 - loss: 0.5552 - val_accuracy: 0.7545 - val_loss: 0.5178\n",
      "Training run 59/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 354ms/step - accuracy: 0.7423 - loss: 0.5475 - val_accuracy: 0.7506 - val_loss: 0.5129\n",
      "Training run 60/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 353ms/step - accuracy: 0.7466 - loss: 0.5385 - val_accuracy: 0.6722 - val_loss: 0.7371\n",
      "Training run 61/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 353ms/step - accuracy: 0.7006 - loss: 0.6129 - val_accuracy: 0.7761 - val_loss: 0.5120\n",
      "Training run 62/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 355ms/step - accuracy: 0.7476 - loss: 0.5403 - val_accuracy: 0.7392 - val_loss: 0.5339\n",
      "Training run 63/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 352ms/step - accuracy: 0.7284 - loss: 0.5472 - val_accuracy: 0.7258 - val_loss: 0.5338\n",
      "Training run 64/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 352ms/step - accuracy: 0.7412 - loss: 0.5423 - val_accuracy: 0.7895 - val_loss: 0.4894\n",
      "Training run 65/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 354ms/step - accuracy: 0.7665 - loss: 0.5088 - val_accuracy: 0.6990 - val_loss: 0.6803\n",
      "Training run 66/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 365ms/step - accuracy: 0.7285 - loss: 0.5459 - val_accuracy: 0.7717 - val_loss: 0.4942\n",
      "Training run 67/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 357ms/step - accuracy: 0.7552 - loss: 0.5070 - val_accuracy: 0.7698 - val_loss: 0.5004\n",
      "Training run 68/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 356ms/step - accuracy: 0.6427 - loss: 1.0016 - val_accuracy: 0.6129 - val_loss: 0.9205\n",
      "Training run 69/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 354ms/step - accuracy: 0.6667 - loss: 0.7515 - val_accuracy: 0.7232 - val_loss: 0.5910\n",
      "Training run 70/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 356ms/step - accuracy: 0.7084 - loss: 0.6246 - val_accuracy: 0.7589 - val_loss: 0.4886\n",
      "Training run 71/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 358ms/step - accuracy: 0.7312 - loss: 0.5416 - val_accuracy: 0.7468 - val_loss: 0.5026\n",
      "Training run 72/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 356ms/step - accuracy: 0.7472 - loss: 0.5244 - val_accuracy: 0.7710 - val_loss: 0.4841\n",
      "Training run 73/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 355ms/step - accuracy: 0.7335 - loss: 0.5436 - val_accuracy: 0.7832 - val_loss: 0.4831\n",
      "Training run 74/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 355ms/step - accuracy: 0.7591 - loss: 0.5144 - val_accuracy: 0.7800 - val_loss: 0.4583\n",
      "Training run 75/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 355ms/step - accuracy: 0.7557 - loss: 0.4932 - val_accuracy: 0.7666 - val_loss: 0.8646\n",
      "Training run 76/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 354ms/step - accuracy: 0.7527 - loss: 0.5235 - val_accuracy: 0.7736 - val_loss: 0.4810\n",
      "Training run 77/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 353ms/step - accuracy: 0.7536 - loss: 0.5089 - val_accuracy: 0.7864 - val_loss: 0.4475\n",
      "Training run 78/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 351ms/step - accuracy: 0.7764 - loss: 0.4784 - val_accuracy: 0.7615 - val_loss: 0.4775\n",
      "Training run 79/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 354ms/step - accuracy: 0.7693 - loss: 0.4695 - val_accuracy: 0.7902 - val_loss: 0.4525\n",
      "Training run 80/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 353ms/step - accuracy: 0.7865 - loss: 0.4640 - val_accuracy: 0.7819 - val_loss: 0.4765\n",
      "Training run 81/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 353ms/step - accuracy: 0.7669 - loss: 0.4807 - val_accuracy: 0.7768 - val_loss: 0.4646\n",
      "Training run 82/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 353ms/step - accuracy: 0.7944 - loss: 0.4354 - val_accuracy: 0.8010 - val_loss: 0.4335\n",
      "Training run 83/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 353ms/step - accuracy: 0.7761 - loss: 0.4620 - val_accuracy: 0.7430 - val_loss: 0.5776\n",
      "Training run 84/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 353ms/step - accuracy: 0.7392 - loss: 0.5357 - val_accuracy: 0.7959 - val_loss: 0.4585\n",
      "Training run 85/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 352ms/step - accuracy: 0.7805 - loss: 0.4612 - val_accuracy: 0.7851 - val_loss: 0.4655\n",
      "Training run 86/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 352ms/step - accuracy: 0.8025 - loss: 0.4395 - val_accuracy: 0.7870 - val_loss: 0.6594\n",
      "Training run 87/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 352ms/step - accuracy: 0.7953 - loss: 0.4492 - val_accuracy: 0.7564 - val_loss: 0.5143\n",
      "Training run 88/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 354ms/step - accuracy: 0.7415 - loss: 2.6802 - val_accuracy: 0.7838 - val_loss: 0.4632\n",
      "Training run 89/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 354ms/step - accuracy: 0.7557 - loss: 0.5038 - val_accuracy: 0.7851 - val_loss: 0.4759\n",
      "Training run 90/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 353ms/step - accuracy: 0.7775 - loss: 0.4648 - val_accuracy: 0.7889 - val_loss: 0.4483\n",
      "Training run 91/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 354ms/step - accuracy: 0.7897 - loss: 0.4526 - val_accuracy: 0.7774 - val_loss: 0.4535\n",
      "Training run 92/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 355ms/step - accuracy: 0.7561 - loss: 0.6470 - val_accuracy: 0.7781 - val_loss: 0.4685\n",
      "Training run 93/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 354ms/step - accuracy: 0.7807 - loss: 0.4819 - val_accuracy: 0.7940 - val_loss: 0.4475\n",
      "Training run 94/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 353ms/step - accuracy: 0.7790 - loss: 0.4784 - val_accuracy: 0.7736 - val_loss: 0.4795\n",
      "Training run 95/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 356ms/step - accuracy: 0.7824 - loss: 0.4630 - val_accuracy: 0.8055 - val_loss: 0.4239\n",
      "Training run 96/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 351ms/step - accuracy: 0.8053 - loss: 0.4311 - val_accuracy: 0.7997 - val_loss: 0.4431\n",
      "Training run 97/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 356ms/step - accuracy: 0.7803 - loss: 0.4519 - val_accuracy: 0.7902 - val_loss: 0.4315\n",
      "Training run 98/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 353ms/step - accuracy: 0.7947 - loss: 0.4291 - val_accuracy: 0.7691 - val_loss: 0.4712\n",
      "Training run 99/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 354ms/step - accuracy: 0.7941 - loss: 0.4388 - val_accuracy: 0.8119 - val_loss: 0.4435\n",
      "Training run 100/100...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 355ms/step - accuracy: 0.7945 - loss: 0.4322 - val_accuracy: 0.7985 - val_loss: 0.4183\n"
     ]
    }
   ],
   "source": [
    "# Custom BalancedDataGenerator class for training\n",
    "class BalancedDataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, X, y, batch_size=32, limit_per_label=1600):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.limit_per_label = limit_per_label\n",
    "        self.classes = np.unique(y)\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.indices) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        X_batch = self.X[indices]\n",
    "        y_batch = self.y[indices]\n",
    "        return X_batch, y_batch\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indices = []\n",
    "        for cls in self.classes:\n",
    "            cls_indices = np.where(self.y == cls)[0]\n",
    "            if len(cls_indices) > self.limit_per_label:\n",
    "                cls_indices = np.random.choice(cls_indices, self.limit_per_label, replace=False)\n",
    "            self.indices.extend(cls_indices)\n",
    "        np.random.shuffle(self.indices)\n",
    "\n",
    "# Custom BalancedDataGenerator class for validation (400 per class)\n",
    "class BalancedValidationGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, X, y, batch_size=32, limit_per_label=400):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.limit_per_label = limit_per_label\n",
    "        self.classes = np.unique(y)\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.indices) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        X_batch = self.X[indices]\n",
    "        y_batch = self.y[indices]\n",
    "        return X_batch, y_batch\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.indices = []\n",
    "        for cls in self.classes:\n",
    "            cls_indices = np.where(self.y == cls)[0]\n",
    "            if len(cls_indices) > self.limit_per_label:\n",
    "                cls_indices = np.random.choice(cls_indices, self.limit_per_label, replace=False)\n",
    "            self.indices.extend(cls_indices)\n",
    "        np.random.shuffle(self.indices)\n",
    "\n",
    "# Create the Conv1D model\n",
    "def create_convnet(input_shape, num_classes, \n",
    "                   num_filters=[128, 128, 128, 128, 128, 128, 128, 128], \n",
    "                   kernel_size=9,\n",
    "                   dense_units1=256, \n",
    "                   dense_units2=128,\n",
    "                   dense_units3=64,\n",
    "                   dropout_rate=0.2,\n",
    "                   padding='same'):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    \n",
    "    # First convolutional layer\n",
    "    model.add(tf.keras.layers.Conv1D(filters=num_filters[0], kernel_size=kernel_size, \n",
    "                                     activation='relu', input_shape=input_shape, padding=padding))\n",
    "    model.add(tf.keras.layers.MaxPooling1D(pool_size=2))\n",
    "    \n",
    "    # Additional convolutional layers\n",
    "    for filters in num_filters[1:]:\n",
    "        model.add(tf.keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, \n",
    "                                         activation='relu', padding=padding))\n",
    "        model.add(tf.keras.layers.MaxPooling1D(pool_size=2))\n",
    "        model.add(tf.keras.layers.Dropout(rate=dropout_rate))\n",
    "    \n",
    "    # Flatten the output and add dense layers\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(units=dense_units1, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(rate=dropout_rate))\n",
    "\n",
    "    # Adding another dense layer\n",
    "    if dense_units2:\n",
    "        model.add(tf.keras.layers.Dense(units=dense_units2, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dropout(rate=dropout_rate))\n",
    "\n",
    "    # Adding another dense layer\n",
    "    if dense_units3:\n",
    "        model.add(tf.keras.layers.Dense(units=dense_units3, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dropout(rate=dropout_rate))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(tf.keras.layers.Dense(units=num_classes, activation='softmax'))\n",
    "\n",
    "    # Optimizer and loss function\n",
    "    optimizer_ = tf.keras.optimizers.AdamW(learning_rate=1e-4) \n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizer_, \n",
    "                  loss='sparse_categorical_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Function to train the model\n",
    "def train_convnet(model, train_dataset, val_dataset, limit_per_label=1600, epochs=1, batch_size=32, patience=5):\n",
    "    # Define early stopping callback\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n",
    "\n",
    "    # Fit the model\n",
    "    history = model.fit(train_dataset,\n",
    "                        validation_data=val_dataset,\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        callbacks=[early_stopping])\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Function to train the model multiple times\n",
    "def train_convnet_many_times(model, train_dataset, val_dataset, epochs_per_run=1, batch_size=32, num_runs=10, limit_per_label=1600):\n",
    "    histories = []\n",
    "    for i in range(num_runs):\n",
    "        print(f\"Training run {i+1}/{num_runs}...\")\n",
    "        history = train_convnet(model, train_dataset, val_dataset, limit_per_label=limit_per_label, epochs=epochs_per_run, batch_size=batch_size)\n",
    "        histories.append(history)\n",
    "    \n",
    "    return histories\n",
    "\n",
    "# Function to print confusion matrix and classification report\n",
    "def print_confusion_matrix(convnet_model, val_spectra, val_labels):   \n",
    "    val_predictions = convnet_model.predict(val_spectra)\n",
    "    predicted_labels = np.argmax(val_predictions, axis=1)\n",
    "    true_labels = np.array(val_labels)\n",
    "\n",
    "    conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(true_labels, predicted_labels))\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Star', 'Binary Star','Galaxy', 'AGN'], yticklabels=['Star', 'Binary Star','Galaxy', 'AGN'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "# Load and preprocess data\n",
    "X = pd.read_pickle(\"Pickles/fusionv0/all.pkl\")\n",
    "y = X[\"label\"]\n",
    "label_mapping = {'star': 0, 'binary_star': 1, 'galaxy': 2, 'agn': 3}\n",
    "y = y.map(label_mapping) if isinstance(y, pd.Series) else np.vectorize(label_mapping.get)(y)\n",
    "\n",
    "X = X.drop([\"parallax\", \"ra\", \"dec\", \"ra_error\", \"dec_error\", \"parallax_error\", \"pmra\", \"pmdec\", \"pmra_error\", \"pmdec_error\", \n",
    "            \"phot_g_mean_flux\", \"flagnopllx\", \"phot_g_mean_flux_error\", \"phot_bp_mean_flux\", \"phot_rp_mean_flux\", \n",
    "            \"phot_bp_mean_flux_error\", \"phot_rp_mean_flux_error\", \"obsid\", \"label\"], axis=1)\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train = np.expand_dims(X_train.to_numpy(), axis=-1)\n",
    "y_train = y_train.to_numpy()\n",
    "X_val = np.expand_dims(X_val.to_numpy(), axis=-1)\n",
    "y_val = y_val.to_numpy()\n",
    "\n",
    "# Clear memory\n",
    "del X, y\n",
    "gc.collect()\n",
    "\n",
    "# Create data generators\n",
    "train_generator = BalancedDataGenerator(X_train, y_train, batch_size=32, limit_per_label=1600)\n",
    "val_generator = BalancedValidationGenerator(X_val, y_val, batch_size=32, limit_per_label=400)\n",
    "\n",
    "# Define input shape and number of classes\n",
    "input_shape = (3748, 1)\n",
    "num_classes = len(np.unique(y_val))\n",
    "\n",
    "# Create and train the model\n",
    "filters=[1024, 512, 512, 512, 512, 512, 256, 256, 256, 256, 256]\n",
    "model = create_convnet(input_shape, num_classes, num_filters=filters, kernel_size=(9,))\n",
    "\n",
    "                          \n",
    "histories = train_convnet_many_times(model, train_generator, val_generator, epochs_per_run=1, num_runs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training run 1/300...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-14 21:07:49.234889: E external/local_xla/xla/service/slow_operation_alarm.cc:65] Trying algorithm eng0{} for conv (f32[1024,512,1,9]{3,2,1,0}, u8[0]{0}) custom-call(f32[1024,32,1,1874]{3,2,1,0}, f32[512,32,1,1874]{3,2,1,0}), window={size=1x1874 pad=0_0x4_4}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2024-10-14 21:07:50.608713: E external/local_xla/xla/service/slow_operation_alarm.cc:133] The operation took 2.373956725s\n",
      "Trying algorithm eng0{} for conv (f32[1024,512,1,9]{3,2,1,0}, u8[0]{0}) custom-call(f32[1024,32,1,1874]{3,2,1,0}, f32[512,32,1,1874]{3,2,1,0}), window={size=1x1874 pad=0_0x4_4}, dim_labels=bf01_oi01->bf01, custom_call_target=\"__cudnn$convForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 242ms/step - accuracy: 0.3272 - loss: 8.4759 - val_accuracy: 0.3565 - val_loss: 1.3476\n",
      "Training run 2/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 235ms/step - accuracy: 0.4235 - loss: 1.3207 - val_accuracy: 0.3514 - val_loss: 1.3204\n",
      "Training run 3/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 235ms/step - accuracy: 0.4344 - loss: 1.2798 - val_accuracy: 0.3884 - val_loss: 1.2599\n",
      "Training run 4/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 238ms/step - accuracy: 0.4212 - loss: 1.3178 - val_accuracy: 0.3705 - val_loss: 1.2751\n",
      "Training run 5/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 237ms/step - accuracy: 0.4345 - loss: 1.1987 - val_accuracy: 0.4037 - val_loss: 1.2103\n",
      "Training run 6/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 240ms/step - accuracy: 0.4492 - loss: 1.1797 - val_accuracy: 0.4254 - val_loss: 1.1879\n",
      "Training run 7/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 239ms/step - accuracy: 0.4535 - loss: 1.1316 - val_accuracy: 0.3992 - val_loss: 1.2242\n",
      "Training run 8/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 241ms/step - accuracy: 0.4928 - loss: 1.0682 - val_accuracy: 0.4311 - val_loss: 1.1161\n",
      "Training run 9/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 239ms/step - accuracy: 0.5056 - loss: 1.0909 - val_accuracy: 0.4783 - val_loss: 1.0759\n",
      "Training run 10/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 231ms/step - accuracy: 0.5405 - loss: 1.0318 - val_accuracy: 0.4662 - val_loss: 1.1232\n",
      "Training run 11/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 232ms/step - accuracy: 0.5158 - loss: 1.2344 - val_accuracy: 0.4088 - val_loss: 1.2725\n",
      "Training run 12/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 231ms/step - accuracy: 0.5384 - loss: 1.0868 - val_accuracy: 0.4847 - val_loss: 1.1117\n",
      "Training run 13/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 229ms/step - accuracy: 0.5735 - loss: 1.0179 - val_accuracy: 0.5676 - val_loss: 0.9934\n",
      "Training run 14/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 229ms/step - accuracy: 0.5683 - loss: 0.9900 - val_accuracy: 0.5045 - val_loss: 1.0939\n",
      "Training run 15/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 229ms/step - accuracy: 0.5743 - loss: 0.9876 - val_accuracy: 0.5670 - val_loss: 1.0026\n",
      "Training run 16/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 230ms/step - accuracy: 0.5801 - loss: 0.9465 - val_accuracy: 0.5198 - val_loss: 1.0348\n",
      "Training run 17/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 230ms/step - accuracy: 0.5994 - loss: 0.9444 - val_accuracy: 0.5293 - val_loss: 0.9798\n",
      "Training run 18/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 229ms/step - accuracy: 0.5896 - loss: 0.9253 - val_accuracy: 0.5319 - val_loss: 0.9976\n",
      "Training run 19/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 230ms/step - accuracy: 0.5953 - loss: 0.9236 - val_accuracy: 0.5848 - val_loss: 0.9120\n",
      "Training run 20/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 230ms/step - accuracy: 0.5865 - loss: 0.9063 - val_accuracy: 0.4656 - val_loss: 1.0823\n",
      "Training run 21/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 230ms/step - accuracy: 0.5840 - loss: 0.9300 - val_accuracy: 0.5510 - val_loss: 1.0124\n",
      "Training run 22/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 229ms/step - accuracy: 0.5863 - loss: 0.9663 - val_accuracy: 0.5619 - val_loss: 1.0114\n",
      "Training run 23/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 229ms/step - accuracy: 0.6055 - loss: 0.8935 - val_accuracy: 0.5893 - val_loss: 0.9027\n",
      "Training run 24/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 230ms/step - accuracy: 0.5999 - loss: 0.8923 - val_accuracy: 0.5950 - val_loss: 0.9083\n",
      "Training run 25/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 231ms/step - accuracy: 0.5967 - loss: 0.8743 - val_accuracy: 0.6295 - val_loss: 0.8217\n",
      "Training run 26/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.6261 - loss: 0.8190 - val_accuracy: 0.5982 - val_loss: 0.8545\n",
      "Training run 27/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 230ms/step - accuracy: 0.6291 - loss: 0.8173 - val_accuracy: 0.6065 - val_loss: 0.8310\n",
      "Training run 28/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 230ms/step - accuracy: 0.6138 - loss: 0.8384 - val_accuracy: 0.6122 - val_loss: 0.8012\n",
      "Training run 29/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 229ms/step - accuracy: 0.5823 - loss: 1.2212 - val_accuracy: 0.5625 - val_loss: 0.9313\n",
      "Training run 30/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 230ms/step - accuracy: 0.6070 - loss: 0.8522 - val_accuracy: 0.5976 - val_loss: 0.9175\n",
      "Training run 31/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 229ms/step - accuracy: 0.6429 - loss: 0.7808 - val_accuracy: 0.6084 - val_loss: 0.8807\n",
      "Training run 32/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 229ms/step - accuracy: 0.6331 - loss: 0.8029 - val_accuracy: 0.6358 - val_loss: 0.8346\n",
      "Training run 33/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 229ms/step - accuracy: 0.6275 - loss: 0.8135 - val_accuracy: 0.6027 - val_loss: 0.8811\n",
      "Training run 34/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 230ms/step - accuracy: 0.6512 - loss: 0.7573 - val_accuracy: 0.6224 - val_loss: 0.7993\n",
      "Training run 35/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 230ms/step - accuracy: 0.6614 - loss: 0.7415 - val_accuracy: 0.6346 - val_loss: 0.7470\n",
      "Training run 36/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 229ms/step - accuracy: 0.6525 - loss: 0.7497 - val_accuracy: 0.5989 - val_loss: 0.8671\n",
      "Training run 37/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 229ms/step - accuracy: 0.6288 - loss: 0.8700 - val_accuracy: 0.6008 - val_loss: 0.8796\n",
      "Training run 38/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 229ms/step - accuracy: 0.6034 - loss: 0.8927 - val_accuracy: 0.5459 - val_loss: 0.9923\n",
      "Training run 39/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 229ms/step - accuracy: 0.6185 - loss: 0.8694 - val_accuracy: 0.6193 - val_loss: 0.7881\n",
      "Training run 40/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 229ms/step - accuracy: 0.6284 - loss: 0.7946 - val_accuracy: 0.6397 - val_loss: 0.7634\n",
      "Training run 41/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 230ms/step - accuracy: 0.6013 - loss: 1.0981 - val_accuracy: 0.5861 - val_loss: 0.9457\n",
      "Training run 42/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 230ms/step - accuracy: 0.6215 - loss: 0.8115 - val_accuracy: 0.6020 - val_loss: 0.9417\n",
      "Training run 43/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 229ms/step - accuracy: 0.6388 - loss: 0.7862 - val_accuracy: 0.6218 - val_loss: 0.8391\n",
      "Training run 44/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 232ms/step - accuracy: 0.6485 - loss: 0.7721 - val_accuracy: 0.6422 - val_loss: 0.7839\n",
      "Training run 45/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 230ms/step - accuracy: 0.6485 - loss: 0.7660 - val_accuracy: 0.6110 - val_loss: 0.8425\n",
      "Training run 46/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.5980 - loss: 0.8600 - val_accuracy: 0.6193 - val_loss: 0.8710\n",
      "Training run 47/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 231ms/step - accuracy: 0.6546 - loss: 0.7788 - val_accuracy: 0.6371 - val_loss: 0.7790\n",
      "Training run 48/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 227ms/step - accuracy: 0.6535 - loss: 0.7694 - val_accuracy: 0.6492 - val_loss: 0.8291\n",
      "Training run 49/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 229ms/step - accuracy: 0.6575 - loss: 0.7430 - val_accuracy: 0.6607 - val_loss: 0.7919\n",
      "Training run 50/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 229ms/step - accuracy: 0.6557 - loss: 0.7730 - val_accuracy: 0.6460 - val_loss: 0.7917\n",
      "Training run 51/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 238ms/step - accuracy: 0.6776 - loss: 0.7285 - val_accuracy: 0.6486 - val_loss: 0.7835\n",
      "Training run 52/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 232ms/step - accuracy: 0.6709 - loss: 0.7233 - val_accuracy: 0.6671 - val_loss: 0.7656\n",
      "Training run 53/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 229ms/step - accuracy: 0.6627 - loss: 0.7457 - val_accuracy: 0.6473 - val_loss: 0.7933\n",
      "Training run 54/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 232ms/step - accuracy: 0.6773 - loss: 0.7419 - val_accuracy: 0.6601 - val_loss: 0.7353\n",
      "Training run 55/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 230ms/step - accuracy: 0.6816 - loss: 0.7019 - val_accuracy: 0.6709 - val_loss: 0.7192\n",
      "Training run 56/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 231ms/step - accuracy: 0.6912 - loss: 0.7149 - val_accuracy: 0.6658 - val_loss: 0.7102\n",
      "Training run 57/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 231ms/step - accuracy: 0.6962 - loss: 0.6978 - val_accuracy: 0.7168 - val_loss: 0.6237\n",
      "Training run 58/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 233ms/step - accuracy: 0.6779 - loss: 0.6820 - val_accuracy: 0.6945 - val_loss: 0.6350\n",
      "Training run 59/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 230ms/step - accuracy: 0.7010 - loss: 0.6396 - val_accuracy: 0.6760 - val_loss: 0.7049\n",
      "Training run 60/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 233ms/step - accuracy: 0.6985 - loss: 0.6718 - val_accuracy: 0.7194 - val_loss: 0.6219\n",
      "Training run 61/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 231ms/step - accuracy: 0.7004 - loss: 0.6355 - val_accuracy: 0.6983 - val_loss: 0.6910\n",
      "Training run 62/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 231ms/step - accuracy: 0.7141 - loss: 0.6252 - val_accuracy: 0.7085 - val_loss: 0.5842\n",
      "Training run 63/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 233ms/step - accuracy: 0.7172 - loss: 0.6001 - val_accuracy: 0.7188 - val_loss: 0.5750\n",
      "Training run 64/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 230ms/step - accuracy: 0.7204 - loss: 0.5899 - val_accuracy: 0.6977 - val_loss: 0.6092\n",
      "Training run 65/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 231ms/step - accuracy: 0.7295 - loss: 0.5976 - val_accuracy: 0.7309 - val_loss: 0.5936\n",
      "Training run 66/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.7285 - loss: 0.5960 - val_accuracy: 0.7296 - val_loss: 0.5717\n",
      "Training run 67/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 232ms/step - accuracy: 0.7326 - loss: 0.5916 - val_accuracy: 0.7341 - val_loss: 0.5520\n",
      "Training run 68/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 227ms/step - accuracy: 0.7322 - loss: 0.5669 - val_accuracy: 0.7066 - val_loss: 0.5739\n",
      "Training run 69/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 231ms/step - accuracy: 0.7452 - loss: 0.5562 - val_accuracy: 0.7264 - val_loss: 0.5275\n",
      "Training run 70/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 231ms/step - accuracy: 0.7445 - loss: 0.5520 - val_accuracy: 0.7449 - val_loss: 0.5354\n",
      "Training run 71/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.7296 - loss: 0.5611 - val_accuracy: 0.7474 - val_loss: 0.5272\n",
      "Training run 72/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 229ms/step - accuracy: 0.7523 - loss: 0.5513 - val_accuracy: 0.7353 - val_loss: 0.5373\n",
      "Training run 73/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 230ms/step - accuracy: 0.7394 - loss: 0.5550 - val_accuracy: 0.7443 - val_loss: 0.5123\n",
      "Training run 74/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.7419 - loss: 0.5522 - val_accuracy: 0.7500 - val_loss: 0.5512\n",
      "Training run 75/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.7299 - loss: 0.6138 - val_accuracy: 0.6843 - val_loss: 0.7096\n",
      "Training run 76/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.7009 - loss: 0.6699 - val_accuracy: 0.7379 - val_loss: 0.5110\n",
      "Training run 77/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.7371 - loss: 0.5688 - val_accuracy: 0.7372 - val_loss: 0.5362\n",
      "Training run 78/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 198ms/step - accuracy: 0.7320 - loss: 0.5632 - val_accuracy: 0.7347 - val_loss: 0.5604\n",
      "Training run 79/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 222ms/step - accuracy: 0.7398 - loss: 0.5542 - val_accuracy: 0.7321 - val_loss: 0.5738\n",
      "Training run 80/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 224ms/step - accuracy: 0.7364 - loss: 0.5639 - val_accuracy: 0.7474 - val_loss: 0.5226\n",
      "Training run 81/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 229ms/step - accuracy: 0.7298 - loss: 0.5472 - val_accuracy: 0.7372 - val_loss: 0.5219\n",
      "Training run 82/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 227ms/step - accuracy: 0.7419 - loss: 0.5393 - val_accuracy: 0.7640 - val_loss: 0.4928\n",
      "Training run 83/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 227ms/step - accuracy: 0.7410 - loss: 0.5292 - val_accuracy: 0.7136 - val_loss: 0.5322\n",
      "Training run 84/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.7430 - loss: 0.5266 - val_accuracy: 0.7672 - val_loss: 0.4804\n",
      "Training run 85/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 227ms/step - accuracy: 0.7661 - loss: 0.5049 - val_accuracy: 0.7315 - val_loss: 0.5553\n",
      "Training run 86/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 227ms/step - accuracy: 0.7582 - loss: 0.5041 - val_accuracy: 0.7710 - val_loss: 0.4834\n",
      "Training run 87/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 226ms/step - accuracy: 0.7402 - loss: 0.5246 - val_accuracy: 0.7666 - val_loss: 0.4909\n",
      "Training run 88/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 226ms/step - accuracy: 0.7624 - loss: 0.5122 - val_accuracy: 0.7685 - val_loss: 0.5395\n",
      "Training run 89/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.7573 - loss: 0.4944 - val_accuracy: 0.7628 - val_loss: 0.4941\n",
      "Training run 90/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 229ms/step - accuracy: 0.7659 - loss: 0.4872 - val_accuracy: 0.7851 - val_loss: 0.4727\n",
      "Training run 91/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.7815 - loss: 0.4741 - val_accuracy: 0.7685 - val_loss: 0.4905\n",
      "Training run 92/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.7639 - loss: 0.4934 - val_accuracy: 0.7895 - val_loss: 0.4579\n",
      "Training run 93/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 226ms/step - accuracy: 0.7725 - loss: 0.4875 - val_accuracy: 0.7781 - val_loss: 0.4665\n",
      "Training run 94/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 226ms/step - accuracy: 0.7578 - loss: 0.4960 - val_accuracy: 0.7946 - val_loss: 0.4456\n",
      "Training run 95/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 230ms/step - accuracy: 0.7737 - loss: 0.4684 - val_accuracy: 0.7781 - val_loss: 0.4710\n",
      "Training run 96/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 227ms/step - accuracy: 0.7745 - loss: 0.4774 - val_accuracy: 0.7589 - val_loss: 0.4985\n",
      "Training run 97/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.7597 - loss: 0.4891 - val_accuracy: 0.7302 - val_loss: 0.5188\n",
      "Training run 98/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.7751 - loss: 0.4824 - val_accuracy: 0.7851 - val_loss: 0.4579\n",
      "Training run 99/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.7722 - loss: 0.4938 - val_accuracy: 0.7717 - val_loss: 0.4766\n",
      "Training run 100/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 227ms/step - accuracy: 0.7738 - loss: 0.4712 - val_accuracy: 0.7857 - val_loss: 0.4476\n",
      "Training run 101/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 227ms/step - accuracy: 0.7868 - loss: 0.4637 - val_accuracy: 0.8093 - val_loss: 0.4303\n",
      "Training run 102/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.7717 - loss: 0.4696 - val_accuracy: 0.7825 - val_loss: 0.4427\n",
      "Training run 103/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 227ms/step - accuracy: 0.7746 - loss: 0.4727 - val_accuracy: 0.7946 - val_loss: 0.4336\n",
      "Training run 104/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 226ms/step - accuracy: 0.7812 - loss: 0.4555 - val_accuracy: 0.7985 - val_loss: 0.4241\n",
      "Training run 105/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 229ms/step - accuracy: 0.7720 - loss: 0.4759 - val_accuracy: 0.7838 - val_loss: 0.4540\n",
      "Training run 106/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.7786 - loss: 0.4607 - val_accuracy: 0.7985 - val_loss: 0.4234\n",
      "Training run 107/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 227ms/step - accuracy: 0.7821 - loss: 0.4435 - val_accuracy: 0.7895 - val_loss: 0.4460\n",
      "Training run 108/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 226ms/step - accuracy: 0.7924 - loss: 0.4373 - val_accuracy: 0.8093 - val_loss: 0.4074\n",
      "Training run 109/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 227ms/step - accuracy: 0.7959 - loss: 0.4511 - val_accuracy: 0.8259 - val_loss: 0.3949\n",
      "Training run 110/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 225ms/step - accuracy: 0.7965 - loss: 0.4235 - val_accuracy: 0.8106 - val_loss: 0.4207\n",
      "Training run 111/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.7673 - loss: 0.4802 - val_accuracy: 0.8029 - val_loss: 0.4260\n",
      "Training run 112/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 227ms/step - accuracy: 0.7426 - loss: 0.8983 - val_accuracy: 0.7602 - val_loss: 0.4836\n",
      "Training run 113/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.7539 - loss: 0.5036 - val_accuracy: 0.7717 - val_loss: 0.4641\n",
      "Training run 114/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 227ms/step - accuracy: 0.7766 - loss: 0.4701 - val_accuracy: 0.7959 - val_loss: 0.4213\n",
      "Training run 115/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 227ms/step - accuracy: 0.7877 - loss: 0.4441 - val_accuracy: 0.8138 - val_loss: 0.4153\n",
      "Training run 116/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 225ms/step - accuracy: 0.7734 - loss: 0.4777 - val_accuracy: 0.7774 - val_loss: 0.4502\n",
      "Training run 117/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.7760 - loss: 0.4649 - val_accuracy: 0.8074 - val_loss: 0.4025\n",
      "Training run 118/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 227ms/step - accuracy: 0.7981 - loss: 0.4367 - val_accuracy: 0.7781 - val_loss: 0.4560\n",
      "Training run 119/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 225ms/step - accuracy: 0.7904 - loss: 0.4466 - val_accuracy: 0.7915 - val_loss: 0.4492\n",
      "Training run 120/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.7774 - loss: 0.4608 - val_accuracy: 0.8074 - val_loss: 0.4238\n",
      "Training run 121/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 227ms/step - accuracy: 0.7980 - loss: 0.4410 - val_accuracy: 0.7985 - val_loss: 0.4076\n",
      "Training run 122/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 227ms/step - accuracy: 0.7926 - loss: 0.4333 - val_accuracy: 0.8119 - val_loss: 0.4254\n",
      "Training run 123/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 227ms/step - accuracy: 0.7947 - loss: 0.4334 - val_accuracy: 0.7825 - val_loss: 0.4361\n",
      "Training run 124/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 227ms/step - accuracy: 0.7854 - loss: 0.4543 - val_accuracy: 0.8061 - val_loss: 0.4057\n",
      "Training run 125/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 227ms/step - accuracy: 0.7945 - loss: 0.4454 - val_accuracy: 0.8055 - val_loss: 0.4609\n",
      "Training run 126/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 226ms/step - accuracy: 0.7907 - loss: 0.4351 - val_accuracy: 0.7953 - val_loss: 0.4393\n",
      "Training run 127/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 225ms/step - accuracy: 0.7990 - loss: 0.4203 - val_accuracy: 0.8157 - val_loss: 0.4080\n",
      "Training run 128/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.7974 - loss: 0.4187 - val_accuracy: 0.8029 - val_loss: 0.4224\n",
      "Training run 129/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 226ms/step - accuracy: 0.7942 - loss: 0.4130 - val_accuracy: 0.8029 - val_loss: 0.4080\n",
      "Training run 130/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 226ms/step - accuracy: 0.8117 - loss: 0.4062 - val_accuracy: 0.8112 - val_loss: 0.4207\n",
      "Training run 131/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 227ms/step - accuracy: 0.7933 - loss: 0.4305 - val_accuracy: 0.7870 - val_loss: 0.4472\n",
      "Training run 132/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 227ms/step - accuracy: 0.8047 - loss: 0.3948 - val_accuracy: 0.8386 - val_loss: 0.3600\n",
      "Training run 133/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 226ms/step - accuracy: 0.8151 - loss: 0.4150 - val_accuracy: 0.8125 - val_loss: 0.3907\n",
      "Training run 134/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 227ms/step - accuracy: 0.8157 - loss: 0.3873 - val_accuracy: 0.8176 - val_loss: 0.4210\n",
      "Training run 135/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 226ms/step - accuracy: 0.8023 - loss: 0.4123 - val_accuracy: 0.8017 - val_loss: 0.4174\n",
      "Training run 136/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 227ms/step - accuracy: 0.8116 - loss: 0.3997 - val_accuracy: 0.8125 - val_loss: 0.3851\n",
      "Training run 137/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 226ms/step - accuracy: 0.8060 - loss: 0.4184 - val_accuracy: 0.8272 - val_loss: 0.3781\n",
      "Training run 138/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 227ms/step - accuracy: 0.8126 - loss: 0.4130 - val_accuracy: 0.8208 - val_loss: 0.3846\n",
      "Training run 139/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 227ms/step - accuracy: 0.8123 - loss: 0.3888 - val_accuracy: 0.8246 - val_loss: 0.3963\n",
      "Training run 140/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 226ms/step - accuracy: 0.7921 - loss: 0.4318 - val_accuracy: 0.7710 - val_loss: 0.5118\n",
      "Training run 141/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 226ms/step - accuracy: 0.7546 - loss: 0.5235 - val_accuracy: 0.7883 - val_loss: 0.4247\n",
      "Training run 142/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.7828 - loss: 0.4482 - val_accuracy: 0.8029 - val_loss: 0.3918\n",
      "Training run 143/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 227ms/step - accuracy: 0.8186 - loss: 0.3910 - val_accuracy: 0.8138 - val_loss: 0.4085\n",
      "Training run 144/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 227ms/step - accuracy: 0.7965 - loss: 0.4283 - val_accuracy: 0.8131 - val_loss: 0.4405\n",
      "Training run 145/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 226ms/step - accuracy: 0.8028 - loss: 0.4235 - val_accuracy: 0.7991 - val_loss: 0.4105\n",
      "Training run 146/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 198ms/step - accuracy: 0.8024 - loss: 0.4007 - val_accuracy: 0.7959 - val_loss: 0.4421\n",
      "Training run 147/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 220ms/step - accuracy: 0.8089 - loss: 0.3976 - val_accuracy: 0.8176 - val_loss: 0.3943\n",
      "Training run 148/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 225ms/step - accuracy: 0.7931 - loss: 0.4354 - val_accuracy: 0.8182 - val_loss: 0.4002\n",
      "Training run 149/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 225ms/step - accuracy: 0.8194 - loss: 0.3933 - val_accuracy: 0.8042 - val_loss: 0.4063\n",
      "Training run 150/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 224ms/step - accuracy: 0.7954 - loss: 0.4237 - val_accuracy: 0.7946 - val_loss: 0.4061\n",
      "Training run 151/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.8190 - loss: 0.4004 - val_accuracy: 0.8176 - val_loss: 0.3911\n",
      "Training run 152/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 223ms/step - accuracy: 0.7985 - loss: 0.4233 - val_accuracy: 0.8055 - val_loss: 0.4232\n",
      "Training run 153/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.7885 - loss: 0.4117 - val_accuracy: 0.8246 - val_loss: 0.3819\n",
      "Training run 154/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 225ms/step - accuracy: 0.8267 - loss: 0.3868 - val_accuracy: 0.8284 - val_loss: 0.3754\n",
      "Training run 155/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 227ms/step - accuracy: 0.8100 - loss: 0.3992 - val_accuracy: 0.8310 - val_loss: 0.3666\n",
      "Training run 156/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 227ms/step - accuracy: 0.8138 - loss: 0.3882 - val_accuracy: 0.8221 - val_loss: 0.3697\n",
      "Training run 157/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 226ms/step - accuracy: 0.8207 - loss: 0.3863 - val_accuracy: 0.8182 - val_loss: 0.3932\n",
      "Training run 158/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 227ms/step - accuracy: 0.8060 - loss: 0.4013 - val_accuracy: 0.7876 - val_loss: 0.4641\n",
      "Training run 159/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 227ms/step - accuracy: 0.7871 - loss: 0.4379 - val_accuracy: 0.8253 - val_loss: 0.3738\n",
      "Training run 160/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 225ms/step - accuracy: 0.8136 - loss: 0.3997 - val_accuracy: 0.8125 - val_loss: 0.3903\n",
      "Training run 161/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 229ms/step - accuracy: 0.8106 - loss: 0.3796 - val_accuracy: 0.8342 - val_loss: 0.3770\n",
      "Training run 162/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 226ms/step - accuracy: 0.8263 - loss: 0.3737 - val_accuracy: 0.8157 - val_loss: 0.4043\n",
      "Training run 163/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 225ms/step - accuracy: 0.8181 - loss: 0.3927 - val_accuracy: 0.8195 - val_loss: 0.4208\n",
      "Training run 164/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 229ms/step - accuracy: 0.8099 - loss: 0.4029 - val_accuracy: 0.8023 - val_loss: 0.4295\n",
      "Training run 165/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 226ms/step - accuracy: 0.8138 - loss: 0.4074 - val_accuracy: 0.8119 - val_loss: 0.4125\n",
      "Training run 166/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 227ms/step - accuracy: 0.8359 - loss: 0.3765 - val_accuracy: 0.8310 - val_loss: 0.3808\n",
      "Training run 167/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 226ms/step - accuracy: 0.8136 - loss: 0.3737 - val_accuracy: 0.8284 - val_loss: 0.3806\n",
      "Training run 168/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.8186 - loss: 0.3770 - val_accuracy: 0.8176 - val_loss: 0.3966\n",
      "Training run 169/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 227ms/step - accuracy: 0.8196 - loss: 0.3803 - val_accuracy: 0.8284 - val_loss: 0.4102\n",
      "Training run 170/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 227ms/step - accuracy: 0.8187 - loss: 0.3848 - val_accuracy: 0.8208 - val_loss: 0.3977\n",
      "Training run 171/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 229ms/step - accuracy: 0.8199 - loss: 0.3869 - val_accuracy: 0.8406 - val_loss: 0.3730\n",
      "Training run 172/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 227ms/step - accuracy: 0.8206 - loss: 0.3857 - val_accuracy: 0.8195 - val_loss: 0.4004\n",
      "Training run 173/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 229ms/step - accuracy: 0.8280 - loss: 0.3750 - val_accuracy: 0.8304 - val_loss: 0.3902\n",
      "Training run 174/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.8232 - loss: 0.3741 - val_accuracy: 0.8131 - val_loss: 0.4047\n",
      "Training run 175/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 230ms/step - accuracy: 0.8223 - loss: 0.3892 - val_accuracy: 0.8189 - val_loss: 0.4068\n",
      "Training run 176/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 229ms/step - accuracy: 0.8155 - loss: 0.3835 - val_accuracy: 0.8221 - val_loss: 0.3953\n",
      "Training run 177/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 226ms/step - accuracy: 0.8222 - loss: 0.3789 - val_accuracy: 0.8080 - val_loss: 0.3981\n",
      "Training run 178/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.8147 - loss: 0.3881 - val_accuracy: 0.8202 - val_loss: 0.3868\n",
      "Training run 179/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 227ms/step - accuracy: 0.8170 - loss: 0.3667 - val_accuracy: 0.8144 - val_loss: 0.4208\n",
      "Training run 180/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 227ms/step - accuracy: 0.8280 - loss: 0.3944 - val_accuracy: 0.8068 - val_loss: 0.3968\n",
      "Training run 181/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 225ms/step - accuracy: 0.8221 - loss: 0.3676 - val_accuracy: 0.8170 - val_loss: 0.3878\n",
      "Training run 182/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.8274 - loss: 0.3689 - val_accuracy: 0.8310 - val_loss: 0.3757\n",
      "Training run 183/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 227ms/step - accuracy: 0.8245 - loss: 0.3693 - val_accuracy: 0.8284 - val_loss: 0.3799\n",
      "Training run 184/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.7837 - loss: 1.8512 - val_accuracy: 0.8112 - val_loss: 0.4017\n",
      "Training run 185/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 226ms/step - accuracy: 0.8050 - loss: 0.4314 - val_accuracy: 0.8208 - val_loss: 0.3886\n",
      "Training run 186/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.8162 - loss: 0.3873 - val_accuracy: 0.8323 - val_loss: 0.3676\n",
      "Training run 187/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.8132 - loss: 0.3961 - val_accuracy: 0.8284 - val_loss: 0.3759\n",
      "Training run 188/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 229ms/step - accuracy: 0.8149 - loss: 0.3906 - val_accuracy: 0.8208 - val_loss: 0.3819\n",
      "Training run 189/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 230ms/step - accuracy: 0.8208 - loss: 0.3842 - val_accuracy: 0.8099 - val_loss: 0.4283\n",
      "Training run 190/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 227ms/step - accuracy: 0.8141 - loss: 0.3937 - val_accuracy: 0.8144 - val_loss: 0.4169\n",
      "Training run 191/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 229ms/step - accuracy: 0.8175 - loss: 0.3774 - val_accuracy: 0.8227 - val_loss: 0.3953\n",
      "Training run 192/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.8258 - loss: 0.3573 - val_accuracy: 0.8087 - val_loss: 0.4772\n",
      "Training run 193/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 227ms/step - accuracy: 0.8254 - loss: 0.3671 - val_accuracy: 0.8272 - val_loss: 0.4210\n",
      "Training run 194/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.8237 - loss: 0.3651 - val_accuracy: 0.8157 - val_loss: 0.3960\n",
      "Training run 195/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 229ms/step - accuracy: 0.8322 - loss: 0.3655 - val_accuracy: 0.8310 - val_loss: 0.3763\n",
      "Training run 196/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 229ms/step - accuracy: 0.8114 - loss: 0.3814 - val_accuracy: 0.8233 - val_loss: 0.3723\n",
      "Training run 197/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.8196 - loss: 0.3659 - val_accuracy: 0.8138 - val_loss: 0.3878\n",
      "Training run 198/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 229ms/step - accuracy: 0.8180 - loss: 0.3823 - val_accuracy: 0.8431 - val_loss: 0.3590\n",
      "Training run 199/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 227ms/step - accuracy: 0.8226 - loss: 0.3711 - val_accuracy: 0.8176 - val_loss: 0.4216\n",
      "Training run 200/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.8361 - loss: 0.3599 - val_accuracy: 0.8182 - val_loss: 0.3966\n",
      "Training run 201/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.8388 - loss: 0.3516 - val_accuracy: 0.8240 - val_loss: 0.3906\n",
      "Training run 202/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.8313 - loss: 0.3674 - val_accuracy: 0.8138 - val_loss: 0.4253\n",
      "Training run 203/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.8217 - loss: 0.3681 - val_accuracy: 0.8335 - val_loss: 0.4013\n",
      "Training run 204/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.8348 - loss: 0.3626 - val_accuracy: 0.8304 - val_loss: 0.3801\n",
      "Training run 205/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.8224 - loss: 0.3867 - val_accuracy: 0.8227 - val_loss: 0.4096\n",
      "Training run 206/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 227ms/step - accuracy: 0.8268 - loss: 0.3607 - val_accuracy: 0.8240 - val_loss: 0.4311\n",
      "Training run 207/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.8328 - loss: 0.3619 - val_accuracy: 0.8291 - val_loss: 0.3880\n",
      "Training run 208/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 229ms/step - accuracy: 0.8359 - loss: 0.3482 - val_accuracy: 0.8323 - val_loss: 0.3670\n",
      "Training run 209/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.8273 - loss: 0.3640 - val_accuracy: 0.8316 - val_loss: 0.3829\n",
      "Training run 210/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.8384 - loss: 0.3498 - val_accuracy: 0.8304 - val_loss: 0.3833\n",
      "Training run 211/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 229ms/step - accuracy: 0.8235 - loss: 0.4323 - val_accuracy: 0.8272 - val_loss: 0.3876\n",
      "Training run 212/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.8344 - loss: 0.4047 - val_accuracy: 0.8265 - val_loss: 0.3918\n",
      "Training run 213/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 229ms/step - accuracy: 0.8142 - loss: 0.3778 - val_accuracy: 0.8335 - val_loss: 0.3722\n",
      "Training run 214/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 230ms/step - accuracy: 0.8277 - loss: 0.3668 - val_accuracy: 0.8272 - val_loss: 0.3890\n",
      "Training run 215/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.8333 - loss: 0.3834 - val_accuracy: 0.8457 - val_loss: 0.3673\n",
      "Training run 216/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 230ms/step - accuracy: 0.8374 - loss: 0.3596 - val_accuracy: 0.8265 - val_loss: 0.3813\n",
      "Training run 217/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 227ms/step - accuracy: 0.8294 - loss: 0.3563 - val_accuracy: 0.8029 - val_loss: 0.4373\n",
      "Training run 218/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 229ms/step - accuracy: 0.8313 - loss: 0.3556 - val_accuracy: 0.8202 - val_loss: 0.3887\n",
      "Training run 219/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.8353 - loss: 0.3446 - val_accuracy: 0.8208 - val_loss: 0.3888\n",
      "Training run 220/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 227ms/step - accuracy: 0.8482 - loss: 0.3390 - val_accuracy: 0.8157 - val_loss: 0.4400\n",
      "Training run 221/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 229ms/step - accuracy: 0.8260 - loss: 0.3563 - val_accuracy: 0.8278 - val_loss: 0.3712\n",
      "Training run 222/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.8314 - loss: 0.3663 - val_accuracy: 0.8227 - val_loss: 0.3790\n",
      "Training run 223/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.8381 - loss: 0.3476 - val_accuracy: 0.8323 - val_loss: 0.3927\n",
      "Training run 224/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.8316 - loss: 0.3664 - val_accuracy: 0.8253 - val_loss: 0.3722\n",
      "Training run 225/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.8412 - loss: 0.3449 - val_accuracy: 0.8406 - val_loss: 0.3785\n",
      "Training run 226/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 229ms/step - accuracy: 0.8428 - loss: 0.3636 - val_accuracy: 0.8017 - val_loss: 0.4133\n",
      "Training run 227/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 227ms/step - accuracy: 0.8231 - loss: 0.3617 - val_accuracy: 0.8386 - val_loss: 0.3577\n",
      "Training run 228/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 229ms/step - accuracy: 0.8171 - loss: 0.3941 - val_accuracy: 0.8233 - val_loss: 0.3962\n",
      "Training run 229/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 229ms/step - accuracy: 0.8293 - loss: 0.3619 - val_accuracy: 0.8214 - val_loss: 0.3797\n",
      "Training run 230/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.8316 - loss: 0.3527 - val_accuracy: 0.8221 - val_loss: 0.3877\n",
      "Training run 231/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 230ms/step - accuracy: 0.8392 - loss: 0.3416 - val_accuracy: 0.8144 - val_loss: 0.3881\n",
      "Training run 232/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 229ms/step - accuracy: 0.8412 - loss: 0.3374 - val_accuracy: 0.8316 - val_loss: 0.3986\n",
      "Training run 233/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 229ms/step - accuracy: 0.8367 - loss: 0.3482 - val_accuracy: 0.8342 - val_loss: 0.3967\n",
      "Training run 234/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.8408 - loss: 0.3329 - val_accuracy: 0.8131 - val_loss: 0.4110\n",
      "Training run 235/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 229ms/step - accuracy: 0.8312 - loss: 0.3565 - val_accuracy: 0.8125 - val_loss: 0.3948\n",
      "Training run 236/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 229ms/step - accuracy: 0.8410 - loss: 0.3382 - val_accuracy: 0.8214 - val_loss: 0.3903\n",
      "Training run 237/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.8429 - loss: 0.3483 - val_accuracy: 0.8361 - val_loss: 0.3686\n",
      "Training run 238/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.8332 - loss: 0.3529 - val_accuracy: 0.8138 - val_loss: 0.4120\n",
      "Training run 239/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.8385 - loss: 0.3499 - val_accuracy: 0.8099 - val_loss: 0.4327\n",
      "Training run 240/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 227ms/step - accuracy: 0.8294 - loss: 0.3550 - val_accuracy: 0.8316 - val_loss: 0.4113\n",
      "Training run 241/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 227ms/step - accuracy: 0.8367 - loss: 0.3404 - val_accuracy: 0.8278 - val_loss: 0.3907\n",
      "Training run 242/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 229ms/step - accuracy: 0.8438 - loss: 0.3332 - val_accuracy: 0.8176 - val_loss: 0.4228\n",
      "Training run 243/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.8365 - loss: 0.3349 - val_accuracy: 0.8418 - val_loss: 0.3683\n",
      "Training run 244/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 229ms/step - accuracy: 0.8426 - loss: 0.3448 - val_accuracy: 0.8284 - val_loss: 0.3720\n",
      "Training run 245/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 229ms/step - accuracy: 0.8416 - loss: 0.3484 - val_accuracy: 0.8233 - val_loss: 0.4260\n",
      "Training run 246/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.8435 - loss: 0.3371 - val_accuracy: 0.8227 - val_loss: 0.3788\n",
      "Training run 247/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.8388 - loss: 0.3449 - val_accuracy: 0.8202 - val_loss: 0.3971\n",
      "Training run 248/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 229ms/step - accuracy: 0.8401 - loss: 0.3500 - val_accuracy: 0.8208 - val_loss: 0.3917\n",
      "Training run 249/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 227ms/step - accuracy: 0.8285 - loss: 0.3599 - val_accuracy: 0.8316 - val_loss: 0.3801\n",
      "Training run 250/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 230ms/step - accuracy: 0.8400 - loss: 0.3366 - val_accuracy: 0.8355 - val_loss: 0.3803\n",
      "Training run 251/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.8426 - loss: 0.3442 - val_accuracy: 0.8316 - val_loss: 0.3782\n",
      "Training run 252/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.8313 - loss: 0.3517 - val_accuracy: 0.8214 - val_loss: 0.3979\n",
      "Training run 253/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 227ms/step - accuracy: 0.8471 - loss: 0.3289 - val_accuracy: 0.8151 - val_loss: 0.4483\n",
      "Training run 254/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.8419 - loss: 0.3343 - val_accuracy: 0.8367 - val_loss: 0.3801\n",
      "Training run 255/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 226ms/step - accuracy: 0.8488 - loss: 0.3267 - val_accuracy: 0.8246 - val_loss: 0.3808\n",
      "Training run 256/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 229ms/step - accuracy: 0.8398 - loss: 0.3453 - val_accuracy: 0.8208 - val_loss: 0.4301\n",
      "Training run 257/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 230ms/step - accuracy: 0.8427 - loss: 0.3515 - val_accuracy: 0.8284 - val_loss: 0.4183\n",
      "Training run 258/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.8285 - loss: 0.3462 - val_accuracy: 0.8438 - val_loss: 0.3819\n",
      "Training run 259/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.8399 - loss: 0.3707 - val_accuracy: 0.8151 - val_loss: 0.4058\n",
      "Training run 260/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 227ms/step - accuracy: 0.8485 - loss: 0.3347 - val_accuracy: 0.8361 - val_loss: 0.3804\n",
      "Training run 261/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 230ms/step - accuracy: 0.8387 - loss: 0.3352 - val_accuracy: 0.8125 - val_loss: 0.3867\n",
      "Training run 262/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.8465 - loss: 0.3314 - val_accuracy: 0.8144 - val_loss: 0.4118\n",
      "Training run 263/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 229ms/step - accuracy: 0.8380 - loss: 0.3341 - val_accuracy: 0.8259 - val_loss: 0.4143\n",
      "Training run 264/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 229ms/step - accuracy: 0.8424 - loss: 0.3433 - val_accuracy: 0.8316 - val_loss: 0.4126\n",
      "Training run 265/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.8449 - loss: 0.3457 - val_accuracy: 0.8214 - val_loss: 0.4258\n",
      "Training run 266/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 227ms/step - accuracy: 0.8486 - loss: 0.3263 - val_accuracy: 0.8278 - val_loss: 0.3857\n",
      "Training run 267/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 229ms/step - accuracy: 0.8413 - loss: 0.3358 - val_accuracy: 0.8227 - val_loss: 0.4012\n",
      "Training run 268/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 227ms/step - accuracy: 0.8460 - loss: 0.3239 - val_accuracy: 0.8208 - val_loss: 0.4243\n",
      "Training run 269/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 227ms/step - accuracy: 0.8454 - loss: 0.3350 - val_accuracy: 0.8233 - val_loss: 0.4246\n",
      "Training run 270/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 230ms/step - accuracy: 0.8452 - loss: 0.3482 - val_accuracy: 0.8310 - val_loss: 0.4180\n",
      "Training run 271/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.8406 - loss: 0.3296 - val_accuracy: 0.8125 - val_loss: 0.4548\n",
      "Training run 272/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.8441 - loss: 0.3452 - val_accuracy: 0.8233 - val_loss: 0.4238\n",
      "Training run 273/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 229ms/step - accuracy: 0.8401 - loss: 0.3395 - val_accuracy: 0.8272 - val_loss: 0.4048\n",
      "Training run 274/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 229ms/step - accuracy: 0.8370 - loss: 0.3495 - val_accuracy: 0.8233 - val_loss: 0.4099\n",
      "Training run 275/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 227ms/step - accuracy: 0.8402 - loss: 0.3265 - val_accuracy: 0.8361 - val_loss: 0.7158\n",
      "Training run 276/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 229ms/step - accuracy: 0.8567 - loss: 0.3146 - val_accuracy: 0.8259 - val_loss: 0.5568\n",
      "Training run 277/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.8474 - loss: 0.3339 - val_accuracy: 0.8304 - val_loss: 0.4148\n",
      "Training run 278/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.8433 - loss: 0.3326 - val_accuracy: 0.8450 - val_loss: 0.3822\n",
      "Training run 279/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.8383 - loss: 0.3421 - val_accuracy: 0.8323 - val_loss: 0.3844\n",
      "Training run 280/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.8457 - loss: 0.3357 - val_accuracy: 0.8265 - val_loss: 0.4277\n",
      "Training run 281/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 229ms/step - accuracy: 0.8378 - loss: 0.3305 - val_accuracy: 0.8195 - val_loss: 0.4330\n",
      "Training run 282/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.8432 - loss: 0.3260 - val_accuracy: 0.8253 - val_loss: 0.4114\n",
      "Training run 283/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 229ms/step - accuracy: 0.8225 - loss: 0.3972 - val_accuracy: 0.8214 - val_loss: 0.4001\n",
      "Training run 284/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 230ms/step - accuracy: 0.8344 - loss: 0.3563 - val_accuracy: 0.8182 - val_loss: 0.4271\n",
      "Training run 285/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 229ms/step - accuracy: 0.8419 - loss: 0.3302 - val_accuracy: 0.8246 - val_loss: 0.8190\n",
      "Training run 286/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 227ms/step - accuracy: 0.8528 - loss: 0.3257 - val_accuracy: 0.8329 - val_loss: 0.4052\n",
      "Training run 287/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 229ms/step - accuracy: 0.8484 - loss: 0.3175 - val_accuracy: 0.8240 - val_loss: 0.7428\n",
      "Training run 288/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 227ms/step - accuracy: 0.8539 - loss: 0.3266 - val_accuracy: 0.8227 - val_loss: 0.4423\n",
      "Training run 289/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 226ms/step - accuracy: 0.8523 - loss: 0.3175 - val_accuracy: 0.8125 - val_loss: 0.4493\n",
      "Training run 290/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 230ms/step - accuracy: 0.8484 - loss: 0.3159 - val_accuracy: 0.8227 - val_loss: 0.4442\n",
      "Training run 291/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.8505 - loss: 0.3178 - val_accuracy: 0.8227 - val_loss: 0.3991\n",
      "Training run 292/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.8380 - loss: 0.3854 - val_accuracy: 0.8431 - val_loss: 0.3899\n",
      "Training run 293/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.8534 - loss: 0.3282 - val_accuracy: 0.8233 - val_loss: 0.4204\n",
      "Training run 294/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.8480 - loss: 0.3443 - val_accuracy: 0.8489 - val_loss: 0.4179\n",
      "Training run 295/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 229ms/step - accuracy: 0.8420 - loss: 0.3307 - val_accuracy: 0.8221 - val_loss: 0.4320\n",
      "Training run 296/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 229ms/step - accuracy: 0.8537 - loss: 0.3175 - val_accuracy: 0.8355 - val_loss: 0.4325\n",
      "Training run 297/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.8392 - loss: 0.3311 - val_accuracy: 0.8482 - val_loss: 0.4272\n",
      "Training run 298/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.8429 - loss: 0.3322 - val_accuracy: 0.8233 - val_loss: 0.3827\n",
      "Training run 299/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 228ms/step - accuracy: 0.8316 - loss: 0.3404 - val_accuracy: 0.8310 - val_loss: 0.4155\n",
      "Training run 300/300...\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 230ms/step - accuracy: 0.8448 - loss: 0.3342 - val_accuracy: 0.8323 - val_loss: 0.4260\n"
     ]
    }
   ],
   "source": [
    "# Create and train the model\n",
    "filters=[1024, 512, 512, 512, 512, 512, 256, 256, 256, 256, 256]\n",
    "model = create_convnet(input_shape, num_classes, num_filters=filters, kernel_size=(9,))            \n",
    "histories = train_convnet_many_times(model, train_generator, val_generator, epochs_per_run=1, num_runs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-15 07:15:16.966809: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 487120064 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1016/1016\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 62ms/step\n",
      "Confusion Matrix:\n",
      "[[14185  2549    12    90]\n",
      " [ 2878  5116     7    32]\n",
      " [    1     1   326    48]\n",
      " [   33    73   328  6813]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.84      0.84     16836\n",
      "           1       0.66      0.64      0.65      8033\n",
      "           2       0.48      0.87      0.62       376\n",
      "           3       0.98      0.94      0.96      7247\n",
      "\n",
      "    accuracy                           0.81     32492\n",
      "   macro avg       0.74      0.82      0.77     32492\n",
      "weighted avg       0.82      0.81      0.81     32492\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_confusion_matrix(model, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deeper dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_8\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_8\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_87 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3748</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,240</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_87 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1874</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_88 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1874</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,719,104</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_88 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">937</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_127 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">937</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_89 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">937</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_89 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">468</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_128 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">468</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_90 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">468</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_90 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">234</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_129 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">234</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_91 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">234</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_91 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">117</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_130 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">117</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_92 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">117</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_92 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">58</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_131 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">58</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_93 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">58</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,179,904</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_93 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_132 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_94 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_94 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_133 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_95 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_95 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_134 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_96 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │       <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_96 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_135 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_97 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │       <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_97 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_136 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_56 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_137 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_57 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_138 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_58 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_139 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_59 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_140 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_60 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_141 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_61 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_142 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_62 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_143 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_63 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_144 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_64 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_145 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_65 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_146 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_66 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,056</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_147 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_67 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,056</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_148 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_68 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_149 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_69 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">272</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_150 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_70 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">272</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_151 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_71 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">136</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_152 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_72 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_153 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_73 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_154 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_74 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_87 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3748\u001b[0m, \u001b[38;5;34m1024\u001b[0m)     │        \u001b[38;5;34m10,240\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_87 (\u001b[38;5;33mMaxPooling1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1874\u001b[0m, \u001b[38;5;34m1024\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_88 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1874\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │     \u001b[38;5;34m4,719,104\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_88 (\u001b[38;5;33mMaxPooling1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m937\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_127 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m937\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_89 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m937\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │     \u001b[38;5;34m2,359,808\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_89 (\u001b[38;5;33mMaxPooling1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m468\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_128 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m468\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_90 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m468\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │     \u001b[38;5;34m2,359,808\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_90 (\u001b[38;5;33mMaxPooling1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m234\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_129 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m234\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_91 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m234\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │     \u001b[38;5;34m2,359,808\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_91 (\u001b[38;5;33mMaxPooling1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m117\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_130 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m117\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_92 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m117\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │     \u001b[38;5;34m2,359,808\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_92 (\u001b[38;5;33mMaxPooling1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m58\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_131 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m58\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_93 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m58\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │     \u001b[38;5;34m1,179,904\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_93 (\u001b[38;5;33mMaxPooling1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_132 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_94 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │       \u001b[38;5;34m590,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_94 (\u001b[38;5;33mMaxPooling1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_133 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_95 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │       \u001b[38;5;34m590,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_95 (\u001b[38;5;33mMaxPooling1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_134 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_96 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │       \u001b[38;5;34m590,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_96 (\u001b[38;5;33mMaxPooling1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_135 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_97 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │       \u001b[38;5;34m590,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_97 (\u001b[38;5;33mMaxPooling1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_136 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_8 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_56 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m65,792\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_137 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_57 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m65,792\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_138 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_58 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m65,792\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_139 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_59 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_140 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_60 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m16,512\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_141 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_61 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m16,512\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_142 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_62 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_143 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_63 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_144 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_64 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_145 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_65 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_146 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_66 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m1,056\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_147 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_67 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m1,056\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_148 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_68 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m528\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_149 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_69 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m272\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_150 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_70 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m272\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_151 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_71 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │           \u001b[38;5;34m136\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_152 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_72 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │            \u001b[38;5;34m72\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_153 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_73 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │            \u001b[38;5;34m72\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_154 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_74 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │            \u001b[38;5;34m36\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">17,994,252</span> (68.64 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m17,994,252\u001b[0m (68.64 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">17,994,252</span> (68.64 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m17,994,252\u001b[0m (68.64 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create and train the model\n",
    "filters=[1024, 512, 512, 512, 512, 512, 256, 256, 256, 256, 256]\n",
    "model = create_convnet(input_shape, num_classes, num_filters=filters, kernel_size=(9,))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training run 1/300...\n"
     ]
    }
   ],
   "source": [
    "histories = train_convnet_many_times(model, train_generator, val_generator, epochs_per_run=1, num_runs=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
