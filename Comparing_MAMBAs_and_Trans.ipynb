{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gc\n",
    "import wandb\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, hamming_loss, roc_auc_score\n",
    "#from mamba_ssm import Mamba2\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalBalancedMultiLabelDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A balanced multi-label dataset that returns (X_spectra, X_gaia, y).\n",
    "    It uses the same balancing strategy as `BalancedMultiLabelDataset`.\n",
    "    \"\"\"\n",
    "    def __init__(self, X_spectra, X_gaia, y, limit_per_label=201):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X_spectra (torch.Tensor): [num_samples, num_spectra_features]\n",
    "            X_gaia (torch.Tensor): [num_samples, num_gaia_features]\n",
    "            y (torch.Tensor): [num_samples, num_classes], multi-hot labels\n",
    "            limit_per_label (int): limit or target number of samples per label\n",
    "        \"\"\"\n",
    "        self.X_spectra = X_spectra\n",
    "        self.X_gaia = X_gaia\n",
    "        self.y = y\n",
    "        self.limit_per_label = limit_per_label\n",
    "        self.num_classes = y.shape[1]\n",
    "        self.indices = self.balance_classes()\n",
    "        \n",
    "    def balance_classes(self):\n",
    "        indices = []\n",
    "        class_counts = torch.sum(self.y, axis=0)\n",
    "        for cls in range(self.num_classes):\n",
    "            cls_indices = np.where(self.y[:, cls] == 1)[0]\n",
    "            if len(cls_indices) < self.limit_per_label:\n",
    "                if len(cls_indices) == 0:\n",
    "                    # No samples for this class\n",
    "                    continue\n",
    "                extra_indices = np.random.choice(\n",
    "                    cls_indices, self.limit_per_label - len(cls_indices), replace=True\n",
    "                )\n",
    "                cls_indices = np.concatenate([cls_indices, extra_indices])\n",
    "            elif len(cls_indices) > self.limit_per_label:\n",
    "                cls_indices = np.random.choice(cls_indices, self.limit_per_label, replace=False)\n",
    "            indices.extend(cls_indices)\n",
    "        indices = np.unique(indices)\n",
    "        np.random.shuffle(indices)\n",
    "        return indices\n",
    "\n",
    "    def re_sample(self):\n",
    "        self.indices = self.balance_classes()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        index = self.indices[idx]\n",
    "        return (\n",
    "            self.X_spectra[index],  # spectra features\n",
    "            self.X_gaia[index],     # gaia features\n",
    "            self.y[index],          # multi-hot labels\n",
    "        )\n",
    "    \n",
    "def calculate_class_weights(y):\n",
    "    if y.ndim > 1:  \n",
    "        class_counts = np.sum(y, axis=0)  \n",
    "    else:\n",
    "        class_counts = np.bincount(y)\n",
    "\n",
    "    total_samples = y.shape[0] if y.ndim > 1 else len(y)\n",
    "    class_counts = np.where(class_counts == 0, 1, class_counts)  # Prevent division by zero\n",
    "    class_weights = total_samples / (len(class_counts) * class_counts)\n",
    "    \n",
    "    return class_weights\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    metrics = {\n",
    "        \"micro_f1\": f1_score(y_true, y_pred, average='micro'),\n",
    "        \"macro_f1\": f1_score(y_true, y_pred, average='macro'),\n",
    "        \"weighted_f1\": f1_score(y_true, y_pred, average='weighted'),\n",
    "        \"micro_precision\": precision_score(y_true, y_pred, average='micro', zero_division=1),\n",
    "        \"macro_precision\": precision_score(y_true, y_pred, average='macro', zero_division=1),\n",
    "        \"weighted_precision\": precision_score(y_true, y_pred, average='weighted', zero_division=1),\n",
    "        \"micro_recall\": recall_score(y_true, y_pred, average='micro'),\n",
    "        \"macro_recall\": recall_score(y_true, y_pred, average='macro'),\n",
    "        \"weighted_recall\": recall_score(y_true, y_pred, average='weighted'),\n",
    "        \"hamming_loss\": hamming_loss(y_true, y_pred)\n",
    "    }\n",
    "    \n",
    "    # Check if there are at least two classes present in y_true\n",
    "    #if len(np.unique(y_true)) > 1:\n",
    "        #metrics[\"roc_auc\"] = roc_auc_score(y_true, y_pred, average='macro', multi_class='ovr')\n",
    "    #else:\n",
    "       # metrics[\"roc_auc\"] = None  # or you can set it to a default value or message\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "class CrossAttentionBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple cross-attention block with a feed-forward sub-layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads=8):\n",
    "        super().__init__()\n",
    "        self.cross_attn = nn.MultiheadAttention(\n",
    "            embed_dim=d_model, \n",
    "            num_heads=n_heads, \n",
    "            batch_first=True\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * d_model, d_model)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x_q, x_kv):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x_q  : (batch_size, seq_len_q, d_model)\n",
    "            x_kv : (batch_size, seq_len_kv, d_model)\n",
    "        \"\"\"\n",
    "        # Cross-attention\n",
    "        attn_output, _ = self.cross_attn(query=x_q, key=x_kv, value=x_kv)\n",
    "        x = self.norm1(x_q + attn_output)\n",
    "\n",
    "        # Feed forward\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = self.norm2(x + ffn_out)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class FeatureTokenizer(nn.Module):\n",
    "    \"\"\"\n",
    "    Splits input features into tokens of a specified dimension.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, token_dim, d_model):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim: Dimension of the input features\n",
    "            token_dim: Dimension of each token\n",
    "            d_model: Model dimension that each token will be embedded to\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.token_dim = token_dim\n",
    "        \n",
    "        # Calculate number of tokens based on input dimension and token dimension\n",
    "        self.num_tokens = (input_dim + token_dim - 1) // token_dim  # Ceiling division\n",
    "        \n",
    "        # Padding to ensure input_dim is divisible by token_dim\n",
    "        self.padded_dim = self.num_tokens * token_dim\n",
    "        \n",
    "        # Linear projection to embed each token to d_model\n",
    "        self.token_embed = nn.Linear(token_dim, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor of shape [batch_size, input_dim]\n",
    "            \n",
    "        Returns:\n",
    "            Tokenized tensor of shape [batch_size, num_tokens, d_model]\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # Pad input if needed\n",
    "        if self.input_dim < self.padded_dim:\n",
    "            padding = torch.zeros(batch_size, self.padded_dim - self.input_dim, \n",
    "                                 dtype=x.dtype, device=x.device)\n",
    "            x = torch.cat([x, padding], dim=1)\n",
    "        \n",
    "        # Reshape into tokens\n",
    "        x = x.view(batch_size, self.num_tokens, self.token_dim)\n",
    "        \n",
    "        # Embed each token to d_model\n",
    "        x = self.token_embed(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "def train_model_fusion(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    test_loader,\n",
    "    num_epochs=100,\n",
    "    lr=2.5e-3,\n",
    "    max_patience=20,\n",
    "    device='cuda'\n",
    "):\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "    total_step_counter = 0 # Initialize step counter to know when to stop updating the schedulers\n",
    "    max_total_steps = num_epochs * len(train_loader)  # Initial estimation of total steps\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, max_lr=lr, total_steps=max_total_steps\n",
    "    )\n",
    "\n",
    "\n",
    "    # Compute class weights based on the training set\n",
    "    all_labels = []\n",
    "    for _, _, y_batch in train_loader:\n",
    "        all_labels.extend(y_batch.cpu().numpy())\n",
    "    \n",
    "    class_weights = calculate_class_weights(np.array(all_labels))\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience = max_patience\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Resample training data\n",
    "        train_loader.dataset.re_sample()\n",
    "\n",
    "        # Update max_total_steps if needed\n",
    "        current_max_steps = (epoch + 1) * len(train_loader) + (num_epochs - epoch - 1) * len(train_loader)\n",
    "        if current_max_steps != max_total_steps:\n",
    "            #print(f\"Adjusting max steps: {max_total_steps} -> {current_max_steps}\")\n",
    "            max_total_steps = current_max_steps\n",
    "\n",
    "        # Recompute class weights if needed\n",
    "        all_labels = []\n",
    "        for _, _, y_batch in train_loader:\n",
    "            all_labels.extend(y_batch.cpu().numpy())\n",
    "        class_weights = calculate_class_weights(np.array(all_labels))\n",
    "        class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights)\n",
    "\n",
    "        # --- Training ---\n",
    "        model.train()\n",
    "        train_loss, train_acc = 0.0, 0.0\n",
    "        for X_spc, X_ga, y_batch in train_loader:\n",
    "            X_spc, X_ga, y_batch = X_spc.to(device), X_ga.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_spc, X_ga)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * X_spc.size(0)\n",
    "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            correct = (predicted == y_batch).float()\n",
    "            train_acc += correct.mean(dim=1).mean().item()\n",
    "\n",
    "            # Only step if we haven't reached max steps\n",
    "            if total_step_counter < max_total_steps:\n",
    "                scheduler.step()\n",
    "                total_step_counter += 1\n",
    "            else:\n",
    "                print(f\"Reached maximum steps ({max_total_steps}), skipping scheduler step\")\n",
    "\n",
    "        # --- Validation ---\n",
    "        model.eval()\n",
    "        val_loss, val_acc = 0.0, 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_spc, X_ga, y_batch in val_loader:\n",
    "                X_spc, X_ga, y_batch = X_spc.to(device), X_ga.to(device), y_batch.to(device)\n",
    "                outputs = model(X_spc, X_ga)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_loss += loss.item() * X_spc.size(0)\n",
    "                predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                correct = (predicted == y_batch).float()\n",
    "                val_acc += correct.mean(dim=1).mean().item()\n",
    "\n",
    "        # --- Test metrics (optional or do after training) ---\n",
    "        test_loss, test_acc = 0.0, 0.0\n",
    "        y_true, y_pred = [], []\n",
    "        with torch.no_grad():\n",
    "            for X_spc, X_ga, y_batch in test_loader:\n",
    "                X_spc, X_ga, y_batch = X_spc.to(device), X_ga.to(device), y_batch.to(device)\n",
    "                outputs = model(X_spc, X_ga)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                test_loss += loss.item() * X_spc.size(0)\n",
    "                \n",
    "                predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                correct = (predicted == y_batch).float()\n",
    "                test_acc += correct.mean(dim=1).mean().item()\n",
    "\n",
    "                y_true.extend(y_batch.cpu().numpy())\n",
    "                y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "        # Compute multi-label metrics as before\n",
    "        all_metrics = calculate_metrics(np.array(y_true), np.array(y_pred))\n",
    "        \n",
    "        # Logging example\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": train_loss / len(train_loader.dataset),\n",
    "            \"val_loss\": val_loss / len(val_loader.dataset),\n",
    "            \"train_acc\": train_acc / len(train_loader),\n",
    "            \"val_acc\": val_acc / len(val_loader),\n",
    "            \"test_loss\": test_loss / len(test_loader.dataset),\n",
    "            \"test_acc\": test_acc / len(test_loader),\n",
    "            \"lr\": get_lr(optimizer),\n",
    "            **all_metrics\n",
    "        })\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience = max_patience\n",
    "            best_model = model.state_dict()\n",
    "        else:\n",
    "            patience -= 1\n",
    "            if patience <= 0:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(best_model)\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for X_spc, X_ga, y_batch in test_loader:\n",
    "            X_spc, X_ga, y_batch = X_spc.to(device), X_ga.to(device), y_batch.to(device)\n",
    "            outputs = model(X_spc, X_ga)\n",
    "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            y_true.extend(y_batch.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parallax                   0\n",
      "ra                         0\n",
      "dec                        0\n",
      "ra_error                   0\n",
      "dec_error                  0\n",
      "parallax_error             0\n",
      "pmra                       0\n",
      "pmdec                      0\n",
      "pmra_error                 0\n",
      "pmdec_error                0\n",
      "phot_g_mean_flux           0\n",
      "flagnopllx                 0\n",
      "phot_g_mean_flux_error     0\n",
      "phot_bp_mean_flux          0\n",
      "phot_rp_mean_flux          0\n",
      "phot_bp_mean_flux_error    0\n",
      "phot_rp_mean_flux_error    0\n",
      "flagnoflux                 0\n",
      "dtype: int64\n",
      "parallax                   0\n",
      "ra                         0\n",
      "dec                        0\n",
      "ra_error                   0\n",
      "dec_error                  0\n",
      "parallax_error             0\n",
      "pmra                       0\n",
      "pmdec                      0\n",
      "pmra_error                 0\n",
      "pmdec_error                0\n",
      "phot_g_mean_flux           0\n",
      "flagnopllx                 0\n",
      "phot_g_mean_flux_error     0\n",
      "phot_bp_mean_flux          0\n",
      "phot_rp_mean_flux          0\n",
      "phot_bp_mean_flux_error    0\n",
      "phot_rp_mean_flux_error    0\n",
      "flagnoflux                 0\n",
      "dtype: int64\n",
      "X_train_spectra shape: torch.Size([87134, 3647])\n",
      "X_val_spectra shape: torch.Size([21784, 3647])\n",
      "X_test_spectra shape: torch.Size([27237, 3647])\n",
      "X_train_gaia shape: torch.Size([87134, 18])\n",
      "X_val_gaia shape: torch.Size([21784, 18])\n",
      "X_test_gaia shape: torch.Size([27237, 18])\n",
      "y_train shape: torch.Size([87134, 55])\n",
      "y_val shape: torch.Size([21784, 55])\n",
      "y_test shape: torch.Size([27237, 55])\n",
      "Train dataset: 291 samples\n",
      "Validation dataset: 236 samples\n",
      "Test dataset: 259 samples\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "batch_limit = int(batch_size / 2.5)\n",
    "\n",
    "# Load datasets\n",
    "#X_train_full = pd.read_pickle(\"Pickles/train_data_transformed2.pkl\")\n",
    "#X_test_full = pd.read_pickle(\"Pickles/test_data_transformed.pkl\")\n",
    "# classes = pd.read_pickle(\"Pickles/Updated_list_of_Classes.pkl\")\n",
    "import pickle\n",
    "# Open them in a cross-platform way\n",
    "with open(\"Pickles/Updated_List_of_Classes_ubuntu.pkl\", \"rb\") as f:\n",
    "    classes = pickle.load(f)  # This reads the actual data\n",
    "with open(\"Pickles/train_data_transformed_ubuntu.pkl\", \"rb\") as f:\n",
    "    X_train_full = pickle.load(f)\n",
    "with open(\"Pickles/test_data_transformed_ubuntu.pkl\", \"rb\") as f:\n",
    "    X_test_full = pickle.load(f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Extract labels\n",
    "y_train_full = X_train_full[classes]\n",
    "y_test = X_test_full[classes]\n",
    "\n",
    "# Drop labels from both datasets\n",
    "X_train_full.drop(classes, axis=1, inplace=True)\n",
    "X_test_full.drop(classes, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Columns for spectral data (assuming all remaining columns after removing Gaia are spectra)\n",
    "gaia_columns = [\"parallax\", \"ra\", \"dec\", \"ra_error\", \"dec_error\", \"parallax_error\", \"pmra\", \"pmdec\", \n",
    "                \"pmra_error\", \"pmdec_error\", \"phot_g_mean_flux\", \"flagnopllx\", \"phot_g_mean_flux_error\", \n",
    "                \"phot_bp_mean_flux\", \"phot_rp_mean_flux\", \"phot_bp_mean_flux_error\", \"phot_rp_mean_flux_error\", \n",
    "                \"flagnoflux\"]\n",
    "\n",
    "# Spectra data (everything that is not Gaia-related) and the column 'otype'\n",
    "X_train_spectra = X_train_full.drop(columns={\"otype\", \"obsid\", *gaia_columns})\n",
    "X_test_spectra = X_test_full.drop(columns={\"otype\", \"obsid\", *gaia_columns})\n",
    "\n",
    "# Gaia data (only the selected columns)\n",
    "X_train_gaia = X_train_full[gaia_columns]\n",
    "X_test_gaia = X_test_full[gaia_columns]\n",
    "\n",
    "# Count nans and infs in x_train_gaia\n",
    "print(X_train_gaia.isnull().sum())\n",
    "print(X_train_gaia.isin([np.inf, -np.inf]).sum())\n",
    "\n",
    "\n",
    "# Free up memory\n",
    "del X_train_full, X_test_full\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "# Split training set into training and validation\n",
    "X_train_spectra, X_val_spectra, X_train_gaia, X_val_gaia, y_train, y_val = train_test_split(\n",
    "    X_train_spectra, X_train_gaia, y_train_full, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Free memory\n",
    "del y_train_full\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "# Convert spectra and Gaia data into PyTorch tensors\n",
    "X_train_spectra = torch.tensor(X_train_spectra.values, dtype=torch.float32)\n",
    "X_val_spectra = torch.tensor(X_val_spectra.values, dtype=torch.float32)\n",
    "X_test_spectra = torch.tensor(X_test_spectra.values, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "X_train_gaia = torch.tensor(X_train_gaia.values, dtype=torch.float32)\n",
    "X_val_gaia = torch.tensor(X_val_gaia.values, dtype=torch.float32)\n",
    "X_test_gaia = torch.tensor(X_test_gaia.values, dtype=torch.float32)\n",
    "\n",
    "y_train = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "# Print dataset shapes\n",
    "print(f\"X_train_spectra shape: {X_train_spectra.shape}\")\n",
    "print(f\"X_val_spectra shape: {X_val_spectra.shape}\")\n",
    "print(f\"X_test_spectra shape: {X_test_spectra.shape}\")\n",
    "\n",
    "print(f\"X_train_gaia shape: {X_train_gaia.shape}\")\n",
    "print(f\"X_val_gaia shape: {X_val_gaia.shape}\")\n",
    "print(f\"X_test_gaia shape: {X_test_gaia.shape}\")\n",
    "train_test_split\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_val shape: {y_val.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "\n",
    "train_dataset = MultiModalBalancedMultiLabelDataset(X_train_spectra, X_train_gaia, y_train, limit_per_label=batch_limit)\n",
    "val_dataset = MultiModalBalancedMultiLabelDataset(X_val_spectra, X_val_gaia, y_val, limit_per_label=batch_limit)\n",
    "test_dataset = MultiModalBalancedMultiLabelDataset(X_test_spectra, X_test_gaia, y_test, limit_per_label=batch_limit)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# print the number of samples in each dataset\n",
    "print(f\"Train dataset: {len(train_dataset)} samples\")\n",
    "print(f\"Validation dataset: {len(val_dataset)} samples\")\n",
    "print(f\"Test dataset: {len(test_dataset)} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 288\n",
      "Val samples: 230\n",
      "Test samples: 256\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train_dataset = MultiModalBalancedMultiLabelDataset(\n",
    "        X_train_spectra, X_train_gaia, y_train, limit_per_label=batch_limit\n",
    "    )\n",
    "    val_dataset = MultiModalBalancedMultiLabelDataset(\n",
    "        X_val_spectra, X_val_gaia, y_val, limit_per_label=batch_limit\n",
    "    )\n",
    "    test_dataset = MultiModalBalancedMultiLabelDataset(\n",
    "        X_test_spectra, X_test_gaia, y_test, limit_per_label=batch_limit\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # print the number of samples in each dataset\n",
    "    print(f\"Train samples: {len(train_dataset)}\")\n",
    "    print(f\"Val samples: {len(val_dataset)}\")\n",
    "    print(f\"Test samples: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from functools import partial\n",
    "\n",
    "# Import the needed components from your MambaOut implementation\n",
    "from timm.models.layers import DropPath\n",
    "\n",
    "class GatedCNNBlock(nn.Module):\n",
    "    \"\"\"Adaptation of GatedCNNBlock for sequence data with dynamic kernel size adaptation\"\"\"\n",
    "    def __init__(self, dim, d_conv=4, expand=2, drop_path=0.):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim, eps=1e-6)\n",
    "        hidden = int(expand * dim)\n",
    "        self.fc1 = nn.Linear(dim, hidden * 2)\n",
    "        self.act = nn.GELU()\n",
    "        \n",
    "        # Store these for dynamic convolution sizing\n",
    "        self.d_conv = d_conv\n",
    "        self.hidden = hidden\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden, dim)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        \n",
    "        # Use simpler approach for sequence length 1 (common case)\n",
    "        # This avoids dynamic convolution creation\n",
    "        if d_conv == 1:\n",
    "            self.use_identity_for_length_1 = True\n",
    "\n",
    "        \n",
    "        # Cache for static convolution with kernel size 1 (for length 1 sequences)\n",
    "        if d_conv == 1:\n",
    "            self.conv1 = nn.Conv1d(\n",
    "                in_channels=hidden,\n",
    "                out_channels=hidden, \n",
    "                kernel_size=1,\n",
    "                padding=0,\n",
    "                groups=hidden\n",
    "            )\n",
    "        else:\n",
    "            # Dynamic convolution for other lengths\n",
    "            self.conv = nn.Conv1d(\n",
    "                in_channels=hidden,\n",
    "                out_channels=hidden, \n",
    "                kernel_size=d_conv,\n",
    "                padding=(d_conv - 1) // 2,\n",
    "                groups=hidden\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input shape: [B, seq_len, dim]\n",
    "        shortcut = x\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        # Split the channels for gating mechanism\n",
    "        x = self.fc1(x)  # [B, seq_len, hidden*2]\n",
    "        g, c = torch.chunk(x, 2, dim=-1)  # Each: [B, seq_len, hidden]\n",
    "        \n",
    "        # Get sequence length\n",
    "        batch_size, seq_len, channels = c.shape\n",
    "        \n",
    "        # Apply gating mechanism\n",
    "        c_permuted = c.permute(0, 2, 1)  # [B, hidden, seq_len]\n",
    "        \n",
    "        # Special case for sequence length 1 \n",
    "        if seq_len == 1 and self.use_identity_for_length_1:\n",
    "            # Use the pre-created kernel size 1 conv, which is like identity but keeps channels\n",
    "            c_conv = self.conv1(c_permuted)\n",
    "        else:\n",
    "            # For other sequence lengths, fallback to kernel size 1 to avoid issues\n",
    "            # The conv1 layer is already initialized and on the correct device\n",
    "            c_conv = self.conv(c_permuted)\n",
    "            c_conv = c_conv[:, :, :seq_len] # Ensure we only take the valid part\n",
    "        \n",
    "        c_final = c_conv.permute(0, 2, 1)  # [B, seq_len, hidden]\n",
    "        \n",
    "        # Gating mechanism\n",
    "        x = self.fc2(self.act(g) * c_final)  # [B, seq_len, dim]\n",
    "        \n",
    "        x = self.drop_path(x)\n",
    "        return x + shortcut\n",
    "    \n",
    "class GatedCNNBlock(nn.Module):\n",
    "    \"\"\"Simplified and fixed GatedCNNBlock that preserves sequence length\"\"\"\n",
    "    def __init__(self, dim, d_conv=4, expand=2, drop_path=0.):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim, eps=1e-6)\n",
    "        hidden = int(expand * dim)\n",
    "        self.fc1 = nn.Linear(dim, hidden * 2)\n",
    "        self.act = nn.GELU()\n",
    "        \n",
    "        # Properly calculate padding to ensure output length matches input length\n",
    "        # For kernel_size k, padding needed is (k-1)/2, rounded up for even kernels\n",
    "        self.d_conv = d_conv\n",
    "        padding = (d_conv - 1) // 2\n",
    "        \n",
    "        # Single convolution with proper padding\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels=hidden,\n",
    "            out_channels=hidden, \n",
    "            kernel_size=d_conv,\n",
    "            padding=padding,\n",
    "            groups=hidden\n",
    "        )\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden, dim)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input shape: [B, seq_len, dim]\n",
    "        shortcut = x\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        # Split for gating\n",
    "        x = self.fc1(x)\n",
    "        g, c = torch.chunk(x, 2, dim=-1)\n",
    "        \n",
    "        # Check shapes before processing\n",
    "        batch_size, seq_len, channels = c.shape\n",
    "        \n",
    "        # Apply convolution\n",
    "        c_permuted = c.permute(0, 2, 1)  # [B, hidden, seq_len]\n",
    "        c_conv = self.conv(c_permuted)\n",
    "        \n",
    "        # Ensure output sequence length matches input\n",
    "        if c_conv.size(2) != seq_len:\n",
    "            if c_conv.size(2) < seq_len:\n",
    "                # Pad if shorter\n",
    "                padding = torch.zeros(\n",
    "                    batch_size, channels, seq_len - c_conv.size(2),\n",
    "                    device=c_conv.device, dtype=c_conv.dtype\n",
    "                )\n",
    "                c_conv = torch.cat([c_conv, padding], dim=2)\n",
    "            else:\n",
    "                # Truncate if longer\n",
    "                c_conv = c_conv[:, :, :seq_len]\n",
    "        \n",
    "        c_final = c_conv.permute(0, 2, 1)  # [B, seq_len, hidden]\n",
    "        \n",
    "        # Perform gating and output projection\n",
    "        x = self.fc2(self.act(g) * c_final)\n",
    "        x = self.drop_path(x)\n",
    "        \n",
    "        return x + shortcut\n",
    "\n",
    "class SequenceMambaOut(nn.Module):\n",
    "    \"\"\"Adaptation of MambaOut for sequence data with a single stage\"\"\"\n",
    "    def __init__(self, d_model,  d_conv=4, expand=2, depth=1, drop_path=0.):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Create a sequence of GatedCNNBlocks\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[GatedCNNBlock(\n",
    "                dim=d_model,\n",
    "                d_conv=d_conv,\n",
    "                expand=expand,\n",
    "                drop_path=drop_path\n",
    "            ) for _ in range(depth)]\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.blocks(x)\n",
    "\n",
    "class CrossAttentionBlock(nn.Module):\n",
    "    def __init__(self, dim, n_heads=8):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=dim,\n",
    "            num_heads=n_heads,\n",
    "            batch_first=True\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, context):\n",
    "        \"\"\"\n",
    "        x: (B, seq_len_x, dim)\n",
    "        context: (B, seq_len_context, dim)\n",
    "        \"\"\"\n",
    "        x_norm = self.norm(x)\n",
    "        attn_output, _ = self.attention(\n",
    "            query=x_norm,\n",
    "            key=context,\n",
    "            value=context\n",
    "        )\n",
    "        return x + attn_output\n",
    "\n",
    "\n",
    "    \n",
    "class StarClassifierFusionMambaOut(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model_spectra,\n",
    "        d_model_gaia,\n",
    "        num_classes,\n",
    "        input_dim_spectra,\n",
    "        input_dim_gaia,\n",
    "        token_dim_spectra,  # New parameter for token size\n",
    "        token_dim_gaia,      # New parameter for token size\n",
    "        n_layers=6,\n",
    "        use_cross_attention=True,\n",
    "        n_cross_attn_heads=8,\n",
    "        d_conv=4,\n",
    "        expand=2,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model_spectra (int): embedding dimension for the spectra MAMBA\n",
    "            d_model_gaia (int): embedding dimension for the gaia MAMBA\n",
    "            num_classes (int): multi-label classification\n",
    "            input_dim_spectra (int): # of features for spectra\n",
    "            input_dim_gaia (int): # of features for gaia\n",
    "            token_dim_spectra (int): size of each token for spectra features\n",
    "            token_dim_gaia (int): size of each token for gaia features\n",
    "            n_layers (int): depth for each MAMBA\n",
    "            use_cross_attention (bool): whether to use cross-attention\n",
    "            n_cross_attn_heads (int): number of heads for cross-attention\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # --- Feature Tokenizers ---\n",
    "        self.tokenizer_spectra = FeatureTokenizer(\n",
    "            input_dim=input_dim_spectra,\n",
    "            token_dim=token_dim_spectra,\n",
    "            d_model=d_model_spectra\n",
    "        )\n",
    "        \n",
    "        self.tokenizer_gaia = FeatureTokenizer(\n",
    "            input_dim=input_dim_gaia,\n",
    "            token_dim=token_dim_gaia,\n",
    "            d_model=d_model_gaia\n",
    "        )\n",
    "\n",
    "        # --- MambaOut for spectra ---\n",
    "        self.mamba_spectra = nn.Sequential(\n",
    "            *[SequenceMambaOut(\n",
    "                d_model=d_model_spectra,\n",
    "                d_conv=d_conv,\n",
    "                expand=expand,\n",
    "                depth=1,\n",
    "                drop_path=0.1 if i > 0 else 0.0,\n",
    "            ) for i in range(n_layers)]\n",
    "        )\n",
    "\n",
    "        # --- MambaOut for gaia ---\n",
    "        self.mamba_gaia = nn.Sequential(\n",
    "            *[SequenceMambaOut(\n",
    "                d_model=d_model_gaia,\n",
    "                d_conv=d_conv,\n",
    "                expand=expand,\n",
    "                depth=1,\n",
    "                drop_path=0.1 if i > 0 else 0.0,\n",
    "            ) for i in range(n_layers)]\n",
    "        )\n",
    "\n",
    "        # --- Cross Attention (Optional) ---\n",
    "        self.use_cross_attention = use_cross_attention\n",
    "        if use_cross_attention:\n",
    "            self.cross_attn_block_spectra = CrossAttentionBlock(d_model_spectra, n_heads=n_cross_attn_heads)\n",
    "            self.cross_attn_block_gaia = CrossAttentionBlock(d_model_gaia, n_heads=n_cross_attn_heads)\n",
    "\n",
    "        # --- Final Classifier ---\n",
    "        fusion_dim = d_model_spectra + d_model_gaia\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(fusion_dim),\n",
    "            nn.Linear(fusion_dim, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x_spectra, x_gaia):\n",
    "        \"\"\"\n",
    "        x_spectra : (batch_size, input_dim_spectra)\n",
    "        x_gaia    : (batch_size, input_dim_gaia)\n",
    "        \"\"\"\n",
    "        # Tokenize input features\n",
    "        # From [batch_size, input_dim] to [batch_size, num_tokens, d_model]\n",
    "        x_spectra = self.tokenizer_spectra(x_spectra)  # (B, num_tokens_spectra, d_model_spectra)\n",
    "        x_gaia = self.tokenizer_gaia(x_gaia)           # (B, num_tokens_gaia, d_model_gaia)\n",
    "\n",
    "        # --- MambaOut encoding (each modality separately) ---\n",
    "        x_spectra = self.mamba_spectra(x_spectra)  # (B, num_tokens_spectra, d_model_spectra)\n",
    "        x_gaia = self.mamba_gaia(x_gaia)           # (B, num_tokens_gaia, d_model_gaia)\n",
    "\n",
    "        # Optionally, use cross-attention to fuse the representations\n",
    "        if self.use_cross_attention:\n",
    "            # Cross-attention from spectra -> gaia\n",
    "            x_spectra_fused = self.cross_attn_block_spectra(x_spectra, x_gaia)\n",
    "            # Cross-attention from gaia -> spectra\n",
    "            x_gaia_fused = self.cross_attn_block_gaia(x_gaia, x_spectra)\n",
    "            \n",
    "            # Update x_spectra and x_gaia\n",
    "            x_spectra = x_spectra_fused\n",
    "            x_gaia = x_gaia_fused\n",
    "        \n",
    "        # --- Pool across sequence dimension ---\n",
    "        x_spectra = x_spectra.mean(dim=1)  # (B, d_model_spectra)\n",
    "        x_gaia = x_gaia.mean(dim=1)        # (B, d_model_gaia)\n",
    "\n",
    "        # --- Late Fusion by Concatenation ---\n",
    "        x_fused = torch.cat([x_spectra, x_gaia], dim=-1)  # (B, d_model_spectra + d_model_gaia)\n",
    "\n",
    "        # --- Final classification ---\n",
    "        logits = self.classifier(x_fused)  # (B, num_classes)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from functools import partial\n",
    "\n",
    "# Rotary Position Embeddings implementation\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_seq_len=4096):\n",
    "        super().__init__()\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        # Generate position embeddings once at initialization\n",
    "        self._generate_embeddings()\n",
    "        \n",
    "    def _generate_embeddings(self):\n",
    "        t = torch.arange(self.max_seq_len, dtype=torch.float)\n",
    "        freqs = torch.einsum('i,j->ij', t, self.inv_freq)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        cos = emb.cos().view(self.max_seq_len, 1, -1)\n",
    "        sin = emb.sin().view(self.max_seq_len, 1, -1)\n",
    "        self.register_buffer('cos_cached', cos)\n",
    "        self.register_buffer('sin_cached', sin)\n",
    "        \n",
    "    def forward(self, seq_len):\n",
    "        return self.cos_cached[:seq_len, ...], self.sin_cached[:seq_len, ...]\n",
    "\n",
    "def rotate_half(x):\n",
    "    \"\"\"Rotate half the hidden dims of the input.\"\"\"\n",
    "    x1, x2 = x.chunk(2, dim=-1)\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin):\n",
    "    \"\"\"Apply rotary position embeddings to q and k tensors.\"\"\"\n",
    "    # Handle the case where q and k have shape [batch_size, seq_len, head_dim]\n",
    "    # or [batch_size, n_heads, seq_len, head_dim]\n",
    "    if q.dim() == 3:\n",
    "        # [batch_size, seq_len, head_dim] -> [batch_size, seq_len, 1, head_dim]\n",
    "        q = q.unsqueeze(2)\n",
    "        k = k.unsqueeze(2)\n",
    "        # After this operation, we squeeze back\n",
    "        squeeze_after = True\n",
    "    else:\n",
    "        squeeze_after = False\n",
    "    \n",
    "    # Reshape cos and sin for proper broadcasting\n",
    "    # [seq_len, 1, head_dim] -> [1, seq_len, 1, head_dim]\n",
    "    cos = cos.unsqueeze(0)\n",
    "    sin = sin.unsqueeze(0)\n",
    "    \n",
    "    # Apply rotation\n",
    "    q_rot = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_rot = (k * cos) + (rotate_half(k) * sin)\n",
    "    \n",
    "    if squeeze_after:\n",
    "        q_rot = q_rot.squeeze(2)\n",
    "        k_rot = k_rot.squeeze(2)\n",
    "    \n",
    "    return q_rot, k_rot\n",
    "\n",
    "class RotarySelfAttention(nn.Module):\n",
    "    \"\"\"Self-attention with rotary position embeddings.\"\"\"\n",
    "    def __init__(self, dim, n_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = dim // n_heads\n",
    "        assert self.head_dim * n_heads == dim, \"dim must be divisible by n_heads\"\n",
    "        \n",
    "        # QKV projections\n",
    "        self.q_proj = nn.Linear(dim, dim)\n",
    "        self.k_proj = nn.Linear(dim, dim)\n",
    "        self.v_proj = nn.Linear(dim, dim)\n",
    "        self.out_proj = nn.Linear(dim, dim)\n",
    "        \n",
    "        # Rotary positional embedding\n",
    "        self.rope = RotaryEmbedding(self.head_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor of shape [batch_size, seq_len, dim]\n",
    "            \n",
    "        Returns:\n",
    "            output: Tensor of same shape as input\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Project to queries, keys, values\n",
    "        q = self.q_proj(x)  # [batch_size, seq_len, dim]\n",
    "        k = self.k_proj(x)  # [batch_size, seq_len, dim]\n",
    "        v = self.v_proj(x)  # [batch_size, seq_len, dim]\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        q = q.view(batch_size, seq_len, self.n_heads, self.head_dim)  \n",
    "        k = k.view(batch_size, seq_len, self.n_heads, self.head_dim)\n",
    "        v = v.view(batch_size, seq_len, self.n_heads, self.head_dim)\n",
    "        \n",
    "        # Get position embeddings\n",
    "        cos, sin = self.rope(seq_len)\n",
    "        \n",
    "        # Apply rotary position embeddings to q and k\n",
    "        q, k = apply_rotary_pos_emb(q, k, cos, sin)\n",
    "        \n",
    "        # Transpose for efficient batch matrix multiplication\n",
    "        q = q.transpose(1, 2)  # [batch_size, n_heads, seq_len, head_dim]\n",
    "        k = k.transpose(1, 2)  # [batch_size, n_heads, seq_len, head_dim]\n",
    "        v = v.transpose(1, 2)  # [batch_size, n_heads, seq_len, head_dim]\n",
    "        \n",
    "        # Compute scaled dot-product attention\n",
    "        attn_weights = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)  # [batch_size, n_heads, seq_len, seq_len]\n",
    "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # Apply attention weights to values\n",
    "        output = torch.matmul(attn_weights, v)  # [batch_size, n_heads, seq_len, head_dim]\n",
    "        \n",
    "        # Reshape back to original format\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.dim)\n",
    "        \n",
    "        # Apply output projection\n",
    "        output = self.out_proj(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Transformer block with rotary self-attention and feed-forward network.\"\"\"\n",
    "    def __init__(self, dim, n_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = RotarySelfAttention(dim, n_heads, dropout)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(dim, 4 * dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(4 * dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor of shape [batch_size, seq_len, dim]\n",
    "        \"\"\"\n",
    "        # Self-attention with residual connection\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        \n",
    "        # FFN with residual connection\n",
    "        x = x + self.ffn(self.norm2(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "class TransformerFeatureExtractor(nn.Module):\n",
    "    \"\"\"Stack of transformer blocks for feature extraction.\"\"\"\n",
    "    def __init__(self, d_model, n_layers=6, n_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(d_model, n_heads, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor of shape [batch_size, seq_len, d_model]\n",
    "        \n",
    "        Returns:\n",
    "            Processed tensor of same shape\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class CrossAttentionBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Cross-attention block to attend from one modality to another.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, n_heads=8):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=dim,\n",
    "            num_heads=n_heads,\n",
    "            batch_first=True\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, context):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Query tensor of shape [batch_size, seq_len_q, dim]\n",
    "            context: Key/value tensor of shape [batch_size, seq_len_kv, dim]\n",
    "        \n",
    "        Returns:\n",
    "            Output tensor of shape [batch_size, seq_len_q, dim]\n",
    "        \"\"\"\n",
    "        x_norm = self.norm(x)\n",
    "        attn_output, _ = self.attention(\n",
    "            query=x_norm,\n",
    "            key=context,\n",
    "            value=context\n",
    "        )\n",
    "        return x + attn_output\n",
    "\n",
    "\n",
    "class StarClassifierFusionTransformer(nn.Module):\n",
    "    \"\"\"Transformer-based feature extractor with tokenization for multi-modal fusion.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model_spectra,\n",
    "        d_model_gaia,\n",
    "        num_classes,\n",
    "        input_dim_spectra,\n",
    "        input_dim_gaia,\n",
    "        token_dim_spectra=64,  # Size of each token for spectra\n",
    "        token_dim_gaia=2,      # Size of each token for gaia\n",
    "        n_layers=6,\n",
    "        n_heads=8,\n",
    "        use_cross_attention=True,\n",
    "        n_cross_attn_heads=8,\n",
    "        dropout=0.1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model_spectra (int): embedding dimension for the spectra Transformer\n",
    "            d_model_gaia (int): embedding dimension for the gaia Transformer\n",
    "            num_classes (int): multi-label classification\n",
    "            input_dim_spectra (int): # of features for spectra\n",
    "            input_dim_gaia (int): # of features for gaia\n",
    "            token_dim_spectra (int): size of each token for spectra features\n",
    "            token_dim_gaia (int): size of each token for gaia features\n",
    "            n_layers (int): depth for each Transformer\n",
    "            n_heads (int): number of attention heads\n",
    "            use_cross_attention (bool): whether to use cross-attention\n",
    "            n_cross_attn_heads (int): number of heads for cross-attention\n",
    "            dropout (float): dropout rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # --- Feature Tokenizers ---\n",
    "        self.tokenizer_spectra = FeatureTokenizer(\n",
    "            input_dim=input_dim_spectra,\n",
    "            token_dim=token_dim_spectra,\n",
    "            d_model=d_model_spectra\n",
    "        )\n",
    "        \n",
    "        self.tokenizer_gaia = FeatureTokenizer(\n",
    "            input_dim=input_dim_gaia,\n",
    "            token_dim=token_dim_gaia,\n",
    "            d_model=d_model_gaia\n",
    "        )\n",
    "\n",
    "        # --- Transformer for spectra ---\n",
    "        self.transformer_spectra = TransformerFeatureExtractor(\n",
    "            d_model=d_model_spectra,\n",
    "            n_layers=n_layers,\n",
    "            n_heads=n_heads,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        # --- Transformer for gaia ---\n",
    "        self.transformer_gaia = TransformerFeatureExtractor(\n",
    "            d_model=d_model_gaia,\n",
    "            n_layers=n_layers,\n",
    "            n_heads=n_heads,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        # --- Cross Attention (Optional) ---\n",
    "        self.use_cross_attention = use_cross_attention\n",
    "        if use_cross_attention:\n",
    "            self.cross_attn_block_spectra = CrossAttentionBlock(d_model_spectra, n_heads=n_cross_attn_heads)\n",
    "            self.cross_attn_block_gaia = CrossAttentionBlock(d_model_gaia, n_heads=n_cross_attn_heads)\n",
    "\n",
    "        # --- Final Classifier ---\n",
    "        fusion_dim = d_model_spectra + d_model_gaia\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(fusion_dim),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(fusion_dim, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x_spectra, x_gaia):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x_spectra: Spectra features of shape [batch_size, input_dim_spectra]\n",
    "            x_gaia: Gaia features of shape [batch_size, input_dim_gaia]\n",
    "            \n",
    "        Returns:\n",
    "            logits: Classification logits of shape [batch_size, num_classes]\n",
    "        \"\"\"\n",
    "        # Tokenize input features\n",
    "        # From [batch_size, input_dim] to [batch_size, num_tokens, d_model]\n",
    "        x_spectra_tokens = self.tokenizer_spectra(x_spectra)\n",
    "        x_gaia_tokens = self.tokenizer_gaia(x_gaia)\n",
    "        \n",
    "        # Process through transformers\n",
    "        x_spectra = self.transformer_spectra(x_spectra_tokens)  # [batch_size, num_tokens_spectra, d_model]\n",
    "        x_gaia = self.transformer_gaia(x_gaia_tokens)          # [batch_size, num_tokens_gaia, d_model]\n",
    "\n",
    "        # Optional cross-attention\n",
    "        if self.use_cross_attention:\n",
    "            x_spectra = self.cross_attn_block_spectra(x_spectra, x_gaia)\n",
    "            x_gaia = self.cross_attn_block_gaia(x_gaia, x_spectra)\n",
    "        \n",
    "        # Global pooling over sequence dimension\n",
    "        x_spectra = x_spectra.mean(dim=1)  # [batch_size, d_model]\n",
    "        x_gaia = x_gaia.mean(dim=1)        # [batch_size, d_model]\n",
    "\n",
    "        # Concatenate for fusion\n",
    "        x_fused = torch.cat([x_spectra, x_gaia], dim=-1)  # [batch_size, 2*d_model]\n",
    "\n",
    "        # Final classification\n",
    "        logits = self.classifier(x_fused)  # [batch_size, num_classes]\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from mamba_ssm import Mamba2\n",
    "\n",
    "\n",
    "class StarClassifierFusionMambaTokenized(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model_spectra,\n",
    "        d_model_gaia,\n",
    "        num_classes,\n",
    "        input_dim_spectra,\n",
    "        input_dim_gaia,\n",
    "        token_dim_spectra=64,  # Size of each token for spectra\n",
    "        token_dim_gaia=2,      # Size of each token for gaia\n",
    "        n_layers=10,\n",
    "        use_cross_attention=True,\n",
    "        n_cross_attn_heads=8,\n",
    "        d_state=256,\n",
    "        d_conv=4,\n",
    "        expand=2,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model_spectra (int): embedding dimension for the spectra MAMBA\n",
    "            d_model_gaia (int): embedding dimension for the gaia MAMBA\n",
    "            num_classes (int): multi-label classification\n",
    "            input_dim_spectra (int): # of features for spectra\n",
    "            input_dim_gaia (int): # of features for gaia\n",
    "            token_dim_spectra (int): size of each token for spectra features\n",
    "            token_dim_gaia (int): size of each token for gaia features\n",
    "            n_layers (int): depth for each MAMBA\n",
    "            use_cross_attention (bool): whether to use cross-attention\n",
    "            n_cross_attn_heads (int): number of heads for cross-attention\n",
    "            d_state (int): state dimension for Mamba\n",
    "            d_conv (int): convolution dimension for Mamba\n",
    "            expand (int): expansion factor for Mamba\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # --- Feature Tokenizers ---\n",
    "        self.tokenizer_spectra = FeatureTokenizer(\n",
    "            input_dim=input_dim_spectra,\n",
    "            token_dim=token_dim_spectra,\n",
    "            d_model=d_model_spectra\n",
    "        )\n",
    "        \n",
    "        self.tokenizer_gaia = FeatureTokenizer(\n",
    "            input_dim=input_dim_gaia,\n",
    "            token_dim=token_dim_gaia,\n",
    "            d_model=d_model_gaia\n",
    "        )\n",
    "\n",
    "        # --- MAMBA 2 for spectra ---\n",
    "        self.mamba_spectra = nn.Sequential(\n",
    "            *[Mamba2(\n",
    "                d_model=d_model_spectra,\n",
    "                d_state=d_state,\n",
    "                d_conv=d_conv,\n",
    "                expand=expand,\n",
    "            ) for _ in range(n_layers)]\n",
    "        )\n",
    "\n",
    "        # --- MAMBA 2 for gaia ---\n",
    "        self.mamba_gaia = nn.Sequential(\n",
    "            *[Mamba2(\n",
    "                d_model=d_model_gaia,\n",
    "                d_state=d_state,\n",
    "                d_conv=d_conv,\n",
    "                expand=expand,\n",
    "            ) for _ in range(n_layers)]\n",
    "        )\n",
    "\n",
    "        # --- Cross Attention (Optional) ---\n",
    "        self.use_cross_attention = use_cross_attention\n",
    "        if use_cross_attention:\n",
    "            self.cross_attn_block_spectra = CrossAttentionBlock(d_model_spectra, n_heads=n_cross_attn_heads)\n",
    "            self.cross_attn_block_gaia = CrossAttentionBlock(d_model_gaia, n_heads=n_cross_attn_heads)\n",
    "\n",
    "        # --- Final Classifier ---\n",
    "        fusion_dim = d_model_spectra + d_model_gaia\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(fusion_dim),\n",
    "            nn.Linear(fusion_dim, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x_spectra, x_gaia):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x_spectra: Spectra features of shape [batch_size, input_dim_spectra]\n",
    "            x_gaia: Gaia features of shape [batch_size, input_dim_gaia]\n",
    "            \n",
    "        Returns:\n",
    "            logits: Classification logits of shape [batch_size, num_classes]\n",
    "        \"\"\"\n",
    "        # Tokenize input features\n",
    "        # From [batch_size, input_dim] to [batch_size, num_tokens, d_model]\n",
    "        x_spectra_tokens = self.tokenizer_spectra(x_spectra)\n",
    "        x_gaia_tokens = self.tokenizer_gaia(x_gaia)\n",
    "        \n",
    "        # Process through Mamba models\n",
    "        x_spectra = self.mamba_spectra(x_spectra_tokens)  # [batch_size, num_tokens_spectra, d_model]\n",
    "        x_gaia = self.mamba_gaia(x_gaia_tokens)          # [batch_size, num_tokens_gaia, d_model]\n",
    "\n",
    "        # Optional cross-attention\n",
    "        if self.use_cross_attention:\n",
    "            x_spectra = self.cross_attn_block_spectra(x_spectra, x_gaia)\n",
    "            x_gaia = self.cross_attn_block_gaia(x_gaia, x_spectra)\n",
    "        \n",
    "        # Global pooling over sequence dimension\n",
    "        x_spectra = x_spectra.mean(dim=1)  # [batch_size, d_model]\n",
    "        x_gaia = x_gaia.mean(dim=1)        # [batch_size, d_model]\n",
    "\n",
    "        # Concatenate for fusion\n",
    "        x_fused = torch.cat([x_spectra, x_gaia], dim=-1)  # [batch_size, 2*d_model]\n",
    "\n",
    "        # Final classification\n",
    "        logits = self.classifier(x_fused)  # [batch_size, num_classes]\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'egoinrign' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43megoinrign\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'egoinrign' is not defined"
     ]
    }
   ],
   "source": [
    "egoinrign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjoaocsgalmeida\u001b[0m (\u001b[33mjoaocsgalmeida-university-of-southampton\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/joao/Documents/Star-Classifier/wandb/run-20250408_150324-ojlnlh6s</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_mamba_out/runs/ojlnlh6s' target=\"_blank\">magic-microwave-18</a></strong> to <a href='https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_mamba_out' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_mamba_out' target=\"_blank\">https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_mamba_out</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_mamba_out/runs/ojlnlh6s' target=\"_blank\">https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_mamba_out/runs/ojlnlh6s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 1.05 GB\n",
      "model size: 4001.071MB\n",
      "Model size: 4001.071 MB\n",
      "StarClassifierFusionMambaOut(\n",
      "  (tokenizer_spectra): FeatureTokenizer(\n",
      "    (token_embed): Linear(in_features=3647, out_features=2048, bias=True)\n",
      "  )\n",
      "  (tokenizer_gaia): FeatureTokenizer(\n",
      "    (token_embed): Linear(in_features=18, out_features=2048, bias=True)\n",
      "  )\n",
      "  (mamba_spectra): Sequential(\n",
      "    (0): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(1,), stride=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(1,), stride=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(1,), stride=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(1,), stride=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(1,), stride=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(1,), stride=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (6): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(1,), stride=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (7): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(1,), stride=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (8): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(1,), stride=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (9): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(1,), stride=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (10): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(1,), stride=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (11): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(1,), stride=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (12): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(1,), stride=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (13): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(1,), stride=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (14): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(1,), stride=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (15): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(1,), stride=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (16): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(1,), stride=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (17): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(1,), stride=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (18): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(1,), stride=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (19): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(1,), stride=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (mamba_gaia): Sequential(\n",
      "    (0): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(1,), stride=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(1,), stride=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(1,), stride=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(1,), stride=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(1,), stride=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(1,), stride=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (6): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(1,), stride=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (7): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(1,), stride=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (8): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(1,), stride=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (9): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(1,), stride=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (10): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(1,), stride=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (11): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(1,), stride=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (12): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(1,), stride=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (13): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(1,), stride=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (14): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(1,), stride=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (15): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(1,), stride=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (16): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(1,), stride=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (17): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(1,), stride=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (18): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(1,), stride=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (19): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(1,), stride=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (cross_attn_block_spectra): CrossAttentionBlock(\n",
      "    (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "    (attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=2048, out_features=2048, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (cross_attn_block_gaia): CrossAttentionBlock(\n",
      "    (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "    (attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=2048, out_features=2048, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): Linear(in_features=4096, out_features=55, bias=True)\n",
      "  )\n",
      ")\n",
      "tokenizer_spectra.token_embed.weight 7469056\n",
      "tokenizer_spectra.token_embed.bias 2048\n",
      "tokenizer_gaia.token_embed.weight 36864\n",
      "tokenizer_gaia.token_embed.bias 2048\n",
      "mamba_spectra.0.blocks.0.norm.weight 2048\n",
      "mamba_spectra.0.blocks.0.norm.bias 2048\n",
      "mamba_spectra.0.blocks.0.fc1.weight 16777216\n",
      "mamba_spectra.0.blocks.0.fc1.bias 8192\n",
      "mamba_spectra.0.blocks.0.conv.weight 4096\n",
      "mamba_spectra.0.blocks.0.conv.bias 4096\n",
      "mamba_spectra.0.blocks.0.fc2.weight 8388608\n",
      "mamba_spectra.0.blocks.0.fc2.bias 2048\n",
      "mamba_spectra.1.blocks.0.norm.weight 2048\n",
      "mamba_spectra.1.blocks.0.norm.bias 2048\n",
      "mamba_spectra.1.blocks.0.fc1.weight 16777216\n",
      "mamba_spectra.1.blocks.0.fc1.bias 8192\n",
      "mamba_spectra.1.blocks.0.conv.weight 4096\n",
      "mamba_spectra.1.blocks.0.conv.bias 4096\n",
      "mamba_spectra.1.blocks.0.fc2.weight 8388608\n",
      "mamba_spectra.1.blocks.0.fc2.bias 2048\n",
      "mamba_spectra.2.blocks.0.norm.weight 2048\n",
      "mamba_spectra.2.blocks.0.norm.bias 2048\n",
      "mamba_spectra.2.blocks.0.fc1.weight 16777216\n",
      "mamba_spectra.2.blocks.0.fc1.bias 8192\n",
      "mamba_spectra.2.blocks.0.conv.weight 4096\n",
      "mamba_spectra.2.blocks.0.conv.bias 4096\n",
      "mamba_spectra.2.blocks.0.fc2.weight 8388608\n",
      "mamba_spectra.2.blocks.0.fc2.bias 2048\n",
      "mamba_spectra.3.blocks.0.norm.weight 2048\n",
      "mamba_spectra.3.blocks.0.norm.bias 2048\n",
      "mamba_spectra.3.blocks.0.fc1.weight 16777216\n",
      "mamba_spectra.3.blocks.0.fc1.bias 8192\n",
      "mamba_spectra.3.blocks.0.conv.weight 4096\n",
      "mamba_spectra.3.blocks.0.conv.bias 4096\n",
      "mamba_spectra.3.blocks.0.fc2.weight 8388608\n",
      "mamba_spectra.3.blocks.0.fc2.bias 2048\n",
      "mamba_spectra.4.blocks.0.norm.weight 2048\n",
      "mamba_spectra.4.blocks.0.norm.bias 2048\n",
      "mamba_spectra.4.blocks.0.fc1.weight 16777216\n",
      "mamba_spectra.4.blocks.0.fc1.bias 8192\n",
      "mamba_spectra.4.blocks.0.conv.weight 4096\n",
      "mamba_spectra.4.blocks.0.conv.bias 4096\n",
      "mamba_spectra.4.blocks.0.fc2.weight 8388608\n",
      "mamba_spectra.4.blocks.0.fc2.bias 2048\n",
      "mamba_spectra.5.blocks.0.norm.weight 2048\n",
      "mamba_spectra.5.blocks.0.norm.bias 2048\n",
      "mamba_spectra.5.blocks.0.fc1.weight 16777216\n",
      "mamba_spectra.5.blocks.0.fc1.bias 8192\n",
      "mamba_spectra.5.blocks.0.conv.weight 4096\n",
      "mamba_spectra.5.blocks.0.conv.bias 4096\n",
      "mamba_spectra.5.blocks.0.fc2.weight 8388608\n",
      "mamba_spectra.5.blocks.0.fc2.bias 2048\n",
      "mamba_spectra.6.blocks.0.norm.weight 2048\n",
      "mamba_spectra.6.blocks.0.norm.bias 2048\n",
      "mamba_spectra.6.blocks.0.fc1.weight 16777216\n",
      "mamba_spectra.6.blocks.0.fc1.bias 8192\n",
      "mamba_spectra.6.blocks.0.conv.weight 4096\n",
      "mamba_spectra.6.blocks.0.conv.bias 4096\n",
      "mamba_spectra.6.blocks.0.fc2.weight 8388608\n",
      "mamba_spectra.6.blocks.0.fc2.bias 2048\n",
      "mamba_spectra.7.blocks.0.norm.weight 2048\n",
      "mamba_spectra.7.blocks.0.norm.bias 2048\n",
      "mamba_spectra.7.blocks.0.fc1.weight 16777216\n",
      "mamba_spectra.7.blocks.0.fc1.bias 8192\n",
      "mamba_spectra.7.blocks.0.conv.weight 4096\n",
      "mamba_spectra.7.blocks.0.conv.bias 4096\n",
      "mamba_spectra.7.blocks.0.fc2.weight 8388608\n",
      "mamba_spectra.7.blocks.0.fc2.bias 2048\n",
      "mamba_spectra.8.blocks.0.norm.weight 2048\n",
      "mamba_spectra.8.blocks.0.norm.bias 2048\n",
      "mamba_spectra.8.blocks.0.fc1.weight 16777216\n",
      "mamba_spectra.8.blocks.0.fc1.bias 8192\n",
      "mamba_spectra.8.blocks.0.conv.weight 4096\n",
      "mamba_spectra.8.blocks.0.conv.bias 4096\n",
      "mamba_spectra.8.blocks.0.fc2.weight 8388608\n",
      "mamba_spectra.8.blocks.0.fc2.bias 2048\n",
      "mamba_spectra.9.blocks.0.norm.weight 2048\n",
      "mamba_spectra.9.blocks.0.norm.bias 2048\n",
      "mamba_spectra.9.blocks.0.fc1.weight 16777216\n",
      "mamba_spectra.9.blocks.0.fc1.bias 8192\n",
      "mamba_spectra.9.blocks.0.conv.weight 4096\n",
      "mamba_spectra.9.blocks.0.conv.bias 4096\n",
      "mamba_spectra.9.blocks.0.fc2.weight 8388608\n",
      "mamba_spectra.9.blocks.0.fc2.bias 2048\n",
      "mamba_spectra.10.blocks.0.norm.weight 2048\n",
      "mamba_spectra.10.blocks.0.norm.bias 2048\n",
      "mamba_spectra.10.blocks.0.fc1.weight 16777216\n",
      "mamba_spectra.10.blocks.0.fc1.bias 8192\n",
      "mamba_spectra.10.blocks.0.conv.weight 4096\n",
      "mamba_spectra.10.blocks.0.conv.bias 4096\n",
      "mamba_spectra.10.blocks.0.fc2.weight 8388608\n",
      "mamba_spectra.10.blocks.0.fc2.bias 2048\n",
      "mamba_spectra.11.blocks.0.norm.weight 2048\n",
      "mamba_spectra.11.blocks.0.norm.bias 2048\n",
      "mamba_spectra.11.blocks.0.fc1.weight 16777216\n",
      "mamba_spectra.11.blocks.0.fc1.bias 8192\n",
      "mamba_spectra.11.blocks.0.conv.weight 4096\n",
      "mamba_spectra.11.blocks.0.conv.bias 4096\n",
      "mamba_spectra.11.blocks.0.fc2.weight 8388608\n",
      "mamba_spectra.11.blocks.0.fc2.bias 2048\n",
      "mamba_spectra.12.blocks.0.norm.weight 2048\n",
      "mamba_spectra.12.blocks.0.norm.bias 2048\n",
      "mamba_spectra.12.blocks.0.fc1.weight 16777216\n",
      "mamba_spectra.12.blocks.0.fc1.bias 8192\n",
      "mamba_spectra.12.blocks.0.conv.weight 4096\n",
      "mamba_spectra.12.blocks.0.conv.bias 4096\n",
      "mamba_spectra.12.blocks.0.fc2.weight 8388608\n",
      "mamba_spectra.12.blocks.0.fc2.bias 2048\n",
      "mamba_spectra.13.blocks.0.norm.weight 2048\n",
      "mamba_spectra.13.blocks.0.norm.bias 2048\n",
      "mamba_spectra.13.blocks.0.fc1.weight 16777216\n",
      "mamba_spectra.13.blocks.0.fc1.bias 8192\n",
      "mamba_spectra.13.blocks.0.conv.weight 4096\n",
      "mamba_spectra.13.blocks.0.conv.bias 4096\n",
      "mamba_spectra.13.blocks.0.fc2.weight 8388608\n",
      "mamba_spectra.13.blocks.0.fc2.bias 2048\n",
      "mamba_spectra.14.blocks.0.norm.weight 2048\n",
      "mamba_spectra.14.blocks.0.norm.bias 2048\n",
      "mamba_spectra.14.blocks.0.fc1.weight 16777216\n",
      "mamba_spectra.14.blocks.0.fc1.bias 8192\n",
      "mamba_spectra.14.blocks.0.conv.weight 4096\n",
      "mamba_spectra.14.blocks.0.conv.bias 4096\n",
      "mamba_spectra.14.blocks.0.fc2.weight 8388608\n",
      "mamba_spectra.14.blocks.0.fc2.bias 2048\n",
      "mamba_spectra.15.blocks.0.norm.weight 2048\n",
      "mamba_spectra.15.blocks.0.norm.bias 2048\n",
      "mamba_spectra.15.blocks.0.fc1.weight 16777216\n",
      "mamba_spectra.15.blocks.0.fc1.bias 8192\n",
      "mamba_spectra.15.blocks.0.conv.weight 4096\n",
      "mamba_spectra.15.blocks.0.conv.bias 4096\n",
      "mamba_spectra.15.blocks.0.fc2.weight 8388608\n",
      "mamba_spectra.15.blocks.0.fc2.bias 2048\n",
      "mamba_spectra.16.blocks.0.norm.weight 2048\n",
      "mamba_spectra.16.blocks.0.norm.bias 2048\n",
      "mamba_spectra.16.blocks.0.fc1.weight 16777216\n",
      "mamba_spectra.16.blocks.0.fc1.bias 8192\n",
      "mamba_spectra.16.blocks.0.conv.weight 4096\n",
      "mamba_spectra.16.blocks.0.conv.bias 4096\n",
      "mamba_spectra.16.blocks.0.fc2.weight 8388608\n",
      "mamba_spectra.16.blocks.0.fc2.bias 2048\n",
      "mamba_spectra.17.blocks.0.norm.weight 2048\n",
      "mamba_spectra.17.blocks.0.norm.bias 2048\n",
      "mamba_spectra.17.blocks.0.fc1.weight 16777216\n",
      "mamba_spectra.17.blocks.0.fc1.bias 8192\n",
      "mamba_spectra.17.blocks.0.conv.weight 4096\n",
      "mamba_spectra.17.blocks.0.conv.bias 4096\n",
      "mamba_spectra.17.blocks.0.fc2.weight 8388608\n",
      "mamba_spectra.17.blocks.0.fc2.bias 2048\n",
      "mamba_spectra.18.blocks.0.norm.weight 2048\n",
      "mamba_spectra.18.blocks.0.norm.bias 2048\n",
      "mamba_spectra.18.blocks.0.fc1.weight 16777216\n",
      "mamba_spectra.18.blocks.0.fc1.bias 8192\n",
      "mamba_spectra.18.blocks.0.conv.weight 4096\n",
      "mamba_spectra.18.blocks.0.conv.bias 4096\n",
      "mamba_spectra.18.blocks.0.fc2.weight 8388608\n",
      "mamba_spectra.18.blocks.0.fc2.bias 2048\n",
      "mamba_spectra.19.blocks.0.norm.weight 2048\n",
      "mamba_spectra.19.blocks.0.norm.bias 2048\n",
      "mamba_spectra.19.blocks.0.fc1.weight 16777216\n",
      "mamba_spectra.19.blocks.0.fc1.bias 8192\n",
      "mamba_spectra.19.blocks.0.conv.weight 4096\n",
      "mamba_spectra.19.blocks.0.conv.bias 4096\n",
      "mamba_spectra.19.blocks.0.fc2.weight 8388608\n",
      "mamba_spectra.19.blocks.0.fc2.bias 2048\n",
      "mamba_gaia.0.blocks.0.norm.weight 2048\n",
      "mamba_gaia.0.blocks.0.norm.bias 2048\n",
      "mamba_gaia.0.blocks.0.fc1.weight 16777216\n",
      "mamba_gaia.0.blocks.0.fc1.bias 8192\n",
      "mamba_gaia.0.blocks.0.conv.weight 4096\n",
      "mamba_gaia.0.blocks.0.conv.bias 4096\n",
      "mamba_gaia.0.blocks.0.fc2.weight 8388608\n",
      "mamba_gaia.0.blocks.0.fc2.bias 2048\n",
      "mamba_gaia.1.blocks.0.norm.weight 2048\n",
      "mamba_gaia.1.blocks.0.norm.bias 2048\n",
      "mamba_gaia.1.blocks.0.fc1.weight 16777216\n",
      "mamba_gaia.1.blocks.0.fc1.bias 8192\n",
      "mamba_gaia.1.blocks.0.conv.weight 4096\n",
      "mamba_gaia.1.blocks.0.conv.bias 4096\n",
      "mamba_gaia.1.blocks.0.fc2.weight 8388608\n",
      "mamba_gaia.1.blocks.0.fc2.bias 2048\n",
      "mamba_gaia.2.blocks.0.norm.weight 2048\n",
      "mamba_gaia.2.blocks.0.norm.bias 2048\n",
      "mamba_gaia.2.blocks.0.fc1.weight 16777216\n",
      "mamba_gaia.2.blocks.0.fc1.bias 8192\n",
      "mamba_gaia.2.blocks.0.conv.weight 4096\n",
      "mamba_gaia.2.blocks.0.conv.bias 4096\n",
      "mamba_gaia.2.blocks.0.fc2.weight 8388608\n",
      "mamba_gaia.2.blocks.0.fc2.bias 2048\n",
      "mamba_gaia.3.blocks.0.norm.weight 2048\n",
      "mamba_gaia.3.blocks.0.norm.bias 2048\n",
      "mamba_gaia.3.blocks.0.fc1.weight 16777216\n",
      "mamba_gaia.3.blocks.0.fc1.bias 8192\n",
      "mamba_gaia.3.blocks.0.conv.weight 4096\n",
      "mamba_gaia.3.blocks.0.conv.bias 4096\n",
      "mamba_gaia.3.blocks.0.fc2.weight 8388608\n",
      "mamba_gaia.3.blocks.0.fc2.bias 2048\n",
      "mamba_gaia.4.blocks.0.norm.weight 2048\n",
      "mamba_gaia.4.blocks.0.norm.bias 2048\n",
      "mamba_gaia.4.blocks.0.fc1.weight 16777216\n",
      "mamba_gaia.4.blocks.0.fc1.bias 8192\n",
      "mamba_gaia.4.blocks.0.conv.weight 4096\n",
      "mamba_gaia.4.blocks.0.conv.bias 4096\n",
      "mamba_gaia.4.blocks.0.fc2.weight 8388608\n",
      "mamba_gaia.4.blocks.0.fc2.bias 2048\n",
      "mamba_gaia.5.blocks.0.norm.weight 2048\n",
      "mamba_gaia.5.blocks.0.norm.bias 2048\n",
      "mamba_gaia.5.blocks.0.fc1.weight 16777216\n",
      "mamba_gaia.5.blocks.0.fc1.bias 8192\n",
      "mamba_gaia.5.blocks.0.conv.weight 4096\n",
      "mamba_gaia.5.blocks.0.conv.bias 4096\n",
      "mamba_gaia.5.blocks.0.fc2.weight 8388608\n",
      "mamba_gaia.5.blocks.0.fc2.bias 2048\n",
      "mamba_gaia.6.blocks.0.norm.weight 2048\n",
      "mamba_gaia.6.blocks.0.norm.bias 2048\n",
      "mamba_gaia.6.blocks.0.fc1.weight 16777216\n",
      "mamba_gaia.6.blocks.0.fc1.bias 8192\n",
      "mamba_gaia.6.blocks.0.conv.weight 4096\n",
      "mamba_gaia.6.blocks.0.conv.bias 4096\n",
      "mamba_gaia.6.blocks.0.fc2.weight 8388608\n",
      "mamba_gaia.6.blocks.0.fc2.bias 2048\n",
      "mamba_gaia.7.blocks.0.norm.weight 2048\n",
      "mamba_gaia.7.blocks.0.norm.bias 2048\n",
      "mamba_gaia.7.blocks.0.fc1.weight 16777216\n",
      "mamba_gaia.7.blocks.0.fc1.bias 8192\n",
      "mamba_gaia.7.blocks.0.conv.weight 4096\n",
      "mamba_gaia.7.blocks.0.conv.bias 4096\n",
      "mamba_gaia.7.blocks.0.fc2.weight 8388608\n",
      "mamba_gaia.7.blocks.0.fc2.bias 2048\n",
      "mamba_gaia.8.blocks.0.norm.weight 2048\n",
      "mamba_gaia.8.blocks.0.norm.bias 2048\n",
      "mamba_gaia.8.blocks.0.fc1.weight 16777216\n",
      "mamba_gaia.8.blocks.0.fc1.bias 8192\n",
      "mamba_gaia.8.blocks.0.conv.weight 4096\n",
      "mamba_gaia.8.blocks.0.conv.bias 4096\n",
      "mamba_gaia.8.blocks.0.fc2.weight 8388608\n",
      "mamba_gaia.8.blocks.0.fc2.bias 2048\n",
      "mamba_gaia.9.blocks.0.norm.weight 2048\n",
      "mamba_gaia.9.blocks.0.norm.bias 2048\n",
      "mamba_gaia.9.blocks.0.fc1.weight 16777216\n",
      "mamba_gaia.9.blocks.0.fc1.bias 8192\n",
      "mamba_gaia.9.blocks.0.conv.weight 4096\n",
      "mamba_gaia.9.blocks.0.conv.bias 4096\n",
      "mamba_gaia.9.blocks.0.fc2.weight 8388608\n",
      "mamba_gaia.9.blocks.0.fc2.bias 2048\n",
      "mamba_gaia.10.blocks.0.norm.weight 2048\n",
      "mamba_gaia.10.blocks.0.norm.bias 2048\n",
      "mamba_gaia.10.blocks.0.fc1.weight 16777216\n",
      "mamba_gaia.10.blocks.0.fc1.bias 8192\n",
      "mamba_gaia.10.blocks.0.conv.weight 4096\n",
      "mamba_gaia.10.blocks.0.conv.bias 4096\n",
      "mamba_gaia.10.blocks.0.fc2.weight 8388608\n",
      "mamba_gaia.10.blocks.0.fc2.bias 2048\n",
      "mamba_gaia.11.blocks.0.norm.weight 2048\n",
      "mamba_gaia.11.blocks.0.norm.bias 2048\n",
      "mamba_gaia.11.blocks.0.fc1.weight 16777216\n",
      "mamba_gaia.11.blocks.0.fc1.bias 8192\n",
      "mamba_gaia.11.blocks.0.conv.weight 4096\n",
      "mamba_gaia.11.blocks.0.conv.bias 4096\n",
      "mamba_gaia.11.blocks.0.fc2.weight 8388608\n",
      "mamba_gaia.11.blocks.0.fc2.bias 2048\n",
      "mamba_gaia.12.blocks.0.norm.weight 2048\n",
      "mamba_gaia.12.blocks.0.norm.bias 2048\n",
      "mamba_gaia.12.blocks.0.fc1.weight 16777216\n",
      "mamba_gaia.12.blocks.0.fc1.bias 8192\n",
      "mamba_gaia.12.blocks.0.conv.weight 4096\n",
      "mamba_gaia.12.blocks.0.conv.bias 4096\n",
      "mamba_gaia.12.blocks.0.fc2.weight 8388608\n",
      "mamba_gaia.12.blocks.0.fc2.bias 2048\n",
      "mamba_gaia.13.blocks.0.norm.weight 2048\n",
      "mamba_gaia.13.blocks.0.norm.bias 2048\n",
      "mamba_gaia.13.blocks.0.fc1.weight 16777216\n",
      "mamba_gaia.13.blocks.0.fc1.bias 8192\n",
      "mamba_gaia.13.blocks.0.conv.weight 4096\n",
      "mamba_gaia.13.blocks.0.conv.bias 4096\n",
      "mamba_gaia.13.blocks.0.fc2.weight 8388608\n",
      "mamba_gaia.13.blocks.0.fc2.bias 2048\n",
      "mamba_gaia.14.blocks.0.norm.weight 2048\n",
      "mamba_gaia.14.blocks.0.norm.bias 2048\n",
      "mamba_gaia.14.blocks.0.fc1.weight 16777216\n",
      "mamba_gaia.14.blocks.0.fc1.bias 8192\n",
      "mamba_gaia.14.blocks.0.conv.weight 4096\n",
      "mamba_gaia.14.blocks.0.conv.bias 4096\n",
      "mamba_gaia.14.blocks.0.fc2.weight 8388608\n",
      "mamba_gaia.14.blocks.0.fc2.bias 2048\n",
      "mamba_gaia.15.blocks.0.norm.weight 2048\n",
      "mamba_gaia.15.blocks.0.norm.bias 2048\n",
      "mamba_gaia.15.blocks.0.fc1.weight 16777216\n",
      "mamba_gaia.15.blocks.0.fc1.bias 8192\n",
      "mamba_gaia.15.blocks.0.conv.weight 4096\n",
      "mamba_gaia.15.blocks.0.conv.bias 4096\n",
      "mamba_gaia.15.blocks.0.fc2.weight 8388608\n",
      "mamba_gaia.15.blocks.0.fc2.bias 2048\n",
      "mamba_gaia.16.blocks.0.norm.weight 2048\n",
      "mamba_gaia.16.blocks.0.norm.bias 2048\n",
      "mamba_gaia.16.blocks.0.fc1.weight 16777216\n",
      "mamba_gaia.16.blocks.0.fc1.bias 8192\n",
      "mamba_gaia.16.blocks.0.conv.weight 4096\n",
      "mamba_gaia.16.blocks.0.conv.bias 4096\n",
      "mamba_gaia.16.blocks.0.fc2.weight 8388608\n",
      "mamba_gaia.16.blocks.0.fc2.bias 2048\n",
      "mamba_gaia.17.blocks.0.norm.weight 2048\n",
      "mamba_gaia.17.blocks.0.norm.bias 2048\n",
      "mamba_gaia.17.blocks.0.fc1.weight 16777216\n",
      "mamba_gaia.17.blocks.0.fc1.bias 8192\n",
      "mamba_gaia.17.blocks.0.conv.weight 4096\n",
      "mamba_gaia.17.blocks.0.conv.bias 4096\n",
      "mamba_gaia.17.blocks.0.fc2.weight 8388608\n",
      "mamba_gaia.17.blocks.0.fc2.bias 2048\n",
      "mamba_gaia.18.blocks.0.norm.weight 2048\n",
      "mamba_gaia.18.blocks.0.norm.bias 2048\n",
      "mamba_gaia.18.blocks.0.fc1.weight 16777216\n",
      "mamba_gaia.18.blocks.0.fc1.bias 8192\n",
      "mamba_gaia.18.blocks.0.conv.weight 4096\n",
      "mamba_gaia.18.blocks.0.conv.bias 4096\n",
      "mamba_gaia.18.blocks.0.fc2.weight 8388608\n",
      "mamba_gaia.18.blocks.0.fc2.bias 2048\n",
      "mamba_gaia.19.blocks.0.norm.weight 2048\n",
      "mamba_gaia.19.blocks.0.norm.bias 2048\n",
      "mamba_gaia.19.blocks.0.fc1.weight 16777216\n",
      "mamba_gaia.19.blocks.0.fc1.bias 8192\n",
      "mamba_gaia.19.blocks.0.conv.weight 4096\n",
      "mamba_gaia.19.blocks.0.conv.bias 4096\n",
      "mamba_gaia.19.blocks.0.fc2.weight 8388608\n",
      "mamba_gaia.19.blocks.0.fc2.bias 2048\n",
      "cross_attn_block_spectra.norm.weight 2048\n",
      "cross_attn_block_spectra.norm.bias 2048\n",
      "cross_attn_block_spectra.attention.in_proj_weight 12582912\n",
      "cross_attn_block_spectra.attention.in_proj_bias 6144\n",
      "cross_attn_block_spectra.attention.out_proj.weight 4194304\n",
      "cross_attn_block_spectra.attention.out_proj.bias 2048\n",
      "cross_attn_block_gaia.norm.weight 2048\n",
      "cross_attn_block_gaia.norm.bias 2048\n",
      "cross_attn_block_gaia.attention.in_proj_weight 12582912\n",
      "cross_attn_block_gaia.attention.in_proj_bias 6144\n",
      "cross_attn_block_gaia.attention.out_proj.weight 4194304\n",
      "cross_attn_block_gaia.attention.out_proj.bias 2048\n",
      "classifier.0.weight 4096\n",
      "classifier.0.bias 4096\n",
      "classifier.1.weight 225280\n",
      "classifier.1.bias 55\n",
      "Total number of parameters: 1048856631\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td></td></tr><tr><td>hamming_loss</td><td></td></tr><tr><td>lr</td><td></td></tr><tr><td>macro_f1</td><td></td></tr><tr><td>macro_precision</td><td></td></tr><tr><td>macro_recall</td><td></td></tr><tr><td>micro_f1</td><td></td></tr><tr><td>micro_precision</td><td></td></tr><tr><td>micro_recall</td><td></td></tr><tr><td>test_acc</td><td></td></tr><tr><td>test_loss</td><td></td></tr><tr><td>train_acc</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>val_acc</td><td></td></tr><tr><td>val_loss</td><td></td></tr><tr><td>weighted_f1</td><td></td></tr><tr><td>weighted_precision</td><td></td></tr><tr><td>weighted_recall</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>546</td></tr><tr><td>hamming_loss</td><td>0.02296</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>macro_f1</td><td>0.4396</td></tr><tr><td>macro_precision</td><td>0.78807</td></tr><tr><td>macro_recall</td><td>0.3945</td></tr><tr><td>micro_f1</td><td>0.61844</td></tr><tr><td>micro_precision</td><td>0.79819</td></tr><tr><td>micro_recall</td><td>0.50476</td></tr><tr><td>test_acc</td><td>0.97582</td></tr><tr><td>test_loss</td><td>0.06547</td></tr><tr><td>train_acc</td><td>0.99078</td></tr><tr><td>train_loss</td><td>0.0123</td></tr><tr><td>val_acc</td><td>0.98089</td></tr><tr><td>val_loss</td><td>0.0406</td></tr><tr><td>weighted_f1</td><td>0.58949</td></tr><tr><td>weighted_precision</td><td>0.82494</td></tr><tr><td>weighted_recall</td><td>0.50476</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">magic-microwave-18</strong> at: <a href='https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_mamba_out/runs/ojlnlh6s' target=\"_blank\">https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_mamba_out/runs/ojlnlh6s</a><br> View project at: <a href='https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_mamba_out' target=\"_blank\">https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_mamba_out</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250408_150324-ojlnlh6s/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Example config\n",
    "    d_model_spectra = 2048\n",
    "    d_model_gaia = 2048\n",
    "    num_classes = 55\n",
    "    input_dim_spectra = 3647\n",
    "    input_dim_gaia = 18\n",
    "    token_dim_spectra = 3647\n",
    "    token_dim_gaia = 18\n",
    "    n_layers = 20\n",
    "    lr = 2.5e-6\n",
    "    patience = 200\n",
    "    num_epochs = 800\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(\"Device:\", device)\n",
    "\n",
    "    # Initialize WandB\n",
    "    wandb.init(project=\"ALLSTARS_multimodal_fusion_mamba_out\")\n",
    "    \n",
    "    config = {\n",
    "        \"num_classes\": num_classes,\n",
    "        \"d_model_spectra\": d_model_spectra,\n",
    "        \"d_model_gaia\": d_model_gaia,\n",
    "        \"input_dim_spectra\": input_dim_spectra,\n",
    "        \"input_dim_gaia\": input_dim_gaia,\n",
    "        \"n_layers\": n_layers,\n",
    "        \"lr\": lr,\n",
    "        \"patience\": patience,\n",
    "        \"num_epochs\": num_epochs\n",
    "    }\n",
    "    wandb.config.update(config)\n",
    "\n",
    "    # Instantiate the fusion model\n",
    "    # Try use_cross_attention=False for late-fusion, True for cross-attention\n",
    "    model_fusion = StarClassifierFusionMambaOut(\n",
    "        d_model_spectra=d_model_spectra,\n",
    "        d_model_gaia=d_model_gaia,\n",
    "        num_classes=num_classes,\n",
    "        input_dim_spectra=input_dim_spectra,\n",
    "        input_dim_gaia=input_dim_gaia,\n",
    "        token_dim_gaia=token_dim_gaia,\n",
    "        token_dim_spectra=token_dim_spectra,\n",
    "        n_layers=n_layers,\n",
    "        d_conv=1,\n",
    "        use_cross_attention=True,  # set to False to compare with late fusion\n",
    "        n_cross_attn_heads=8\n",
    "    )\n",
    "    model_fusion.to(device)\n",
    "\n",
    "    # Print size of model in GB\n",
    "    print(f\"Model size: {sum(p.numel() for p in model_fusion.parameters()) / 1e9:.2f} GB\")\n",
    "    param_size = 0\n",
    "    for param in model_fusion.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    buffer_size = 0\n",
    "    for buffer in model_fusion.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "    size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "    print('model size: {:.3f}MB'.format(size_all_mb))\n",
    "    # Compute parameter size\n",
    "    param_size = sum(p.nelement() * p.element_size() for p in model_fusion.parameters())\n",
    "\n",
    "    # Compute buffer size\n",
    "    buffer_size = sum(b.nelement() * b.element_size() for b in model_fusion.buffers())\n",
    "\n",
    "    # Total size in MB\n",
    "    total_size_mb = (param_size + buffer_size) / (1024 ** 2)\n",
    "    print(f\"Model size: {total_size_mb:.3f} MB\")\n",
    "\n",
    "    print(model_fusion)\n",
    "    # print number of parameters per layer\n",
    "    for name, param in model_fusion.named_parameters():\n",
    "        print(name, param.numel())\n",
    "    print(\"Total number of parameters:\", sum(p.numel() for p in model_fusion.parameters() if p.requires_grad))\n",
    "\n",
    "    # Train the fusion model\n",
    "    trained_fusion_model = train_model_fusion(\n",
    "        model=model_fusion,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader,\n",
    "        num_epochs=num_epochs,\n",
    "        lr=lr,\n",
    "        max_patience=patience,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "# Save the model\n",
    "torch.save(trained_fusion_model.state_dict(), \"Models/model_fusion_mambaoutv3.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MambaOut 19+18 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjoaocsgalmeida\u001b[0m (\u001b[33mjoaocsgalmeida-university-of-southampton\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/joao/Documents/Star-Classifier/wandb/run-20250408_112323-8mq8on95</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_MambaOut_18_19/runs/8mq8on95' target=\"_blank\">graceful-glitter-6</a></strong> to <a href='https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_MambaOut_18_19' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_MambaOut_18_19' target=\"_blank\">https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_MambaOut_18_19</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_MambaOut_18_19/runs/8mq8on95' target=\"_blank\">https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_MambaOut_18_19/runs/8mq8on95</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of spectra tokens: 19\n",
      "Number of gaia tokens: 18\n",
      "StarClassifierFusionMambaOut(\n",
      "  (tokenizer_spectra): FeatureTokenizer(\n",
      "    (token_embed): Linear(in_features=192, out_features=2048, bias=True)\n",
      "  )\n",
      "  (tokenizer_gaia): FeatureTokenizer(\n",
      "    (token_embed): Linear(in_features=1, out_features=2048, bias=True)\n",
      "  )\n",
      "  (mamba_spectra): Sequential(\n",
      "    (0): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(4,), stride=(1,), padding=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(4,), stride=(1,), padding=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(4,), stride=(1,), padding=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(4,), stride=(1,), padding=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(4,), stride=(1,), padding=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(4,), stride=(1,), padding=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (6): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(4,), stride=(1,), padding=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (7): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(4,), stride=(1,), padding=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (8): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(4,), stride=(1,), padding=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (9): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(4,), stride=(1,), padding=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (10): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(4,), stride=(1,), padding=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (11): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(4,), stride=(1,), padding=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (12): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(4,), stride=(1,), padding=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (13): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(4,), stride=(1,), padding=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (14): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(4,), stride=(1,), padding=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (15): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(4,), stride=(1,), padding=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (16): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(4,), stride=(1,), padding=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (17): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(4,), stride=(1,), padding=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (18): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(4,), stride=(1,), padding=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (19): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(4,), stride=(1,), padding=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (mamba_gaia): Sequential(\n",
      "    (0): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(4,), stride=(1,), padding=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(4,), stride=(1,), padding=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(4,), stride=(1,), padding=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(4,), stride=(1,), padding=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(4,), stride=(1,), padding=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(4,), stride=(1,), padding=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (6): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(4,), stride=(1,), padding=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (7): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(4,), stride=(1,), padding=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (8): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(4,), stride=(1,), padding=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (9): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(4,), stride=(1,), padding=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (10): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(4,), stride=(1,), padding=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (11): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(4,), stride=(1,), padding=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (12): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(4,), stride=(1,), padding=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (13): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(4,), stride=(1,), padding=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (14): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(4,), stride=(1,), padding=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (15): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(4,), stride=(1,), padding=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (16): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(4,), stride=(1,), padding=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (17): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(4,), stride=(1,), padding=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (18): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(4,), stride=(1,), padding=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (19): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((2048,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(4096, 4096, kernel_size=(4,), stride=(1,), padding=(1,), groups=4096)\n",
      "          (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (cross_attn_block_spectra): CrossAttentionBlock(\n",
      "    (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "    (attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=2048, out_features=2048, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (cross_attn_block_gaia): CrossAttentionBlock(\n",
      "    (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "    (attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=2048, out_features=2048, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): Linear(in_features=4096, out_features=55, bias=True)\n",
      "  )\n",
      ")\n",
      "tokenizer_spectra.token_embed.weight 393216\n",
      "tokenizer_spectra.token_embed.bias 2048\n",
      "tokenizer_gaia.token_embed.weight 2048\n",
      "tokenizer_gaia.token_embed.bias 2048\n",
      "mamba_spectra.0.blocks.0.norm.weight 2048\n",
      "mamba_spectra.0.blocks.0.norm.bias 2048\n",
      "mamba_spectra.0.blocks.0.fc1.weight 16777216\n",
      "mamba_spectra.0.blocks.0.fc1.bias 8192\n",
      "mamba_spectra.0.blocks.0.conv.weight 16384\n",
      "mamba_spectra.0.blocks.0.conv.bias 4096\n",
      "mamba_spectra.0.blocks.0.fc2.weight 8388608\n",
      "mamba_spectra.0.blocks.0.fc2.bias 2048\n",
      "mamba_spectra.1.blocks.0.norm.weight 2048\n",
      "mamba_spectra.1.blocks.0.norm.bias 2048\n",
      "mamba_spectra.1.blocks.0.fc1.weight 16777216\n",
      "mamba_spectra.1.blocks.0.fc1.bias 8192\n",
      "mamba_spectra.1.blocks.0.conv.weight 16384\n",
      "mamba_spectra.1.blocks.0.conv.bias 4096\n",
      "mamba_spectra.1.blocks.0.fc2.weight 8388608\n",
      "mamba_spectra.1.blocks.0.fc2.bias 2048\n",
      "mamba_spectra.2.blocks.0.norm.weight 2048\n",
      "mamba_spectra.2.blocks.0.norm.bias 2048\n",
      "mamba_spectra.2.blocks.0.fc1.weight 16777216\n",
      "mamba_spectra.2.blocks.0.fc1.bias 8192\n",
      "mamba_spectra.2.blocks.0.conv.weight 16384\n",
      "mamba_spectra.2.blocks.0.conv.bias 4096\n",
      "mamba_spectra.2.blocks.0.fc2.weight 8388608\n",
      "mamba_spectra.2.blocks.0.fc2.bias 2048\n",
      "mamba_spectra.3.blocks.0.norm.weight 2048\n",
      "mamba_spectra.3.blocks.0.norm.bias 2048\n",
      "mamba_spectra.3.blocks.0.fc1.weight 16777216\n",
      "mamba_spectra.3.blocks.0.fc1.bias 8192\n",
      "mamba_spectra.3.blocks.0.conv.weight 16384\n",
      "mamba_spectra.3.blocks.0.conv.bias 4096\n",
      "mamba_spectra.3.blocks.0.fc2.weight 8388608\n",
      "mamba_spectra.3.blocks.0.fc2.bias 2048\n",
      "mamba_spectra.4.blocks.0.norm.weight 2048\n",
      "mamba_spectra.4.blocks.0.norm.bias 2048\n",
      "mamba_spectra.4.blocks.0.fc1.weight 16777216\n",
      "mamba_spectra.4.blocks.0.fc1.bias 8192\n",
      "mamba_spectra.4.blocks.0.conv.weight 16384\n",
      "mamba_spectra.4.blocks.0.conv.bias 4096\n",
      "mamba_spectra.4.blocks.0.fc2.weight 8388608\n",
      "mamba_spectra.4.blocks.0.fc2.bias 2048\n",
      "mamba_spectra.5.blocks.0.norm.weight 2048\n",
      "mamba_spectra.5.blocks.0.norm.bias 2048\n",
      "mamba_spectra.5.blocks.0.fc1.weight 16777216\n",
      "mamba_spectra.5.blocks.0.fc1.bias 8192\n",
      "mamba_spectra.5.blocks.0.conv.weight 16384\n",
      "mamba_spectra.5.blocks.0.conv.bias 4096\n",
      "mamba_spectra.5.blocks.0.fc2.weight 8388608\n",
      "mamba_spectra.5.blocks.0.fc2.bias 2048\n",
      "mamba_spectra.6.blocks.0.norm.weight 2048\n",
      "mamba_spectra.6.blocks.0.norm.bias 2048\n",
      "mamba_spectra.6.blocks.0.fc1.weight 16777216\n",
      "mamba_spectra.6.blocks.0.fc1.bias 8192\n",
      "mamba_spectra.6.blocks.0.conv.weight 16384\n",
      "mamba_spectra.6.blocks.0.conv.bias 4096\n",
      "mamba_spectra.6.blocks.0.fc2.weight 8388608\n",
      "mamba_spectra.6.blocks.0.fc2.bias 2048\n",
      "mamba_spectra.7.blocks.0.norm.weight 2048\n",
      "mamba_spectra.7.blocks.0.norm.bias 2048\n",
      "mamba_spectra.7.blocks.0.fc1.weight 16777216\n",
      "mamba_spectra.7.blocks.0.fc1.bias 8192\n",
      "mamba_spectra.7.blocks.0.conv.weight 16384\n",
      "mamba_spectra.7.blocks.0.conv.bias 4096\n",
      "mamba_spectra.7.blocks.0.fc2.weight 8388608\n",
      "mamba_spectra.7.blocks.0.fc2.bias 2048\n",
      "mamba_spectra.8.blocks.0.norm.weight 2048\n",
      "mamba_spectra.8.blocks.0.norm.bias 2048\n",
      "mamba_spectra.8.blocks.0.fc1.weight 16777216\n",
      "mamba_spectra.8.blocks.0.fc1.bias 8192\n",
      "mamba_spectra.8.blocks.0.conv.weight 16384\n",
      "mamba_spectra.8.blocks.0.conv.bias 4096\n",
      "mamba_spectra.8.blocks.0.fc2.weight 8388608\n",
      "mamba_spectra.8.blocks.0.fc2.bias 2048\n",
      "mamba_spectra.9.blocks.0.norm.weight 2048\n",
      "mamba_spectra.9.blocks.0.norm.bias 2048\n",
      "mamba_spectra.9.blocks.0.fc1.weight 16777216\n",
      "mamba_spectra.9.blocks.0.fc1.bias 8192\n",
      "mamba_spectra.9.blocks.0.conv.weight 16384\n",
      "mamba_spectra.9.blocks.0.conv.bias 4096\n",
      "mamba_spectra.9.blocks.0.fc2.weight 8388608\n",
      "mamba_spectra.9.blocks.0.fc2.bias 2048\n",
      "mamba_spectra.10.blocks.0.norm.weight 2048\n",
      "mamba_spectra.10.blocks.0.norm.bias 2048\n",
      "mamba_spectra.10.blocks.0.fc1.weight 16777216\n",
      "mamba_spectra.10.blocks.0.fc1.bias 8192\n",
      "mamba_spectra.10.blocks.0.conv.weight 16384\n",
      "mamba_spectra.10.blocks.0.conv.bias 4096\n",
      "mamba_spectra.10.blocks.0.fc2.weight 8388608\n",
      "mamba_spectra.10.blocks.0.fc2.bias 2048\n",
      "mamba_spectra.11.blocks.0.norm.weight 2048\n",
      "mamba_spectra.11.blocks.0.norm.bias 2048\n",
      "mamba_spectra.11.blocks.0.fc1.weight 16777216\n",
      "mamba_spectra.11.blocks.0.fc1.bias 8192\n",
      "mamba_spectra.11.blocks.0.conv.weight 16384\n",
      "mamba_spectra.11.blocks.0.conv.bias 4096\n",
      "mamba_spectra.11.blocks.0.fc2.weight 8388608\n",
      "mamba_spectra.11.blocks.0.fc2.bias 2048\n",
      "mamba_spectra.12.blocks.0.norm.weight 2048\n",
      "mamba_spectra.12.blocks.0.norm.bias 2048\n",
      "mamba_spectra.12.blocks.0.fc1.weight 16777216\n",
      "mamba_spectra.12.blocks.0.fc1.bias 8192\n",
      "mamba_spectra.12.blocks.0.conv.weight 16384\n",
      "mamba_spectra.12.blocks.0.conv.bias 4096\n",
      "mamba_spectra.12.blocks.0.fc2.weight 8388608\n",
      "mamba_spectra.12.blocks.0.fc2.bias 2048\n",
      "mamba_spectra.13.blocks.0.norm.weight 2048\n",
      "mamba_spectra.13.blocks.0.norm.bias 2048\n",
      "mamba_spectra.13.blocks.0.fc1.weight 16777216\n",
      "mamba_spectra.13.blocks.0.fc1.bias 8192\n",
      "mamba_spectra.13.blocks.0.conv.weight 16384\n",
      "mamba_spectra.13.blocks.0.conv.bias 4096\n",
      "mamba_spectra.13.blocks.0.fc2.weight 8388608\n",
      "mamba_spectra.13.blocks.0.fc2.bias 2048\n",
      "mamba_spectra.14.blocks.0.norm.weight 2048\n",
      "mamba_spectra.14.blocks.0.norm.bias 2048\n",
      "mamba_spectra.14.blocks.0.fc1.weight 16777216\n",
      "mamba_spectra.14.blocks.0.fc1.bias 8192\n",
      "mamba_spectra.14.blocks.0.conv.weight 16384\n",
      "mamba_spectra.14.blocks.0.conv.bias 4096\n",
      "mamba_spectra.14.blocks.0.fc2.weight 8388608\n",
      "mamba_spectra.14.blocks.0.fc2.bias 2048\n",
      "mamba_spectra.15.blocks.0.norm.weight 2048\n",
      "mamba_spectra.15.blocks.0.norm.bias 2048\n",
      "mamba_spectra.15.blocks.0.fc1.weight 16777216\n",
      "mamba_spectra.15.blocks.0.fc1.bias 8192\n",
      "mamba_spectra.15.blocks.0.conv.weight 16384\n",
      "mamba_spectra.15.blocks.0.conv.bias 4096\n",
      "mamba_spectra.15.blocks.0.fc2.weight 8388608\n",
      "mamba_spectra.15.blocks.0.fc2.bias 2048\n",
      "mamba_spectra.16.blocks.0.norm.weight 2048\n",
      "mamba_spectra.16.blocks.0.norm.bias 2048\n",
      "mamba_spectra.16.blocks.0.fc1.weight 16777216\n",
      "mamba_spectra.16.blocks.0.fc1.bias 8192\n",
      "mamba_spectra.16.blocks.0.conv.weight 16384\n",
      "mamba_spectra.16.blocks.0.conv.bias 4096\n",
      "mamba_spectra.16.blocks.0.fc2.weight 8388608\n",
      "mamba_spectra.16.blocks.0.fc2.bias 2048\n",
      "mamba_spectra.17.blocks.0.norm.weight 2048\n",
      "mamba_spectra.17.blocks.0.norm.bias 2048\n",
      "mamba_spectra.17.blocks.0.fc1.weight 16777216\n",
      "mamba_spectra.17.blocks.0.fc1.bias 8192\n",
      "mamba_spectra.17.blocks.0.conv.weight 16384\n",
      "mamba_spectra.17.blocks.0.conv.bias 4096\n",
      "mamba_spectra.17.blocks.0.fc2.weight 8388608\n",
      "mamba_spectra.17.blocks.0.fc2.bias 2048\n",
      "mamba_spectra.18.blocks.0.norm.weight 2048\n",
      "mamba_spectra.18.blocks.0.norm.bias 2048\n",
      "mamba_spectra.18.blocks.0.fc1.weight 16777216\n",
      "mamba_spectra.18.blocks.0.fc1.bias 8192\n",
      "mamba_spectra.18.blocks.0.conv.weight 16384\n",
      "mamba_spectra.18.blocks.0.conv.bias 4096\n",
      "mamba_spectra.18.blocks.0.fc2.weight 8388608\n",
      "mamba_spectra.18.blocks.0.fc2.bias 2048\n",
      "mamba_spectra.19.blocks.0.norm.weight 2048\n",
      "mamba_spectra.19.blocks.0.norm.bias 2048\n",
      "mamba_spectra.19.blocks.0.fc1.weight 16777216\n",
      "mamba_spectra.19.blocks.0.fc1.bias 8192\n",
      "mamba_spectra.19.blocks.0.conv.weight 16384\n",
      "mamba_spectra.19.blocks.0.conv.bias 4096\n",
      "mamba_spectra.19.blocks.0.fc2.weight 8388608\n",
      "mamba_spectra.19.blocks.0.fc2.bias 2048\n",
      "mamba_gaia.0.blocks.0.norm.weight 2048\n",
      "mamba_gaia.0.blocks.0.norm.bias 2048\n",
      "mamba_gaia.0.blocks.0.fc1.weight 16777216\n",
      "mamba_gaia.0.blocks.0.fc1.bias 8192\n",
      "mamba_gaia.0.blocks.0.conv.weight 16384\n",
      "mamba_gaia.0.blocks.0.conv.bias 4096\n",
      "mamba_gaia.0.blocks.0.fc2.weight 8388608\n",
      "mamba_gaia.0.blocks.0.fc2.bias 2048\n",
      "mamba_gaia.1.blocks.0.norm.weight 2048\n",
      "mamba_gaia.1.blocks.0.norm.bias 2048\n",
      "mamba_gaia.1.blocks.0.fc1.weight 16777216\n",
      "mamba_gaia.1.blocks.0.fc1.bias 8192\n",
      "mamba_gaia.1.blocks.0.conv.weight 16384\n",
      "mamba_gaia.1.blocks.0.conv.bias 4096\n",
      "mamba_gaia.1.blocks.0.fc2.weight 8388608\n",
      "mamba_gaia.1.blocks.0.fc2.bias 2048\n",
      "mamba_gaia.2.blocks.0.norm.weight 2048\n",
      "mamba_gaia.2.blocks.0.norm.bias 2048\n",
      "mamba_gaia.2.blocks.0.fc1.weight 16777216\n",
      "mamba_gaia.2.blocks.0.fc1.bias 8192\n",
      "mamba_gaia.2.blocks.0.conv.weight 16384\n",
      "mamba_gaia.2.blocks.0.conv.bias 4096\n",
      "mamba_gaia.2.blocks.0.fc2.weight 8388608\n",
      "mamba_gaia.2.blocks.0.fc2.bias 2048\n",
      "mamba_gaia.3.blocks.0.norm.weight 2048\n",
      "mamba_gaia.3.blocks.0.norm.bias 2048\n",
      "mamba_gaia.3.blocks.0.fc1.weight 16777216\n",
      "mamba_gaia.3.blocks.0.fc1.bias 8192\n",
      "mamba_gaia.3.blocks.0.conv.weight 16384\n",
      "mamba_gaia.3.blocks.0.conv.bias 4096\n",
      "mamba_gaia.3.blocks.0.fc2.weight 8388608\n",
      "mamba_gaia.3.blocks.0.fc2.bias 2048\n",
      "mamba_gaia.4.blocks.0.norm.weight 2048\n",
      "mamba_gaia.4.blocks.0.norm.bias 2048\n",
      "mamba_gaia.4.blocks.0.fc1.weight 16777216\n",
      "mamba_gaia.4.blocks.0.fc1.bias 8192\n",
      "mamba_gaia.4.blocks.0.conv.weight 16384\n",
      "mamba_gaia.4.blocks.0.conv.bias 4096\n",
      "mamba_gaia.4.blocks.0.fc2.weight 8388608\n",
      "mamba_gaia.4.blocks.0.fc2.bias 2048\n",
      "mamba_gaia.5.blocks.0.norm.weight 2048\n",
      "mamba_gaia.5.blocks.0.norm.bias 2048\n",
      "mamba_gaia.5.blocks.0.fc1.weight 16777216\n",
      "mamba_gaia.5.blocks.0.fc1.bias 8192\n",
      "mamba_gaia.5.blocks.0.conv.weight 16384\n",
      "mamba_gaia.5.blocks.0.conv.bias 4096\n",
      "mamba_gaia.5.blocks.0.fc2.weight 8388608\n",
      "mamba_gaia.5.blocks.0.fc2.bias 2048\n",
      "mamba_gaia.6.blocks.0.norm.weight 2048\n",
      "mamba_gaia.6.blocks.0.norm.bias 2048\n",
      "mamba_gaia.6.blocks.0.fc1.weight 16777216\n",
      "mamba_gaia.6.blocks.0.fc1.bias 8192\n",
      "mamba_gaia.6.blocks.0.conv.weight 16384\n",
      "mamba_gaia.6.blocks.0.conv.bias 4096\n",
      "mamba_gaia.6.blocks.0.fc2.weight 8388608\n",
      "mamba_gaia.6.blocks.0.fc2.bias 2048\n",
      "mamba_gaia.7.blocks.0.norm.weight 2048\n",
      "mamba_gaia.7.blocks.0.norm.bias 2048\n",
      "mamba_gaia.7.blocks.0.fc1.weight 16777216\n",
      "mamba_gaia.7.blocks.0.fc1.bias 8192\n",
      "mamba_gaia.7.blocks.0.conv.weight 16384\n",
      "mamba_gaia.7.blocks.0.conv.bias 4096\n",
      "mamba_gaia.7.blocks.0.fc2.weight 8388608\n",
      "mamba_gaia.7.blocks.0.fc2.bias 2048\n",
      "mamba_gaia.8.blocks.0.norm.weight 2048\n",
      "mamba_gaia.8.blocks.0.norm.bias 2048\n",
      "mamba_gaia.8.blocks.0.fc1.weight 16777216\n",
      "mamba_gaia.8.blocks.0.fc1.bias 8192\n",
      "mamba_gaia.8.blocks.0.conv.weight 16384\n",
      "mamba_gaia.8.blocks.0.conv.bias 4096\n",
      "mamba_gaia.8.blocks.0.fc2.weight 8388608\n",
      "mamba_gaia.8.blocks.0.fc2.bias 2048\n",
      "mamba_gaia.9.blocks.0.norm.weight 2048\n",
      "mamba_gaia.9.blocks.0.norm.bias 2048\n",
      "mamba_gaia.9.blocks.0.fc1.weight 16777216\n",
      "mamba_gaia.9.blocks.0.fc1.bias 8192\n",
      "mamba_gaia.9.blocks.0.conv.weight 16384\n",
      "mamba_gaia.9.blocks.0.conv.bias 4096\n",
      "mamba_gaia.9.blocks.0.fc2.weight 8388608\n",
      "mamba_gaia.9.blocks.0.fc2.bias 2048\n",
      "mamba_gaia.10.blocks.0.norm.weight 2048\n",
      "mamba_gaia.10.blocks.0.norm.bias 2048\n",
      "mamba_gaia.10.blocks.0.fc1.weight 16777216\n",
      "mamba_gaia.10.blocks.0.fc1.bias 8192\n",
      "mamba_gaia.10.blocks.0.conv.weight 16384\n",
      "mamba_gaia.10.blocks.0.conv.bias 4096\n",
      "mamba_gaia.10.blocks.0.fc2.weight 8388608\n",
      "mamba_gaia.10.blocks.0.fc2.bias 2048\n",
      "mamba_gaia.11.blocks.0.norm.weight 2048\n",
      "mamba_gaia.11.blocks.0.norm.bias 2048\n",
      "mamba_gaia.11.blocks.0.fc1.weight 16777216\n",
      "mamba_gaia.11.blocks.0.fc1.bias 8192\n",
      "mamba_gaia.11.blocks.0.conv.weight 16384\n",
      "mamba_gaia.11.blocks.0.conv.bias 4096\n",
      "mamba_gaia.11.blocks.0.fc2.weight 8388608\n",
      "mamba_gaia.11.blocks.0.fc2.bias 2048\n",
      "mamba_gaia.12.blocks.0.norm.weight 2048\n",
      "mamba_gaia.12.blocks.0.norm.bias 2048\n",
      "mamba_gaia.12.blocks.0.fc1.weight 16777216\n",
      "mamba_gaia.12.blocks.0.fc1.bias 8192\n",
      "mamba_gaia.12.blocks.0.conv.weight 16384\n",
      "mamba_gaia.12.blocks.0.conv.bias 4096\n",
      "mamba_gaia.12.blocks.0.fc2.weight 8388608\n",
      "mamba_gaia.12.blocks.0.fc2.bias 2048\n",
      "mamba_gaia.13.blocks.0.norm.weight 2048\n",
      "mamba_gaia.13.blocks.0.norm.bias 2048\n",
      "mamba_gaia.13.blocks.0.fc1.weight 16777216\n",
      "mamba_gaia.13.blocks.0.fc1.bias 8192\n",
      "mamba_gaia.13.blocks.0.conv.weight 16384\n",
      "mamba_gaia.13.blocks.0.conv.bias 4096\n",
      "mamba_gaia.13.blocks.0.fc2.weight 8388608\n",
      "mamba_gaia.13.blocks.0.fc2.bias 2048\n",
      "mamba_gaia.14.blocks.0.norm.weight 2048\n",
      "mamba_gaia.14.blocks.0.norm.bias 2048\n",
      "mamba_gaia.14.blocks.0.fc1.weight 16777216\n",
      "mamba_gaia.14.blocks.0.fc1.bias 8192\n",
      "mamba_gaia.14.blocks.0.conv.weight 16384\n",
      "mamba_gaia.14.blocks.0.conv.bias 4096\n",
      "mamba_gaia.14.blocks.0.fc2.weight 8388608\n",
      "mamba_gaia.14.blocks.0.fc2.bias 2048\n",
      "mamba_gaia.15.blocks.0.norm.weight 2048\n",
      "mamba_gaia.15.blocks.0.norm.bias 2048\n",
      "mamba_gaia.15.blocks.0.fc1.weight 16777216\n",
      "mamba_gaia.15.blocks.0.fc1.bias 8192\n",
      "mamba_gaia.15.blocks.0.conv.weight 16384\n",
      "mamba_gaia.15.blocks.0.conv.bias 4096\n",
      "mamba_gaia.15.blocks.0.fc2.weight 8388608\n",
      "mamba_gaia.15.blocks.0.fc2.bias 2048\n",
      "mamba_gaia.16.blocks.0.norm.weight 2048\n",
      "mamba_gaia.16.blocks.0.norm.bias 2048\n",
      "mamba_gaia.16.blocks.0.fc1.weight 16777216\n",
      "mamba_gaia.16.blocks.0.fc1.bias 8192\n",
      "mamba_gaia.16.blocks.0.conv.weight 16384\n",
      "mamba_gaia.16.blocks.0.conv.bias 4096\n",
      "mamba_gaia.16.blocks.0.fc2.weight 8388608\n",
      "mamba_gaia.16.blocks.0.fc2.bias 2048\n",
      "mamba_gaia.17.blocks.0.norm.weight 2048\n",
      "mamba_gaia.17.blocks.0.norm.bias 2048\n",
      "mamba_gaia.17.blocks.0.fc1.weight 16777216\n",
      "mamba_gaia.17.blocks.0.fc1.bias 8192\n",
      "mamba_gaia.17.blocks.0.conv.weight 16384\n",
      "mamba_gaia.17.blocks.0.conv.bias 4096\n",
      "mamba_gaia.17.blocks.0.fc2.weight 8388608\n",
      "mamba_gaia.17.blocks.0.fc2.bias 2048\n",
      "mamba_gaia.18.blocks.0.norm.weight 2048\n",
      "mamba_gaia.18.blocks.0.norm.bias 2048\n",
      "mamba_gaia.18.blocks.0.fc1.weight 16777216\n",
      "mamba_gaia.18.blocks.0.fc1.bias 8192\n",
      "mamba_gaia.18.blocks.0.conv.weight 16384\n",
      "mamba_gaia.18.blocks.0.conv.bias 4096\n",
      "mamba_gaia.18.blocks.0.fc2.weight 8388608\n",
      "mamba_gaia.18.blocks.0.fc2.bias 2048\n",
      "mamba_gaia.19.blocks.0.norm.weight 2048\n",
      "mamba_gaia.19.blocks.0.norm.bias 2048\n",
      "mamba_gaia.19.blocks.0.fc1.weight 16777216\n",
      "mamba_gaia.19.blocks.0.fc1.bias 8192\n",
      "mamba_gaia.19.blocks.0.conv.weight 16384\n",
      "mamba_gaia.19.blocks.0.conv.bias 4096\n",
      "mamba_gaia.19.blocks.0.fc2.weight 8388608\n",
      "mamba_gaia.19.blocks.0.fc2.bias 2048\n",
      "cross_attn_block_spectra.norm.weight 2048\n",
      "cross_attn_block_spectra.norm.bias 2048\n",
      "cross_attn_block_spectra.attention.in_proj_weight 12582912\n",
      "cross_attn_block_spectra.attention.in_proj_bias 6144\n",
      "cross_attn_block_spectra.attention.out_proj.weight 4194304\n",
      "cross_attn_block_spectra.attention.out_proj.bias 2048\n",
      "cross_attn_block_gaia.norm.weight 2048\n",
      "cross_attn_block_gaia.norm.bias 2048\n",
      "cross_attn_block_gaia.attention.in_proj_weight 12582912\n",
      "cross_attn_block_gaia.attention.in_proj_bias 6144\n",
      "cross_attn_block_gaia.attention.out_proj.weight 4194304\n",
      "cross_attn_block_gaia.attention.out_proj.bias 2048\n",
      "classifier.0.weight 4096\n",
      "classifier.0.bias 4096\n",
      "classifier.1.weight 225280\n",
      "classifier.1.bias 55\n",
      "Model size: 3975.821 MB\n",
      "Total parameters: 1042237495\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td></td></tr><tr><td>hamming_loss</td><td></td></tr><tr><td>lr</td><td></td></tr><tr><td>macro_f1</td><td></td></tr><tr><td>macro_precision</td><td></td></tr><tr><td>macro_recall</td><td></td></tr><tr><td>micro_f1</td><td></td></tr><tr><td>micro_precision</td><td></td></tr><tr><td>micro_recall</td><td></td></tr><tr><td>test_acc</td><td></td></tr><tr><td>test_loss</td><td></td></tr><tr><td>train_acc</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>val_acc</td><td></td></tr><tr><td>val_loss</td><td></td></tr><tr><td>weighted_f1</td><td></td></tr><tr><td>weighted_precision</td><td></td></tr><tr><td>weighted_recall</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>568</td></tr><tr><td>hamming_loss</td><td>0.02656</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>macro_f1</td><td>0.36749</td></tr><tr><td>macro_precision</td><td>0.83075</td></tr><tr><td>macro_recall</td><td>0.31212</td></tr><tr><td>micro_f1</td><td>0.5163</td></tr><tr><td>micro_precision</td><td>0.77647</td></tr><tr><td>micro_recall</td><td>0.38672</td></tr><tr><td>test_acc</td><td>0.97342</td></tr><tr><td>test_loss</td><td>0.07104</td></tr><tr><td>train_acc</td><td>0.98834</td></tr><tr><td>train_loss</td><td>0.01616</td></tr><tr><td>val_acc</td><td>0.97659</td></tr><tr><td>val_loss</td><td>0.04545</td></tr><tr><td>weighted_f1</td><td>0.48033</td></tr><tr><td>weighted_precision</td><td>0.82744</td></tr><tr><td>weighted_recall</td><td>0.38672</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">graceful-glitter-6</strong> at: <a href='https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_MambaOut_18_19/runs/8mq8on95' target=\"_blank\">https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_MambaOut_18_19/runs/8mq8on95</a><br> View project at: <a href='https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_MambaOut_18_19' target=\"_blank\">https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_MambaOut_18_19</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250408_112323-8mq8on95/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Configuration for tokenized transformer model\n",
    "    d_model_spectra = 2048\n",
    "    d_model_gaia = 2048\n",
    "    num_classes = 55\n",
    "    input_dim_spectra = 3647\n",
    "    input_dim_gaia = 18\n",
    "    d_conv = 4\n",
    "    \n",
    "    # Token dimensions - these control the sequence length\n",
    "    token_dim_spectra = 192  # Will create ~19 tokens for spectra (3647/192)\n",
    "    token_dim_gaia = 1      # Will create 18 tokens for gaia (18/1)\n",
    "    \n",
    "    n_layers = 20  \n",
    "    n_heads = 8\n",
    "    \n",
    "    lr = 2.5e-6\n",
    "    patience = 200\n",
    "    num_epochs = 800\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(\"Device:\", device)\n",
    "    \n",
    "    # Initialize WandB\n",
    "    wandb.init(project=\"ALLSTARS_multimodal_fusion_MambaOut_18_19\")\n",
    "    config = {\n",
    "        \"model_type\": \"transformer_tokenized\",\n",
    "        \"num_classes\": num_classes,\n",
    "        \"d_model_spectra\": d_model_spectra,\n",
    "        \"d_model_gaia\": d_model_gaia,\n",
    "        \"input_dim_spectra\": input_dim_spectra,\n",
    "        \"input_dim_gaia\": input_dim_gaia,\n",
    "        \"token_dim_spectra\": token_dim_spectra,\n",
    "        \"token_dim_gaia\": token_dim_gaia,\n",
    "        \"num_tokens_spectra\": (input_dim_spectra + token_dim_spectra - 1) // token_dim_spectra,\n",
    "        \"num_tokens_gaia\": (input_dim_gaia + token_dim_gaia - 1) // token_dim_gaia,\n",
    "        \"n_layers\": n_layers,\n",
    "        \"n_heads\": n_heads,\n",
    "        \"use_cross_attention\": True,\n",
    "        \"lr\": lr,\n",
    "        \"patience\": patience,\n",
    "        \"num_epochs\": num_epochs\n",
    "    }\n",
    "    wandb.config.update(config)\n",
    "    \n",
    "    # Instantiate the tokenized MAMBAOut model\n",
    "    model_MambaOut = StarClassifierFusionMambaOut(\n",
    "        d_model_spectra=d_model_spectra,\n",
    "        d_model_gaia=d_model_gaia,\n",
    "        num_classes=num_classes,\n",
    "        input_dim_spectra=input_dim_spectra,\n",
    "        input_dim_gaia=input_dim_gaia,\n",
    "        token_dim_spectra=token_dim_spectra,\n",
    "        token_dim_gaia=token_dim_gaia,\n",
    "        n_layers=n_layers,\n",
    "        d_conv=d_conv,\n",
    "        use_cross_attention=True,\n",
    "        n_cross_attn_heads=8)\n",
    "    model_MambaOut.to(device)\n",
    "    \n",
    "    # Print model statistics\n",
    "    print(f\"Number of spectra tokens: {(input_dim_spectra + token_dim_spectra - 1) // token_dim_spectra}\")\n",
    "    print(f\"Number of gaia tokens: {(input_dim_gaia + token_dim_gaia - 1) // token_dim_gaia}\")\n",
    "    \n",
    "    # Compute parameter size\n",
    "    param_size = 0\n",
    "    for param in model_MambaOut.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    \n",
    "    # Compute buffer size\n",
    "    buffer_size = 0\n",
    "    for buffer in model_MambaOut.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "    print(model_MambaOut)\n",
    "    # print number of parameters per layer\n",
    "    for name, param in model_MambaOut.named_parameters():\n",
    "        print(name, param.numel())\n",
    "    \n",
    "    # Total size in MB\n",
    "    total_size_mb = (param_size + buffer_size) / (1024 ** 2)\n",
    "    print(f\"Model size: {total_size_mb:.3f} MB\")\n",
    "    print(f\"Total parameters: {sum(p.numel() for p in model_MambaOut.parameters() if p.requires_grad)}\")\n",
    "    \n",
    "    # Train the MAMBAOUT model\n",
    "    trained_MAMBAOut_model = train_model_fusion(\n",
    "        model=model_MambaOut,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader,\n",
    "        num_epochs=num_epochs,\n",
    "        lr=lr,\n",
    "        max_patience=patience,\n",
    "        device=device\n",
    "    )\n",
    "    wandb.finish()\n",
    "    \n",
    "    # Save the model\n",
    "    torch.save(trained_MAMBAOut_model.state_dict(), \"Models/model_fusion_MambaOut_18_19_v2.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MambaOut Most Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjoaocsgalmeida\u001b[0m (\u001b[33mjoaocsgalmeida-university-of-southampton\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/joao/Documents/Star-Classifier/wandb/run-20250407_155943-4wkrik43</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_transformer_tokenized/runs/4wkrik43' target=\"_blank\">fast-sun-45</a></strong> to <a href='https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_transformer_tokenized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_transformer_tokenized' target=\"_blank\">https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_transformer_tokenized</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_transformer_tokenized/runs/4wkrik43' target=\"_blank\">https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_transformer_tokenized/runs/4wkrik43</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of spectra tokens: 521\n",
      "Number of gaia tokens: 18\n",
      "StarClassifierFusionMambaOut(\n",
      "  (tokenizer_spectra): FeatureTokenizer(\n",
      "    (token_embed): Linear(in_features=7, out_features=1536, bias=True)\n",
      "  )\n",
      "  (tokenizer_gaia): FeatureTokenizer(\n",
      "    (token_embed): Linear(in_features=1, out_features=1536, bias=True)\n",
      "  )\n",
      "  (mamba_spectra): Sequential(\n",
      "    (0): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(3072, 3072, kernel_size=(32,), stride=(1,), padding=(15,), groups=3072)\n",
      "          (fc2): Linear(in_features=3072, out_features=1536, bias=True)\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(3072, 3072, kernel_size=(32,), stride=(1,), padding=(15,), groups=3072)\n",
      "          (fc2): Linear(in_features=3072, out_features=1536, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(3072, 3072, kernel_size=(32,), stride=(1,), padding=(15,), groups=3072)\n",
      "          (fc2): Linear(in_features=3072, out_features=1536, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(3072, 3072, kernel_size=(32,), stride=(1,), padding=(15,), groups=3072)\n",
      "          (fc2): Linear(in_features=3072, out_features=1536, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(3072, 3072, kernel_size=(32,), stride=(1,), padding=(15,), groups=3072)\n",
      "          (fc2): Linear(in_features=3072, out_features=1536, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(3072, 3072, kernel_size=(32,), stride=(1,), padding=(15,), groups=3072)\n",
      "          (fc2): Linear(in_features=3072, out_features=1536, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (6): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(3072, 3072, kernel_size=(32,), stride=(1,), padding=(15,), groups=3072)\n",
      "          (fc2): Linear(in_features=3072, out_features=1536, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (7): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(3072, 3072, kernel_size=(32,), stride=(1,), padding=(15,), groups=3072)\n",
      "          (fc2): Linear(in_features=3072, out_features=1536, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (8): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(3072, 3072, kernel_size=(32,), stride=(1,), padding=(15,), groups=3072)\n",
      "          (fc2): Linear(in_features=3072, out_features=1536, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (9): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(3072, 3072, kernel_size=(32,), stride=(1,), padding=(15,), groups=3072)\n",
      "          (fc2): Linear(in_features=3072, out_features=1536, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (10): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(3072, 3072, kernel_size=(32,), stride=(1,), padding=(15,), groups=3072)\n",
      "          (fc2): Linear(in_features=3072, out_features=1536, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (11): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(3072, 3072, kernel_size=(32,), stride=(1,), padding=(15,), groups=3072)\n",
      "          (fc2): Linear(in_features=3072, out_features=1536, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (12): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(3072, 3072, kernel_size=(32,), stride=(1,), padding=(15,), groups=3072)\n",
      "          (fc2): Linear(in_features=3072, out_features=1536, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (13): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(3072, 3072, kernel_size=(32,), stride=(1,), padding=(15,), groups=3072)\n",
      "          (fc2): Linear(in_features=3072, out_features=1536, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (14): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(3072, 3072, kernel_size=(32,), stride=(1,), padding=(15,), groups=3072)\n",
      "          (fc2): Linear(in_features=3072, out_features=1536, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (15): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(3072, 3072, kernel_size=(32,), stride=(1,), padding=(15,), groups=3072)\n",
      "          (fc2): Linear(in_features=3072, out_features=1536, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (16): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(3072, 3072, kernel_size=(32,), stride=(1,), padding=(15,), groups=3072)\n",
      "          (fc2): Linear(in_features=3072, out_features=1536, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (17): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(3072, 3072, kernel_size=(32,), stride=(1,), padding=(15,), groups=3072)\n",
      "          (fc2): Linear(in_features=3072, out_features=1536, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (18): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(3072, 3072, kernel_size=(32,), stride=(1,), padding=(15,), groups=3072)\n",
      "          (fc2): Linear(in_features=3072, out_features=1536, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (19): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(3072, 3072, kernel_size=(32,), stride=(1,), padding=(15,), groups=3072)\n",
      "          (fc2): Linear(in_features=3072, out_features=1536, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (mamba_gaia): Sequential(\n",
      "    (0): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(3072, 3072, kernel_size=(32,), stride=(1,), padding=(15,), groups=3072)\n",
      "          (fc2): Linear(in_features=3072, out_features=1536, bias=True)\n",
      "          (drop_path): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(3072, 3072, kernel_size=(32,), stride=(1,), padding=(15,), groups=3072)\n",
      "          (fc2): Linear(in_features=3072, out_features=1536, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(3072, 3072, kernel_size=(32,), stride=(1,), padding=(15,), groups=3072)\n",
      "          (fc2): Linear(in_features=3072, out_features=1536, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(3072, 3072, kernel_size=(32,), stride=(1,), padding=(15,), groups=3072)\n",
      "          (fc2): Linear(in_features=3072, out_features=1536, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(3072, 3072, kernel_size=(32,), stride=(1,), padding=(15,), groups=3072)\n",
      "          (fc2): Linear(in_features=3072, out_features=1536, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(3072, 3072, kernel_size=(32,), stride=(1,), padding=(15,), groups=3072)\n",
      "          (fc2): Linear(in_features=3072, out_features=1536, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (6): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(3072, 3072, kernel_size=(32,), stride=(1,), padding=(15,), groups=3072)\n",
      "          (fc2): Linear(in_features=3072, out_features=1536, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (7): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(3072, 3072, kernel_size=(32,), stride=(1,), padding=(15,), groups=3072)\n",
      "          (fc2): Linear(in_features=3072, out_features=1536, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (8): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(3072, 3072, kernel_size=(32,), stride=(1,), padding=(15,), groups=3072)\n",
      "          (fc2): Linear(in_features=3072, out_features=1536, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (9): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(3072, 3072, kernel_size=(32,), stride=(1,), padding=(15,), groups=3072)\n",
      "          (fc2): Linear(in_features=3072, out_features=1536, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (10): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(3072, 3072, kernel_size=(32,), stride=(1,), padding=(15,), groups=3072)\n",
      "          (fc2): Linear(in_features=3072, out_features=1536, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (11): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(3072, 3072, kernel_size=(32,), stride=(1,), padding=(15,), groups=3072)\n",
      "          (fc2): Linear(in_features=3072, out_features=1536, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (12): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(3072, 3072, kernel_size=(32,), stride=(1,), padding=(15,), groups=3072)\n",
      "          (fc2): Linear(in_features=3072, out_features=1536, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (13): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(3072, 3072, kernel_size=(32,), stride=(1,), padding=(15,), groups=3072)\n",
      "          (fc2): Linear(in_features=3072, out_features=1536, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (14): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(3072, 3072, kernel_size=(32,), stride=(1,), padding=(15,), groups=3072)\n",
      "          (fc2): Linear(in_features=3072, out_features=1536, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (15): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(3072, 3072, kernel_size=(32,), stride=(1,), padding=(15,), groups=3072)\n",
      "          (fc2): Linear(in_features=3072, out_features=1536, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (16): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(3072, 3072, kernel_size=(32,), stride=(1,), padding=(15,), groups=3072)\n",
      "          (fc2): Linear(in_features=3072, out_features=1536, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (17): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(3072, 3072, kernel_size=(32,), stride=(1,), padding=(15,), groups=3072)\n",
      "          (fc2): Linear(in_features=3072, out_features=1536, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (18): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(3072, 3072, kernel_size=(32,), stride=(1,), padding=(15,), groups=3072)\n",
      "          (fc2): Linear(in_features=3072, out_features=1536, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (19): SequenceMambaOut(\n",
      "      (blocks): Sequential(\n",
      "        (0): GatedCNNBlock(\n",
      "          (norm): LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (conv): Conv1d(3072, 3072, kernel_size=(32,), stride=(1,), padding=(15,), groups=3072)\n",
      "          (fc2): Linear(in_features=3072, out_features=1536, bias=True)\n",
      "          (drop_path): DropPath(drop_prob=0.100)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (cross_attn_block_spectra): CrossAttentionBlock(\n",
      "    (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "    (attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=1536, out_features=1536, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (cross_attn_block_gaia): CrossAttentionBlock(\n",
      "    (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "    (attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=1536, out_features=1536, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): Linear(in_features=3072, out_features=55, bias=True)\n",
      "  )\n",
      ")\n",
      "tokenizer_spectra.token_embed.weight 10752\n",
      "tokenizer_spectra.token_embed.bias 1536\n",
      "tokenizer_gaia.token_embed.weight 1536\n",
      "tokenizer_gaia.token_embed.bias 1536\n",
      "mamba_spectra.0.blocks.0.norm.weight 1536\n",
      "mamba_spectra.0.blocks.0.norm.bias 1536\n",
      "mamba_spectra.0.blocks.0.fc1.weight 9437184\n",
      "mamba_spectra.0.blocks.0.fc1.bias 6144\n",
      "mamba_spectra.0.blocks.0.conv.weight 98304\n",
      "mamba_spectra.0.blocks.0.conv.bias 3072\n",
      "mamba_spectra.0.blocks.0.fc2.weight 4718592\n",
      "mamba_spectra.0.blocks.0.fc2.bias 1536\n",
      "mamba_spectra.1.blocks.0.norm.weight 1536\n",
      "mamba_spectra.1.blocks.0.norm.bias 1536\n",
      "mamba_spectra.1.blocks.0.fc1.weight 9437184\n",
      "mamba_spectra.1.blocks.0.fc1.bias 6144\n",
      "mamba_spectra.1.blocks.0.conv.weight 98304\n",
      "mamba_spectra.1.blocks.0.conv.bias 3072\n",
      "mamba_spectra.1.blocks.0.fc2.weight 4718592\n",
      "mamba_spectra.1.blocks.0.fc2.bias 1536\n",
      "mamba_spectra.2.blocks.0.norm.weight 1536\n",
      "mamba_spectra.2.blocks.0.norm.bias 1536\n",
      "mamba_spectra.2.blocks.0.fc1.weight 9437184\n",
      "mamba_spectra.2.blocks.0.fc1.bias 6144\n",
      "mamba_spectra.2.blocks.0.conv.weight 98304\n",
      "mamba_spectra.2.blocks.0.conv.bias 3072\n",
      "mamba_spectra.2.blocks.0.fc2.weight 4718592\n",
      "mamba_spectra.2.blocks.0.fc2.bias 1536\n",
      "mamba_spectra.3.blocks.0.norm.weight 1536\n",
      "mamba_spectra.3.blocks.0.norm.bias 1536\n",
      "mamba_spectra.3.blocks.0.fc1.weight 9437184\n",
      "mamba_spectra.3.blocks.0.fc1.bias 6144\n",
      "mamba_spectra.3.blocks.0.conv.weight 98304\n",
      "mamba_spectra.3.blocks.0.conv.bias 3072\n",
      "mamba_spectra.3.blocks.0.fc2.weight 4718592\n",
      "mamba_spectra.3.blocks.0.fc2.bias 1536\n",
      "mamba_spectra.4.blocks.0.norm.weight 1536\n",
      "mamba_spectra.4.blocks.0.norm.bias 1536\n",
      "mamba_spectra.4.blocks.0.fc1.weight 9437184\n",
      "mamba_spectra.4.blocks.0.fc1.bias 6144\n",
      "mamba_spectra.4.blocks.0.conv.weight 98304\n",
      "mamba_spectra.4.blocks.0.conv.bias 3072\n",
      "mamba_spectra.4.blocks.0.fc2.weight 4718592\n",
      "mamba_spectra.4.blocks.0.fc2.bias 1536\n",
      "mamba_spectra.5.blocks.0.norm.weight 1536\n",
      "mamba_spectra.5.blocks.0.norm.bias 1536\n",
      "mamba_spectra.5.blocks.0.fc1.weight 9437184\n",
      "mamba_spectra.5.blocks.0.fc1.bias 6144\n",
      "mamba_spectra.5.blocks.0.conv.weight 98304\n",
      "mamba_spectra.5.blocks.0.conv.bias 3072\n",
      "mamba_spectra.5.blocks.0.fc2.weight 4718592\n",
      "mamba_spectra.5.blocks.0.fc2.bias 1536\n",
      "mamba_spectra.6.blocks.0.norm.weight 1536\n",
      "mamba_spectra.6.blocks.0.norm.bias 1536\n",
      "mamba_spectra.6.blocks.0.fc1.weight 9437184\n",
      "mamba_spectra.6.blocks.0.fc1.bias 6144\n",
      "mamba_spectra.6.blocks.0.conv.weight 98304\n",
      "mamba_spectra.6.blocks.0.conv.bias 3072\n",
      "mamba_spectra.6.blocks.0.fc2.weight 4718592\n",
      "mamba_spectra.6.blocks.0.fc2.bias 1536\n",
      "mamba_spectra.7.blocks.0.norm.weight 1536\n",
      "mamba_spectra.7.blocks.0.norm.bias 1536\n",
      "mamba_spectra.7.blocks.0.fc1.weight 9437184\n",
      "mamba_spectra.7.blocks.0.fc1.bias 6144\n",
      "mamba_spectra.7.blocks.0.conv.weight 98304\n",
      "mamba_spectra.7.blocks.0.conv.bias 3072\n",
      "mamba_spectra.7.blocks.0.fc2.weight 4718592\n",
      "mamba_spectra.7.blocks.0.fc2.bias 1536\n",
      "mamba_spectra.8.blocks.0.norm.weight 1536\n",
      "mamba_spectra.8.blocks.0.norm.bias 1536\n",
      "mamba_spectra.8.blocks.0.fc1.weight 9437184\n",
      "mamba_spectra.8.blocks.0.fc1.bias 6144\n",
      "mamba_spectra.8.blocks.0.conv.weight 98304\n",
      "mamba_spectra.8.blocks.0.conv.bias 3072\n",
      "mamba_spectra.8.blocks.0.fc2.weight 4718592\n",
      "mamba_spectra.8.blocks.0.fc2.bias 1536\n",
      "mamba_spectra.9.blocks.0.norm.weight 1536\n",
      "mamba_spectra.9.blocks.0.norm.bias 1536\n",
      "mamba_spectra.9.blocks.0.fc1.weight 9437184\n",
      "mamba_spectra.9.blocks.0.fc1.bias 6144\n",
      "mamba_spectra.9.blocks.0.conv.weight 98304\n",
      "mamba_spectra.9.blocks.0.conv.bias 3072\n",
      "mamba_spectra.9.blocks.0.fc2.weight 4718592\n",
      "mamba_spectra.9.blocks.0.fc2.bias 1536\n",
      "mamba_spectra.10.blocks.0.norm.weight 1536\n",
      "mamba_spectra.10.blocks.0.norm.bias 1536\n",
      "mamba_spectra.10.blocks.0.fc1.weight 9437184\n",
      "mamba_spectra.10.blocks.0.fc1.bias 6144\n",
      "mamba_spectra.10.blocks.0.conv.weight 98304\n",
      "mamba_spectra.10.blocks.0.conv.bias 3072\n",
      "mamba_spectra.10.blocks.0.fc2.weight 4718592\n",
      "mamba_spectra.10.blocks.0.fc2.bias 1536\n",
      "mamba_spectra.11.blocks.0.norm.weight 1536\n",
      "mamba_spectra.11.blocks.0.norm.bias 1536\n",
      "mamba_spectra.11.blocks.0.fc1.weight 9437184\n",
      "mamba_spectra.11.blocks.0.fc1.bias 6144\n",
      "mamba_spectra.11.blocks.0.conv.weight 98304\n",
      "mamba_spectra.11.blocks.0.conv.bias 3072\n",
      "mamba_spectra.11.blocks.0.fc2.weight 4718592\n",
      "mamba_spectra.11.blocks.0.fc2.bias 1536\n",
      "mamba_spectra.12.blocks.0.norm.weight 1536\n",
      "mamba_spectra.12.blocks.0.norm.bias 1536\n",
      "mamba_spectra.12.blocks.0.fc1.weight 9437184\n",
      "mamba_spectra.12.blocks.0.fc1.bias 6144\n",
      "mamba_spectra.12.blocks.0.conv.weight 98304\n",
      "mamba_spectra.12.blocks.0.conv.bias 3072\n",
      "mamba_spectra.12.blocks.0.fc2.weight 4718592\n",
      "mamba_spectra.12.blocks.0.fc2.bias 1536\n",
      "mamba_spectra.13.blocks.0.norm.weight 1536\n",
      "mamba_spectra.13.blocks.0.norm.bias 1536\n",
      "mamba_spectra.13.blocks.0.fc1.weight 9437184\n",
      "mamba_spectra.13.blocks.0.fc1.bias 6144\n",
      "mamba_spectra.13.blocks.0.conv.weight 98304\n",
      "mamba_spectra.13.blocks.0.conv.bias 3072\n",
      "mamba_spectra.13.blocks.0.fc2.weight 4718592\n",
      "mamba_spectra.13.blocks.0.fc2.bias 1536\n",
      "mamba_spectra.14.blocks.0.norm.weight 1536\n",
      "mamba_spectra.14.blocks.0.norm.bias 1536\n",
      "mamba_spectra.14.blocks.0.fc1.weight 9437184\n",
      "mamba_spectra.14.blocks.0.fc1.bias 6144\n",
      "mamba_spectra.14.blocks.0.conv.weight 98304\n",
      "mamba_spectra.14.blocks.0.conv.bias 3072\n",
      "mamba_spectra.14.blocks.0.fc2.weight 4718592\n",
      "mamba_spectra.14.blocks.0.fc2.bias 1536\n",
      "mamba_spectra.15.blocks.0.norm.weight 1536\n",
      "mamba_spectra.15.blocks.0.norm.bias 1536\n",
      "mamba_spectra.15.blocks.0.fc1.weight 9437184\n",
      "mamba_spectra.15.blocks.0.fc1.bias 6144\n",
      "mamba_spectra.15.blocks.0.conv.weight 98304\n",
      "mamba_spectra.15.blocks.0.conv.bias 3072\n",
      "mamba_spectra.15.blocks.0.fc2.weight 4718592\n",
      "mamba_spectra.15.blocks.0.fc2.bias 1536\n",
      "mamba_spectra.16.blocks.0.norm.weight 1536\n",
      "mamba_spectra.16.blocks.0.norm.bias 1536\n",
      "mamba_spectra.16.blocks.0.fc1.weight 9437184\n",
      "mamba_spectra.16.blocks.0.fc1.bias 6144\n",
      "mamba_spectra.16.blocks.0.conv.weight 98304\n",
      "mamba_spectra.16.blocks.0.conv.bias 3072\n",
      "mamba_spectra.16.blocks.0.fc2.weight 4718592\n",
      "mamba_spectra.16.blocks.0.fc2.bias 1536\n",
      "mamba_spectra.17.blocks.0.norm.weight 1536\n",
      "mamba_spectra.17.blocks.0.norm.bias 1536\n",
      "mamba_spectra.17.blocks.0.fc1.weight 9437184\n",
      "mamba_spectra.17.blocks.0.fc1.bias 6144\n",
      "mamba_spectra.17.blocks.0.conv.weight 98304\n",
      "mamba_spectra.17.blocks.0.conv.bias 3072\n",
      "mamba_spectra.17.blocks.0.fc2.weight 4718592\n",
      "mamba_spectra.17.blocks.0.fc2.bias 1536\n",
      "mamba_spectra.18.blocks.0.norm.weight 1536\n",
      "mamba_spectra.18.blocks.0.norm.bias 1536\n",
      "mamba_spectra.18.blocks.0.fc1.weight 9437184\n",
      "mamba_spectra.18.blocks.0.fc1.bias 6144\n",
      "mamba_spectra.18.blocks.0.conv.weight 98304\n",
      "mamba_spectra.18.blocks.0.conv.bias 3072\n",
      "mamba_spectra.18.blocks.0.fc2.weight 4718592\n",
      "mamba_spectra.18.blocks.0.fc2.bias 1536\n",
      "mamba_spectra.19.blocks.0.norm.weight 1536\n",
      "mamba_spectra.19.blocks.0.norm.bias 1536\n",
      "mamba_spectra.19.blocks.0.fc1.weight 9437184\n",
      "mamba_spectra.19.blocks.0.fc1.bias 6144\n",
      "mamba_spectra.19.blocks.0.conv.weight 98304\n",
      "mamba_spectra.19.blocks.0.conv.bias 3072\n",
      "mamba_spectra.19.blocks.0.fc2.weight 4718592\n",
      "mamba_spectra.19.blocks.0.fc2.bias 1536\n",
      "mamba_gaia.0.blocks.0.norm.weight 1536\n",
      "mamba_gaia.0.blocks.0.norm.bias 1536\n",
      "mamba_gaia.0.blocks.0.fc1.weight 9437184\n",
      "mamba_gaia.0.blocks.0.fc1.bias 6144\n",
      "mamba_gaia.0.blocks.0.conv.weight 98304\n",
      "mamba_gaia.0.blocks.0.conv.bias 3072\n",
      "mamba_gaia.0.blocks.0.fc2.weight 4718592\n",
      "mamba_gaia.0.blocks.0.fc2.bias 1536\n",
      "mamba_gaia.1.blocks.0.norm.weight 1536\n",
      "mamba_gaia.1.blocks.0.norm.bias 1536\n",
      "mamba_gaia.1.blocks.0.fc1.weight 9437184\n",
      "mamba_gaia.1.blocks.0.fc1.bias 6144\n",
      "mamba_gaia.1.blocks.0.conv.weight 98304\n",
      "mamba_gaia.1.blocks.0.conv.bias 3072\n",
      "mamba_gaia.1.blocks.0.fc2.weight 4718592\n",
      "mamba_gaia.1.blocks.0.fc2.bias 1536\n",
      "mamba_gaia.2.blocks.0.norm.weight 1536\n",
      "mamba_gaia.2.blocks.0.norm.bias 1536\n",
      "mamba_gaia.2.blocks.0.fc1.weight 9437184\n",
      "mamba_gaia.2.blocks.0.fc1.bias 6144\n",
      "mamba_gaia.2.blocks.0.conv.weight 98304\n",
      "mamba_gaia.2.blocks.0.conv.bias 3072\n",
      "mamba_gaia.2.blocks.0.fc2.weight 4718592\n",
      "mamba_gaia.2.blocks.0.fc2.bias 1536\n",
      "mamba_gaia.3.blocks.0.norm.weight 1536\n",
      "mamba_gaia.3.blocks.0.norm.bias 1536\n",
      "mamba_gaia.3.blocks.0.fc1.weight 9437184\n",
      "mamba_gaia.3.blocks.0.fc1.bias 6144\n",
      "mamba_gaia.3.blocks.0.conv.weight 98304\n",
      "mamba_gaia.3.blocks.0.conv.bias 3072\n",
      "mamba_gaia.3.blocks.0.fc2.weight 4718592\n",
      "mamba_gaia.3.blocks.0.fc2.bias 1536\n",
      "mamba_gaia.4.blocks.0.norm.weight 1536\n",
      "mamba_gaia.4.blocks.0.norm.bias 1536\n",
      "mamba_gaia.4.blocks.0.fc1.weight 9437184\n",
      "mamba_gaia.4.blocks.0.fc1.bias 6144\n",
      "mamba_gaia.4.blocks.0.conv.weight 98304\n",
      "mamba_gaia.4.blocks.0.conv.bias 3072\n",
      "mamba_gaia.4.blocks.0.fc2.weight 4718592\n",
      "mamba_gaia.4.blocks.0.fc2.bias 1536\n",
      "mamba_gaia.5.blocks.0.norm.weight 1536\n",
      "mamba_gaia.5.blocks.0.norm.bias 1536\n",
      "mamba_gaia.5.blocks.0.fc1.weight 9437184\n",
      "mamba_gaia.5.blocks.0.fc1.bias 6144\n",
      "mamba_gaia.5.blocks.0.conv.weight 98304\n",
      "mamba_gaia.5.blocks.0.conv.bias 3072\n",
      "mamba_gaia.5.blocks.0.fc2.weight 4718592\n",
      "mamba_gaia.5.blocks.0.fc2.bias 1536\n",
      "mamba_gaia.6.blocks.0.norm.weight 1536\n",
      "mamba_gaia.6.blocks.0.norm.bias 1536\n",
      "mamba_gaia.6.blocks.0.fc1.weight 9437184\n",
      "mamba_gaia.6.blocks.0.fc1.bias 6144\n",
      "mamba_gaia.6.blocks.0.conv.weight 98304\n",
      "mamba_gaia.6.blocks.0.conv.bias 3072\n",
      "mamba_gaia.6.blocks.0.fc2.weight 4718592\n",
      "mamba_gaia.6.blocks.0.fc2.bias 1536\n",
      "mamba_gaia.7.blocks.0.norm.weight 1536\n",
      "mamba_gaia.7.blocks.0.norm.bias 1536\n",
      "mamba_gaia.7.blocks.0.fc1.weight 9437184\n",
      "mamba_gaia.7.blocks.0.fc1.bias 6144\n",
      "mamba_gaia.7.blocks.0.conv.weight 98304\n",
      "mamba_gaia.7.blocks.0.conv.bias 3072\n",
      "mamba_gaia.7.blocks.0.fc2.weight 4718592\n",
      "mamba_gaia.7.blocks.0.fc2.bias 1536\n",
      "mamba_gaia.8.blocks.0.norm.weight 1536\n",
      "mamba_gaia.8.blocks.0.norm.bias 1536\n",
      "mamba_gaia.8.blocks.0.fc1.weight 9437184\n",
      "mamba_gaia.8.blocks.0.fc1.bias 6144\n",
      "mamba_gaia.8.blocks.0.conv.weight 98304\n",
      "mamba_gaia.8.blocks.0.conv.bias 3072\n",
      "mamba_gaia.8.blocks.0.fc2.weight 4718592\n",
      "mamba_gaia.8.blocks.0.fc2.bias 1536\n",
      "mamba_gaia.9.blocks.0.norm.weight 1536\n",
      "mamba_gaia.9.blocks.0.norm.bias 1536\n",
      "mamba_gaia.9.blocks.0.fc1.weight 9437184\n",
      "mamba_gaia.9.blocks.0.fc1.bias 6144\n",
      "mamba_gaia.9.blocks.0.conv.weight 98304\n",
      "mamba_gaia.9.blocks.0.conv.bias 3072\n",
      "mamba_gaia.9.blocks.0.fc2.weight 4718592\n",
      "mamba_gaia.9.blocks.0.fc2.bias 1536\n",
      "mamba_gaia.10.blocks.0.norm.weight 1536\n",
      "mamba_gaia.10.blocks.0.norm.bias 1536\n",
      "mamba_gaia.10.blocks.0.fc1.weight 9437184\n",
      "mamba_gaia.10.blocks.0.fc1.bias 6144\n",
      "mamba_gaia.10.blocks.0.conv.weight 98304\n",
      "mamba_gaia.10.blocks.0.conv.bias 3072\n",
      "mamba_gaia.10.blocks.0.fc2.weight 4718592\n",
      "mamba_gaia.10.blocks.0.fc2.bias 1536\n",
      "mamba_gaia.11.blocks.0.norm.weight 1536\n",
      "mamba_gaia.11.blocks.0.norm.bias 1536\n",
      "mamba_gaia.11.blocks.0.fc1.weight 9437184\n",
      "mamba_gaia.11.blocks.0.fc1.bias 6144\n",
      "mamba_gaia.11.blocks.0.conv.weight 98304\n",
      "mamba_gaia.11.blocks.0.conv.bias 3072\n",
      "mamba_gaia.11.blocks.0.fc2.weight 4718592\n",
      "mamba_gaia.11.blocks.0.fc2.bias 1536\n",
      "mamba_gaia.12.blocks.0.norm.weight 1536\n",
      "mamba_gaia.12.blocks.0.norm.bias 1536\n",
      "mamba_gaia.12.blocks.0.fc1.weight 9437184\n",
      "mamba_gaia.12.blocks.0.fc1.bias 6144\n",
      "mamba_gaia.12.blocks.0.conv.weight 98304\n",
      "mamba_gaia.12.blocks.0.conv.bias 3072\n",
      "mamba_gaia.12.blocks.0.fc2.weight 4718592\n",
      "mamba_gaia.12.blocks.0.fc2.bias 1536\n",
      "mamba_gaia.13.blocks.0.norm.weight 1536\n",
      "mamba_gaia.13.blocks.0.norm.bias 1536\n",
      "mamba_gaia.13.blocks.0.fc1.weight 9437184\n",
      "mamba_gaia.13.blocks.0.fc1.bias 6144\n",
      "mamba_gaia.13.blocks.0.conv.weight 98304\n",
      "mamba_gaia.13.blocks.0.conv.bias 3072\n",
      "mamba_gaia.13.blocks.0.fc2.weight 4718592\n",
      "mamba_gaia.13.blocks.0.fc2.bias 1536\n",
      "mamba_gaia.14.blocks.0.norm.weight 1536\n",
      "mamba_gaia.14.blocks.0.norm.bias 1536\n",
      "mamba_gaia.14.blocks.0.fc1.weight 9437184\n",
      "mamba_gaia.14.blocks.0.fc1.bias 6144\n",
      "mamba_gaia.14.blocks.0.conv.weight 98304\n",
      "mamba_gaia.14.blocks.0.conv.bias 3072\n",
      "mamba_gaia.14.blocks.0.fc2.weight 4718592\n",
      "mamba_gaia.14.blocks.0.fc2.bias 1536\n",
      "mamba_gaia.15.blocks.0.norm.weight 1536\n",
      "mamba_gaia.15.blocks.0.norm.bias 1536\n",
      "mamba_gaia.15.blocks.0.fc1.weight 9437184\n",
      "mamba_gaia.15.blocks.0.fc1.bias 6144\n",
      "mamba_gaia.15.blocks.0.conv.weight 98304\n",
      "mamba_gaia.15.blocks.0.conv.bias 3072\n",
      "mamba_gaia.15.blocks.0.fc2.weight 4718592\n",
      "mamba_gaia.15.blocks.0.fc2.bias 1536\n",
      "mamba_gaia.16.blocks.0.norm.weight 1536\n",
      "mamba_gaia.16.blocks.0.norm.bias 1536\n",
      "mamba_gaia.16.blocks.0.fc1.weight 9437184\n",
      "mamba_gaia.16.blocks.0.fc1.bias 6144\n",
      "mamba_gaia.16.blocks.0.conv.weight 98304\n",
      "mamba_gaia.16.blocks.0.conv.bias 3072\n",
      "mamba_gaia.16.blocks.0.fc2.weight 4718592\n",
      "mamba_gaia.16.blocks.0.fc2.bias 1536\n",
      "mamba_gaia.17.blocks.0.norm.weight 1536\n",
      "mamba_gaia.17.blocks.0.norm.bias 1536\n",
      "mamba_gaia.17.blocks.0.fc1.weight 9437184\n",
      "mamba_gaia.17.blocks.0.fc1.bias 6144\n",
      "mamba_gaia.17.blocks.0.conv.weight 98304\n",
      "mamba_gaia.17.blocks.0.conv.bias 3072\n",
      "mamba_gaia.17.blocks.0.fc2.weight 4718592\n",
      "mamba_gaia.17.blocks.0.fc2.bias 1536\n",
      "mamba_gaia.18.blocks.0.norm.weight 1536\n",
      "mamba_gaia.18.blocks.0.norm.bias 1536\n",
      "mamba_gaia.18.blocks.0.fc1.weight 9437184\n",
      "mamba_gaia.18.blocks.0.fc1.bias 6144\n",
      "mamba_gaia.18.blocks.0.conv.weight 98304\n",
      "mamba_gaia.18.blocks.0.conv.bias 3072\n",
      "mamba_gaia.18.blocks.0.fc2.weight 4718592\n",
      "mamba_gaia.18.blocks.0.fc2.bias 1536\n",
      "mamba_gaia.19.blocks.0.norm.weight 1536\n",
      "mamba_gaia.19.blocks.0.norm.bias 1536\n",
      "mamba_gaia.19.blocks.0.fc1.weight 9437184\n",
      "mamba_gaia.19.blocks.0.fc1.bias 6144\n",
      "mamba_gaia.19.blocks.0.conv.weight 98304\n",
      "mamba_gaia.19.blocks.0.conv.bias 3072\n",
      "mamba_gaia.19.blocks.0.fc2.weight 4718592\n",
      "mamba_gaia.19.blocks.0.fc2.bias 1536\n",
      "cross_attn_block_spectra.norm.weight 1536\n",
      "cross_attn_block_spectra.norm.bias 1536\n",
      "cross_attn_block_spectra.attention.in_proj_weight 7077888\n",
      "cross_attn_block_spectra.attention.in_proj_bias 4608\n",
      "cross_attn_block_spectra.attention.out_proj.weight 2359296\n",
      "cross_attn_block_spectra.attention.out_proj.bias 1536\n",
      "cross_attn_block_gaia.norm.weight 1536\n",
      "cross_attn_block_gaia.norm.bias 1536\n",
      "cross_attn_block_gaia.attention.in_proj_weight 7077888\n",
      "cross_attn_block_gaia.attention.in_proj_bias 4608\n",
      "cross_attn_block_gaia.attention.out_proj.weight 2359296\n",
      "cross_attn_block_gaia.attention.out_proj.bias 1536\n",
      "classifier.0.weight 3072\n",
      "classifier.0.bias 3072\n",
      "classifier.1.weight 168960\n",
      "classifier.1.bias 55\n",
      "Model size: 2249.906 MB\n",
      "Total parameters: 589799479\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td></td></tr><tr><td>hamming_loss</td><td></td></tr><tr><td>lr</td><td></td></tr><tr><td>macro_f1</td><td></td></tr><tr><td>macro_precision</td><td></td></tr><tr><td>macro_recall</td><td></td></tr><tr><td>micro_f1</td><td></td></tr><tr><td>micro_precision</td><td></td></tr><tr><td>micro_recall</td><td></td></tr><tr><td>test_acc</td><td></td></tr><tr><td>test_loss</td><td></td></tr><tr><td>train_acc</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>val_acc</td><td></td></tr><tr><td>val_loss</td><td></td></tr><tr><td>weighted_f1</td><td></td></tr><tr><td>weighted_precision</td><td></td></tr><tr><td>weighted_recall</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>437</td></tr><tr><td>hamming_loss</td><td>0.02831</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>macro_f1</td><td>0.34189</td></tr><tr><td>macro_precision</td><td>0.77948</td></tr><tr><td>macro_recall</td><td>0.28316</td></tr><tr><td>micro_f1</td><td>0.44784</td></tr><tr><td>micro_precision</td><td>0.78155</td></tr><tr><td>micro_recall</td><td>0.31384</td></tr><tr><td>test_acc</td><td>0.9717</td></tr><tr><td>test_loss</td><td>0.05836</td></tr><tr><td>train_acc</td><td>0.9808</td></tr><tr><td>train_loss</td><td>0.02739</td></tr><tr><td>val_acc</td><td>0.97279</td></tr><tr><td>val_loss</td><td>0.04623</td></tr><tr><td>weighted_f1</td><td>0.40584</td></tr><tr><td>weighted_precision</td><td>0.82116</td></tr><tr><td>weighted_recall</td><td>0.31384</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fast-sun-45</strong> at: <a href='https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_transformer_tokenized/runs/4wkrik43' target=\"_blank\">https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_transformer_tokenized/runs/4wkrik43</a><br> View project at: <a href='https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_transformer_tokenized' target=\"_blank\">https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_transformer_tokenized</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250407_155943-4wkrik43/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration for tokenized transformer model, reduced embedding dimension\n",
    "    d_model_spectra = 1536\n",
    "    d_model_gaia = 1536\n",
    "    num_classes = 55\n",
    "    input_dim_spectra = 3647\n",
    "    input_dim_gaia = 18\n",
    "    \n",
    "    # Token dimensions - these control the sequence length\n",
    "    token_dim_spectra = 7  # Will create 522 tokens for spectra (3647/7)\n",
    "    token_dim_gaia = 1      # Will create 18 tokens for gaia (18/1)\n",
    "    \n",
    "    n_layers = 20  \n",
    "    n_heads = 8\n",
    "    \n",
    "    lr = 2.5e-6\n",
    "    patience = 200\n",
    "    num_epochs = 800\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(\"Device:\", device)\n",
    "    \n",
    "    # Initialize WandB\n",
    "    wandb.init(project=\"ALLSTARS_multimodal_fusion_transformer_tokenized\")\n",
    "    config = {\n",
    "        \"model_type\": \"transformer_tokenized\",\n",
    "        \"num_classes\": num_classes,\n",
    "        \"d_model_spectra\": d_model_spectra,\n",
    "        \"d_model_gaia\": d_model_gaia,\n",
    "        \"input_dim_spectra\": input_dim_spectra,\n",
    "        \"input_dim_gaia\": input_dim_gaia,\n",
    "        \"token_dim_spectra\": token_dim_spectra,\n",
    "        \"token_dim_gaia\": token_dim_gaia,\n",
    "        \"num_tokens_spectra\": (input_dim_spectra + token_dim_spectra - 1) // token_dim_spectra,\n",
    "        \"num_tokens_gaia\": (input_dim_gaia + token_dim_gaia - 1) // token_dim_gaia,\n",
    "        \"n_layers\": n_layers,\n",
    "        \"n_heads\": n_heads,\n",
    "        \"use_cross_attention\": True,\n",
    "        \"lr\": lr,\n",
    "        \"patience\": patience,\n",
    "        \"num_epochs\": num_epochs\n",
    "    }\n",
    "    wandb.config.update(config)\n",
    "    \n",
    "    # Instantiate the tokenized transformer model\n",
    "    model_MambaOut = StarClassifierFusionMambaOut(\n",
    "        d_model_spectra=d_model_spectra,\n",
    "        d_model_gaia=d_model_gaia,\n",
    "        num_classes=num_classes,\n",
    "        input_dim_spectra=input_dim_spectra,\n",
    "        input_dim_gaia=input_dim_gaia,\n",
    "        token_dim_spectra=token_dim_spectra,\n",
    "        token_dim_gaia=token_dim_gaia,\n",
    "        n_layers=n_layers,\n",
    "        use_cross_attention=True,\n",
    "        d_conv=32,\n",
    "        n_cross_attn_heads=8\n",
    "    )\n",
    "    model_MambaOut.to(device)\n",
    "\n",
    "    \n",
    "    # Print model statistics\n",
    "    print(f\"Number of spectra tokens: {(input_dim_spectra + token_dim_spectra - 1) // token_dim_spectra}\")\n",
    "    print(f\"Number of gaia tokens: {(input_dim_gaia + token_dim_gaia - 1) // token_dim_gaia}\")\n",
    "    \n",
    "    # Compute parameter size\n",
    "    param_size = 0\n",
    "    for param in model_MambaOut.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    \n",
    "    # Compute buffer size\n",
    "    buffer_size = 0\n",
    "    for buffer in model_MambaOut.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "    print(model_MambaOut)\n",
    "    # print number of parameters per layer\n",
    "    for name, param in model_MambaOut.named_parameters():\n",
    "        print(name, param.numel())\n",
    "    \n",
    "    # Total size in MB\n",
    "    total_size_mb = (param_size + buffer_size) / (1024 ** 2)\n",
    "    print(f\"Model size: {total_size_mb:.3f} MB\")\n",
    "    print(f\"Total parameters: {sum(p.numel() for p in model_MambaOut.parameters() if p.requires_grad)}\")\n",
    "    \n",
    "    # Train the transformer model\n",
    "    trained_transformer_model = train_model_fusion(\n",
    "        model=model_MambaOut,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader,\n",
    "        num_epochs=num_epochs,\n",
    "        lr=lr,\n",
    "        max_patience=patience,\n",
    "        device=device\n",
    "    )\n",
    "    wandb.finish()\n",
    "    \n",
    "    # Save the model\n",
    "    torch.save(trained_transformer_model.state_dict(), \"Models/model_fusion_MambaOut_Many_tokens.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3536932824.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[9], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    breakit up\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "breakit up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer 1 token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjoaocsgalmeida\u001b[0m (\u001b[33mjoaocsgalmeida-university-of-southampton\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/joao/Documents/Star-Classifier/wandb/run-20250408_162919-dbhq8sn4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_transformer/runs/dbhq8sn4' target=\"_blank\">dark-gorge-13</a></strong> to <a href='https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_transformer' target=\"_blank\">https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_transformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_transformer/runs/dbhq8sn4' target=\"_blank\">https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_transformer/runs/dbhq8sn4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 1.041 GB\n",
      "Model size: 4131.557 MB\n",
      "StarClassifierFusionTransformer(\n",
      "  (tokenizer_spectra): FeatureTokenizer(\n",
      "    (token_embed): Linear(in_features=64, out_features=2048, bias=True)\n",
      "  )\n",
      "  (tokenizer_gaia): FeatureTokenizer(\n",
      "    (token_embed): Linear(in_features=2, out_features=2048, bias=True)\n",
      "  )\n",
      "  (transformer_spectra): TransformerFeatureExtractor(\n",
      "    (layers): ModuleList(\n",
      "      (0-9): 10 x TransformerBlock(\n",
      "        (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): RotarySelfAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (rope): RotaryEmbedding()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "        (ffn): Sequential(\n",
      "          (0): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.1, inplace=False)\n",
      "          (3): Linear(in_features=8192, out_features=2048, bias=True)\n",
      "          (4): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (transformer_gaia): TransformerFeatureExtractor(\n",
      "    (layers): ModuleList(\n",
      "      (0-9): 10 x TransformerBlock(\n",
      "        (norm1): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): RotarySelfAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (out_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "          (rope): RotaryEmbedding()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "        (ffn): Sequential(\n",
      "          (0): Linear(in_features=2048, out_features=8192, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.1, inplace=False)\n",
      "          (3): Linear(in_features=8192, out_features=2048, bias=True)\n",
      "          (4): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (cross_attn_block_spectra): CrossAttentionBlock(\n",
      "    (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "    (attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=2048, out_features=2048, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (cross_attn_block_gaia): CrossAttentionBlock(\n",
      "    (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "    (attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=2048, out_features=2048, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): Dropout(p=0.1, inplace=False)\n",
      "    (2): Linear(in_features=4096, out_features=55, bias=True)\n",
      "  )\n",
      ")\n",
      "tokenizer_spectra.token_embed.weight 131072\n",
      "tokenizer_spectra.token_embed.bias 2048\n",
      "tokenizer_gaia.token_embed.weight 4096\n",
      "tokenizer_gaia.token_embed.bias 2048\n",
      "transformer_spectra.layers.0.norm1.weight 2048\n",
      "transformer_spectra.layers.0.norm1.bias 2048\n",
      "transformer_spectra.layers.0.attn.q_proj.weight 4194304\n",
      "transformer_spectra.layers.0.attn.q_proj.bias 2048\n",
      "transformer_spectra.layers.0.attn.k_proj.weight 4194304\n",
      "transformer_spectra.layers.0.attn.k_proj.bias 2048\n",
      "transformer_spectra.layers.0.attn.v_proj.weight 4194304\n",
      "transformer_spectra.layers.0.attn.v_proj.bias 2048\n",
      "transformer_spectra.layers.0.attn.out_proj.weight 4194304\n",
      "transformer_spectra.layers.0.attn.out_proj.bias 2048\n",
      "transformer_spectra.layers.0.norm2.weight 2048\n",
      "transformer_spectra.layers.0.norm2.bias 2048\n",
      "transformer_spectra.layers.0.ffn.0.weight 16777216\n",
      "transformer_spectra.layers.0.ffn.0.bias 8192\n",
      "transformer_spectra.layers.0.ffn.3.weight 16777216\n",
      "transformer_spectra.layers.0.ffn.3.bias 2048\n",
      "transformer_spectra.layers.1.norm1.weight 2048\n",
      "transformer_spectra.layers.1.norm1.bias 2048\n",
      "transformer_spectra.layers.1.attn.q_proj.weight 4194304\n",
      "transformer_spectra.layers.1.attn.q_proj.bias 2048\n",
      "transformer_spectra.layers.1.attn.k_proj.weight 4194304\n",
      "transformer_spectra.layers.1.attn.k_proj.bias 2048\n",
      "transformer_spectra.layers.1.attn.v_proj.weight 4194304\n",
      "transformer_spectra.layers.1.attn.v_proj.bias 2048\n",
      "transformer_spectra.layers.1.attn.out_proj.weight 4194304\n",
      "transformer_spectra.layers.1.attn.out_proj.bias 2048\n",
      "transformer_spectra.layers.1.norm2.weight 2048\n",
      "transformer_spectra.layers.1.norm2.bias 2048\n",
      "transformer_spectra.layers.1.ffn.0.weight 16777216\n",
      "transformer_spectra.layers.1.ffn.0.bias 8192\n",
      "transformer_spectra.layers.1.ffn.3.weight 16777216\n",
      "transformer_spectra.layers.1.ffn.3.bias 2048\n",
      "transformer_spectra.layers.2.norm1.weight 2048\n",
      "transformer_spectra.layers.2.norm1.bias 2048\n",
      "transformer_spectra.layers.2.attn.q_proj.weight 4194304\n",
      "transformer_spectra.layers.2.attn.q_proj.bias 2048\n",
      "transformer_spectra.layers.2.attn.k_proj.weight 4194304\n",
      "transformer_spectra.layers.2.attn.k_proj.bias 2048\n",
      "transformer_spectra.layers.2.attn.v_proj.weight 4194304\n",
      "transformer_spectra.layers.2.attn.v_proj.bias 2048\n",
      "transformer_spectra.layers.2.attn.out_proj.weight 4194304\n",
      "transformer_spectra.layers.2.attn.out_proj.bias 2048\n",
      "transformer_spectra.layers.2.norm2.weight 2048\n",
      "transformer_spectra.layers.2.norm2.bias 2048\n",
      "transformer_spectra.layers.2.ffn.0.weight 16777216\n",
      "transformer_spectra.layers.2.ffn.0.bias 8192\n",
      "transformer_spectra.layers.2.ffn.3.weight 16777216\n",
      "transformer_spectra.layers.2.ffn.3.bias 2048\n",
      "transformer_spectra.layers.3.norm1.weight 2048\n",
      "transformer_spectra.layers.3.norm1.bias 2048\n",
      "transformer_spectra.layers.3.attn.q_proj.weight 4194304\n",
      "transformer_spectra.layers.3.attn.q_proj.bias 2048\n",
      "transformer_spectra.layers.3.attn.k_proj.weight 4194304\n",
      "transformer_spectra.layers.3.attn.k_proj.bias 2048\n",
      "transformer_spectra.layers.3.attn.v_proj.weight 4194304\n",
      "transformer_spectra.layers.3.attn.v_proj.bias 2048\n",
      "transformer_spectra.layers.3.attn.out_proj.weight 4194304\n",
      "transformer_spectra.layers.3.attn.out_proj.bias 2048\n",
      "transformer_spectra.layers.3.norm2.weight 2048\n",
      "transformer_spectra.layers.3.norm2.bias 2048\n",
      "transformer_spectra.layers.3.ffn.0.weight 16777216\n",
      "transformer_spectra.layers.3.ffn.0.bias 8192\n",
      "transformer_spectra.layers.3.ffn.3.weight 16777216\n",
      "transformer_spectra.layers.3.ffn.3.bias 2048\n",
      "transformer_spectra.layers.4.norm1.weight 2048\n",
      "transformer_spectra.layers.4.norm1.bias 2048\n",
      "transformer_spectra.layers.4.attn.q_proj.weight 4194304\n",
      "transformer_spectra.layers.4.attn.q_proj.bias 2048\n",
      "transformer_spectra.layers.4.attn.k_proj.weight 4194304\n",
      "transformer_spectra.layers.4.attn.k_proj.bias 2048\n",
      "transformer_spectra.layers.4.attn.v_proj.weight 4194304\n",
      "transformer_spectra.layers.4.attn.v_proj.bias 2048\n",
      "transformer_spectra.layers.4.attn.out_proj.weight 4194304\n",
      "transformer_spectra.layers.4.attn.out_proj.bias 2048\n",
      "transformer_spectra.layers.4.norm2.weight 2048\n",
      "transformer_spectra.layers.4.norm2.bias 2048\n",
      "transformer_spectra.layers.4.ffn.0.weight 16777216\n",
      "transformer_spectra.layers.4.ffn.0.bias 8192\n",
      "transformer_spectra.layers.4.ffn.3.weight 16777216\n",
      "transformer_spectra.layers.4.ffn.3.bias 2048\n",
      "transformer_spectra.layers.5.norm1.weight 2048\n",
      "transformer_spectra.layers.5.norm1.bias 2048\n",
      "transformer_spectra.layers.5.attn.q_proj.weight 4194304\n",
      "transformer_spectra.layers.5.attn.q_proj.bias 2048\n",
      "transformer_spectra.layers.5.attn.k_proj.weight 4194304\n",
      "transformer_spectra.layers.5.attn.k_proj.bias 2048\n",
      "transformer_spectra.layers.5.attn.v_proj.weight 4194304\n",
      "transformer_spectra.layers.5.attn.v_proj.bias 2048\n",
      "transformer_spectra.layers.5.attn.out_proj.weight 4194304\n",
      "transformer_spectra.layers.5.attn.out_proj.bias 2048\n",
      "transformer_spectra.layers.5.norm2.weight 2048\n",
      "transformer_spectra.layers.5.norm2.bias 2048\n",
      "transformer_spectra.layers.5.ffn.0.weight 16777216\n",
      "transformer_spectra.layers.5.ffn.0.bias 8192\n",
      "transformer_spectra.layers.5.ffn.3.weight 16777216\n",
      "transformer_spectra.layers.5.ffn.3.bias 2048\n",
      "transformer_spectra.layers.6.norm1.weight 2048\n",
      "transformer_spectra.layers.6.norm1.bias 2048\n",
      "transformer_spectra.layers.6.attn.q_proj.weight 4194304\n",
      "transformer_spectra.layers.6.attn.q_proj.bias 2048\n",
      "transformer_spectra.layers.6.attn.k_proj.weight 4194304\n",
      "transformer_spectra.layers.6.attn.k_proj.bias 2048\n",
      "transformer_spectra.layers.6.attn.v_proj.weight 4194304\n",
      "transformer_spectra.layers.6.attn.v_proj.bias 2048\n",
      "transformer_spectra.layers.6.attn.out_proj.weight 4194304\n",
      "transformer_spectra.layers.6.attn.out_proj.bias 2048\n",
      "transformer_spectra.layers.6.norm2.weight 2048\n",
      "transformer_spectra.layers.6.norm2.bias 2048\n",
      "transformer_spectra.layers.6.ffn.0.weight 16777216\n",
      "transformer_spectra.layers.6.ffn.0.bias 8192\n",
      "transformer_spectra.layers.6.ffn.3.weight 16777216\n",
      "transformer_spectra.layers.6.ffn.3.bias 2048\n",
      "transformer_spectra.layers.7.norm1.weight 2048\n",
      "transformer_spectra.layers.7.norm1.bias 2048\n",
      "transformer_spectra.layers.7.attn.q_proj.weight 4194304\n",
      "transformer_spectra.layers.7.attn.q_proj.bias 2048\n",
      "transformer_spectra.layers.7.attn.k_proj.weight 4194304\n",
      "transformer_spectra.layers.7.attn.k_proj.bias 2048\n",
      "transformer_spectra.layers.7.attn.v_proj.weight 4194304\n",
      "transformer_spectra.layers.7.attn.v_proj.bias 2048\n",
      "transformer_spectra.layers.7.attn.out_proj.weight 4194304\n",
      "transformer_spectra.layers.7.attn.out_proj.bias 2048\n",
      "transformer_spectra.layers.7.norm2.weight 2048\n",
      "transformer_spectra.layers.7.norm2.bias 2048\n",
      "transformer_spectra.layers.7.ffn.0.weight 16777216\n",
      "transformer_spectra.layers.7.ffn.0.bias 8192\n",
      "transformer_spectra.layers.7.ffn.3.weight 16777216\n",
      "transformer_spectra.layers.7.ffn.3.bias 2048\n",
      "transformer_spectra.layers.8.norm1.weight 2048\n",
      "transformer_spectra.layers.8.norm1.bias 2048\n",
      "transformer_spectra.layers.8.attn.q_proj.weight 4194304\n",
      "transformer_spectra.layers.8.attn.q_proj.bias 2048\n",
      "transformer_spectra.layers.8.attn.k_proj.weight 4194304\n",
      "transformer_spectra.layers.8.attn.k_proj.bias 2048\n",
      "transformer_spectra.layers.8.attn.v_proj.weight 4194304\n",
      "transformer_spectra.layers.8.attn.v_proj.bias 2048\n",
      "transformer_spectra.layers.8.attn.out_proj.weight 4194304\n",
      "transformer_spectra.layers.8.attn.out_proj.bias 2048\n",
      "transformer_spectra.layers.8.norm2.weight 2048\n",
      "transformer_spectra.layers.8.norm2.bias 2048\n",
      "transformer_spectra.layers.8.ffn.0.weight 16777216\n",
      "transformer_spectra.layers.8.ffn.0.bias 8192\n",
      "transformer_spectra.layers.8.ffn.3.weight 16777216\n",
      "transformer_spectra.layers.8.ffn.3.bias 2048\n",
      "transformer_spectra.layers.9.norm1.weight 2048\n",
      "transformer_spectra.layers.9.norm1.bias 2048\n",
      "transformer_spectra.layers.9.attn.q_proj.weight 4194304\n",
      "transformer_spectra.layers.9.attn.q_proj.bias 2048\n",
      "transformer_spectra.layers.9.attn.k_proj.weight 4194304\n",
      "transformer_spectra.layers.9.attn.k_proj.bias 2048\n",
      "transformer_spectra.layers.9.attn.v_proj.weight 4194304\n",
      "transformer_spectra.layers.9.attn.v_proj.bias 2048\n",
      "transformer_spectra.layers.9.attn.out_proj.weight 4194304\n",
      "transformer_spectra.layers.9.attn.out_proj.bias 2048\n",
      "transformer_spectra.layers.9.norm2.weight 2048\n",
      "transformer_spectra.layers.9.norm2.bias 2048\n",
      "transformer_spectra.layers.9.ffn.0.weight 16777216\n",
      "transformer_spectra.layers.9.ffn.0.bias 8192\n",
      "transformer_spectra.layers.9.ffn.3.weight 16777216\n",
      "transformer_spectra.layers.9.ffn.3.bias 2048\n",
      "transformer_gaia.layers.0.norm1.weight 2048\n",
      "transformer_gaia.layers.0.norm1.bias 2048\n",
      "transformer_gaia.layers.0.attn.q_proj.weight 4194304\n",
      "transformer_gaia.layers.0.attn.q_proj.bias 2048\n",
      "transformer_gaia.layers.0.attn.k_proj.weight 4194304\n",
      "transformer_gaia.layers.0.attn.k_proj.bias 2048\n",
      "transformer_gaia.layers.0.attn.v_proj.weight 4194304\n",
      "transformer_gaia.layers.0.attn.v_proj.bias 2048\n",
      "transformer_gaia.layers.0.attn.out_proj.weight 4194304\n",
      "transformer_gaia.layers.0.attn.out_proj.bias 2048\n",
      "transformer_gaia.layers.0.norm2.weight 2048\n",
      "transformer_gaia.layers.0.norm2.bias 2048\n",
      "transformer_gaia.layers.0.ffn.0.weight 16777216\n",
      "transformer_gaia.layers.0.ffn.0.bias 8192\n",
      "transformer_gaia.layers.0.ffn.3.weight 16777216\n",
      "transformer_gaia.layers.0.ffn.3.bias 2048\n",
      "transformer_gaia.layers.1.norm1.weight 2048\n",
      "transformer_gaia.layers.1.norm1.bias 2048\n",
      "transformer_gaia.layers.1.attn.q_proj.weight 4194304\n",
      "transformer_gaia.layers.1.attn.q_proj.bias 2048\n",
      "transformer_gaia.layers.1.attn.k_proj.weight 4194304\n",
      "transformer_gaia.layers.1.attn.k_proj.bias 2048\n",
      "transformer_gaia.layers.1.attn.v_proj.weight 4194304\n",
      "transformer_gaia.layers.1.attn.v_proj.bias 2048\n",
      "transformer_gaia.layers.1.attn.out_proj.weight 4194304\n",
      "transformer_gaia.layers.1.attn.out_proj.bias 2048\n",
      "transformer_gaia.layers.1.norm2.weight 2048\n",
      "transformer_gaia.layers.1.norm2.bias 2048\n",
      "transformer_gaia.layers.1.ffn.0.weight 16777216\n",
      "transformer_gaia.layers.1.ffn.0.bias 8192\n",
      "transformer_gaia.layers.1.ffn.3.weight 16777216\n",
      "transformer_gaia.layers.1.ffn.3.bias 2048\n",
      "transformer_gaia.layers.2.norm1.weight 2048\n",
      "transformer_gaia.layers.2.norm1.bias 2048\n",
      "transformer_gaia.layers.2.attn.q_proj.weight 4194304\n",
      "transformer_gaia.layers.2.attn.q_proj.bias 2048\n",
      "transformer_gaia.layers.2.attn.k_proj.weight 4194304\n",
      "transformer_gaia.layers.2.attn.k_proj.bias 2048\n",
      "transformer_gaia.layers.2.attn.v_proj.weight 4194304\n",
      "transformer_gaia.layers.2.attn.v_proj.bias 2048\n",
      "transformer_gaia.layers.2.attn.out_proj.weight 4194304\n",
      "transformer_gaia.layers.2.attn.out_proj.bias 2048\n",
      "transformer_gaia.layers.2.norm2.weight 2048\n",
      "transformer_gaia.layers.2.norm2.bias 2048\n",
      "transformer_gaia.layers.2.ffn.0.weight 16777216\n",
      "transformer_gaia.layers.2.ffn.0.bias 8192\n",
      "transformer_gaia.layers.2.ffn.3.weight 16777216\n",
      "transformer_gaia.layers.2.ffn.3.bias 2048\n",
      "transformer_gaia.layers.3.norm1.weight 2048\n",
      "transformer_gaia.layers.3.norm1.bias 2048\n",
      "transformer_gaia.layers.3.attn.q_proj.weight 4194304\n",
      "transformer_gaia.layers.3.attn.q_proj.bias 2048\n",
      "transformer_gaia.layers.3.attn.k_proj.weight 4194304\n",
      "transformer_gaia.layers.3.attn.k_proj.bias 2048\n",
      "transformer_gaia.layers.3.attn.v_proj.weight 4194304\n",
      "transformer_gaia.layers.3.attn.v_proj.bias 2048\n",
      "transformer_gaia.layers.3.attn.out_proj.weight 4194304\n",
      "transformer_gaia.layers.3.attn.out_proj.bias 2048\n",
      "transformer_gaia.layers.3.norm2.weight 2048\n",
      "transformer_gaia.layers.3.norm2.bias 2048\n",
      "transformer_gaia.layers.3.ffn.0.weight 16777216\n",
      "transformer_gaia.layers.3.ffn.0.bias 8192\n",
      "transformer_gaia.layers.3.ffn.3.weight 16777216\n",
      "transformer_gaia.layers.3.ffn.3.bias 2048\n",
      "transformer_gaia.layers.4.norm1.weight 2048\n",
      "transformer_gaia.layers.4.norm1.bias 2048\n",
      "transformer_gaia.layers.4.attn.q_proj.weight 4194304\n",
      "transformer_gaia.layers.4.attn.q_proj.bias 2048\n",
      "transformer_gaia.layers.4.attn.k_proj.weight 4194304\n",
      "transformer_gaia.layers.4.attn.k_proj.bias 2048\n",
      "transformer_gaia.layers.4.attn.v_proj.weight 4194304\n",
      "transformer_gaia.layers.4.attn.v_proj.bias 2048\n",
      "transformer_gaia.layers.4.attn.out_proj.weight 4194304\n",
      "transformer_gaia.layers.4.attn.out_proj.bias 2048\n",
      "transformer_gaia.layers.4.norm2.weight 2048\n",
      "transformer_gaia.layers.4.norm2.bias 2048\n",
      "transformer_gaia.layers.4.ffn.0.weight 16777216\n",
      "transformer_gaia.layers.4.ffn.0.bias 8192\n",
      "transformer_gaia.layers.4.ffn.3.weight 16777216\n",
      "transformer_gaia.layers.4.ffn.3.bias 2048\n",
      "transformer_gaia.layers.5.norm1.weight 2048\n",
      "transformer_gaia.layers.5.norm1.bias 2048\n",
      "transformer_gaia.layers.5.attn.q_proj.weight 4194304\n",
      "transformer_gaia.layers.5.attn.q_proj.bias 2048\n",
      "transformer_gaia.layers.5.attn.k_proj.weight 4194304\n",
      "transformer_gaia.layers.5.attn.k_proj.bias 2048\n",
      "transformer_gaia.layers.5.attn.v_proj.weight 4194304\n",
      "transformer_gaia.layers.5.attn.v_proj.bias 2048\n",
      "transformer_gaia.layers.5.attn.out_proj.weight 4194304\n",
      "transformer_gaia.layers.5.attn.out_proj.bias 2048\n",
      "transformer_gaia.layers.5.norm2.weight 2048\n",
      "transformer_gaia.layers.5.norm2.bias 2048\n",
      "transformer_gaia.layers.5.ffn.0.weight 16777216\n",
      "transformer_gaia.layers.5.ffn.0.bias 8192\n",
      "transformer_gaia.layers.5.ffn.3.weight 16777216\n",
      "transformer_gaia.layers.5.ffn.3.bias 2048\n",
      "transformer_gaia.layers.6.norm1.weight 2048\n",
      "transformer_gaia.layers.6.norm1.bias 2048\n",
      "transformer_gaia.layers.6.attn.q_proj.weight 4194304\n",
      "transformer_gaia.layers.6.attn.q_proj.bias 2048\n",
      "transformer_gaia.layers.6.attn.k_proj.weight 4194304\n",
      "transformer_gaia.layers.6.attn.k_proj.bias 2048\n",
      "transformer_gaia.layers.6.attn.v_proj.weight 4194304\n",
      "transformer_gaia.layers.6.attn.v_proj.bias 2048\n",
      "transformer_gaia.layers.6.attn.out_proj.weight 4194304\n",
      "transformer_gaia.layers.6.attn.out_proj.bias 2048\n",
      "transformer_gaia.layers.6.norm2.weight 2048\n",
      "transformer_gaia.layers.6.norm2.bias 2048\n",
      "transformer_gaia.layers.6.ffn.0.weight 16777216\n",
      "transformer_gaia.layers.6.ffn.0.bias 8192\n",
      "transformer_gaia.layers.6.ffn.3.weight 16777216\n",
      "transformer_gaia.layers.6.ffn.3.bias 2048\n",
      "transformer_gaia.layers.7.norm1.weight 2048\n",
      "transformer_gaia.layers.7.norm1.bias 2048\n",
      "transformer_gaia.layers.7.attn.q_proj.weight 4194304\n",
      "transformer_gaia.layers.7.attn.q_proj.bias 2048\n",
      "transformer_gaia.layers.7.attn.k_proj.weight 4194304\n",
      "transformer_gaia.layers.7.attn.k_proj.bias 2048\n",
      "transformer_gaia.layers.7.attn.v_proj.weight 4194304\n",
      "transformer_gaia.layers.7.attn.v_proj.bias 2048\n",
      "transformer_gaia.layers.7.attn.out_proj.weight 4194304\n",
      "transformer_gaia.layers.7.attn.out_proj.bias 2048\n",
      "transformer_gaia.layers.7.norm2.weight 2048\n",
      "transformer_gaia.layers.7.norm2.bias 2048\n",
      "transformer_gaia.layers.7.ffn.0.weight 16777216\n",
      "transformer_gaia.layers.7.ffn.0.bias 8192\n",
      "transformer_gaia.layers.7.ffn.3.weight 16777216\n",
      "transformer_gaia.layers.7.ffn.3.bias 2048\n",
      "transformer_gaia.layers.8.norm1.weight 2048\n",
      "transformer_gaia.layers.8.norm1.bias 2048\n",
      "transformer_gaia.layers.8.attn.q_proj.weight 4194304\n",
      "transformer_gaia.layers.8.attn.q_proj.bias 2048\n",
      "transformer_gaia.layers.8.attn.k_proj.weight 4194304\n",
      "transformer_gaia.layers.8.attn.k_proj.bias 2048\n",
      "transformer_gaia.layers.8.attn.v_proj.weight 4194304\n",
      "transformer_gaia.layers.8.attn.v_proj.bias 2048\n",
      "transformer_gaia.layers.8.attn.out_proj.weight 4194304\n",
      "transformer_gaia.layers.8.attn.out_proj.bias 2048\n",
      "transformer_gaia.layers.8.norm2.weight 2048\n",
      "transformer_gaia.layers.8.norm2.bias 2048\n",
      "transformer_gaia.layers.8.ffn.0.weight 16777216\n",
      "transformer_gaia.layers.8.ffn.0.bias 8192\n",
      "transformer_gaia.layers.8.ffn.3.weight 16777216\n",
      "transformer_gaia.layers.8.ffn.3.bias 2048\n",
      "transformer_gaia.layers.9.norm1.weight 2048\n",
      "transformer_gaia.layers.9.norm1.bias 2048\n",
      "transformer_gaia.layers.9.attn.q_proj.weight 4194304\n",
      "transformer_gaia.layers.9.attn.q_proj.bias 2048\n",
      "transformer_gaia.layers.9.attn.k_proj.weight 4194304\n",
      "transformer_gaia.layers.9.attn.k_proj.bias 2048\n",
      "transformer_gaia.layers.9.attn.v_proj.weight 4194304\n",
      "transformer_gaia.layers.9.attn.v_proj.bias 2048\n",
      "transformer_gaia.layers.9.attn.out_proj.weight 4194304\n",
      "transformer_gaia.layers.9.attn.out_proj.bias 2048\n",
      "transformer_gaia.layers.9.norm2.weight 2048\n",
      "transformer_gaia.layers.9.norm2.bias 2048\n",
      "transformer_gaia.layers.9.ffn.0.weight 16777216\n",
      "transformer_gaia.layers.9.ffn.0.bias 8192\n",
      "transformer_gaia.layers.9.ffn.3.weight 16777216\n",
      "transformer_gaia.layers.9.ffn.3.bias 2048\n",
      "cross_attn_block_spectra.norm.weight 2048\n",
      "cross_attn_block_spectra.norm.bias 2048\n",
      "cross_attn_block_spectra.attention.in_proj_weight 12582912\n",
      "cross_attn_block_spectra.attention.in_proj_bias 6144\n",
      "cross_attn_block_spectra.attention.out_proj.weight 4194304\n",
      "cross_attn_block_spectra.attention.out_proj.bias 2048\n",
      "cross_attn_block_gaia.norm.weight 2048\n",
      "cross_attn_block_gaia.norm.bias 2048\n",
      "cross_attn_block_gaia.attention.in_proj_weight 12582912\n",
      "cross_attn_block_gaia.attention.in_proj_bias 6144\n",
      "cross_attn_block_gaia.attention.out_proj.weight 4194304\n",
      "cross_attn_block_gaia.attention.out_proj.bias 2048\n",
      "classifier.0.weight 4096\n",
      "classifier.0.bias 4096\n",
      "classifier.2.weight 225280\n",
      "classifier.2.bias 55\n",
      "Total number of parameters: 1041117239\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td></td></tr><tr><td>hamming_loss</td><td></td></tr><tr><td>lr</td><td></td></tr><tr><td>macro_f1</td><td></td></tr><tr><td>macro_precision</td><td></td></tr><tr><td>macro_recall</td><td></td></tr><tr><td>micro_f1</td><td></td></tr><tr><td>micro_precision</td><td></td></tr><tr><td>micro_recall</td><td></td></tr><tr><td>test_acc</td><td></td></tr><tr><td>test_loss</td><td></td></tr><tr><td>train_acc</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>val_acc</td><td></td></tr><tr><td>val_loss</td><td></td></tr><tr><td>weighted_f1</td><td></td></tr><tr><td>weighted_precision</td><td></td></tr><tr><td>weighted_recall</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>640</td></tr><tr><td>hamming_loss</td><td>0.02656</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>macro_f1</td><td>0.41904</td></tr><tr><td>macro_precision</td><td>0.67243</td></tr><tr><td>macro_recall</td><td>0.39741</td></tr><tr><td>micro_f1</td><td>0.57306</td></tr><tr><td>micro_precision</td><td>0.70112</td></tr><tr><td>micro_recall</td><td>0.48456</td></tr><tr><td>test_acc</td><td>0.97344</td></tr><tr><td>test_loss</td><td>0.06958</td></tr><tr><td>train_acc</td><td>0.98803</td></tr><tr><td>train_loss</td><td>0.01617</td></tr><tr><td>val_acc</td><td>0.97851</td></tr><tr><td>val_loss</td><td>0.04676</td></tr><tr><td>weighted_f1</td><td>0.54924</td></tr><tr><td>weighted_precision</td><td>0.7294</td></tr><tr><td>weighted_recall</td><td>0.48456</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dark-gorge-13</strong> at: <a href='https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_transformer/runs/dbhq8sn4' target=\"_blank\">https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_transformer/runs/dbhq8sn4</a><br> View project at: <a href='https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_transformer' target=\"_blank\">https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_transformer</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250408_162919-dbhq8sn4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Example config for transformer-based model\n",
    "    d_model_spectra = 2048\n",
    "    d_model_gaia = 2048\n",
    "    num_classes = 55\n",
    "    input_dim_spectra = 3647\n",
    "    input_dim_gaia = 18\n",
    "    n_layers = 10  # Match the number of layers with MAMBA models for fair comparison\n",
    "    lr = 2.5e-6\n",
    "    patience = 200\n",
    "    num_epochs = 800\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(\"Device:\", device)\n",
    "    \n",
    "    # Initialize WandB\n",
    "    wandb.init(project=\"ALLSTARS_multimodal_fusion_transformer\",\n",
    "    config = {\n",
    "        \"model_type\": \"transformer\",\n",
    "        \"num_classes\": num_classes,\n",
    "        \"d_model_spectra\": d_model_spectra,\n",
    "        \"d_model_gaia\": d_model_gaia,\n",
    "        \"input_dim_spectra\": input_dim_spectra,\n",
    "        \"input_dim_gaia\": input_dim_gaia,\n",
    "        \"n_layers\": n_layers,\n",
    "        \"n_heads\": 8,  # Number of attention heads\n",
    "        \"use_cross_attention\": True,\n",
    "        \"lr\": lr,\n",
    "        \"patience\": patience,\n",
    "        \"num_epochs\": num_epochs\n",
    "    })\n",
    "    \n",
    "    # Instantiate the transformer model\n",
    "    model_transformer = StarClassifierFusionTransformer(\n",
    "        d_model_spectra=d_model_spectra,\n",
    "        d_model_gaia=d_model_gaia,\n",
    "        num_classes=num_classes,\n",
    "        input_dim_spectra=input_dim_spectra,\n",
    "        input_dim_gaia=input_dim_gaia,\n",
    "        n_layers=n_layers,\n",
    "        n_heads=8,\n",
    "        use_cross_attention=True,  # set to False to compare with late fusion\n",
    "        n_cross_attn_heads=8,\n",
    "        dropout=0.1\n",
    "    )\n",
    "    model_transformer.to(device)\n",
    "    \n",
    "    # Print model statistics\n",
    "    print(f\"Model size: {sum(p.numel() for p in model_transformer.parameters()) / 1e9:.3f} GB\")\n",
    "    \n",
    "    # Compute parameter size\n",
    "    param_size = 0\n",
    "    for param in model_transformer.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    \n",
    "    # Compute buffer size\n",
    "    buffer_size = 0\n",
    "    for buffer in model_transformer.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "    \n",
    "    # Total size in MB\n",
    "    total_size_mb = (param_size + buffer_size) / (1024 ** 2)\n",
    "    print(f\"Model size: {total_size_mb:.3f} MB\")\n",
    "    \n",
    "    # Print model architecture\n",
    "    print(model_transformer)\n",
    "    \n",
    "    # Print number of parameters per layer\n",
    "    for name, param in model_transformer.named_parameters():\n",
    "        print(name, param.numel())\n",
    "    print(\"Total number of parameters:\", sum(p.numel() for p in model_transformer.parameters() if p.requires_grad))\n",
    "    \n",
    "    # Train the transformer model\n",
    "    trained_transformer_model = train_model_fusion(\n",
    "        model=model_transformer,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader,\n",
    "        num_epochs=num_epochs,\n",
    "        lr=lr,\n",
    "        max_patience=patience,\n",
    "        device=device\n",
    "    )\n",
    "    wandb.finish()\n",
    "    \n",
    "    # Save the model\n",
    "    torch.save(trained_transformer_model.state_dict(), \"Models/model_fusion_transformer_1token_v2.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer with 19 + 18 Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjoaocsgalmeida\u001b[0m (\u001b[33mjoaocsgalmeida-university-of-southampton\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/joao/Documents/Star-Classifier/wandb/run-20250405_134613-i4au3sfd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_transformer_tokenized/runs/i4au3sfd' target=\"_blank\">bajoran-seven-4</a></strong> to <a href='https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_transformer_tokenized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_transformer_tokenized' target=\"_blank\">https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_transformer_tokenized</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_transformer_tokenized/runs/i4au3sfd' target=\"_blank\">https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_transformer_tokenized/runs/i4au3sfd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of spectra tokens: 19\n",
      "Number of gaia tokens: 18\n",
      "Model size: 4052.549 MB\n",
      "Total parameters: 1041377335\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td></td></tr><tr><td>hamming_loss</td><td></td></tr><tr><td>lr</td><td></td></tr><tr><td>macro_f1</td><td></td></tr><tr><td>macro_precision</td><td></td></tr><tr><td>macro_recall</td><td></td></tr><tr><td>micro_f1</td><td></td></tr><tr><td>micro_precision</td><td></td></tr><tr><td>micro_recall</td><td></td></tr><tr><td>test_acc</td><td></td></tr><tr><td>test_loss</td><td></td></tr><tr><td>train_acc</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>val_acc</td><td></td></tr><tr><td>val_loss</td><td></td></tr><tr><td>weighted_f1</td><td></td></tr><tr><td>weighted_precision</td><td></td></tr><tr><td>weighted_recall</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>659</td></tr><tr><td>hamming_loss</td><td>0.02185</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>macro_f1</td><td>0.44726</td></tr><tr><td>macro_precision</td><td>0.6264</td></tr><tr><td>macro_recall</td><td>0.42024</td></tr><tr><td>micro_f1</td><td>0.63076</td></tr><tr><td>micro_precision</td><td>0.77981</td></tr><tr><td>micro_recall</td><td>0.52955</td></tr><tr><td>test_acc</td><td>0.97823</td></tr><tr><td>test_loss</td><td>0.05599</td></tr><tr><td>train_acc</td><td>0.98275</td></tr><tr><td>train_loss</td><td>0.01919</td></tr><tr><td>val_acc</td><td>0.97719</td></tr><tr><td>val_loss</td><td>0.04196</td></tr><tr><td>weighted_f1</td><td>0.62373</td></tr><tr><td>weighted_precision</td><td>0.81097</td></tr><tr><td>weighted_recall</td><td>0.52955</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">bajoran-seven-4</strong> at: <a href='https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_transformer_tokenized/runs/i4au3sfd' target=\"_blank\">https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_transformer_tokenized/runs/i4au3sfd</a><br> View project at: <a href='https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_transformer_tokenized' target=\"_blank\">https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_transformer_tokenized</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250405_134613-i4au3sfd/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Configuration for tokenized transformer model\n",
    "    d_model_spectra = 2048\n",
    "    d_model_gaia = 2048\n",
    "    num_classes = 55\n",
    "    input_dim_spectra = 3647\n",
    "    input_dim_gaia = 18\n",
    "    \n",
    "    # Token dimensions - these control the sequence length\n",
    "    token_dim_spectra = 192  # Will create ~19 tokens for spectra (3647/192)\n",
    "    token_dim_gaia = 1      # Will create 18 tokens for gaia (18/1)\n",
    "    \n",
    "    n_layers = 10  # Reduced number of layers since we have more tokens\n",
    "    n_heads = 8\n",
    "    \n",
    "    lr = 1e-6\n",
    "    patience = 200\n",
    "    num_epochs = 800\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(\"Device:\", device)\n",
    "    \n",
    "    # Initialize WandB\n",
    "    wandb.init(project=\"ALLSTARS_multimodal_fusion_transformer_tokenized\")\n",
    "    config = {\n",
    "        \"model_type\": \"transformer_tokenized\",\n",
    "        \"num_classes\": num_classes,\n",
    "        \"d_model_spectra\": d_model_spectra,\n",
    "        \"d_model_gaia\": d_model_gaia,\n",
    "        \"input_dim_spectra\": input_dim_spectra,\n",
    "        \"input_dim_gaia\": input_dim_gaia,\n",
    "        \"token_dim_spectra\": token_dim_spectra,\n",
    "        \"token_dim_gaia\": token_dim_gaia,\n",
    "        \"num_tokens_spectra\": (input_dim_spectra + token_dim_spectra - 1) // token_dim_spectra,\n",
    "        \"num_tokens_gaia\": (input_dim_gaia + token_dim_gaia - 1) // token_dim_gaia,\n",
    "        \"n_layers\": n_layers,\n",
    "        \"n_heads\": n_heads,\n",
    "        \"use_cross_attention\": True,\n",
    "        \"lr\": lr,\n",
    "        \"patience\": patience,\n",
    "        \"num_epochs\": num_epochs\n",
    "    }\n",
    "    wandb.config.update(config)\n",
    "    \n",
    "    # Instantiate the tokenized transformer model\n",
    "    model_transformer = StarClassifierFusionTransformer(\n",
    "        d_model_spectra=d_model_spectra,\n",
    "        d_model_gaia=d_model_gaia,\n",
    "        num_classes=num_classes,\n",
    "        input_dim_spectra=input_dim_spectra,\n",
    "        input_dim_gaia=input_dim_gaia,\n",
    "        token_dim_spectra=token_dim_spectra,\n",
    "        token_dim_gaia=token_dim_gaia,\n",
    "        n_layers=n_layers,\n",
    "        n_heads=n_heads,\n",
    "        use_cross_attention=True,\n",
    "        n_cross_attn_heads=8,\n",
    "        dropout=0.1\n",
    "    )\n",
    "    model_transformer.to(device)\n",
    "    \n",
    "    # Print model statistics\n",
    "    print(f\"Number of spectra tokens: {(input_dim_spectra + token_dim_spectra - 1) // token_dim_spectra}\")\n",
    "    print(f\"Number of gaia tokens: {(input_dim_gaia + token_dim_gaia - 1) // token_dim_gaia}\")\n",
    "    \n",
    "    # Compute parameter size\n",
    "    param_size = 0\n",
    "    for param in model_transformer.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    \n",
    "    # Compute buffer size\n",
    "    buffer_size = 0\n",
    "    for buffer in model_transformer.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "    print(model_fusion)\n",
    "    # print number of parameters per layer\n",
    "    for name, param in model_fusion.named_parameters():\n",
    "        print(name, param.numel())\n",
    "    \n",
    "    # Total size in MB\n",
    "    total_size_mb = (param_size + buffer_size) / (1024 ** 2)\n",
    "    print(f\"Model size: {total_size_mb:.3f} MB\")\n",
    "    print(f\"Total parameters: {sum(p.numel() for p in model_transformer.parameters() if p.requires_grad)}\")\n",
    "    \n",
    "    # Train the transformer model\n",
    "    trained_transformer_model = train_model_fusion(\n",
    "        model=model_transformer,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader,\n",
    "        num_epochs=num_epochs,\n",
    "        lr=lr,\n",
    "        max_patience=patience,\n",
    "        device=device\n",
    "    )\n",
    "    wandb.finish()\n",
    "    \n",
    "    # Save the model\n",
    "    torch.save(trained_transformer_model.state_dict(), \"Models/model_fusion_transformer_tokenized.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer with 522 + 18 Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjoaocsgalmeida\u001b[0m (\u001b[33mjoaocsgalmeida-university-of-southampton\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/joao/Documents/Star-Classifier/wandb/run-20250407_222139-pjsl3eea</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_transformer_522/runs/pjsl3eea' target=\"_blank\">fancy-wood-10</a></strong> to <a href='https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_transformer_522' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_transformer_522' target=\"_blank\">https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_transformer_522</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_transformer_522/runs/pjsl3eea' target=\"_blank\">https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_transformer_522/runs/pjsl3eea</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of spectra tokens: 521\n",
      "Number of gaia tokens: 18\n",
      "StarClassifierFusionTransformer(\n",
      "  (tokenizer_spectra): FeatureTokenizer(\n",
      "    (token_embed): Linear(in_features=7, out_features=1536, bias=True)\n",
      "  )\n",
      "  (tokenizer_gaia): FeatureTokenizer(\n",
      "    (token_embed): Linear(in_features=1, out_features=1536, bias=True)\n",
      "  )\n",
      "  (transformer_spectra): TransformerFeatureExtractor(\n",
      "    (layers): ModuleList(\n",
      "      (0-9): 10 x TransformerBlock(\n",
      "        (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): RotarySelfAttention(\n",
      "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (rope): RotaryEmbedding()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "        (ffn): Sequential(\n",
      "          (0): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.1, inplace=False)\n",
      "          (3): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "          (4): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (transformer_gaia): TransformerFeatureExtractor(\n",
      "    (layers): ModuleList(\n",
      "      (0-9): 10 x TransformerBlock(\n",
      "        (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): RotarySelfAttention(\n",
      "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (k_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (v_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (out_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (rope): RotaryEmbedding()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "        (ffn): Sequential(\n",
      "          (0): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.1, inplace=False)\n",
      "          (3): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "          (4): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (cross_attn_block_spectra): CrossAttentionBlock(\n",
      "    (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "    (attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=1536, out_features=1536, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (cross_attn_block_gaia): CrossAttentionBlock(\n",
      "    (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "    (attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=1536, out_features=1536, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): Dropout(p=0.1, inplace=False)\n",
      "    (2): Linear(in_features=3072, out_features=55, bias=True)\n",
      "  )\n",
      ")\n",
      "tokenizer_spectra.token_embed.weight 10752\n",
      "tokenizer_spectra.token_embed.bias 1536\n",
      "tokenizer_gaia.token_embed.weight 1536\n",
      "tokenizer_gaia.token_embed.bias 1536\n",
      "transformer_spectra.layers.0.norm1.weight 1536\n",
      "transformer_spectra.layers.0.norm1.bias 1536\n",
      "transformer_spectra.layers.0.attn.q_proj.weight 2359296\n",
      "transformer_spectra.layers.0.attn.q_proj.bias 1536\n",
      "transformer_spectra.layers.0.attn.k_proj.weight 2359296\n",
      "transformer_spectra.layers.0.attn.k_proj.bias 1536\n",
      "transformer_spectra.layers.0.attn.v_proj.weight 2359296\n",
      "transformer_spectra.layers.0.attn.v_proj.bias 1536\n",
      "transformer_spectra.layers.0.attn.out_proj.weight 2359296\n",
      "transformer_spectra.layers.0.attn.out_proj.bias 1536\n",
      "transformer_spectra.layers.0.norm2.weight 1536\n",
      "transformer_spectra.layers.0.norm2.bias 1536\n",
      "transformer_spectra.layers.0.ffn.0.weight 9437184\n",
      "transformer_spectra.layers.0.ffn.0.bias 6144\n",
      "transformer_spectra.layers.0.ffn.3.weight 9437184\n",
      "transformer_spectra.layers.0.ffn.3.bias 1536\n",
      "transformer_spectra.layers.1.norm1.weight 1536\n",
      "transformer_spectra.layers.1.norm1.bias 1536\n",
      "transformer_spectra.layers.1.attn.q_proj.weight 2359296\n",
      "transformer_spectra.layers.1.attn.q_proj.bias 1536\n",
      "transformer_spectra.layers.1.attn.k_proj.weight 2359296\n",
      "transformer_spectra.layers.1.attn.k_proj.bias 1536\n",
      "transformer_spectra.layers.1.attn.v_proj.weight 2359296\n",
      "transformer_spectra.layers.1.attn.v_proj.bias 1536\n",
      "transformer_spectra.layers.1.attn.out_proj.weight 2359296\n",
      "transformer_spectra.layers.1.attn.out_proj.bias 1536\n",
      "transformer_spectra.layers.1.norm2.weight 1536\n",
      "transformer_spectra.layers.1.norm2.bias 1536\n",
      "transformer_spectra.layers.1.ffn.0.weight 9437184\n",
      "transformer_spectra.layers.1.ffn.0.bias 6144\n",
      "transformer_spectra.layers.1.ffn.3.weight 9437184\n",
      "transformer_spectra.layers.1.ffn.3.bias 1536\n",
      "transformer_spectra.layers.2.norm1.weight 1536\n",
      "transformer_spectra.layers.2.norm1.bias 1536\n",
      "transformer_spectra.layers.2.attn.q_proj.weight 2359296\n",
      "transformer_spectra.layers.2.attn.q_proj.bias 1536\n",
      "transformer_spectra.layers.2.attn.k_proj.weight 2359296\n",
      "transformer_spectra.layers.2.attn.k_proj.bias 1536\n",
      "transformer_spectra.layers.2.attn.v_proj.weight 2359296\n",
      "transformer_spectra.layers.2.attn.v_proj.bias 1536\n",
      "transformer_spectra.layers.2.attn.out_proj.weight 2359296\n",
      "transformer_spectra.layers.2.attn.out_proj.bias 1536\n",
      "transformer_spectra.layers.2.norm2.weight 1536\n",
      "transformer_spectra.layers.2.norm2.bias 1536\n",
      "transformer_spectra.layers.2.ffn.0.weight 9437184\n",
      "transformer_spectra.layers.2.ffn.0.bias 6144\n",
      "transformer_spectra.layers.2.ffn.3.weight 9437184\n",
      "transformer_spectra.layers.2.ffn.3.bias 1536\n",
      "transformer_spectra.layers.3.norm1.weight 1536\n",
      "transformer_spectra.layers.3.norm1.bias 1536\n",
      "transformer_spectra.layers.3.attn.q_proj.weight 2359296\n",
      "transformer_spectra.layers.3.attn.q_proj.bias 1536\n",
      "transformer_spectra.layers.3.attn.k_proj.weight 2359296\n",
      "transformer_spectra.layers.3.attn.k_proj.bias 1536\n",
      "transformer_spectra.layers.3.attn.v_proj.weight 2359296\n",
      "transformer_spectra.layers.3.attn.v_proj.bias 1536\n",
      "transformer_spectra.layers.3.attn.out_proj.weight 2359296\n",
      "transformer_spectra.layers.3.attn.out_proj.bias 1536\n",
      "transformer_spectra.layers.3.norm2.weight 1536\n",
      "transformer_spectra.layers.3.norm2.bias 1536\n",
      "transformer_spectra.layers.3.ffn.0.weight 9437184\n",
      "transformer_spectra.layers.3.ffn.0.bias 6144\n",
      "transformer_spectra.layers.3.ffn.3.weight 9437184\n",
      "transformer_spectra.layers.3.ffn.3.bias 1536\n",
      "transformer_spectra.layers.4.norm1.weight 1536\n",
      "transformer_spectra.layers.4.norm1.bias 1536\n",
      "transformer_spectra.layers.4.attn.q_proj.weight 2359296\n",
      "transformer_spectra.layers.4.attn.q_proj.bias 1536\n",
      "transformer_spectra.layers.4.attn.k_proj.weight 2359296\n",
      "transformer_spectra.layers.4.attn.k_proj.bias 1536\n",
      "transformer_spectra.layers.4.attn.v_proj.weight 2359296\n",
      "transformer_spectra.layers.4.attn.v_proj.bias 1536\n",
      "transformer_spectra.layers.4.attn.out_proj.weight 2359296\n",
      "transformer_spectra.layers.4.attn.out_proj.bias 1536\n",
      "transformer_spectra.layers.4.norm2.weight 1536\n",
      "transformer_spectra.layers.4.norm2.bias 1536\n",
      "transformer_spectra.layers.4.ffn.0.weight 9437184\n",
      "transformer_spectra.layers.4.ffn.0.bias 6144\n",
      "transformer_spectra.layers.4.ffn.3.weight 9437184\n",
      "transformer_spectra.layers.4.ffn.3.bias 1536\n",
      "transformer_spectra.layers.5.norm1.weight 1536\n",
      "transformer_spectra.layers.5.norm1.bias 1536\n",
      "transformer_spectra.layers.5.attn.q_proj.weight 2359296\n",
      "transformer_spectra.layers.5.attn.q_proj.bias 1536\n",
      "transformer_spectra.layers.5.attn.k_proj.weight 2359296\n",
      "transformer_spectra.layers.5.attn.k_proj.bias 1536\n",
      "transformer_spectra.layers.5.attn.v_proj.weight 2359296\n",
      "transformer_spectra.layers.5.attn.v_proj.bias 1536\n",
      "transformer_spectra.layers.5.attn.out_proj.weight 2359296\n",
      "transformer_spectra.layers.5.attn.out_proj.bias 1536\n",
      "transformer_spectra.layers.5.norm2.weight 1536\n",
      "transformer_spectra.layers.5.norm2.bias 1536\n",
      "transformer_spectra.layers.5.ffn.0.weight 9437184\n",
      "transformer_spectra.layers.5.ffn.0.bias 6144\n",
      "transformer_spectra.layers.5.ffn.3.weight 9437184\n",
      "transformer_spectra.layers.5.ffn.3.bias 1536\n",
      "transformer_spectra.layers.6.norm1.weight 1536\n",
      "transformer_spectra.layers.6.norm1.bias 1536\n",
      "transformer_spectra.layers.6.attn.q_proj.weight 2359296\n",
      "transformer_spectra.layers.6.attn.q_proj.bias 1536\n",
      "transformer_spectra.layers.6.attn.k_proj.weight 2359296\n",
      "transformer_spectra.layers.6.attn.k_proj.bias 1536\n",
      "transformer_spectra.layers.6.attn.v_proj.weight 2359296\n",
      "transformer_spectra.layers.6.attn.v_proj.bias 1536\n",
      "transformer_spectra.layers.6.attn.out_proj.weight 2359296\n",
      "transformer_spectra.layers.6.attn.out_proj.bias 1536\n",
      "transformer_spectra.layers.6.norm2.weight 1536\n",
      "transformer_spectra.layers.6.norm2.bias 1536\n",
      "transformer_spectra.layers.6.ffn.0.weight 9437184\n",
      "transformer_spectra.layers.6.ffn.0.bias 6144\n",
      "transformer_spectra.layers.6.ffn.3.weight 9437184\n",
      "transformer_spectra.layers.6.ffn.3.bias 1536\n",
      "transformer_spectra.layers.7.norm1.weight 1536\n",
      "transformer_spectra.layers.7.norm1.bias 1536\n",
      "transformer_spectra.layers.7.attn.q_proj.weight 2359296\n",
      "transformer_spectra.layers.7.attn.q_proj.bias 1536\n",
      "transformer_spectra.layers.7.attn.k_proj.weight 2359296\n",
      "transformer_spectra.layers.7.attn.k_proj.bias 1536\n",
      "transformer_spectra.layers.7.attn.v_proj.weight 2359296\n",
      "transformer_spectra.layers.7.attn.v_proj.bias 1536\n",
      "transformer_spectra.layers.7.attn.out_proj.weight 2359296\n",
      "transformer_spectra.layers.7.attn.out_proj.bias 1536\n",
      "transformer_spectra.layers.7.norm2.weight 1536\n",
      "transformer_spectra.layers.7.norm2.bias 1536\n",
      "transformer_spectra.layers.7.ffn.0.weight 9437184\n",
      "transformer_spectra.layers.7.ffn.0.bias 6144\n",
      "transformer_spectra.layers.7.ffn.3.weight 9437184\n",
      "transformer_spectra.layers.7.ffn.3.bias 1536\n",
      "transformer_spectra.layers.8.norm1.weight 1536\n",
      "transformer_spectra.layers.8.norm1.bias 1536\n",
      "transformer_spectra.layers.8.attn.q_proj.weight 2359296\n",
      "transformer_spectra.layers.8.attn.q_proj.bias 1536\n",
      "transformer_spectra.layers.8.attn.k_proj.weight 2359296\n",
      "transformer_spectra.layers.8.attn.k_proj.bias 1536\n",
      "transformer_spectra.layers.8.attn.v_proj.weight 2359296\n",
      "transformer_spectra.layers.8.attn.v_proj.bias 1536\n",
      "transformer_spectra.layers.8.attn.out_proj.weight 2359296\n",
      "transformer_spectra.layers.8.attn.out_proj.bias 1536\n",
      "transformer_spectra.layers.8.norm2.weight 1536\n",
      "transformer_spectra.layers.8.norm2.bias 1536\n",
      "transformer_spectra.layers.8.ffn.0.weight 9437184\n",
      "transformer_spectra.layers.8.ffn.0.bias 6144\n",
      "transformer_spectra.layers.8.ffn.3.weight 9437184\n",
      "transformer_spectra.layers.8.ffn.3.bias 1536\n",
      "transformer_spectra.layers.9.norm1.weight 1536\n",
      "transformer_spectra.layers.9.norm1.bias 1536\n",
      "transformer_spectra.layers.9.attn.q_proj.weight 2359296\n",
      "transformer_spectra.layers.9.attn.q_proj.bias 1536\n",
      "transformer_spectra.layers.9.attn.k_proj.weight 2359296\n",
      "transformer_spectra.layers.9.attn.k_proj.bias 1536\n",
      "transformer_spectra.layers.9.attn.v_proj.weight 2359296\n",
      "transformer_spectra.layers.9.attn.v_proj.bias 1536\n",
      "transformer_spectra.layers.9.attn.out_proj.weight 2359296\n",
      "transformer_spectra.layers.9.attn.out_proj.bias 1536\n",
      "transformer_spectra.layers.9.norm2.weight 1536\n",
      "transformer_spectra.layers.9.norm2.bias 1536\n",
      "transformer_spectra.layers.9.ffn.0.weight 9437184\n",
      "transformer_spectra.layers.9.ffn.0.bias 6144\n",
      "transformer_spectra.layers.9.ffn.3.weight 9437184\n",
      "transformer_spectra.layers.9.ffn.3.bias 1536\n",
      "transformer_gaia.layers.0.norm1.weight 1536\n",
      "transformer_gaia.layers.0.norm1.bias 1536\n",
      "transformer_gaia.layers.0.attn.q_proj.weight 2359296\n",
      "transformer_gaia.layers.0.attn.q_proj.bias 1536\n",
      "transformer_gaia.layers.0.attn.k_proj.weight 2359296\n",
      "transformer_gaia.layers.0.attn.k_proj.bias 1536\n",
      "transformer_gaia.layers.0.attn.v_proj.weight 2359296\n",
      "transformer_gaia.layers.0.attn.v_proj.bias 1536\n",
      "transformer_gaia.layers.0.attn.out_proj.weight 2359296\n",
      "transformer_gaia.layers.0.attn.out_proj.bias 1536\n",
      "transformer_gaia.layers.0.norm2.weight 1536\n",
      "transformer_gaia.layers.0.norm2.bias 1536\n",
      "transformer_gaia.layers.0.ffn.0.weight 9437184\n",
      "transformer_gaia.layers.0.ffn.0.bias 6144\n",
      "transformer_gaia.layers.0.ffn.3.weight 9437184\n",
      "transformer_gaia.layers.0.ffn.3.bias 1536\n",
      "transformer_gaia.layers.1.norm1.weight 1536\n",
      "transformer_gaia.layers.1.norm1.bias 1536\n",
      "transformer_gaia.layers.1.attn.q_proj.weight 2359296\n",
      "transformer_gaia.layers.1.attn.q_proj.bias 1536\n",
      "transformer_gaia.layers.1.attn.k_proj.weight 2359296\n",
      "transformer_gaia.layers.1.attn.k_proj.bias 1536\n",
      "transformer_gaia.layers.1.attn.v_proj.weight 2359296\n",
      "transformer_gaia.layers.1.attn.v_proj.bias 1536\n",
      "transformer_gaia.layers.1.attn.out_proj.weight 2359296\n",
      "transformer_gaia.layers.1.attn.out_proj.bias 1536\n",
      "transformer_gaia.layers.1.norm2.weight 1536\n",
      "transformer_gaia.layers.1.norm2.bias 1536\n",
      "transformer_gaia.layers.1.ffn.0.weight 9437184\n",
      "transformer_gaia.layers.1.ffn.0.bias 6144\n",
      "transformer_gaia.layers.1.ffn.3.weight 9437184\n",
      "transformer_gaia.layers.1.ffn.3.bias 1536\n",
      "transformer_gaia.layers.2.norm1.weight 1536\n",
      "transformer_gaia.layers.2.norm1.bias 1536\n",
      "transformer_gaia.layers.2.attn.q_proj.weight 2359296\n",
      "transformer_gaia.layers.2.attn.q_proj.bias 1536\n",
      "transformer_gaia.layers.2.attn.k_proj.weight 2359296\n",
      "transformer_gaia.layers.2.attn.k_proj.bias 1536\n",
      "transformer_gaia.layers.2.attn.v_proj.weight 2359296\n",
      "transformer_gaia.layers.2.attn.v_proj.bias 1536\n",
      "transformer_gaia.layers.2.attn.out_proj.weight 2359296\n",
      "transformer_gaia.layers.2.attn.out_proj.bias 1536\n",
      "transformer_gaia.layers.2.norm2.weight 1536\n",
      "transformer_gaia.layers.2.norm2.bias 1536\n",
      "transformer_gaia.layers.2.ffn.0.weight 9437184\n",
      "transformer_gaia.layers.2.ffn.0.bias 6144\n",
      "transformer_gaia.layers.2.ffn.3.weight 9437184\n",
      "transformer_gaia.layers.2.ffn.3.bias 1536\n",
      "transformer_gaia.layers.3.norm1.weight 1536\n",
      "transformer_gaia.layers.3.norm1.bias 1536\n",
      "transformer_gaia.layers.3.attn.q_proj.weight 2359296\n",
      "transformer_gaia.layers.3.attn.q_proj.bias 1536\n",
      "transformer_gaia.layers.3.attn.k_proj.weight 2359296\n",
      "transformer_gaia.layers.3.attn.k_proj.bias 1536\n",
      "transformer_gaia.layers.3.attn.v_proj.weight 2359296\n",
      "transformer_gaia.layers.3.attn.v_proj.bias 1536\n",
      "transformer_gaia.layers.3.attn.out_proj.weight 2359296\n",
      "transformer_gaia.layers.3.attn.out_proj.bias 1536\n",
      "transformer_gaia.layers.3.norm2.weight 1536\n",
      "transformer_gaia.layers.3.norm2.bias 1536\n",
      "transformer_gaia.layers.3.ffn.0.weight 9437184\n",
      "transformer_gaia.layers.3.ffn.0.bias 6144\n",
      "transformer_gaia.layers.3.ffn.3.weight 9437184\n",
      "transformer_gaia.layers.3.ffn.3.bias 1536\n",
      "transformer_gaia.layers.4.norm1.weight 1536\n",
      "transformer_gaia.layers.4.norm1.bias 1536\n",
      "transformer_gaia.layers.4.attn.q_proj.weight 2359296\n",
      "transformer_gaia.layers.4.attn.q_proj.bias 1536\n",
      "transformer_gaia.layers.4.attn.k_proj.weight 2359296\n",
      "transformer_gaia.layers.4.attn.k_proj.bias 1536\n",
      "transformer_gaia.layers.4.attn.v_proj.weight 2359296\n",
      "transformer_gaia.layers.4.attn.v_proj.bias 1536\n",
      "transformer_gaia.layers.4.attn.out_proj.weight 2359296\n",
      "transformer_gaia.layers.4.attn.out_proj.bias 1536\n",
      "transformer_gaia.layers.4.norm2.weight 1536\n",
      "transformer_gaia.layers.4.norm2.bias 1536\n",
      "transformer_gaia.layers.4.ffn.0.weight 9437184\n",
      "transformer_gaia.layers.4.ffn.0.bias 6144\n",
      "transformer_gaia.layers.4.ffn.3.weight 9437184\n",
      "transformer_gaia.layers.4.ffn.3.bias 1536\n",
      "transformer_gaia.layers.5.norm1.weight 1536\n",
      "transformer_gaia.layers.5.norm1.bias 1536\n",
      "transformer_gaia.layers.5.attn.q_proj.weight 2359296\n",
      "transformer_gaia.layers.5.attn.q_proj.bias 1536\n",
      "transformer_gaia.layers.5.attn.k_proj.weight 2359296\n",
      "transformer_gaia.layers.5.attn.k_proj.bias 1536\n",
      "transformer_gaia.layers.5.attn.v_proj.weight 2359296\n",
      "transformer_gaia.layers.5.attn.v_proj.bias 1536\n",
      "transformer_gaia.layers.5.attn.out_proj.weight 2359296\n",
      "transformer_gaia.layers.5.attn.out_proj.bias 1536\n",
      "transformer_gaia.layers.5.norm2.weight 1536\n",
      "transformer_gaia.layers.5.norm2.bias 1536\n",
      "transformer_gaia.layers.5.ffn.0.weight 9437184\n",
      "transformer_gaia.layers.5.ffn.0.bias 6144\n",
      "transformer_gaia.layers.5.ffn.3.weight 9437184\n",
      "transformer_gaia.layers.5.ffn.3.bias 1536\n",
      "transformer_gaia.layers.6.norm1.weight 1536\n",
      "transformer_gaia.layers.6.norm1.bias 1536\n",
      "transformer_gaia.layers.6.attn.q_proj.weight 2359296\n",
      "transformer_gaia.layers.6.attn.q_proj.bias 1536\n",
      "transformer_gaia.layers.6.attn.k_proj.weight 2359296\n",
      "transformer_gaia.layers.6.attn.k_proj.bias 1536\n",
      "transformer_gaia.layers.6.attn.v_proj.weight 2359296\n",
      "transformer_gaia.layers.6.attn.v_proj.bias 1536\n",
      "transformer_gaia.layers.6.attn.out_proj.weight 2359296\n",
      "transformer_gaia.layers.6.attn.out_proj.bias 1536\n",
      "transformer_gaia.layers.6.norm2.weight 1536\n",
      "transformer_gaia.layers.6.norm2.bias 1536\n",
      "transformer_gaia.layers.6.ffn.0.weight 9437184\n",
      "transformer_gaia.layers.6.ffn.0.bias 6144\n",
      "transformer_gaia.layers.6.ffn.3.weight 9437184\n",
      "transformer_gaia.layers.6.ffn.3.bias 1536\n",
      "transformer_gaia.layers.7.norm1.weight 1536\n",
      "transformer_gaia.layers.7.norm1.bias 1536\n",
      "transformer_gaia.layers.7.attn.q_proj.weight 2359296\n",
      "transformer_gaia.layers.7.attn.q_proj.bias 1536\n",
      "transformer_gaia.layers.7.attn.k_proj.weight 2359296\n",
      "transformer_gaia.layers.7.attn.k_proj.bias 1536\n",
      "transformer_gaia.layers.7.attn.v_proj.weight 2359296\n",
      "transformer_gaia.layers.7.attn.v_proj.bias 1536\n",
      "transformer_gaia.layers.7.attn.out_proj.weight 2359296\n",
      "transformer_gaia.layers.7.attn.out_proj.bias 1536\n",
      "transformer_gaia.layers.7.norm2.weight 1536\n",
      "transformer_gaia.layers.7.norm2.bias 1536\n",
      "transformer_gaia.layers.7.ffn.0.weight 9437184\n",
      "transformer_gaia.layers.7.ffn.0.bias 6144\n",
      "transformer_gaia.layers.7.ffn.3.weight 9437184\n",
      "transformer_gaia.layers.7.ffn.3.bias 1536\n",
      "transformer_gaia.layers.8.norm1.weight 1536\n",
      "transformer_gaia.layers.8.norm1.bias 1536\n",
      "transformer_gaia.layers.8.attn.q_proj.weight 2359296\n",
      "transformer_gaia.layers.8.attn.q_proj.bias 1536\n",
      "transformer_gaia.layers.8.attn.k_proj.weight 2359296\n",
      "transformer_gaia.layers.8.attn.k_proj.bias 1536\n",
      "transformer_gaia.layers.8.attn.v_proj.weight 2359296\n",
      "transformer_gaia.layers.8.attn.v_proj.bias 1536\n",
      "transformer_gaia.layers.8.attn.out_proj.weight 2359296\n",
      "transformer_gaia.layers.8.attn.out_proj.bias 1536\n",
      "transformer_gaia.layers.8.norm2.weight 1536\n",
      "transformer_gaia.layers.8.norm2.bias 1536\n",
      "transformer_gaia.layers.8.ffn.0.weight 9437184\n",
      "transformer_gaia.layers.8.ffn.0.bias 6144\n",
      "transformer_gaia.layers.8.ffn.3.weight 9437184\n",
      "transformer_gaia.layers.8.ffn.3.bias 1536\n",
      "transformer_gaia.layers.9.norm1.weight 1536\n",
      "transformer_gaia.layers.9.norm1.bias 1536\n",
      "transformer_gaia.layers.9.attn.q_proj.weight 2359296\n",
      "transformer_gaia.layers.9.attn.q_proj.bias 1536\n",
      "transformer_gaia.layers.9.attn.k_proj.weight 2359296\n",
      "transformer_gaia.layers.9.attn.k_proj.bias 1536\n",
      "transformer_gaia.layers.9.attn.v_proj.weight 2359296\n",
      "transformer_gaia.layers.9.attn.v_proj.bias 1536\n",
      "transformer_gaia.layers.9.attn.out_proj.weight 2359296\n",
      "transformer_gaia.layers.9.attn.out_proj.bias 1536\n",
      "transformer_gaia.layers.9.norm2.weight 1536\n",
      "transformer_gaia.layers.9.norm2.bias 1536\n",
      "transformer_gaia.layers.9.ffn.0.weight 9437184\n",
      "transformer_gaia.layers.9.ffn.0.bias 6144\n",
      "transformer_gaia.layers.9.ffn.3.weight 9437184\n",
      "transformer_gaia.layers.9.ffn.3.bias 1536\n",
      "cross_attn_block_spectra.norm.weight 1536\n",
      "cross_attn_block_spectra.norm.bias 1536\n",
      "cross_attn_block_spectra.attention.in_proj_weight 7077888\n",
      "cross_attn_block_spectra.attention.in_proj_bias 4608\n",
      "cross_attn_block_spectra.attention.out_proj.weight 2359296\n",
      "cross_attn_block_spectra.attention.out_proj.bias 1536\n",
      "cross_attn_block_gaia.norm.weight 1536\n",
      "cross_attn_block_gaia.norm.bias 1536\n",
      "cross_attn_block_gaia.attention.in_proj_weight 7077888\n",
      "cross_attn_block_gaia.attention.in_proj_bias 4608\n",
      "cross_attn_block_gaia.attention.out_proj.weight 2359296\n",
      "cross_attn_block_gaia.attention.out_proj.bias 1536\n",
      "classifier.0.weight 3072\n",
      "classifier.0.bias 3072\n",
      "classifier.2.weight 168960\n",
      "classifier.2.bias 55\n",
      "Model size: 2354.328 MB\n",
      "Total parameters: 585713719\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n",
      "Reached maximum steps (14400), skipping scheduler step\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td></td></tr><tr><td>hamming_loss</td><td></td></tr><tr><td>lr</td><td></td></tr><tr><td>macro_f1</td><td></td></tr><tr><td>macro_precision</td><td></td></tr><tr><td>macro_recall</td><td></td></tr><tr><td>micro_f1</td><td></td></tr><tr><td>micro_precision</td><td></td></tr><tr><td>micro_recall</td><td></td></tr><tr><td>test_acc</td><td></td></tr><tr><td>test_loss</td><td></td></tr><tr><td>train_acc</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>val_acc</td><td></td></tr><tr><td>val_loss</td><td></td></tr><tr><td>weighted_f1</td><td></td></tr><tr><td>weighted_precision</td><td></td></tr><tr><td>weighted_recall</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>799</td></tr><tr><td>hamming_loss</td><td>0.03313</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>macro_f1</td><td>0.27559</td></tr><tr><td>macro_precision</td><td>0.68441</td></tr><tr><td>macro_recall</td><td>0.23068</td></tr><tr><td>micro_f1</td><td>0.34986</td></tr><tr><td>micro_precision</td><td>0.63184</td></tr><tr><td>micro_recall</td><td>0.2419</td></tr><tr><td>test_acc</td><td>0.96671</td></tr><tr><td>test_loss</td><td>0.07077</td></tr><tr><td>train_acc</td><td>0.96993</td></tr><tr><td>train_loss</td><td>0.04091</td></tr><tr><td>val_acc</td><td>0.96949</td></tr><tr><td>val_loss</td><td>0.05461</td></tr><tr><td>weighted_f1</td><td>0.31844</td></tr><tr><td>weighted_precision</td><td>0.76269</td></tr><tr><td>weighted_recall</td><td>0.2419</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fancy-wood-10</strong> at: <a href='https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_transformer_522/runs/pjsl3eea' target=\"_blank\">https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_transformer_522/runs/pjsl3eea</a><br> View project at: <a href='https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_transformer_522' target=\"_blank\">https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_transformer_522</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250407_222139-pjsl3eea/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Configuration for tokenized transformer model\n",
    "    d_model_spectra = 1536\n",
    "    d_model_gaia = 1536\n",
    "    num_classes = 55\n",
    "    input_dim_spectra = 3647\n",
    "    input_dim_gaia = 18\n",
    "    \n",
    "    # Token dimensions - these control the sequence length\n",
    "    token_dim_spectra = 7 # Will create ~522 tokens for spectra (3647/7)\n",
    "    token_dim_gaia = 1      # Will create 18 tokens for gaia (18/1)\n",
    "    \n",
    "    n_layers = 10 # Reduced number of layers since we have more tokens\n",
    "    n_heads = 8\n",
    "    \n",
    "    lr = 1e-6\n",
    "    patience = 200\n",
    "    num_epochs = 800\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(\"Device:\", device)\n",
    "    \n",
    "    # Initialize WandB\n",
    "    wandb.init(project=\"ALLSTARS_multimodal_fusion_transformer_522\")\n",
    "    config = {\n",
    "        \"model_type\": \"transformer_tokenized\",\n",
    "        \"num_classes\": num_classes,\n",
    "        \"d_model_spectra\": d_model_spectra,\n",
    "        \"d_model_gaia\": d_model_gaia,\n",
    "        \"input_dim_spectra\": input_dim_spectra,\n",
    "        \"input_dim_gaia\": input_dim_gaia,\n",
    "        \"token_dim_spectra\": token_dim_spectra,\n",
    "        \"token_dim_gaia\": token_dim_gaia,\n",
    "        \"num_tokens_spectra\": (input_dim_spectra + token_dim_spectra - 1) // token_dim_spectra,\n",
    "        \"num_tokens_gaia\": (input_dim_gaia + token_dim_gaia - 1) // token_dim_gaia,\n",
    "        \"n_layers\": n_layers,\n",
    "        \"n_heads\": n_heads,\n",
    "        \"use_cross_attention\": True,\n",
    "        \"lr\": lr,\n",
    "        \"patience\": patience,\n",
    "        \"num_epochs\": num_epochs\n",
    "    }\n",
    "    wandb.config.update(config)\n",
    "    \n",
    "    # Instantiate the tokenized transformer model\n",
    "    model_transformer = StarClassifierFusionTransformer(\n",
    "        d_model_spectra=d_model_spectra,\n",
    "        d_model_gaia=d_model_gaia,\n",
    "        num_classes=num_classes,\n",
    "        input_dim_spectra=input_dim_spectra,\n",
    "        input_dim_gaia=input_dim_gaia,\n",
    "        token_dim_spectra=token_dim_spectra,\n",
    "        token_dim_gaia=token_dim_gaia,\n",
    "        n_layers=n_layers,\n",
    "        n_heads=n_heads,\n",
    "        use_cross_attention=True,\n",
    "        n_cross_attn_heads=8,\n",
    "        dropout=0.1\n",
    "    )\n",
    "    model_transformer.to(device)\n",
    "    \n",
    "    # Print model statistics\n",
    "    print(f\"Number of spectra tokens: {(input_dim_spectra + token_dim_spectra - 1) // token_dim_spectra}\")\n",
    "    print(f\"Number of gaia tokens: {(input_dim_gaia + token_dim_gaia - 1) // token_dim_gaia}\")\n",
    "    \n",
    "    # Compute parameter size\n",
    "    param_size = 0\n",
    "    for param in model_transformer.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    \n",
    "    # Compute buffer size\n",
    "    buffer_size = 0\n",
    "    for buffer in model_transformer.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "    print(model_transformer)\n",
    "    # print number of parameters per layer\n",
    "    for name, param in model_transformer.named_parameters():\n",
    "        print(name, param.numel())\n",
    "    \n",
    "    # Total size in MB\n",
    "    total_size_mb = (param_size + buffer_size) / (1024 ** 2)\n",
    "    print(f\"Model size: {total_size_mb:.3f} MB\")\n",
    "    print(f\"Total parameters: {sum(p.numel() for p in model_transformer.parameters() if p.requires_grad)}\")\n",
    "    \n",
    "    # Train the transformer model\n",
    "    trained_transformer_model = train_model_fusion(\n",
    "        model=model_transformer,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader,\n",
    "        num_epochs=num_epochs,\n",
    "        lr=lr,\n",
    "        max_patience=patience,\n",
    "        device=device\n",
    "    )\n",
    "    wandb.finish()\n",
    "    \n",
    "    # Save the model\n",
    "    torch.save(trained_transformer_model.state_dict(), \"Models/model_fusion_transformer_tokenized_many_tokens.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAMBA2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAMBAIN (MAMBA2) 522 + 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Model size: 0.62 GB\n",
      "model size: 2367.272MB\n",
      "Model size: 2367.272 MB\n",
      "StarClassifierFusionMambaTokenized(\n",
      "  (tokenizer_spectra): FeatureTokenizer(\n",
      "    (token_embed): Linear(in_features=7, out_features=1536, bias=True)\n",
      "  )\n",
      "  (tokenizer_gaia): FeatureTokenizer(\n",
      "    (token_embed): Linear(in_features=1, out_features=1536, bias=True)\n",
      "  )\n",
      "  (mamba_spectra): Sequential(\n",
      "    (0): Mamba2(\n",
      "      (in_proj): Linear(in_features=1536, out_features=6704, bias=False)\n",
      "      (conv1d): Conv1d(3584, 3584, kernel_size=(4,), stride=(1,), padding=(3,), groups=3584)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=3072, out_features=1536, bias=False)\n",
      "    )\n",
      "    (1): Mamba2(\n",
      "      (in_proj): Linear(in_features=1536, out_features=6704, bias=False)\n",
      "      (conv1d): Conv1d(3584, 3584, kernel_size=(4,), stride=(1,), padding=(3,), groups=3584)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=3072, out_features=1536, bias=False)\n",
      "    )\n",
      "    (2): Mamba2(\n",
      "      (in_proj): Linear(in_features=1536, out_features=6704, bias=False)\n",
      "      (conv1d): Conv1d(3584, 3584, kernel_size=(4,), stride=(1,), padding=(3,), groups=3584)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=3072, out_features=1536, bias=False)\n",
      "    )\n",
      "    (3): Mamba2(\n",
      "      (in_proj): Linear(in_features=1536, out_features=6704, bias=False)\n",
      "      (conv1d): Conv1d(3584, 3584, kernel_size=(4,), stride=(1,), padding=(3,), groups=3584)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=3072, out_features=1536, bias=False)\n",
      "    )\n",
      "    (4): Mamba2(\n",
      "      (in_proj): Linear(in_features=1536, out_features=6704, bias=False)\n",
      "      (conv1d): Conv1d(3584, 3584, kernel_size=(4,), stride=(1,), padding=(3,), groups=3584)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=3072, out_features=1536, bias=False)\n",
      "    )\n",
      "    (5): Mamba2(\n",
      "      (in_proj): Linear(in_features=1536, out_features=6704, bias=False)\n",
      "      (conv1d): Conv1d(3584, 3584, kernel_size=(4,), stride=(1,), padding=(3,), groups=3584)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=3072, out_features=1536, bias=False)\n",
      "    )\n",
      "    (6): Mamba2(\n",
      "      (in_proj): Linear(in_features=1536, out_features=6704, bias=False)\n",
      "      (conv1d): Conv1d(3584, 3584, kernel_size=(4,), stride=(1,), padding=(3,), groups=3584)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=3072, out_features=1536, bias=False)\n",
      "    )\n",
      "    (7): Mamba2(\n",
      "      (in_proj): Linear(in_features=1536, out_features=6704, bias=False)\n",
      "      (conv1d): Conv1d(3584, 3584, kernel_size=(4,), stride=(1,), padding=(3,), groups=3584)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=3072, out_features=1536, bias=False)\n",
      "    )\n",
      "    (8): Mamba2(\n",
      "      (in_proj): Linear(in_features=1536, out_features=6704, bias=False)\n",
      "      (conv1d): Conv1d(3584, 3584, kernel_size=(4,), stride=(1,), padding=(3,), groups=3584)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=3072, out_features=1536, bias=False)\n",
      "    )\n",
      "    (9): Mamba2(\n",
      "      (in_proj): Linear(in_features=1536, out_features=6704, bias=False)\n",
      "      (conv1d): Conv1d(3584, 3584, kernel_size=(4,), stride=(1,), padding=(3,), groups=3584)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=3072, out_features=1536, bias=False)\n",
      "    )\n",
      "    (10): Mamba2(\n",
      "      (in_proj): Linear(in_features=1536, out_features=6704, bias=False)\n",
      "      (conv1d): Conv1d(3584, 3584, kernel_size=(4,), stride=(1,), padding=(3,), groups=3584)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=3072, out_features=1536, bias=False)\n",
      "    )\n",
      "    (11): Mamba2(\n",
      "      (in_proj): Linear(in_features=1536, out_features=6704, bias=False)\n",
      "      (conv1d): Conv1d(3584, 3584, kernel_size=(4,), stride=(1,), padding=(3,), groups=3584)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=3072, out_features=1536, bias=False)\n",
      "    )\n",
      "    (12): Mamba2(\n",
      "      (in_proj): Linear(in_features=1536, out_features=6704, bias=False)\n",
      "      (conv1d): Conv1d(3584, 3584, kernel_size=(4,), stride=(1,), padding=(3,), groups=3584)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=3072, out_features=1536, bias=False)\n",
      "    )\n",
      "    (13): Mamba2(\n",
      "      (in_proj): Linear(in_features=1536, out_features=6704, bias=False)\n",
      "      (conv1d): Conv1d(3584, 3584, kernel_size=(4,), stride=(1,), padding=(3,), groups=3584)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=3072, out_features=1536, bias=False)\n",
      "    )\n",
      "    (14): Mamba2(\n",
      "      (in_proj): Linear(in_features=1536, out_features=6704, bias=False)\n",
      "      (conv1d): Conv1d(3584, 3584, kernel_size=(4,), stride=(1,), padding=(3,), groups=3584)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=3072, out_features=1536, bias=False)\n",
      "    )\n",
      "    (15): Mamba2(\n",
      "      (in_proj): Linear(in_features=1536, out_features=6704, bias=False)\n",
      "      (conv1d): Conv1d(3584, 3584, kernel_size=(4,), stride=(1,), padding=(3,), groups=3584)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=3072, out_features=1536, bias=False)\n",
      "    )\n",
      "    (16): Mamba2(\n",
      "      (in_proj): Linear(in_features=1536, out_features=6704, bias=False)\n",
      "      (conv1d): Conv1d(3584, 3584, kernel_size=(4,), stride=(1,), padding=(3,), groups=3584)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=3072, out_features=1536, bias=False)\n",
      "    )\n",
      "    (17): Mamba2(\n",
      "      (in_proj): Linear(in_features=1536, out_features=6704, bias=False)\n",
      "      (conv1d): Conv1d(3584, 3584, kernel_size=(4,), stride=(1,), padding=(3,), groups=3584)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=3072, out_features=1536, bias=False)\n",
      "    )\n",
      "    (18): Mamba2(\n",
      "      (in_proj): Linear(in_features=1536, out_features=6704, bias=False)\n",
      "      (conv1d): Conv1d(3584, 3584, kernel_size=(4,), stride=(1,), padding=(3,), groups=3584)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=3072, out_features=1536, bias=False)\n",
      "    )\n",
      "    (19): Mamba2(\n",
      "      (in_proj): Linear(in_features=1536, out_features=6704, bias=False)\n",
      "      (conv1d): Conv1d(3584, 3584, kernel_size=(4,), stride=(1,), padding=(3,), groups=3584)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=3072, out_features=1536, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (mamba_gaia): Sequential(\n",
      "    (0): Mamba2(\n",
      "      (in_proj): Linear(in_features=1536, out_features=6704, bias=False)\n",
      "      (conv1d): Conv1d(3584, 3584, kernel_size=(4,), stride=(1,), padding=(3,), groups=3584)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=3072, out_features=1536, bias=False)\n",
      "    )\n",
      "    (1): Mamba2(\n",
      "      (in_proj): Linear(in_features=1536, out_features=6704, bias=False)\n",
      "      (conv1d): Conv1d(3584, 3584, kernel_size=(4,), stride=(1,), padding=(3,), groups=3584)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=3072, out_features=1536, bias=False)\n",
      "    )\n",
      "    (2): Mamba2(\n",
      "      (in_proj): Linear(in_features=1536, out_features=6704, bias=False)\n",
      "      (conv1d): Conv1d(3584, 3584, kernel_size=(4,), stride=(1,), padding=(3,), groups=3584)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=3072, out_features=1536, bias=False)\n",
      "    )\n",
      "    (3): Mamba2(\n",
      "      (in_proj): Linear(in_features=1536, out_features=6704, bias=False)\n",
      "      (conv1d): Conv1d(3584, 3584, kernel_size=(4,), stride=(1,), padding=(3,), groups=3584)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=3072, out_features=1536, bias=False)\n",
      "    )\n",
      "    (4): Mamba2(\n",
      "      (in_proj): Linear(in_features=1536, out_features=6704, bias=False)\n",
      "      (conv1d): Conv1d(3584, 3584, kernel_size=(4,), stride=(1,), padding=(3,), groups=3584)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=3072, out_features=1536, bias=False)\n",
      "    )\n",
      "    (5): Mamba2(\n",
      "      (in_proj): Linear(in_features=1536, out_features=6704, bias=False)\n",
      "      (conv1d): Conv1d(3584, 3584, kernel_size=(4,), stride=(1,), padding=(3,), groups=3584)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=3072, out_features=1536, bias=False)\n",
      "    )\n",
      "    (6): Mamba2(\n",
      "      (in_proj): Linear(in_features=1536, out_features=6704, bias=False)\n",
      "      (conv1d): Conv1d(3584, 3584, kernel_size=(4,), stride=(1,), padding=(3,), groups=3584)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=3072, out_features=1536, bias=False)\n",
      "    )\n",
      "    (7): Mamba2(\n",
      "      (in_proj): Linear(in_features=1536, out_features=6704, bias=False)\n",
      "      (conv1d): Conv1d(3584, 3584, kernel_size=(4,), stride=(1,), padding=(3,), groups=3584)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=3072, out_features=1536, bias=False)\n",
      "    )\n",
      "    (8): Mamba2(\n",
      "      (in_proj): Linear(in_features=1536, out_features=6704, bias=False)\n",
      "      (conv1d): Conv1d(3584, 3584, kernel_size=(4,), stride=(1,), padding=(3,), groups=3584)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=3072, out_features=1536, bias=False)\n",
      "    )\n",
      "    (9): Mamba2(\n",
      "      (in_proj): Linear(in_features=1536, out_features=6704, bias=False)\n",
      "      (conv1d): Conv1d(3584, 3584, kernel_size=(4,), stride=(1,), padding=(3,), groups=3584)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=3072, out_features=1536, bias=False)\n",
      "    )\n",
      "    (10): Mamba2(\n",
      "      (in_proj): Linear(in_features=1536, out_features=6704, bias=False)\n",
      "      (conv1d): Conv1d(3584, 3584, kernel_size=(4,), stride=(1,), padding=(3,), groups=3584)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=3072, out_features=1536, bias=False)\n",
      "    )\n",
      "    (11): Mamba2(\n",
      "      (in_proj): Linear(in_features=1536, out_features=6704, bias=False)\n",
      "      (conv1d): Conv1d(3584, 3584, kernel_size=(4,), stride=(1,), padding=(3,), groups=3584)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=3072, out_features=1536, bias=False)\n",
      "    )\n",
      "    (12): Mamba2(\n",
      "      (in_proj): Linear(in_features=1536, out_features=6704, bias=False)\n",
      "      (conv1d): Conv1d(3584, 3584, kernel_size=(4,), stride=(1,), padding=(3,), groups=3584)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=3072, out_features=1536, bias=False)\n",
      "    )\n",
      "    (13): Mamba2(\n",
      "      (in_proj): Linear(in_features=1536, out_features=6704, bias=False)\n",
      "      (conv1d): Conv1d(3584, 3584, kernel_size=(4,), stride=(1,), padding=(3,), groups=3584)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=3072, out_features=1536, bias=False)\n",
      "    )\n",
      "    (14): Mamba2(\n",
      "      (in_proj): Linear(in_features=1536, out_features=6704, bias=False)\n",
      "      (conv1d): Conv1d(3584, 3584, kernel_size=(4,), stride=(1,), padding=(3,), groups=3584)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=3072, out_features=1536, bias=False)\n",
      "    )\n",
      "    (15): Mamba2(\n",
      "      (in_proj): Linear(in_features=1536, out_features=6704, bias=False)\n",
      "      (conv1d): Conv1d(3584, 3584, kernel_size=(4,), stride=(1,), padding=(3,), groups=3584)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=3072, out_features=1536, bias=False)\n",
      "    )\n",
      "    (16): Mamba2(\n",
      "      (in_proj): Linear(in_features=1536, out_features=6704, bias=False)\n",
      "      (conv1d): Conv1d(3584, 3584, kernel_size=(4,), stride=(1,), padding=(3,), groups=3584)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=3072, out_features=1536, bias=False)\n",
      "    )\n",
      "    (17): Mamba2(\n",
      "      (in_proj): Linear(in_features=1536, out_features=6704, bias=False)\n",
      "      (conv1d): Conv1d(3584, 3584, kernel_size=(4,), stride=(1,), padding=(3,), groups=3584)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=3072, out_features=1536, bias=False)\n",
      "    )\n",
      "    (18): Mamba2(\n",
      "      (in_proj): Linear(in_features=1536, out_features=6704, bias=False)\n",
      "      (conv1d): Conv1d(3584, 3584, kernel_size=(4,), stride=(1,), padding=(3,), groups=3584)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=3072, out_features=1536, bias=False)\n",
      "    )\n",
      "    (19): Mamba2(\n",
      "      (in_proj): Linear(in_features=1536, out_features=6704, bias=False)\n",
      "      (conv1d): Conv1d(3584, 3584, kernel_size=(4,), stride=(1,), padding=(3,), groups=3584)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=3072, out_features=1536, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (cross_attn_block_spectra): CrossAttentionBlock(\n",
      "    (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "    (attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=1536, out_features=1536, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (cross_attn_block_gaia): CrossAttentionBlock(\n",
      "    (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "    (attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=1536, out_features=1536, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): Linear(in_features=3072, out_features=55, bias=True)\n",
      "  )\n",
      ")\n",
      "tokenizer_spectra.token_embed.weight 10752\n",
      "tokenizer_spectra.token_embed.bias 1536\n",
      "tokenizer_gaia.token_embed.weight 1536\n",
      "tokenizer_gaia.token_embed.bias 1536\n",
      "mamba_spectra.0.dt_bias 48\n",
      "mamba_spectra.0.A_log 48\n",
      "mamba_spectra.0.D 48\n",
      "mamba_spectra.0.in_proj.weight 10297344\n",
      "mamba_spectra.0.conv1d.weight 14336\n",
      "mamba_spectra.0.conv1d.bias 3584\n",
      "mamba_spectra.0.norm.weight 3072\n",
      "mamba_spectra.0.out_proj.weight 4718592\n",
      "mamba_spectra.1.dt_bias 48\n",
      "mamba_spectra.1.A_log 48\n",
      "mamba_spectra.1.D 48\n",
      "mamba_spectra.1.in_proj.weight 10297344\n",
      "mamba_spectra.1.conv1d.weight 14336\n",
      "mamba_spectra.1.conv1d.bias 3584\n",
      "mamba_spectra.1.norm.weight 3072\n",
      "mamba_spectra.1.out_proj.weight 4718592\n",
      "mamba_spectra.2.dt_bias 48\n",
      "mamba_spectra.2.A_log 48\n",
      "mamba_spectra.2.D 48\n",
      "mamba_spectra.2.in_proj.weight 10297344\n",
      "mamba_spectra.2.conv1d.weight 14336\n",
      "mamba_spectra.2.conv1d.bias 3584\n",
      "mamba_spectra.2.norm.weight 3072\n",
      "mamba_spectra.2.out_proj.weight 4718592\n",
      "mamba_spectra.3.dt_bias 48\n",
      "mamba_spectra.3.A_log 48\n",
      "mamba_spectra.3.D 48\n",
      "mamba_spectra.3.in_proj.weight 10297344\n",
      "mamba_spectra.3.conv1d.weight 14336\n",
      "mamba_spectra.3.conv1d.bias 3584\n",
      "mamba_spectra.3.norm.weight 3072\n",
      "mamba_spectra.3.out_proj.weight 4718592\n",
      "mamba_spectra.4.dt_bias 48\n",
      "mamba_spectra.4.A_log 48\n",
      "mamba_spectra.4.D 48\n",
      "mamba_spectra.4.in_proj.weight 10297344\n",
      "mamba_spectra.4.conv1d.weight 14336\n",
      "mamba_spectra.4.conv1d.bias 3584\n",
      "mamba_spectra.4.norm.weight 3072\n",
      "mamba_spectra.4.out_proj.weight 4718592\n",
      "mamba_spectra.5.dt_bias 48\n",
      "mamba_spectra.5.A_log 48\n",
      "mamba_spectra.5.D 48\n",
      "mamba_spectra.5.in_proj.weight 10297344\n",
      "mamba_spectra.5.conv1d.weight 14336\n",
      "mamba_spectra.5.conv1d.bias 3584\n",
      "mamba_spectra.5.norm.weight 3072\n",
      "mamba_spectra.5.out_proj.weight 4718592\n",
      "mamba_spectra.6.dt_bias 48\n",
      "mamba_spectra.6.A_log 48\n",
      "mamba_spectra.6.D 48\n",
      "mamba_spectra.6.in_proj.weight 10297344\n",
      "mamba_spectra.6.conv1d.weight 14336\n",
      "mamba_spectra.6.conv1d.bias 3584\n",
      "mamba_spectra.6.norm.weight 3072\n",
      "mamba_spectra.6.out_proj.weight 4718592\n",
      "mamba_spectra.7.dt_bias 48\n",
      "mamba_spectra.7.A_log 48\n",
      "mamba_spectra.7.D 48\n",
      "mamba_spectra.7.in_proj.weight 10297344\n",
      "mamba_spectra.7.conv1d.weight 14336\n",
      "mamba_spectra.7.conv1d.bias 3584\n",
      "mamba_spectra.7.norm.weight 3072\n",
      "mamba_spectra.7.out_proj.weight 4718592\n",
      "mamba_spectra.8.dt_bias 48\n",
      "mamba_spectra.8.A_log 48\n",
      "mamba_spectra.8.D 48\n",
      "mamba_spectra.8.in_proj.weight 10297344\n",
      "mamba_spectra.8.conv1d.weight 14336\n",
      "mamba_spectra.8.conv1d.bias 3584\n",
      "mamba_spectra.8.norm.weight 3072\n",
      "mamba_spectra.8.out_proj.weight 4718592\n",
      "mamba_spectra.9.dt_bias 48\n",
      "mamba_spectra.9.A_log 48\n",
      "mamba_spectra.9.D 48\n",
      "mamba_spectra.9.in_proj.weight 10297344\n",
      "mamba_spectra.9.conv1d.weight 14336\n",
      "mamba_spectra.9.conv1d.bias 3584\n",
      "mamba_spectra.9.norm.weight 3072\n",
      "mamba_spectra.9.out_proj.weight 4718592\n",
      "mamba_spectra.10.dt_bias 48\n",
      "mamba_spectra.10.A_log 48\n",
      "mamba_spectra.10.D 48\n",
      "mamba_spectra.10.in_proj.weight 10297344\n",
      "mamba_spectra.10.conv1d.weight 14336\n",
      "mamba_spectra.10.conv1d.bias 3584\n",
      "mamba_spectra.10.norm.weight 3072\n",
      "mamba_spectra.10.out_proj.weight 4718592\n",
      "mamba_spectra.11.dt_bias 48\n",
      "mamba_spectra.11.A_log 48\n",
      "mamba_spectra.11.D 48\n",
      "mamba_spectra.11.in_proj.weight 10297344\n",
      "mamba_spectra.11.conv1d.weight 14336\n",
      "mamba_spectra.11.conv1d.bias 3584\n",
      "mamba_spectra.11.norm.weight 3072\n",
      "mamba_spectra.11.out_proj.weight 4718592\n",
      "mamba_spectra.12.dt_bias 48\n",
      "mamba_spectra.12.A_log 48\n",
      "mamba_spectra.12.D 48\n",
      "mamba_spectra.12.in_proj.weight 10297344\n",
      "mamba_spectra.12.conv1d.weight 14336\n",
      "mamba_spectra.12.conv1d.bias 3584\n",
      "mamba_spectra.12.norm.weight 3072\n",
      "mamba_spectra.12.out_proj.weight 4718592\n",
      "mamba_spectra.13.dt_bias 48\n",
      "mamba_spectra.13.A_log 48\n",
      "mamba_spectra.13.D 48\n",
      "mamba_spectra.13.in_proj.weight 10297344\n",
      "mamba_spectra.13.conv1d.weight 14336\n",
      "mamba_spectra.13.conv1d.bias 3584\n",
      "mamba_spectra.13.norm.weight 3072\n",
      "mamba_spectra.13.out_proj.weight 4718592\n",
      "mamba_spectra.14.dt_bias 48\n",
      "mamba_spectra.14.A_log 48\n",
      "mamba_spectra.14.D 48\n",
      "mamba_spectra.14.in_proj.weight 10297344\n",
      "mamba_spectra.14.conv1d.weight 14336\n",
      "mamba_spectra.14.conv1d.bias 3584\n",
      "mamba_spectra.14.norm.weight 3072\n",
      "mamba_spectra.14.out_proj.weight 4718592\n",
      "mamba_spectra.15.dt_bias 48\n",
      "mamba_spectra.15.A_log 48\n",
      "mamba_spectra.15.D 48\n",
      "mamba_spectra.15.in_proj.weight 10297344\n",
      "mamba_spectra.15.conv1d.weight 14336\n",
      "mamba_spectra.15.conv1d.bias 3584\n",
      "mamba_spectra.15.norm.weight 3072\n",
      "mamba_spectra.15.out_proj.weight 4718592\n",
      "mamba_spectra.16.dt_bias 48\n",
      "mamba_spectra.16.A_log 48\n",
      "mamba_spectra.16.D 48\n",
      "mamba_spectra.16.in_proj.weight 10297344\n",
      "mamba_spectra.16.conv1d.weight 14336\n",
      "mamba_spectra.16.conv1d.bias 3584\n",
      "mamba_spectra.16.norm.weight 3072\n",
      "mamba_spectra.16.out_proj.weight 4718592\n",
      "mamba_spectra.17.dt_bias 48\n",
      "mamba_spectra.17.A_log 48\n",
      "mamba_spectra.17.D 48\n",
      "mamba_spectra.17.in_proj.weight 10297344\n",
      "mamba_spectra.17.conv1d.weight 14336\n",
      "mamba_spectra.17.conv1d.bias 3584\n",
      "mamba_spectra.17.norm.weight 3072\n",
      "mamba_spectra.17.out_proj.weight 4718592\n",
      "mamba_spectra.18.dt_bias 48\n",
      "mamba_spectra.18.A_log 48\n",
      "mamba_spectra.18.D 48\n",
      "mamba_spectra.18.in_proj.weight 10297344\n",
      "mamba_spectra.18.conv1d.weight 14336\n",
      "mamba_spectra.18.conv1d.bias 3584\n",
      "mamba_spectra.18.norm.weight 3072\n",
      "mamba_spectra.18.out_proj.weight 4718592\n",
      "mamba_spectra.19.dt_bias 48\n",
      "mamba_spectra.19.A_log 48\n",
      "mamba_spectra.19.D 48\n",
      "mamba_spectra.19.in_proj.weight 10297344\n",
      "mamba_spectra.19.conv1d.weight 14336\n",
      "mamba_spectra.19.conv1d.bias 3584\n",
      "mamba_spectra.19.norm.weight 3072\n",
      "mamba_spectra.19.out_proj.weight 4718592\n",
      "mamba_gaia.0.dt_bias 48\n",
      "mamba_gaia.0.A_log 48\n",
      "mamba_gaia.0.D 48\n",
      "mamba_gaia.0.in_proj.weight 10297344\n",
      "mamba_gaia.0.conv1d.weight 14336\n",
      "mamba_gaia.0.conv1d.bias 3584\n",
      "mamba_gaia.0.norm.weight 3072\n",
      "mamba_gaia.0.out_proj.weight 4718592\n",
      "mamba_gaia.1.dt_bias 48\n",
      "mamba_gaia.1.A_log 48\n",
      "mamba_gaia.1.D 48\n",
      "mamba_gaia.1.in_proj.weight 10297344\n",
      "mamba_gaia.1.conv1d.weight 14336\n",
      "mamba_gaia.1.conv1d.bias 3584\n",
      "mamba_gaia.1.norm.weight 3072\n",
      "mamba_gaia.1.out_proj.weight 4718592\n",
      "mamba_gaia.2.dt_bias 48\n",
      "mamba_gaia.2.A_log 48\n",
      "mamba_gaia.2.D 48\n",
      "mamba_gaia.2.in_proj.weight 10297344\n",
      "mamba_gaia.2.conv1d.weight 14336\n",
      "mamba_gaia.2.conv1d.bias 3584\n",
      "mamba_gaia.2.norm.weight 3072\n",
      "mamba_gaia.2.out_proj.weight 4718592\n",
      "mamba_gaia.3.dt_bias 48\n",
      "mamba_gaia.3.A_log 48\n",
      "mamba_gaia.3.D 48\n",
      "mamba_gaia.3.in_proj.weight 10297344\n",
      "mamba_gaia.3.conv1d.weight 14336\n",
      "mamba_gaia.3.conv1d.bias 3584\n",
      "mamba_gaia.3.norm.weight 3072\n",
      "mamba_gaia.3.out_proj.weight 4718592\n",
      "mamba_gaia.4.dt_bias 48\n",
      "mamba_gaia.4.A_log 48\n",
      "mamba_gaia.4.D 48\n",
      "mamba_gaia.4.in_proj.weight 10297344\n",
      "mamba_gaia.4.conv1d.weight 14336\n",
      "mamba_gaia.4.conv1d.bias 3584\n",
      "mamba_gaia.4.norm.weight 3072\n",
      "mamba_gaia.4.out_proj.weight 4718592\n",
      "mamba_gaia.5.dt_bias 48\n",
      "mamba_gaia.5.A_log 48\n",
      "mamba_gaia.5.D 48\n",
      "mamba_gaia.5.in_proj.weight 10297344\n",
      "mamba_gaia.5.conv1d.weight 14336\n",
      "mamba_gaia.5.conv1d.bias 3584\n",
      "mamba_gaia.5.norm.weight 3072\n",
      "mamba_gaia.5.out_proj.weight 4718592\n",
      "mamba_gaia.6.dt_bias 48\n",
      "mamba_gaia.6.A_log 48\n",
      "mamba_gaia.6.D 48\n",
      "mamba_gaia.6.in_proj.weight 10297344\n",
      "mamba_gaia.6.conv1d.weight 14336\n",
      "mamba_gaia.6.conv1d.bias 3584\n",
      "mamba_gaia.6.norm.weight 3072\n",
      "mamba_gaia.6.out_proj.weight 4718592\n",
      "mamba_gaia.7.dt_bias 48\n",
      "mamba_gaia.7.A_log 48\n",
      "mamba_gaia.7.D 48\n",
      "mamba_gaia.7.in_proj.weight 10297344\n",
      "mamba_gaia.7.conv1d.weight 14336\n",
      "mamba_gaia.7.conv1d.bias 3584\n",
      "mamba_gaia.7.norm.weight 3072\n",
      "mamba_gaia.7.out_proj.weight 4718592\n",
      "mamba_gaia.8.dt_bias 48\n",
      "mamba_gaia.8.A_log 48\n",
      "mamba_gaia.8.D 48\n",
      "mamba_gaia.8.in_proj.weight 10297344\n",
      "mamba_gaia.8.conv1d.weight 14336\n",
      "mamba_gaia.8.conv1d.bias 3584\n",
      "mamba_gaia.8.norm.weight 3072\n",
      "mamba_gaia.8.out_proj.weight 4718592\n",
      "mamba_gaia.9.dt_bias 48\n",
      "mamba_gaia.9.A_log 48\n",
      "mamba_gaia.9.D 48\n",
      "mamba_gaia.9.in_proj.weight 10297344\n",
      "mamba_gaia.9.conv1d.weight 14336\n",
      "mamba_gaia.9.conv1d.bias 3584\n",
      "mamba_gaia.9.norm.weight 3072\n",
      "mamba_gaia.9.out_proj.weight 4718592\n",
      "mamba_gaia.10.dt_bias 48\n",
      "mamba_gaia.10.A_log 48\n",
      "mamba_gaia.10.D 48\n",
      "mamba_gaia.10.in_proj.weight 10297344\n",
      "mamba_gaia.10.conv1d.weight 14336\n",
      "mamba_gaia.10.conv1d.bias 3584\n",
      "mamba_gaia.10.norm.weight 3072\n",
      "mamba_gaia.10.out_proj.weight 4718592\n",
      "mamba_gaia.11.dt_bias 48\n",
      "mamba_gaia.11.A_log 48\n",
      "mamba_gaia.11.D 48\n",
      "mamba_gaia.11.in_proj.weight 10297344\n",
      "mamba_gaia.11.conv1d.weight 14336\n",
      "mamba_gaia.11.conv1d.bias 3584\n",
      "mamba_gaia.11.norm.weight 3072\n",
      "mamba_gaia.11.out_proj.weight 4718592\n",
      "mamba_gaia.12.dt_bias 48\n",
      "mamba_gaia.12.A_log 48\n",
      "mamba_gaia.12.D 48\n",
      "mamba_gaia.12.in_proj.weight 10297344\n",
      "mamba_gaia.12.conv1d.weight 14336\n",
      "mamba_gaia.12.conv1d.bias 3584\n",
      "mamba_gaia.12.norm.weight 3072\n",
      "mamba_gaia.12.out_proj.weight 4718592\n",
      "mamba_gaia.13.dt_bias 48\n",
      "mamba_gaia.13.A_log 48\n",
      "mamba_gaia.13.D 48\n",
      "mamba_gaia.13.in_proj.weight 10297344\n",
      "mamba_gaia.13.conv1d.weight 14336\n",
      "mamba_gaia.13.conv1d.bias 3584\n",
      "mamba_gaia.13.norm.weight 3072\n",
      "mamba_gaia.13.out_proj.weight 4718592\n",
      "mamba_gaia.14.dt_bias 48\n",
      "mamba_gaia.14.A_log 48\n",
      "mamba_gaia.14.D 48\n",
      "mamba_gaia.14.in_proj.weight 10297344\n",
      "mamba_gaia.14.conv1d.weight 14336\n",
      "mamba_gaia.14.conv1d.bias 3584\n",
      "mamba_gaia.14.norm.weight 3072\n",
      "mamba_gaia.14.out_proj.weight 4718592\n",
      "mamba_gaia.15.dt_bias 48\n",
      "mamba_gaia.15.A_log 48\n",
      "mamba_gaia.15.D 48\n",
      "mamba_gaia.15.in_proj.weight 10297344\n",
      "mamba_gaia.15.conv1d.weight 14336\n",
      "mamba_gaia.15.conv1d.bias 3584\n",
      "mamba_gaia.15.norm.weight 3072\n",
      "mamba_gaia.15.out_proj.weight 4718592\n",
      "mamba_gaia.16.dt_bias 48\n",
      "mamba_gaia.16.A_log 48\n",
      "mamba_gaia.16.D 48\n",
      "mamba_gaia.16.in_proj.weight 10297344\n",
      "mamba_gaia.16.conv1d.weight 14336\n",
      "mamba_gaia.16.conv1d.bias 3584\n",
      "mamba_gaia.16.norm.weight 3072\n",
      "mamba_gaia.16.out_proj.weight 4718592\n",
      "mamba_gaia.17.dt_bias 48\n",
      "mamba_gaia.17.A_log 48\n",
      "mamba_gaia.17.D 48\n",
      "mamba_gaia.17.in_proj.weight 10297344\n",
      "mamba_gaia.17.conv1d.weight 14336\n",
      "mamba_gaia.17.conv1d.bias 3584\n",
      "mamba_gaia.17.norm.weight 3072\n",
      "mamba_gaia.17.out_proj.weight 4718592\n",
      "mamba_gaia.18.dt_bias 48\n",
      "mamba_gaia.18.A_log 48\n",
      "mamba_gaia.18.D 48\n",
      "mamba_gaia.18.in_proj.weight 10297344\n",
      "mamba_gaia.18.conv1d.weight 14336\n",
      "mamba_gaia.18.conv1d.bias 3584\n",
      "mamba_gaia.18.norm.weight 3072\n",
      "mamba_gaia.18.out_proj.weight 4718592\n",
      "mamba_gaia.19.dt_bias 48\n",
      "mamba_gaia.19.A_log 48\n",
      "mamba_gaia.19.D 48\n",
      "mamba_gaia.19.in_proj.weight 10297344\n",
      "mamba_gaia.19.conv1d.weight 14336\n",
      "mamba_gaia.19.conv1d.bias 3584\n",
      "mamba_gaia.19.norm.weight 3072\n",
      "mamba_gaia.19.out_proj.weight 4718592\n",
      "cross_attn_block_spectra.norm.weight 1536\n",
      "cross_attn_block_spectra.norm.bias 1536\n",
      "cross_attn_block_spectra.attention.in_proj_weight 7077888\n",
      "cross_attn_block_spectra.attention.in_proj_bias 4608\n",
      "cross_attn_block_spectra.attention.out_proj.weight 2359296\n",
      "cross_attn_block_spectra.attention.out_proj.bias 1536\n",
      "cross_attn_block_gaia.norm.weight 1536\n",
      "cross_attn_block_gaia.norm.bias 1536\n",
      "cross_attn_block_gaia.attention.in_proj_weight 7077888\n",
      "cross_attn_block_gaia.attention.in_proj_bias 4608\n",
      "cross_attn_block_gaia.attention.out_proj.weight 2359296\n",
      "cross_attn_block_gaia.attention.out_proj.bias 1536\n",
      "classifier.0.weight 3072\n",
      "classifier.0.bias 3072\n",
      "classifier.1.weight 168960\n",
      "classifier.1.bias 55\n",
      "Total number of parameters: 620566199\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td></td></tr><tr><td>hamming_loss</td><td></td></tr><tr><td>lr</td><td></td></tr><tr><td>macro_f1</td><td></td></tr><tr><td>macro_precision</td><td></td></tr><tr><td>macro_recall</td><td></td></tr><tr><td>micro_f1</td><td></td></tr><tr><td>micro_precision</td><td></td></tr><tr><td>micro_recall</td><td></td></tr><tr><td>test_acc</td><td></td></tr><tr><td>test_loss</td><td></td></tr><tr><td>train_acc</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>val_acc</td><td></td></tr><tr><td>val_loss</td><td></td></tr><tr><td>weighted_f1</td><td></td></tr><tr><td>weighted_precision</td><td></td></tr><tr><td>weighted_recall</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>449</td></tr><tr><td>hamming_loss</td><td>0.02646</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>macro_f1</td><td>0.33348</td></tr><tr><td>macro_precision</td><td>0.88376</td></tr><tr><td>macro_recall</td><td>0.27981</td></tr><tr><td>micro_f1</td><td>0.49046</td></tr><tr><td>micro_precision</td><td>0.83333</td></tr><tr><td>micro_recall</td><td>0.34749</td></tr><tr><td>test_acc</td><td>0.97299</td></tr><tr><td>test_loss</td><td>0.06882</td></tr><tr><td>train_acc</td><td>0.98762</td></tr><tr><td>train_loss</td><td>0.01649</td></tr><tr><td>val_acc</td><td>0.97359</td></tr><tr><td>val_loss</td><td>0.0524</td></tr><tr><td>weighted_f1</td><td>0.4434</td></tr><tr><td>weighted_precision</td><td>0.88343</td></tr><tr><td>weighted_recall</td><td>0.34749</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">xindi-t-pol-8</strong> at: <a href='https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_mamba_v2/runs/cziossxp' target=\"_blank\">https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_mamba_v2/runs/cziossxp</a><br> View project at: <a href='https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_mamba_v2' target=\"_blank\">https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_mamba_v2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250405_230929-cziossxp/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Example config\n",
    "    d_model_spectra = 1536\n",
    "    d_model_gaia = 1536\n",
    "    num_classes = 55\n",
    "    input_dim_spectra = 3647\n",
    "    input_dim_gaia = 18\n",
    "    n_layers = 20\n",
    "    d_state = 16 # State dimension for MAMBA\n",
    "    d_conv = 4  # Convolution dimension for MAMBA\n",
    "    expand = 2  # Expansion factor for MAMBA\n",
    "\n",
    "    # Token dimensions - these control the sequence length\n",
    "    token_dim_spectra = 7    # Will create 522 tokens for spectra (3647/7)\n",
    "    token_dim_gaia = 1       # Will create 18 tokens for gaia (18/1)\n",
    "\n",
    "    lr = 2.5e-6\n",
    "    patience = 200\n",
    "    num_epochs = 800\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(\"Device:\", device)\n",
    "\n",
    "    # Initialize WandB\n",
    "    wandb.init(project=\"ALLSTARS_multimodal_fusion_mamba_v2\")\n",
    "    \n",
    "    config = {\n",
    "        \"num_classes\": num_classes,\n",
    "        \"d_model_spectra\": d_model_spectra,\n",
    "        \"d_model_gaia\": d_model_gaia,\n",
    "        \"input_dim_spectra\": input_dim_spectra,\n",
    "        \"input_dim_gaia\": input_dim_gaia,\n",
    "        \"n_layers\": n_layers,\n",
    "        \"lr\": lr,\n",
    "        \"patience\": patience,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"d_state\": d_state,\n",
    "        \"d_conv\": d_conv,\n",
    "        \"expand\": expand,\n",
    "        \"token_dim_gaia\": token_dim_gaia,\n",
    "        \"token_dim_spectra\": token_dim_spectra,\n",
    "    }\n",
    "    wandb.config.update(config)\n",
    "\n",
    "    # Instantiate the fusion model\n",
    "    # Try use_cross_attention=False for late-fusion, True for cross-attention\n",
    "    model_fusion = StarClassifierFusionMambaTokenized(\n",
    "        d_model_spectra=d_model_spectra,\n",
    "        d_model_gaia=d_model_gaia,\n",
    "        num_classes=num_classes,\n",
    "        input_dim_spectra=input_dim_spectra,\n",
    "        input_dim_gaia=input_dim_gaia,\n",
    "        token_dim_spectra=token_dim_spectra,\n",
    "        token_dim_gaia=token_dim_gaia,\n",
    "        n_layers=n_layers,\n",
    "        use_cross_attention=True,  # set to False to compare with late fusion\n",
    "        n_cross_attn_heads=8\n",
    "    )\n",
    "    model_fusion.to(device)\n",
    "\n",
    "    # Print size of model in GB\n",
    "    print(f\"Model size: {sum(p.numel() for p in model_fusion.parameters()) / 1e9:.2f} GB\")\n",
    "    param_size = 0\n",
    "    for param in model_fusion.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    buffer_size = 0\n",
    "    for buffer in model_fusion.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "    size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "    print('model size: {:.3f}MB'.format(size_all_mb))\n",
    "    # Compute parameter size\n",
    "    param_size = sum(p.nelement() * p.element_size() for p in model_fusion.parameters())\n",
    "\n",
    "    # Compute buffer size\n",
    "    buffer_size = sum(b.nelement() * b.element_size() for b in model_fusion.buffers())\n",
    "\n",
    "    # Total size in MB\n",
    "    total_size_mb = (param_size + buffer_size) / (1024 ** 2)\n",
    "    print(f\"Model size: {total_size_mb:.3f} MB\")\n",
    "\n",
    "    print(model_fusion)\n",
    "    # print number of parameters per layer\n",
    "    for name, param in model_fusion.named_parameters():\n",
    "        print(name, param.numel())\n",
    "    print(\"Total number of parameters:\", sum(p.numel() for p in model_fusion.parameters() if p.requires_grad))\n",
    "\n",
    "    # Train the fusion model\n",
    "    trained_fusion_model = train_model_fusion(\n",
    "        model=model_fusion,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader,\n",
    "        num_epochs=num_epochs,\n",
    "        lr=lr,\n",
    "        max_patience=patience,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "# Save the model\n",
    "torch.save(trained_fusion_model.state_dict(), \"Models/model_fusion_mamba_maxtokens.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mamba 19 + 18 Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjoaocsgalmeida\u001b[0m (\u001b[33mjoaocsgalmeida-university-of-southampton\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/joao/Documents/Star-Classifier/wandb/run-20250406_190234-yqo0yv3z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_mamba_v2/runs/yqo0yv3z' target=\"_blank\">cardassian-daedalus-13</a></strong> to <a href='https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_mamba_v2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_mamba_v2' target=\"_blank\">https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_mamba_v2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_mamba_v2/runs/yqo0yv3z' target=\"_blank\">https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_mamba_v2/runs/yqo0yv3z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 1.09 GB\n",
      "model size: 4153.686MB\n",
      "Model size: 4153.686 MB\n",
      "StarClassifierFusionMambaTokenized(\n",
      "  (tokenizer_spectra): FeatureTokenizer(\n",
      "    (token_embed): Linear(in_features=64, out_features=2048, bias=True)\n",
      "  )\n",
      "  (tokenizer_gaia): FeatureTokenizer(\n",
      "    (token_embed): Linear(in_features=2, out_features=2048, bias=True)\n",
      "  )\n",
      "  (mamba_spectra): Sequential(\n",
      "    (0): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (1): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (2): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (3): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (4): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (5): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (6): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (7): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (8): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (9): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (10): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (11): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (12): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (13): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (14): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (15): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (16): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (17): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (18): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (19): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (mamba_gaia): Sequential(\n",
      "    (0): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (1): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (2): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (3): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (4): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (5): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (6): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (7): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (8): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (9): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (10): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (11): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (12): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (13): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (14): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (15): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (16): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (17): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (18): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (19): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (cross_attn_block_spectra): CrossAttentionBlock(\n",
      "    (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "    (attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=2048, out_features=2048, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (cross_attn_block_gaia): CrossAttentionBlock(\n",
      "    (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "    (attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=2048, out_features=2048, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): Linear(in_features=4096, out_features=55, bias=True)\n",
      "  )\n",
      ")\n",
      "tokenizer_spectra.token_embed.weight 131072\n",
      "tokenizer_spectra.token_embed.bias 2048\n",
      "tokenizer_gaia.token_embed.weight 4096\n",
      "tokenizer_gaia.token_embed.bias 2048\n",
      "mamba_spectra.0.dt_bias 64\n",
      "mamba_spectra.0.A_log 64\n",
      "mamba_spectra.0.D 64\n",
      "mamba_spectra.0.in_proj.weight 17956864\n",
      "mamba_spectra.0.conv1d.weight 18432\n",
      "mamba_spectra.0.conv1d.bias 4608\n",
      "mamba_spectra.0.norm.weight 4096\n",
      "mamba_spectra.0.out_proj.weight 8388608\n",
      "mamba_spectra.1.dt_bias 64\n",
      "mamba_spectra.1.A_log 64\n",
      "mamba_spectra.1.D 64\n",
      "mamba_spectra.1.in_proj.weight 17956864\n",
      "mamba_spectra.1.conv1d.weight 18432\n",
      "mamba_spectra.1.conv1d.bias 4608\n",
      "mamba_spectra.1.norm.weight 4096\n",
      "mamba_spectra.1.out_proj.weight 8388608\n",
      "mamba_spectra.2.dt_bias 64\n",
      "mamba_spectra.2.A_log 64\n",
      "mamba_spectra.2.D 64\n",
      "mamba_spectra.2.in_proj.weight 17956864\n",
      "mamba_spectra.2.conv1d.weight 18432\n",
      "mamba_spectra.2.conv1d.bias 4608\n",
      "mamba_spectra.2.norm.weight 4096\n",
      "mamba_spectra.2.out_proj.weight 8388608\n",
      "mamba_spectra.3.dt_bias 64\n",
      "mamba_spectra.3.A_log 64\n",
      "mamba_spectra.3.D 64\n",
      "mamba_spectra.3.in_proj.weight 17956864\n",
      "mamba_spectra.3.conv1d.weight 18432\n",
      "mamba_spectra.3.conv1d.bias 4608\n",
      "mamba_spectra.3.norm.weight 4096\n",
      "mamba_spectra.3.out_proj.weight 8388608\n",
      "mamba_spectra.4.dt_bias 64\n",
      "mamba_spectra.4.A_log 64\n",
      "mamba_spectra.4.D 64\n",
      "mamba_spectra.4.in_proj.weight 17956864\n",
      "mamba_spectra.4.conv1d.weight 18432\n",
      "mamba_spectra.4.conv1d.bias 4608\n",
      "mamba_spectra.4.norm.weight 4096\n",
      "mamba_spectra.4.out_proj.weight 8388608\n",
      "mamba_spectra.5.dt_bias 64\n",
      "mamba_spectra.5.A_log 64\n",
      "mamba_spectra.5.D 64\n",
      "mamba_spectra.5.in_proj.weight 17956864\n",
      "mamba_spectra.5.conv1d.weight 18432\n",
      "mamba_spectra.5.conv1d.bias 4608\n",
      "mamba_spectra.5.norm.weight 4096\n",
      "mamba_spectra.5.out_proj.weight 8388608\n",
      "mamba_spectra.6.dt_bias 64\n",
      "mamba_spectra.6.A_log 64\n",
      "mamba_spectra.6.D 64\n",
      "mamba_spectra.6.in_proj.weight 17956864\n",
      "mamba_spectra.6.conv1d.weight 18432\n",
      "mamba_spectra.6.conv1d.bias 4608\n",
      "mamba_spectra.6.norm.weight 4096\n",
      "mamba_spectra.6.out_proj.weight 8388608\n",
      "mamba_spectra.7.dt_bias 64\n",
      "mamba_spectra.7.A_log 64\n",
      "mamba_spectra.7.D 64\n",
      "mamba_spectra.7.in_proj.weight 17956864\n",
      "mamba_spectra.7.conv1d.weight 18432\n",
      "mamba_spectra.7.conv1d.bias 4608\n",
      "mamba_spectra.7.norm.weight 4096\n",
      "mamba_spectra.7.out_proj.weight 8388608\n",
      "mamba_spectra.8.dt_bias 64\n",
      "mamba_spectra.8.A_log 64\n",
      "mamba_spectra.8.D 64\n",
      "mamba_spectra.8.in_proj.weight 17956864\n",
      "mamba_spectra.8.conv1d.weight 18432\n",
      "mamba_spectra.8.conv1d.bias 4608\n",
      "mamba_spectra.8.norm.weight 4096\n",
      "mamba_spectra.8.out_proj.weight 8388608\n",
      "mamba_spectra.9.dt_bias 64\n",
      "mamba_spectra.9.A_log 64\n",
      "mamba_spectra.9.D 64\n",
      "mamba_spectra.9.in_proj.weight 17956864\n",
      "mamba_spectra.9.conv1d.weight 18432\n",
      "mamba_spectra.9.conv1d.bias 4608\n",
      "mamba_spectra.9.norm.weight 4096\n",
      "mamba_spectra.9.out_proj.weight 8388608\n",
      "mamba_spectra.10.dt_bias 64\n",
      "mamba_spectra.10.A_log 64\n",
      "mamba_spectra.10.D 64\n",
      "mamba_spectra.10.in_proj.weight 17956864\n",
      "mamba_spectra.10.conv1d.weight 18432\n",
      "mamba_spectra.10.conv1d.bias 4608\n",
      "mamba_spectra.10.norm.weight 4096\n",
      "mamba_spectra.10.out_proj.weight 8388608\n",
      "mamba_spectra.11.dt_bias 64\n",
      "mamba_spectra.11.A_log 64\n",
      "mamba_spectra.11.D 64\n",
      "mamba_spectra.11.in_proj.weight 17956864\n",
      "mamba_spectra.11.conv1d.weight 18432\n",
      "mamba_spectra.11.conv1d.bias 4608\n",
      "mamba_spectra.11.norm.weight 4096\n",
      "mamba_spectra.11.out_proj.weight 8388608\n",
      "mamba_spectra.12.dt_bias 64\n",
      "mamba_spectra.12.A_log 64\n",
      "mamba_spectra.12.D 64\n",
      "mamba_spectra.12.in_proj.weight 17956864\n",
      "mamba_spectra.12.conv1d.weight 18432\n",
      "mamba_spectra.12.conv1d.bias 4608\n",
      "mamba_spectra.12.norm.weight 4096\n",
      "mamba_spectra.12.out_proj.weight 8388608\n",
      "mamba_spectra.13.dt_bias 64\n",
      "mamba_spectra.13.A_log 64\n",
      "mamba_spectra.13.D 64\n",
      "mamba_spectra.13.in_proj.weight 17956864\n",
      "mamba_spectra.13.conv1d.weight 18432\n",
      "mamba_spectra.13.conv1d.bias 4608\n",
      "mamba_spectra.13.norm.weight 4096\n",
      "mamba_spectra.13.out_proj.weight 8388608\n",
      "mamba_spectra.14.dt_bias 64\n",
      "mamba_spectra.14.A_log 64\n",
      "mamba_spectra.14.D 64\n",
      "mamba_spectra.14.in_proj.weight 17956864\n",
      "mamba_spectra.14.conv1d.weight 18432\n",
      "mamba_spectra.14.conv1d.bias 4608\n",
      "mamba_spectra.14.norm.weight 4096\n",
      "mamba_spectra.14.out_proj.weight 8388608\n",
      "mamba_spectra.15.dt_bias 64\n",
      "mamba_spectra.15.A_log 64\n",
      "mamba_spectra.15.D 64\n",
      "mamba_spectra.15.in_proj.weight 17956864\n",
      "mamba_spectra.15.conv1d.weight 18432\n",
      "mamba_spectra.15.conv1d.bias 4608\n",
      "mamba_spectra.15.norm.weight 4096\n",
      "mamba_spectra.15.out_proj.weight 8388608\n",
      "mamba_spectra.16.dt_bias 64\n",
      "mamba_spectra.16.A_log 64\n",
      "mamba_spectra.16.D 64\n",
      "mamba_spectra.16.in_proj.weight 17956864\n",
      "mamba_spectra.16.conv1d.weight 18432\n",
      "mamba_spectra.16.conv1d.bias 4608\n",
      "mamba_spectra.16.norm.weight 4096\n",
      "mamba_spectra.16.out_proj.weight 8388608\n",
      "mamba_spectra.17.dt_bias 64\n",
      "mamba_spectra.17.A_log 64\n",
      "mamba_spectra.17.D 64\n",
      "mamba_spectra.17.in_proj.weight 17956864\n",
      "mamba_spectra.17.conv1d.weight 18432\n",
      "mamba_spectra.17.conv1d.bias 4608\n",
      "mamba_spectra.17.norm.weight 4096\n",
      "mamba_spectra.17.out_proj.weight 8388608\n",
      "mamba_spectra.18.dt_bias 64\n",
      "mamba_spectra.18.A_log 64\n",
      "mamba_spectra.18.D 64\n",
      "mamba_spectra.18.in_proj.weight 17956864\n",
      "mamba_spectra.18.conv1d.weight 18432\n",
      "mamba_spectra.18.conv1d.bias 4608\n",
      "mamba_spectra.18.norm.weight 4096\n",
      "mamba_spectra.18.out_proj.weight 8388608\n",
      "mamba_spectra.19.dt_bias 64\n",
      "mamba_spectra.19.A_log 64\n",
      "mamba_spectra.19.D 64\n",
      "mamba_spectra.19.in_proj.weight 17956864\n",
      "mamba_spectra.19.conv1d.weight 18432\n",
      "mamba_spectra.19.conv1d.bias 4608\n",
      "mamba_spectra.19.norm.weight 4096\n",
      "mamba_spectra.19.out_proj.weight 8388608\n",
      "mamba_gaia.0.dt_bias 64\n",
      "mamba_gaia.0.A_log 64\n",
      "mamba_gaia.0.D 64\n",
      "mamba_gaia.0.in_proj.weight 17956864\n",
      "mamba_gaia.0.conv1d.weight 18432\n",
      "mamba_gaia.0.conv1d.bias 4608\n",
      "mamba_gaia.0.norm.weight 4096\n",
      "mamba_gaia.0.out_proj.weight 8388608\n",
      "mamba_gaia.1.dt_bias 64\n",
      "mamba_gaia.1.A_log 64\n",
      "mamba_gaia.1.D 64\n",
      "mamba_gaia.1.in_proj.weight 17956864\n",
      "mamba_gaia.1.conv1d.weight 18432\n",
      "mamba_gaia.1.conv1d.bias 4608\n",
      "mamba_gaia.1.norm.weight 4096\n",
      "mamba_gaia.1.out_proj.weight 8388608\n",
      "mamba_gaia.2.dt_bias 64\n",
      "mamba_gaia.2.A_log 64\n",
      "mamba_gaia.2.D 64\n",
      "mamba_gaia.2.in_proj.weight 17956864\n",
      "mamba_gaia.2.conv1d.weight 18432\n",
      "mamba_gaia.2.conv1d.bias 4608\n",
      "mamba_gaia.2.norm.weight 4096\n",
      "mamba_gaia.2.out_proj.weight 8388608\n",
      "mamba_gaia.3.dt_bias 64\n",
      "mamba_gaia.3.A_log 64\n",
      "mamba_gaia.3.D 64\n",
      "mamba_gaia.3.in_proj.weight 17956864\n",
      "mamba_gaia.3.conv1d.weight 18432\n",
      "mamba_gaia.3.conv1d.bias 4608\n",
      "mamba_gaia.3.norm.weight 4096\n",
      "mamba_gaia.3.out_proj.weight 8388608\n",
      "mamba_gaia.4.dt_bias 64\n",
      "mamba_gaia.4.A_log 64\n",
      "mamba_gaia.4.D 64\n",
      "mamba_gaia.4.in_proj.weight 17956864\n",
      "mamba_gaia.4.conv1d.weight 18432\n",
      "mamba_gaia.4.conv1d.bias 4608\n",
      "mamba_gaia.4.norm.weight 4096\n",
      "mamba_gaia.4.out_proj.weight 8388608\n",
      "mamba_gaia.5.dt_bias 64\n",
      "mamba_gaia.5.A_log 64\n",
      "mamba_gaia.5.D 64\n",
      "mamba_gaia.5.in_proj.weight 17956864\n",
      "mamba_gaia.5.conv1d.weight 18432\n",
      "mamba_gaia.5.conv1d.bias 4608\n",
      "mamba_gaia.5.norm.weight 4096\n",
      "mamba_gaia.5.out_proj.weight 8388608\n",
      "mamba_gaia.6.dt_bias 64\n",
      "mamba_gaia.6.A_log 64\n",
      "mamba_gaia.6.D 64\n",
      "mamba_gaia.6.in_proj.weight 17956864\n",
      "mamba_gaia.6.conv1d.weight 18432\n",
      "mamba_gaia.6.conv1d.bias 4608\n",
      "mamba_gaia.6.norm.weight 4096\n",
      "mamba_gaia.6.out_proj.weight 8388608\n",
      "mamba_gaia.7.dt_bias 64\n",
      "mamba_gaia.7.A_log 64\n",
      "mamba_gaia.7.D 64\n",
      "mamba_gaia.7.in_proj.weight 17956864\n",
      "mamba_gaia.7.conv1d.weight 18432\n",
      "mamba_gaia.7.conv1d.bias 4608\n",
      "mamba_gaia.7.norm.weight 4096\n",
      "mamba_gaia.7.out_proj.weight 8388608\n",
      "mamba_gaia.8.dt_bias 64\n",
      "mamba_gaia.8.A_log 64\n",
      "mamba_gaia.8.D 64\n",
      "mamba_gaia.8.in_proj.weight 17956864\n",
      "mamba_gaia.8.conv1d.weight 18432\n",
      "mamba_gaia.8.conv1d.bias 4608\n",
      "mamba_gaia.8.norm.weight 4096\n",
      "mamba_gaia.8.out_proj.weight 8388608\n",
      "mamba_gaia.9.dt_bias 64\n",
      "mamba_gaia.9.A_log 64\n",
      "mamba_gaia.9.D 64\n",
      "mamba_gaia.9.in_proj.weight 17956864\n",
      "mamba_gaia.9.conv1d.weight 18432\n",
      "mamba_gaia.9.conv1d.bias 4608\n",
      "mamba_gaia.9.norm.weight 4096\n",
      "mamba_gaia.9.out_proj.weight 8388608\n",
      "mamba_gaia.10.dt_bias 64\n",
      "mamba_gaia.10.A_log 64\n",
      "mamba_gaia.10.D 64\n",
      "mamba_gaia.10.in_proj.weight 17956864\n",
      "mamba_gaia.10.conv1d.weight 18432\n",
      "mamba_gaia.10.conv1d.bias 4608\n",
      "mamba_gaia.10.norm.weight 4096\n",
      "mamba_gaia.10.out_proj.weight 8388608\n",
      "mamba_gaia.11.dt_bias 64\n",
      "mamba_gaia.11.A_log 64\n",
      "mamba_gaia.11.D 64\n",
      "mamba_gaia.11.in_proj.weight 17956864\n",
      "mamba_gaia.11.conv1d.weight 18432\n",
      "mamba_gaia.11.conv1d.bias 4608\n",
      "mamba_gaia.11.norm.weight 4096\n",
      "mamba_gaia.11.out_proj.weight 8388608\n",
      "mamba_gaia.12.dt_bias 64\n",
      "mamba_gaia.12.A_log 64\n",
      "mamba_gaia.12.D 64\n",
      "mamba_gaia.12.in_proj.weight 17956864\n",
      "mamba_gaia.12.conv1d.weight 18432\n",
      "mamba_gaia.12.conv1d.bias 4608\n",
      "mamba_gaia.12.norm.weight 4096\n",
      "mamba_gaia.12.out_proj.weight 8388608\n",
      "mamba_gaia.13.dt_bias 64\n",
      "mamba_gaia.13.A_log 64\n",
      "mamba_gaia.13.D 64\n",
      "mamba_gaia.13.in_proj.weight 17956864\n",
      "mamba_gaia.13.conv1d.weight 18432\n",
      "mamba_gaia.13.conv1d.bias 4608\n",
      "mamba_gaia.13.norm.weight 4096\n",
      "mamba_gaia.13.out_proj.weight 8388608\n",
      "mamba_gaia.14.dt_bias 64\n",
      "mamba_gaia.14.A_log 64\n",
      "mamba_gaia.14.D 64\n",
      "mamba_gaia.14.in_proj.weight 17956864\n",
      "mamba_gaia.14.conv1d.weight 18432\n",
      "mamba_gaia.14.conv1d.bias 4608\n",
      "mamba_gaia.14.norm.weight 4096\n",
      "mamba_gaia.14.out_proj.weight 8388608\n",
      "mamba_gaia.15.dt_bias 64\n",
      "mamba_gaia.15.A_log 64\n",
      "mamba_gaia.15.D 64\n",
      "mamba_gaia.15.in_proj.weight 17956864\n",
      "mamba_gaia.15.conv1d.weight 18432\n",
      "mamba_gaia.15.conv1d.bias 4608\n",
      "mamba_gaia.15.norm.weight 4096\n",
      "mamba_gaia.15.out_proj.weight 8388608\n",
      "mamba_gaia.16.dt_bias 64\n",
      "mamba_gaia.16.A_log 64\n",
      "mamba_gaia.16.D 64\n",
      "mamba_gaia.16.in_proj.weight 17956864\n",
      "mamba_gaia.16.conv1d.weight 18432\n",
      "mamba_gaia.16.conv1d.bias 4608\n",
      "mamba_gaia.16.norm.weight 4096\n",
      "mamba_gaia.16.out_proj.weight 8388608\n",
      "mamba_gaia.17.dt_bias 64\n",
      "mamba_gaia.17.A_log 64\n",
      "mamba_gaia.17.D 64\n",
      "mamba_gaia.17.in_proj.weight 17956864\n",
      "mamba_gaia.17.conv1d.weight 18432\n",
      "mamba_gaia.17.conv1d.bias 4608\n",
      "mamba_gaia.17.norm.weight 4096\n",
      "mamba_gaia.17.out_proj.weight 8388608\n",
      "mamba_gaia.18.dt_bias 64\n",
      "mamba_gaia.18.A_log 64\n",
      "mamba_gaia.18.D 64\n",
      "mamba_gaia.18.in_proj.weight 17956864\n",
      "mamba_gaia.18.conv1d.weight 18432\n",
      "mamba_gaia.18.conv1d.bias 4608\n",
      "mamba_gaia.18.norm.weight 4096\n",
      "mamba_gaia.18.out_proj.weight 8388608\n",
      "mamba_gaia.19.dt_bias 64\n",
      "mamba_gaia.19.A_log 64\n",
      "mamba_gaia.19.D 64\n",
      "mamba_gaia.19.in_proj.weight 17956864\n",
      "mamba_gaia.19.conv1d.weight 18432\n",
      "mamba_gaia.19.conv1d.bias 4608\n",
      "mamba_gaia.19.norm.weight 4096\n",
      "mamba_gaia.19.out_proj.weight 8388608\n",
      "cross_attn_block_spectra.norm.weight 2048\n",
      "cross_attn_block_spectra.norm.bias 2048\n",
      "cross_attn_block_spectra.attention.in_proj_weight 12582912\n",
      "cross_attn_block_spectra.attention.in_proj_bias 6144\n",
      "cross_attn_block_spectra.attention.out_proj.weight 4194304\n",
      "cross_attn_block_spectra.attention.out_proj.bias 2048\n",
      "cross_attn_block_gaia.norm.weight 2048\n",
      "cross_attn_block_gaia.norm.bias 2048\n",
      "cross_attn_block_gaia.attention.in_proj_weight 12582912\n",
      "cross_attn_block_gaia.attention.in_proj_bias 6144\n",
      "cross_attn_block_gaia.attention.out_proj.weight 4194304\n",
      "cross_attn_block_gaia.attention.out_proj.bias 2048\n",
      "classifier.0.weight 4096\n",
      "classifier.0.bias 4096\n",
      "classifier.1.weight 225280\n",
      "classifier.1.bias 55\n",
      "Total number of parameters: 1088863799\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td></td></tr><tr><td>hamming_loss</td><td></td></tr><tr><td>lr</td><td></td></tr><tr><td>macro_f1</td><td></td></tr><tr><td>macro_precision</td><td></td></tr><tr><td>macro_recall</td><td></td></tr><tr><td>micro_f1</td><td></td></tr><tr><td>micro_precision</td><td></td></tr><tr><td>micro_recall</td><td></td></tr><tr><td>test_acc</td><td></td></tr><tr><td>test_loss</td><td></td></tr><tr><td>train_acc</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>val_acc</td><td></td></tr><tr><td>val_loss</td><td></td></tr><tr><td>weighted_f1</td><td></td></tr><tr><td>weighted_precision</td><td></td></tr><tr><td>weighted_recall</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>521</td></tr><tr><td>hamming_loss</td><td>0.02368</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>macro_f1</td><td>0.40767</td></tr><tr><td>macro_precision</td><td>0.87328</td></tr><tr><td>macro_recall</td><td>0.35113</td></tr><tr><td>micro_f1</td><td>0.58924</td></tr><tr><td>micro_precision</td><td>0.82534</td></tr><tr><td>micro_recall</td><td>0.45817</td></tr><tr><td>test_acc</td><td>0.97614</td></tr><tr><td>test_loss</td><td>0.07066</td></tr><tr><td>train_acc</td><td>0.99043</td></tr><tr><td>train_loss</td><td>0.01242</td></tr><tr><td>val_acc</td><td>0.97624</td></tr><tr><td>val_loss</td><td>0.05286</td></tr><tr><td>weighted_f1</td><td>0.55451</td></tr><tr><td>weighted_precision</td><td>0.865</td></tr><tr><td>weighted_recall</td><td>0.45817</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">cardassian-daedalus-13</strong> at: <a href='https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_mamba_v2/runs/yqo0yv3z' target=\"_blank\">https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_mamba_v2/runs/yqo0yv3z</a><br> View project at: <a href='https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_mamba_v2' target=\"_blank\">https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_mamba_v2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250406_190234-yqo0yv3z/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Clean Cache of GPU\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Example config\n",
    "    d_model_spectra = 2048\n",
    "    d_model_gaia = 2048\n",
    "    num_classes = 55\n",
    "    input_dim_spectra = 3647\n",
    "    input_dim_gaia = 18\n",
    "    n_layers = 20\n",
    "    d_state = 32 # State dimension for MAMBA\n",
    "    d_conv = 4  # Convolution dimension for MAMBA\n",
    "    expand = 2  # Expansion factor for MAMBA\n",
    "\n",
    "    # Token dimensions - these control the sequence length\n",
    "    token_dim_spectra = 192  # Will create ~19 tokens for spectra (3647/192)\n",
    "    token_dim_gaia = 1       # Will create 18 tokens for gaia (18/1)\n",
    "\n",
    "    lr = 2.5e-6\n",
    "    patience = 200\n",
    "    num_epochs = 800\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(\"Device:\", device)\n",
    "\n",
    "    # Initialize WandB\n",
    "    wandb.init(project=\"ALLSTARS_multimodal_fusion_mamba_v2\")\n",
    "    \n",
    "    config = {\n",
    "        \"num_classes\": num_classes,\n",
    "        \"d_model_spectra\": d_model_spectra,\n",
    "        \"d_model_gaia\": d_model_gaia,\n",
    "        \"input_dim_spectra\": input_dim_spectra,\n",
    "        \"input_dim_gaia\": input_dim_gaia,\n",
    "        \"n_layers\": n_layers,\n",
    "        \"lr\": lr,\n",
    "        \"patience\": patience,\n",
    "        \"num_epochs\": num_epochs\n",
    "    }\n",
    "    wandb.config.update(config)\n",
    "\n",
    "    # Instantiate the fusion model\n",
    "    # Try use_cross_attention=False for late-fusion, True for cross-attention\n",
    "    model_fusion = StarClassifierFusionMambaTokenized(\n",
    "        d_model_spectra=d_model_spectra,\n",
    "        d_model_gaia=d_model_gaia,\n",
    "        num_classes=num_classes,\n",
    "        input_dim_spectra=input_dim_spectra,\n",
    "        input_dim_gaia=input_dim_gaia,\n",
    "        n_layers=n_layers,\n",
    "        use_cross_attention=True,  # set to False to compare with late fusion\n",
    "        n_cross_attn_heads=8\n",
    "    )\n",
    "    model_fusion.to(device)\n",
    "\n",
    "    # Print size of model in GB\n",
    "    print(f\"Model size: {sum(p.numel() for p in model_fusion.parameters()) / 1e9:.2f} GB\")\n",
    "    param_size = 0\n",
    "    for param in model_fusion.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    buffer_size = 0\n",
    "    for buffer in model_fusion.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "    size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "    print('model size: {:.3f}MB'.format(size_all_mb))\n",
    "    # Compute parameter size\n",
    "    param_size = sum(p.nelement() * p.element_size() for p in model_fusion.parameters())\n",
    "\n",
    "    # Compute buffer size\n",
    "    buffer_size = sum(b.nelement() * b.element_size() for b in model_fusion.buffers())\n",
    "\n",
    "    # Total size in MB\n",
    "    total_size_mb = (param_size + buffer_size) / (1024 ** 2)\n",
    "    print(f\"Model size: {total_size_mb:.3f} MB\")\n",
    "\n",
    "    print(model_fusion)\n",
    "    # print number of parameters per layer\n",
    "    for name, param in model_fusion.named_parameters():\n",
    "        print(name, param.numel())\n",
    "    print(\"Total number of parameters:\", sum(p.numel() for p in model_fusion.parameters() if p.requires_grad))\n",
    "\n",
    "    # Train the fusion model\n",
    "    trained_fusion_model = train_model_fusion(\n",
    "        model=model_fusion,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader,\n",
    "        num_epochs=num_epochs,\n",
    "        lr=lr,\n",
    "        max_patience=patience,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "# Save the model\n",
    "torch.save(trained_fusion_model.state_dict(), \"Models/model_fusion_mamba_19_18.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mamba 1 token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjoaocsgalmeida\u001b[0m (\u001b[33mjoaocsgalmeida-university-of-southampton\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/joao/Documents/Star-Classifier/wandb/run-20250407_203003-zst2sggk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_mamba_v2/runs/zst2sggk' target=\"_blank\">sandy-energy-14</a></strong> to <a href='https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_mamba_v2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_mamba_v2' target=\"_blank\">https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_mamba_v2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_mamba_v2/runs/zst2sggk' target=\"_blank\">https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_mamba_v2/runs/zst2sggk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 1.09 GB\n",
      "model size: 4153.686MB\n",
      "Model size: 4153.686 MB\n",
      "StarClassifierFusionMambaTokenized(\n",
      "  (tokenizer_spectra): FeatureTokenizer(\n",
      "    (token_embed): Linear(in_features=64, out_features=2048, bias=True)\n",
      "  )\n",
      "  (tokenizer_gaia): FeatureTokenizer(\n",
      "    (token_embed): Linear(in_features=2, out_features=2048, bias=True)\n",
      "  )\n",
      "  (mamba_spectra): Sequential(\n",
      "    (0): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (1): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (2): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (3): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (4): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (5): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (6): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (7): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (8): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (9): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (10): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (11): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (12): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (13): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (14): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (15): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (16): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (17): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (18): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (19): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (mamba_gaia): Sequential(\n",
      "    (0): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (1): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (2): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (3): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (4): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (5): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (6): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (7): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (8): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (9): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (10): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (11): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (12): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (13): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (14): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (15): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (16): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (17): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (18): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "    (19): Mamba2(\n",
      "      (in_proj): Linear(in_features=2048, out_features=8768, bias=False)\n",
      "      (conv1d): Conv1d(4608, 4608, kernel_size=(4,), stride=(1,), padding=(3,), groups=4608)\n",
      "      (act): SiLU()\n",
      "      (norm): RMSNorm()\n",
      "      (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (cross_attn_block_spectra): CrossAttentionBlock(\n",
      "    (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "    (attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=2048, out_features=2048, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (cross_attn_block_gaia): CrossAttentionBlock(\n",
      "    (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "    (attention): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=2048, out_features=2048, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): Linear(in_features=4096, out_features=55, bias=True)\n",
      "  )\n",
      ")\n",
      "tokenizer_spectra.token_embed.weight 131072\n",
      "tokenizer_spectra.token_embed.bias 2048\n",
      "tokenizer_gaia.token_embed.weight 4096\n",
      "tokenizer_gaia.token_embed.bias 2048\n",
      "mamba_spectra.0.dt_bias 64\n",
      "mamba_spectra.0.A_log 64\n",
      "mamba_spectra.0.D 64\n",
      "mamba_spectra.0.in_proj.weight 17956864\n",
      "mamba_spectra.0.conv1d.weight 18432\n",
      "mamba_spectra.0.conv1d.bias 4608\n",
      "mamba_spectra.0.norm.weight 4096\n",
      "mamba_spectra.0.out_proj.weight 8388608\n",
      "mamba_spectra.1.dt_bias 64\n",
      "mamba_spectra.1.A_log 64\n",
      "mamba_spectra.1.D 64\n",
      "mamba_spectra.1.in_proj.weight 17956864\n",
      "mamba_spectra.1.conv1d.weight 18432\n",
      "mamba_spectra.1.conv1d.bias 4608\n",
      "mamba_spectra.1.norm.weight 4096\n",
      "mamba_spectra.1.out_proj.weight 8388608\n",
      "mamba_spectra.2.dt_bias 64\n",
      "mamba_spectra.2.A_log 64\n",
      "mamba_spectra.2.D 64\n",
      "mamba_spectra.2.in_proj.weight 17956864\n",
      "mamba_spectra.2.conv1d.weight 18432\n",
      "mamba_spectra.2.conv1d.bias 4608\n",
      "mamba_spectra.2.norm.weight 4096\n",
      "mamba_spectra.2.out_proj.weight 8388608\n",
      "mamba_spectra.3.dt_bias 64\n",
      "mamba_spectra.3.A_log 64\n",
      "mamba_spectra.3.D 64\n",
      "mamba_spectra.3.in_proj.weight 17956864\n",
      "mamba_spectra.3.conv1d.weight 18432\n",
      "mamba_spectra.3.conv1d.bias 4608\n",
      "mamba_spectra.3.norm.weight 4096\n",
      "mamba_spectra.3.out_proj.weight 8388608\n",
      "mamba_spectra.4.dt_bias 64\n",
      "mamba_spectra.4.A_log 64\n",
      "mamba_spectra.4.D 64\n",
      "mamba_spectra.4.in_proj.weight 17956864\n",
      "mamba_spectra.4.conv1d.weight 18432\n",
      "mamba_spectra.4.conv1d.bias 4608\n",
      "mamba_spectra.4.norm.weight 4096\n",
      "mamba_spectra.4.out_proj.weight 8388608\n",
      "mamba_spectra.5.dt_bias 64\n",
      "mamba_spectra.5.A_log 64\n",
      "mamba_spectra.5.D 64\n",
      "mamba_spectra.5.in_proj.weight 17956864\n",
      "mamba_spectra.5.conv1d.weight 18432\n",
      "mamba_spectra.5.conv1d.bias 4608\n",
      "mamba_spectra.5.norm.weight 4096\n",
      "mamba_spectra.5.out_proj.weight 8388608\n",
      "mamba_spectra.6.dt_bias 64\n",
      "mamba_spectra.6.A_log 64\n",
      "mamba_spectra.6.D 64\n",
      "mamba_spectra.6.in_proj.weight 17956864\n",
      "mamba_spectra.6.conv1d.weight 18432\n",
      "mamba_spectra.6.conv1d.bias 4608\n",
      "mamba_spectra.6.norm.weight 4096\n",
      "mamba_spectra.6.out_proj.weight 8388608\n",
      "mamba_spectra.7.dt_bias 64\n",
      "mamba_spectra.7.A_log 64\n",
      "mamba_spectra.7.D 64\n",
      "mamba_spectra.7.in_proj.weight 17956864\n",
      "mamba_spectra.7.conv1d.weight 18432\n",
      "mamba_spectra.7.conv1d.bias 4608\n",
      "mamba_spectra.7.norm.weight 4096\n",
      "mamba_spectra.7.out_proj.weight 8388608\n",
      "mamba_spectra.8.dt_bias 64\n",
      "mamba_spectra.8.A_log 64\n",
      "mamba_spectra.8.D 64\n",
      "mamba_spectra.8.in_proj.weight 17956864\n",
      "mamba_spectra.8.conv1d.weight 18432\n",
      "mamba_spectra.8.conv1d.bias 4608\n",
      "mamba_spectra.8.norm.weight 4096\n",
      "mamba_spectra.8.out_proj.weight 8388608\n",
      "mamba_spectra.9.dt_bias 64\n",
      "mamba_spectra.9.A_log 64\n",
      "mamba_spectra.9.D 64\n",
      "mamba_spectra.9.in_proj.weight 17956864\n",
      "mamba_spectra.9.conv1d.weight 18432\n",
      "mamba_spectra.9.conv1d.bias 4608\n",
      "mamba_spectra.9.norm.weight 4096\n",
      "mamba_spectra.9.out_proj.weight 8388608\n",
      "mamba_spectra.10.dt_bias 64\n",
      "mamba_spectra.10.A_log 64\n",
      "mamba_spectra.10.D 64\n",
      "mamba_spectra.10.in_proj.weight 17956864\n",
      "mamba_spectra.10.conv1d.weight 18432\n",
      "mamba_spectra.10.conv1d.bias 4608\n",
      "mamba_spectra.10.norm.weight 4096\n",
      "mamba_spectra.10.out_proj.weight 8388608\n",
      "mamba_spectra.11.dt_bias 64\n",
      "mamba_spectra.11.A_log 64\n",
      "mamba_spectra.11.D 64\n",
      "mamba_spectra.11.in_proj.weight 17956864\n",
      "mamba_spectra.11.conv1d.weight 18432\n",
      "mamba_spectra.11.conv1d.bias 4608\n",
      "mamba_spectra.11.norm.weight 4096\n",
      "mamba_spectra.11.out_proj.weight 8388608\n",
      "mamba_spectra.12.dt_bias 64\n",
      "mamba_spectra.12.A_log 64\n",
      "mamba_spectra.12.D 64\n",
      "mamba_spectra.12.in_proj.weight 17956864\n",
      "mamba_spectra.12.conv1d.weight 18432\n",
      "mamba_spectra.12.conv1d.bias 4608\n",
      "mamba_spectra.12.norm.weight 4096\n",
      "mamba_spectra.12.out_proj.weight 8388608\n",
      "mamba_spectra.13.dt_bias 64\n",
      "mamba_spectra.13.A_log 64\n",
      "mamba_spectra.13.D 64\n",
      "mamba_spectra.13.in_proj.weight 17956864\n",
      "mamba_spectra.13.conv1d.weight 18432\n",
      "mamba_spectra.13.conv1d.bias 4608\n",
      "mamba_spectra.13.norm.weight 4096\n",
      "mamba_spectra.13.out_proj.weight 8388608\n",
      "mamba_spectra.14.dt_bias 64\n",
      "mamba_spectra.14.A_log 64\n",
      "mamba_spectra.14.D 64\n",
      "mamba_spectra.14.in_proj.weight 17956864\n",
      "mamba_spectra.14.conv1d.weight 18432\n",
      "mamba_spectra.14.conv1d.bias 4608\n",
      "mamba_spectra.14.norm.weight 4096\n",
      "mamba_spectra.14.out_proj.weight 8388608\n",
      "mamba_spectra.15.dt_bias 64\n",
      "mamba_spectra.15.A_log 64\n",
      "mamba_spectra.15.D 64\n",
      "mamba_spectra.15.in_proj.weight 17956864\n",
      "mamba_spectra.15.conv1d.weight 18432\n",
      "mamba_spectra.15.conv1d.bias 4608\n",
      "mamba_spectra.15.norm.weight 4096\n",
      "mamba_spectra.15.out_proj.weight 8388608\n",
      "mamba_spectra.16.dt_bias 64\n",
      "mamba_spectra.16.A_log 64\n",
      "mamba_spectra.16.D 64\n",
      "mamba_spectra.16.in_proj.weight 17956864\n",
      "mamba_spectra.16.conv1d.weight 18432\n",
      "mamba_spectra.16.conv1d.bias 4608\n",
      "mamba_spectra.16.norm.weight 4096\n",
      "mamba_spectra.16.out_proj.weight 8388608\n",
      "mamba_spectra.17.dt_bias 64\n",
      "mamba_spectra.17.A_log 64\n",
      "mamba_spectra.17.D 64\n",
      "mamba_spectra.17.in_proj.weight 17956864\n",
      "mamba_spectra.17.conv1d.weight 18432\n",
      "mamba_spectra.17.conv1d.bias 4608\n",
      "mamba_spectra.17.norm.weight 4096\n",
      "mamba_spectra.17.out_proj.weight 8388608\n",
      "mamba_spectra.18.dt_bias 64\n",
      "mamba_spectra.18.A_log 64\n",
      "mamba_spectra.18.D 64\n",
      "mamba_spectra.18.in_proj.weight 17956864\n",
      "mamba_spectra.18.conv1d.weight 18432\n",
      "mamba_spectra.18.conv1d.bias 4608\n",
      "mamba_spectra.18.norm.weight 4096\n",
      "mamba_spectra.18.out_proj.weight 8388608\n",
      "mamba_spectra.19.dt_bias 64\n",
      "mamba_spectra.19.A_log 64\n",
      "mamba_spectra.19.D 64\n",
      "mamba_spectra.19.in_proj.weight 17956864\n",
      "mamba_spectra.19.conv1d.weight 18432\n",
      "mamba_spectra.19.conv1d.bias 4608\n",
      "mamba_spectra.19.norm.weight 4096\n",
      "mamba_spectra.19.out_proj.weight 8388608\n",
      "mamba_gaia.0.dt_bias 64\n",
      "mamba_gaia.0.A_log 64\n",
      "mamba_gaia.0.D 64\n",
      "mamba_gaia.0.in_proj.weight 17956864\n",
      "mamba_gaia.0.conv1d.weight 18432\n",
      "mamba_gaia.0.conv1d.bias 4608\n",
      "mamba_gaia.0.norm.weight 4096\n",
      "mamba_gaia.0.out_proj.weight 8388608\n",
      "mamba_gaia.1.dt_bias 64\n",
      "mamba_gaia.1.A_log 64\n",
      "mamba_gaia.1.D 64\n",
      "mamba_gaia.1.in_proj.weight 17956864\n",
      "mamba_gaia.1.conv1d.weight 18432\n",
      "mamba_gaia.1.conv1d.bias 4608\n",
      "mamba_gaia.1.norm.weight 4096\n",
      "mamba_gaia.1.out_proj.weight 8388608\n",
      "mamba_gaia.2.dt_bias 64\n",
      "mamba_gaia.2.A_log 64\n",
      "mamba_gaia.2.D 64\n",
      "mamba_gaia.2.in_proj.weight 17956864\n",
      "mamba_gaia.2.conv1d.weight 18432\n",
      "mamba_gaia.2.conv1d.bias 4608\n",
      "mamba_gaia.2.norm.weight 4096\n",
      "mamba_gaia.2.out_proj.weight 8388608\n",
      "mamba_gaia.3.dt_bias 64\n",
      "mamba_gaia.3.A_log 64\n",
      "mamba_gaia.3.D 64\n",
      "mamba_gaia.3.in_proj.weight 17956864\n",
      "mamba_gaia.3.conv1d.weight 18432\n",
      "mamba_gaia.3.conv1d.bias 4608\n",
      "mamba_gaia.3.norm.weight 4096\n",
      "mamba_gaia.3.out_proj.weight 8388608\n",
      "mamba_gaia.4.dt_bias 64\n",
      "mamba_gaia.4.A_log 64\n",
      "mamba_gaia.4.D 64\n",
      "mamba_gaia.4.in_proj.weight 17956864\n",
      "mamba_gaia.4.conv1d.weight 18432\n",
      "mamba_gaia.4.conv1d.bias 4608\n",
      "mamba_gaia.4.norm.weight 4096\n",
      "mamba_gaia.4.out_proj.weight 8388608\n",
      "mamba_gaia.5.dt_bias 64\n",
      "mamba_gaia.5.A_log 64\n",
      "mamba_gaia.5.D 64\n",
      "mamba_gaia.5.in_proj.weight 17956864\n",
      "mamba_gaia.5.conv1d.weight 18432\n",
      "mamba_gaia.5.conv1d.bias 4608\n",
      "mamba_gaia.5.norm.weight 4096\n",
      "mamba_gaia.5.out_proj.weight 8388608\n",
      "mamba_gaia.6.dt_bias 64\n",
      "mamba_gaia.6.A_log 64\n",
      "mamba_gaia.6.D 64\n",
      "mamba_gaia.6.in_proj.weight 17956864\n",
      "mamba_gaia.6.conv1d.weight 18432\n",
      "mamba_gaia.6.conv1d.bias 4608\n",
      "mamba_gaia.6.norm.weight 4096\n",
      "mamba_gaia.6.out_proj.weight 8388608\n",
      "mamba_gaia.7.dt_bias 64\n",
      "mamba_gaia.7.A_log 64\n",
      "mamba_gaia.7.D 64\n",
      "mamba_gaia.7.in_proj.weight 17956864\n",
      "mamba_gaia.7.conv1d.weight 18432\n",
      "mamba_gaia.7.conv1d.bias 4608\n",
      "mamba_gaia.7.norm.weight 4096\n",
      "mamba_gaia.7.out_proj.weight 8388608\n",
      "mamba_gaia.8.dt_bias 64\n",
      "mamba_gaia.8.A_log 64\n",
      "mamba_gaia.8.D 64\n",
      "mamba_gaia.8.in_proj.weight 17956864\n",
      "mamba_gaia.8.conv1d.weight 18432\n",
      "mamba_gaia.8.conv1d.bias 4608\n",
      "mamba_gaia.8.norm.weight 4096\n",
      "mamba_gaia.8.out_proj.weight 8388608\n",
      "mamba_gaia.9.dt_bias 64\n",
      "mamba_gaia.9.A_log 64\n",
      "mamba_gaia.9.D 64\n",
      "mamba_gaia.9.in_proj.weight 17956864\n",
      "mamba_gaia.9.conv1d.weight 18432\n",
      "mamba_gaia.9.conv1d.bias 4608\n",
      "mamba_gaia.9.norm.weight 4096\n",
      "mamba_gaia.9.out_proj.weight 8388608\n",
      "mamba_gaia.10.dt_bias 64\n",
      "mamba_gaia.10.A_log 64\n",
      "mamba_gaia.10.D 64\n",
      "mamba_gaia.10.in_proj.weight 17956864\n",
      "mamba_gaia.10.conv1d.weight 18432\n",
      "mamba_gaia.10.conv1d.bias 4608\n",
      "mamba_gaia.10.norm.weight 4096\n",
      "mamba_gaia.10.out_proj.weight 8388608\n",
      "mamba_gaia.11.dt_bias 64\n",
      "mamba_gaia.11.A_log 64\n",
      "mamba_gaia.11.D 64\n",
      "mamba_gaia.11.in_proj.weight 17956864\n",
      "mamba_gaia.11.conv1d.weight 18432\n",
      "mamba_gaia.11.conv1d.bias 4608\n",
      "mamba_gaia.11.norm.weight 4096\n",
      "mamba_gaia.11.out_proj.weight 8388608\n",
      "mamba_gaia.12.dt_bias 64\n",
      "mamba_gaia.12.A_log 64\n",
      "mamba_gaia.12.D 64\n",
      "mamba_gaia.12.in_proj.weight 17956864\n",
      "mamba_gaia.12.conv1d.weight 18432\n",
      "mamba_gaia.12.conv1d.bias 4608\n",
      "mamba_gaia.12.norm.weight 4096\n",
      "mamba_gaia.12.out_proj.weight 8388608\n",
      "mamba_gaia.13.dt_bias 64\n",
      "mamba_gaia.13.A_log 64\n",
      "mamba_gaia.13.D 64\n",
      "mamba_gaia.13.in_proj.weight 17956864\n",
      "mamba_gaia.13.conv1d.weight 18432\n",
      "mamba_gaia.13.conv1d.bias 4608\n",
      "mamba_gaia.13.norm.weight 4096\n",
      "mamba_gaia.13.out_proj.weight 8388608\n",
      "mamba_gaia.14.dt_bias 64\n",
      "mamba_gaia.14.A_log 64\n",
      "mamba_gaia.14.D 64\n",
      "mamba_gaia.14.in_proj.weight 17956864\n",
      "mamba_gaia.14.conv1d.weight 18432\n",
      "mamba_gaia.14.conv1d.bias 4608\n",
      "mamba_gaia.14.norm.weight 4096\n",
      "mamba_gaia.14.out_proj.weight 8388608\n",
      "mamba_gaia.15.dt_bias 64\n",
      "mamba_gaia.15.A_log 64\n",
      "mamba_gaia.15.D 64\n",
      "mamba_gaia.15.in_proj.weight 17956864\n",
      "mamba_gaia.15.conv1d.weight 18432\n",
      "mamba_gaia.15.conv1d.bias 4608\n",
      "mamba_gaia.15.norm.weight 4096\n",
      "mamba_gaia.15.out_proj.weight 8388608\n",
      "mamba_gaia.16.dt_bias 64\n",
      "mamba_gaia.16.A_log 64\n",
      "mamba_gaia.16.D 64\n",
      "mamba_gaia.16.in_proj.weight 17956864\n",
      "mamba_gaia.16.conv1d.weight 18432\n",
      "mamba_gaia.16.conv1d.bias 4608\n",
      "mamba_gaia.16.norm.weight 4096\n",
      "mamba_gaia.16.out_proj.weight 8388608\n",
      "mamba_gaia.17.dt_bias 64\n",
      "mamba_gaia.17.A_log 64\n",
      "mamba_gaia.17.D 64\n",
      "mamba_gaia.17.in_proj.weight 17956864\n",
      "mamba_gaia.17.conv1d.weight 18432\n",
      "mamba_gaia.17.conv1d.bias 4608\n",
      "mamba_gaia.17.norm.weight 4096\n",
      "mamba_gaia.17.out_proj.weight 8388608\n",
      "mamba_gaia.18.dt_bias 64\n",
      "mamba_gaia.18.A_log 64\n",
      "mamba_gaia.18.D 64\n",
      "mamba_gaia.18.in_proj.weight 17956864\n",
      "mamba_gaia.18.conv1d.weight 18432\n",
      "mamba_gaia.18.conv1d.bias 4608\n",
      "mamba_gaia.18.norm.weight 4096\n",
      "mamba_gaia.18.out_proj.weight 8388608\n",
      "mamba_gaia.19.dt_bias 64\n",
      "mamba_gaia.19.A_log 64\n",
      "mamba_gaia.19.D 64\n",
      "mamba_gaia.19.in_proj.weight 17956864\n",
      "mamba_gaia.19.conv1d.weight 18432\n",
      "mamba_gaia.19.conv1d.bias 4608\n",
      "mamba_gaia.19.norm.weight 4096\n",
      "mamba_gaia.19.out_proj.weight 8388608\n",
      "cross_attn_block_spectra.norm.weight 2048\n",
      "cross_attn_block_spectra.norm.bias 2048\n",
      "cross_attn_block_spectra.attention.in_proj_weight 12582912\n",
      "cross_attn_block_spectra.attention.in_proj_bias 6144\n",
      "cross_attn_block_spectra.attention.out_proj.weight 4194304\n",
      "cross_attn_block_spectra.attention.out_proj.bias 2048\n",
      "cross_attn_block_gaia.norm.weight 2048\n",
      "cross_attn_block_gaia.norm.bias 2048\n",
      "cross_attn_block_gaia.attention.in_proj_weight 12582912\n",
      "cross_attn_block_gaia.attention.in_proj_bias 6144\n",
      "cross_attn_block_gaia.attention.out_proj.weight 4194304\n",
      "cross_attn_block_gaia.attention.out_proj.bias 2048\n",
      "classifier.0.weight 4096\n",
      "classifier.0.bias 4096\n",
      "classifier.1.weight 225280\n",
      "classifier.1.bias 55\n",
      "Total number of parameters: 1088863799\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td></td></tr><tr><td>hamming_loss</td><td></td></tr><tr><td>lr</td><td></td></tr><tr><td>macro_f1</td><td></td></tr><tr><td>macro_precision</td><td></td></tr><tr><td>macro_recall</td><td></td></tr><tr><td>micro_f1</td><td></td></tr><tr><td>micro_precision</td><td></td></tr><tr><td>micro_recall</td><td></td></tr><tr><td>test_acc</td><td></td></tr><tr><td>test_loss</td><td></td></tr><tr><td>train_acc</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>val_acc</td><td></td></tr><tr><td>val_loss</td><td></td></tr><tr><td>weighted_f1</td><td></td></tr><tr><td>weighted_precision</td><td></td></tr><tr><td>weighted_recall</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>476</td></tr><tr><td>hamming_loss</td><td>0.02393</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>macro_f1</td><td>0.38904</td></tr><tr><td>macro_precision</td><td>0.8911</td></tr><tr><td>macro_recall</td><td>0.33105</td></tr><tr><td>micro_f1</td><td>0.56739</td></tr><tr><td>micro_precision</td><td>0.83712</td></tr><tr><td>micro_recall</td><td>0.42913</td></tr><tr><td>test_acc</td><td>0.97607</td></tr><tr><td>test_loss</td><td>0.07114</td></tr><tr><td>train_acc</td><td>0.98895</td></tr><tr><td>train_loss</td><td>0.01347</td></tr><tr><td>val_acc</td><td>0.97618</td></tr><tr><td>val_loss</td><td>0.05095</td></tr><tr><td>weighted_f1</td><td>0.52905</td></tr><tr><td>weighted_precision</td><td>0.88934</td></tr><tr><td>weighted_recall</td><td>0.42913</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sandy-energy-14</strong> at: <a href='https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_mamba_v2/runs/zst2sggk' target=\"_blank\">https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_mamba_v2/runs/zst2sggk</a><br> View project at: <a href='https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_mamba_v2' target=\"_blank\">https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_multimodal_fusion_mamba_v2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250407_203003-zst2sggk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Clean Cache of GPU\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Example config\n",
    "    d_model_spectra = 2048\n",
    "    d_model_gaia = 2048\n",
    "    num_classes = 55\n",
    "    input_dim_spectra = 3647\n",
    "    input_dim_gaia = 18\n",
    "    n_layers = 20\n",
    "    d_state = 32 # State dimension for MAMBA\n",
    "    d_conv = 2  # Convolution dimension for MAMBA\n",
    "    expand = 2  # Expansion factor for MAMBA\n",
    "\n",
    "    # Token dimensions - these control the sequence length\n",
    "    token_dim_spectra = 3647  # Will create 1 token for spectra (3647/3647)\n",
    "    token_dim_gaia = 18       # Will create 1 token for gaia (18/1)\n",
    "\n",
    "    lr = 2.5e-6\n",
    "    patience = 200\n",
    "    num_epochs = 800\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(\"Device:\", device)\n",
    "\n",
    "    # Initialize WandB\n",
    "    wandb.init(project=\"ALLSTARS_multimodal_fusion_mamba_v2\")\n",
    "    \n",
    "    config = {\n",
    "        \"num_classes\": num_classes,\n",
    "        \"d_model_spectra\": d_model_spectra,\n",
    "        \"d_model_gaia\": d_model_gaia,\n",
    "        \"input_dim_spectra\": input_dim_spectra,\n",
    "        \"input_dim_gaia\": input_dim_gaia,\n",
    "        \"n_layers\": n_layers,\n",
    "        \"lr\": lr,\n",
    "        \"patience\": patience,\n",
    "        \"num_epochs\": num_epochs\n",
    "    }\n",
    "    wandb.config.update(config)\n",
    "\n",
    "    # Instantiate the fusion model\n",
    "    # Try use_cross_attention=False for late-fusion, True for cross-attention\n",
    "    model_fusion = StarClassifierFusionMambaTokenized(\n",
    "        d_model_spectra=d_model_spectra,\n",
    "        d_model_gaia=d_model_gaia,\n",
    "        num_classes=num_classes,\n",
    "        input_dim_spectra=input_dim_spectra,\n",
    "        input_dim_gaia=input_dim_gaia,\n",
    "        n_layers=n_layers,\n",
    "        use_cross_attention=True,  # set to False to compare with late fusion\n",
    "        n_cross_attn_heads=8\n",
    "    )\n",
    "    model_fusion.to(device)\n",
    "\n",
    "    # Print size of model in GB\n",
    "    print(f\"Model size: {sum(p.numel() for p in model_fusion.parameters()) / 1e9:.2f} GB\")\n",
    "    param_size = 0\n",
    "    for param in model_fusion.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    buffer_size = 0\n",
    "    for buffer in model_fusion.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "    size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "    print('model size: {:.3f}MB'.format(size_all_mb))\n",
    "    # Compute parameter size\n",
    "    param_size = sum(p.nelement() * p.element_size() for p in model_fusion.parameters())\n",
    "\n",
    "    # Compute buffer size\n",
    "    buffer_size = sum(b.nelement() * b.element_size() for b in model_fusion.buffers())\n",
    "\n",
    "    # Total size in MB\n",
    "    total_size_mb = (param_size + buffer_size) / (1024 ** 2)\n",
    "    print(f\"Model size: {total_size_mb:.3f} MB\")\n",
    "\n",
    "    print(model_fusion)\n",
    "    # print number of parameters per layer\n",
    "    for name, param in model_fusion.named_parameters():\n",
    "        print(name, param.numel())\n",
    "    print(\"Total number of parameters:\", sum(p.numel() for p in model_fusion.parameters() if p.requires_grad))\n",
    "\n",
    "    # Train the fusion model\n",
    "    trained_fusion_model = train_model_fusion(\n",
    "        model=model_fusion,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader,\n",
    "        num_epochs=num_epochs,\n",
    "        lr=lr,\n",
    "        max_patience=patience,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "# Save the model\n",
    "torch.save(trained_fusion_model.state_dict(), \"Models/model_fusion_mamba_1_token.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
