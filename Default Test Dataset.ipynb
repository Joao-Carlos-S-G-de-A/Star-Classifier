{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from astroquery.gaia import Gaia\n",
    "from astroquery.simbad import Simbad\n",
    "from astropy.coordinates import SkyCoord\n",
    "import astropy.units as u\n",
    "import numpy as np\n",
    "import pyvo as vo\n",
    "import pickle\n",
    "from astroquery.vizier import Vizier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetGAIAData(GaiaDR3SourceIDs):\n",
    "    try:\n",
    "        qry = f\"SELECT * FROM gaiadr3.gaia_source gs WHERE gs.source_id in ({GaiaDR3SourceIDs});\"\n",
    "        job = Gaia.launch_job_async(qry)\n",
    "        tblGaia = job.get_results()\n",
    "        dfGaia = tblGaia.to_pandas()\n",
    "        return dfGaia\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return pd.DataFrame()  # Return an empty DataFrame in case of error\n",
    "\n",
    "def split_ids_into_chunks(id_string, chunk_size=50000):\n",
    "    id_list = id_string.split(', ')\n",
    "    chunks = [', '.join(id_list[i:i + chunk_size]) for i in range(0, len(id_list), chunk_size)]\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting AGN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting SIMBAD labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(136855, 2)\n",
      "(819002, 2)\n"
     ]
    }
   ],
   "source": [
    "criteria_ = \"otype = 'AGN..' \"\n",
    "\n",
    "# Initialize SIMBAD\n",
    "simbad = Simbad()\n",
    "simbad.ROW_LIMIT = -1\n",
    "simbad.add_votable_fields(\"ids\", \"otype\")\n",
    "\n",
    "# Define coordinates (random)\n",
    "coordinates = SkyCoord(0, -90, unit=(\"deg\", \"deg\"))\n",
    "coordinates2 = SkyCoord(0, 90, unit=(\"deg\", \"deg\"))\n",
    "\n",
    "# Query region 1st half\n",
    "result = simbad.query_region(coordinates, radius=\"90d0m\",\n",
    "                           #  criteria=\"otype = 'Ma*..' OR otype = 'MS*..' OR otype = 'Y*O..' OR otype = 'Ev*..' \")\n",
    "                           criteria=criteria_)\n",
    "filtered_result = result[\"ids\", \"otype\"] # Keep only the columns we need\n",
    "result_df = filtered_result.to_pandas() # Convert result to a Pandas DataFrame\n",
    "print(result_df.shape)\n",
    "filtered_result = result_df.dropna() # Remove rows with any NULL values\n",
    "data1 = filtered_result.to_numpy() # Convert to numpy array\n",
    "\n",
    "\n",
    "# Query region 2nd half\n",
    "result2 = simbad.query_region(coordinates2, radius=\"90d0m\",\n",
    "                             #criteria=\"otype = 'Ma*..' OR otype = 'MS*..' OR otype = 'Y*O..' OR otype = 'Ev*..' \")\n",
    "                             criteria=criteria_)\n",
    "filtered_result2 = result2[\"ids\", \"otype\"] # Keep only the columns we need\n",
    "result_df2 = filtered_result2.to_pandas() # Convert result to a Pandas DataFrame\n",
    "print(result_df2.shape)\n",
    "filtered_result2 = result_df2.dropna() # Remove rows with any NULL values\n",
    "data2 = filtered_result2.to_numpy() # Convert to numpy array\n",
    "\n",
    "# Combine the two arrays & save\n",
    "#otypes_agn = np.row_stack((data1, data2))\n",
    "#np.save(\"fullsky4catsunfiltered\", simbad_data)\n",
    "\n",
    "# without filtering\n",
    "otypes_agn = pd.concat([result_df, result_df2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting coresponding Gaia data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jcwin\\AppData\\Local\\Temp\\ipykernel_10088\\1695916639.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  simbad_data['gaia_id'] = simbad_data['gaia_id'].astype(str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jcwin\\AppData\\Local\\Temp\\ipykernel_10088\\1695916639.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  gaia_data['source_id'] = gaia_data['source_id'].astype(str)\n"
     ]
    }
   ],
   "source": [
    "# Concatenate the DataFrames\n",
    "simbad_data = otypes_agn\n",
    "\n",
    "# Filter SIMBAD data to only include rows where 'ids' contains 'Gaia DR3'\n",
    "simbad_data['gaia_id'] = simbad_data['ids'].apply(lambda x: next((id for id in x.split('|') if id.startswith('Gaia DR3')), None))\n",
    "\n",
    "# Remove 'Gaia DR3' prefix and drop rows with NaN values in 'gaia_id'\n",
    "simbad_data['gaia_id'] = simbad_data['gaia_id'].str.lstrip('Gaia DR3')\n",
    "simbad_data = simbad_data.dropna(subset=['gaia_id'])\n",
    "\n",
    "# Ensure 'gaia_id' is a string\n",
    "simbad_data['gaia_id'] = simbad_data['gaia_id'].astype(str)\n",
    "\n",
    "# Split the Gaia DR3 source IDs into chunks of 30,000 IDs each\n",
    "GaiaDR3SourceIDs = ', '.join(simbad_data['gaia_id'].astype(str))\n",
    "chunks = split_ids_into_chunks(GaiaDR3SourceIDs)\n",
    "\n",
    "# Initialize an empty DataFrame to store all data\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "# Process each chunk and append the results to the combined DataFrame\n",
    "for chunk in chunks:\n",
    "    dfGaia = GetGAIAData(chunk)\n",
    "    combined_df = pd.concat([combined_df, dfGaia], ignore_index=True)\n",
    "\n",
    "# Convert the combined DataFrame to a NumPy array if needed\n",
    "combined_matrix = combined_df.to_numpy()\n",
    "\n",
    "# GAIA data\n",
    "gaia_data = combined_df[[\"source_id\", \"ra\", \"ra_error\", \"dec\", \"dec_error\", \"phot_g_mean_flux\", \"phot_g_mean_flux_error\", \"pmra\", \"pmra_error\", \"pmdec\", \"pmdec_error\", \"parallax\", \"parallax_error\", \"phot_bp_mean_flux\", \"phot_bp_mean_flux_error\", \"phot_rp_mean_flux\", \"phot_rp_mean_flux_error\"]]\n",
    "\n",
    "# Convert Gaia source_id to string\n",
    "gaia_data['source_id'] = gaia_data['source_id'].astype(str)\n",
    "\n",
    "# Merge Gaia and SIMBAD data on matching IDs\n",
    "agn_data = pd.merge(gaia_data, simbad_data, left_on='source_id', right_on='gaia_id', how='inner')\n",
    "agn_data.to_pickle(\"agn_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"agn_data.pkl\", \"rb\") as f:\n",
    "    agn_data = pickle.load(f)\n",
    "agn_data_high_otype = agn_data.copy()\n",
    "agn_data_high_otype = agn_data_high_otype.assign(otype= \"AGN\")\n",
    "agn_data_high_otype = agn_data_high_otype[[\"source_id\", \"ra\", \"ra_error\", \"dec\", \"dec_error\", \"phot_g_mean_flux\", \"phot_g_mean_flux_error\", \"pmra\", \"pmra_error\", \"pmdec\", \"pmdec_error\", \"parallax\", \"parallax_error\", \"phot_bp_mean_flux\", \"phot_bp_mean_flux_error\", \"phot_rp_mean_flux\", \"phot_rp_mean_flux_error\", \"otype\"]]\n",
    "agn_data_high_otype.to_pickle(\"agn_data_high_otype.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting non-AGN Galaxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57267, 2)\n",
      "(42307, 2)\n"
     ]
    }
   ],
   "source": [
    "criteria_ = \"otype = 'LSB..' OR otype = 'bCG..' OR otype = 'SBG..' OR otype = 'H2G..' OR otype = 'EmG..'\"\n",
    "\n",
    "# Initialize SIMBAD\n",
    "simbad = Simbad()\n",
    "simbad.ROW_LIMIT = -1\n",
    "simbad.add_votable_fields(\"ids\", \"otype\")\n",
    "\n",
    "# Define coordinates (random)\n",
    "coordinates = SkyCoord(0, -90, unit=(\"deg\", \"deg\"))\n",
    "coordinates2 = SkyCoord(0, 90, unit=(\"deg\", \"deg\"))\n",
    "\n",
    "# Query region 1st half\n",
    "result = simbad.query_region(coordinates, radius=\"90d0m\",\n",
    "                           #  criteria=\"otype = 'Ma*..' OR otype = 'MS*..' OR otype = 'Y*O..' OR otype = 'Ev*..' \")\n",
    "                           criteria=criteria_)\n",
    "filtered_result = result[\"ids\", \"otype\"] # Keep only the columns we need\n",
    "result_df = filtered_result.to_pandas() # Convert result to a Pandas DataFrame\n",
    "print(result_df.shape)\n",
    "filtered_result = result_df.dropna() # Remove rows with any NULL values\n",
    "data1 = filtered_result.to_numpy() # Convert to numpy array\n",
    "\n",
    "\n",
    "# Query region 2nd half\n",
    "result2 = simbad.query_region(coordinates2, radius=\"90d0m\",\n",
    "                             #criteria=\"otype = 'Ma*..' OR otype = 'MS*..' OR otype = 'Y*O..' OR otype = 'Ev*..' \")\n",
    "                             criteria=criteria_)\n",
    "filtered_result2 = result2[\"ids\", \"otype\"] # Keep only the columns we need\n",
    "result_df2 = filtered_result2.to_pandas() # Convert result to a Pandas DataFrame\n",
    "print(result_df2.shape)\n",
    "filtered_result2 = result_df2.dropna() # Remove rows with any NULL values\n",
    "data2 = filtered_result2.to_numpy() # Convert to numpy array\n",
    "\n",
    "# Combine the two arrays & save\n",
    "#otypes_agn = np.row_stack((data1, data2))\n",
    "#np.save(\"fullsky4catsunfiltered\", simbad_data)\n",
    "\n",
    "# without filtering\n",
    "otypes_gal = pd.concat([result_df, result_df2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting corresponding gaia data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jcwin\\AppData\\Local\\Temp\\ipykernel_10088\\1415820394.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  simbad_data['gaia_id'] = simbad_data['gaia_id'].astype(str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Query finished. [astroquery.utils.tap.core]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jcwin\\AppData\\Local\\Temp\\ipykernel_10088\\1415820394.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  gaia_data['source_id'] = gaia_data['source_id'].astype(str)\n"
     ]
    }
   ],
   "source": [
    "# Concatenate the DataFrames\n",
    "simbad_data = otypes_gal\n",
    "\n",
    "# Filter SIMBAD data to only include rows where 'ids' contains 'Gaia DR3'\n",
    "simbad_data['gaia_id'] = simbad_data['ids'].apply(lambda x: next((id for id in x.split('|') if id.startswith('Gaia DR3')), None))\n",
    "\n",
    "# Remove 'Gaia DR3' prefix and drop rows with NaN values in 'gaia_id'\n",
    "simbad_data['gaia_id'] = simbad_data['gaia_id'].str.lstrip('Gaia DR3')\n",
    "simbad_data = simbad_data.dropna(subset=['gaia_id'])\n",
    "\n",
    "# Ensure 'gaia_id' is a string\n",
    "simbad_data['gaia_id'] = simbad_data['gaia_id'].astype(str)\n",
    "\n",
    "# Split the Gaia DR3 source IDs into chunks of 30,000 IDs each\n",
    "GaiaDR3SourceIDs = ', '.join(simbad_data['gaia_id'].astype(str))\n",
    "chunks = split_ids_into_chunks(GaiaDR3SourceIDs)\n",
    "\n",
    "# Initialize an empty DataFrame to store all data\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "# Process each chunk and append the results to the combined DataFrame\n",
    "for chunk in chunks:\n",
    "    dfGaia = GetGAIAData(chunk)\n",
    "    combined_df = pd.concat([combined_df, dfGaia], ignore_index=True)\n",
    "\n",
    "# Convert the combined DataFrame to a NumPy array if needed\n",
    "combined_matrix = combined_df.to_numpy()\n",
    "\n",
    "# GAIA data\n",
    "gaia_data = combined_df[[\"source_id\", \"ra\", \"ra_error\", \"dec\", \"dec_error\", \"phot_g_mean_flux\", \"phot_g_mean_flux_error\", \"pmra\", \"pmra_error\", \"pmdec\", \"pmdec_error\", \"parallax\", \"parallax_error\", \"phot_bp_mean_flux\", \"phot_bp_mean_flux_error\", \"phot_rp_mean_flux\", \"phot_rp_mean_flux_error\"]]\n",
    "\n",
    "# Convert Gaia source_id to string\n",
    "gaia_data['source_id'] = gaia_data['source_id'].astype(str)\n",
    "\n",
    "# Merge Gaia and SIMBAD data on matching IDs\n",
    "gal_data = pd.merge(gaia_data, simbad_data, left_on='source_id', right_on='gaia_id', how='inner')\n",
    "gal_data.to_pickle(\"gal_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33531, 18)\n"
     ]
    }
   ],
   "source": [
    "with open(\"gal_data.pkl\", \"rb\") as f:\n",
    "    gal_data = pickle.load(f)\n",
    "gal_data_high_otype = gal_data.copy()\n",
    "gal_data_high_otype = gal_data_high_otype.assign(otype= \"GAL\") \n",
    "gal_data_high_otype = gal_data_high_otype[[\"source_id\", \"ra\", \"ra_error\", \"dec\", \"dec_error\", \"phot_g_mean_flux\", \"phot_g_mean_flux_error\", \"pmra\", \"pmra_error\", \"pmdec\", \"pmdec_error\", \"parallax\", \"parallax_error\", \"phot_bp_mean_flux\", \"phot_bp_mean_flux_error\", \"phot_rp_mean_flux\", \"phot_rp_mean_flux_error\", \"otype\"]]\n",
    "print(gal_data_high_otype.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Binaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1446685, 2)\n",
      "(917897, 2)\n"
     ]
    }
   ],
   "source": [
    "criteria_ = \"otype = '**..'\"\n",
    "\n",
    "# Initialize SIMBAD\n",
    "simbad = Simbad()\n",
    "simbad.ROW_LIMIT = -1\n",
    "simbad.add_votable_fields(\"ids\", \"otype\")\n",
    "\n",
    "# Define coordinates (random)\n",
    "coordinates = SkyCoord(0, -90, unit=(\"deg\", \"deg\"))\n",
    "coordinates2 = SkyCoord(0, 90, unit=(\"deg\", \"deg\"))\n",
    "\n",
    "# Query region 1st half\n",
    "result = simbad.query_region(coordinates, radius=\"90d0m\",\n",
    "                           #  criteria=\"otype = 'Ma*..' OR otype = 'MS*..' OR otype = 'Y*O..' OR otype = 'Ev*..' \")\n",
    "                           criteria=criteria_)\n",
    "filtered_result = result[\"ids\", \"otype\"] # Keep only the columns we need\n",
    "result_df = filtered_result.to_pandas() # Convert result to a Pandas DataFrame\n",
    "print(result_df.shape)\n",
    "filtered_result = result_df.dropna() # Remove rows with any NULL values\n",
    "data1 = filtered_result.to_numpy() # Convert to numpy array\n",
    "\n",
    "\n",
    "# Query region 2nd half\n",
    "result2 = simbad.query_region(coordinates2, radius=\"90d0m\",\n",
    "                             #criteria=\"otype = 'Ma*..' OR otype = 'MS*..' OR otype = 'Y*O..' OR otype = 'Ev*..' \")\n",
    "                             criteria=criteria_)\n",
    "filtered_result2 = result2[\"ids\", \"otype\"] # Keep only the columns we need\n",
    "result_df2 = filtered_result2.to_pandas() # Convert result to a Pandas DataFrame\n",
    "print(result_df2.shape)\n",
    "filtered_result2 = result_df2.dropna() # Remove rows with any NULL values\n",
    "data2 = filtered_result2.to_numpy() # Convert to numpy array\n",
    "\n",
    "# Combine the two arrays & save\n",
    "#otypes_agn = np.row_stack((data1, data2))\n",
    "#np.save(\"fullsky4catsunfiltered\", simbad_data)\n",
    "\n",
    "# without filtering\n",
    "otypes_bin = pd.concat([result_df, result_df2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting coresponding gaia data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jcwin\\AppData\\Local\\Temp\\ipykernel_10088\\249499439.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  simbad_data['gaia_id'] = simbad_data['gaia_id'].astype(str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jcwin\\AppData\\Local\\Temp\\ipykernel_10088\\249499439.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  gaia_data['source_id'] = gaia_data['source_id'].astype(str)\n"
     ]
    }
   ],
   "source": [
    "# Concatenate the DataFrames\n",
    "simbad_data = otypes_bin\n",
    "\n",
    "# Filter SIMBAD data to only include rows where 'ids' contains 'Gaia DR3'\n",
    "simbad_data['gaia_id'] = simbad_data['ids'].apply(lambda x: next((id for id in x.split('|') if id.startswith('Gaia DR3')), None))\n",
    "\n",
    "# Remove 'Gaia DR3' prefix and drop rows with NaN values in 'gaia_id'\n",
    "simbad_data['gaia_id'] = simbad_data['gaia_id'].str.lstrip('Gaia DR3')\n",
    "simbad_data = simbad_data.dropna(subset=['gaia_id'])\n",
    "\n",
    "# Ensure 'gaia_id' is a string\n",
    "simbad_data['gaia_id'] = simbad_data['gaia_id'].astype(str)\n",
    "\n",
    "# Split the Gaia DR3 source IDs into chunks of 30,000 IDs each\n",
    "GaiaDR3SourceIDs = ', '.join(simbad_data['gaia_id'].astype(str))\n",
    "chunks = split_ids_into_chunks(GaiaDR3SourceIDs)\n",
    "\n",
    "# Initialize an empty DataFrame to store all data\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "# Process each chunk and append the results to the combined DataFrame\n",
    "for chunk in chunks:\n",
    "    dfGaia = GetGAIAData(chunk)\n",
    "    combined_df = pd.concat([combined_df, dfGaia], ignore_index=True)\n",
    "\n",
    "# Convert the combined DataFrame to a NumPy array if needed\n",
    "combined_matrix = combined_df.to_numpy()\n",
    "\n",
    "# GAIA data\n",
    "gaia_data = combined_df[[\"source_id\", \"ra\", \"ra_error\", \"dec\", \"dec_error\", \"phot_g_mean_flux\", \"phot_g_mean_flux_error\", \"pmra\", \"pmra_error\", \"pmdec\", \"pmdec_error\", \"parallax\", \"parallax_error\", \"phot_bp_mean_flux\", \"phot_bp_mean_flux_error\", \"phot_rp_mean_flux\", \"phot_rp_mean_flux_error\"]]\n",
    "\n",
    "# Convert Gaia source_id to string\n",
    "gaia_data['source_id'] = gaia_data['source_id'].astype(str)\n",
    "\n",
    "# Merge Gaia and SIMBAD data on matching IDs\n",
    "bin_data = pd.merge(gaia_data, simbad_data, left_on='source_id', right_on='gaia_id', how='inner')\n",
    "bin_data.to_pickle(\"bin_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1700440, 18)\n"
     ]
    }
   ],
   "source": [
    "with open(\"bin_data.pkl\", \"rb\") as f:\n",
    "    bin_data = pickle.load(f)\n",
    "bin_data_high_otype = bin_data.copy()\n",
    "bin_data_high_otype = bin_data_high_otype.assign(otype= \"BIN\")\n",
    "bin_data_high_otype = bin_data_high_otype[[\"source_id\", \"ra\", \"ra_error\", \"dec\", \"dec_error\", \"phot_g_mean_flux\", \"phot_g_mean_flux_error\", \"pmra\", \"pmra_error\", \"pmdec\", \"pmdec_error\", \"parallax\", \"parallax_error\", \"phot_bp_mean_flux\", \"phot_bp_mean_flux_error\", \"phot_rp_mean_flux\", \"phot_rp_mean_flux_error\", \"otype\"]]\n",
    "print(bin_data_high_otype.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting data for single stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1237922, 2)\n",
      "(810162, 2)\n"
     ]
    }
   ],
   "source": [
    "criteria_ = \"otype = 'Ma*..' OR otype = 'MS*..' OR otype = 'Y*O..' OR otype = 'Ev*..' OR otype = 'Pe*..' OR otype = 'SN*' OR otype = 'LM*' OR otype = 'V*' OR otype = 'Em*' OR otype = 'PM*' OR otype = 'HV*'\"\n",
    "\n",
    "# Initialize SIMBAD\n",
    "simbad = Simbad()\n",
    "simbad.ROW_LIMIT = -1\n",
    "simbad.add_votable_fields(\"ids\", \"otype\")\n",
    "\n",
    "# Define coordinates (random)\n",
    "coordinates = SkyCoord(0, -90, unit=(\"deg\", \"deg\"))\n",
    "coordinates2 = SkyCoord(0, 90, unit=(\"deg\", \"deg\"))\n",
    "\n",
    "# Query region 1st half\n",
    "result = simbad.query_region(coordinates, radius=\"90d0m\",\n",
    "                           #  criteria=\"otype = 'Ma*..' OR otype = 'MS*..' OR otype = 'Y*O..' OR otype = 'Ev*..' \")\n",
    "                           criteria=criteria_)\n",
    "filtered_result = result[\"ids\", \"otype\"] # Keep only the columns we need\n",
    "result_df = filtered_result.to_pandas() # Convert result to a Pandas DataFrame\n",
    "print(result_df.shape)\n",
    "filtered_result = result_df.dropna() # Remove rows with any NULL values\n",
    "data1 = filtered_result.to_numpy() # Convert to numpy array\n",
    "\n",
    "\n",
    "# Query region 2nd half\n",
    "result2 = simbad.query_region(coordinates2, radius=\"90d0m\",\n",
    "                             #criteria=\"otype = 'Ma*..' OR otype = 'MS*..' OR otype = 'Y*O..' OR otype = 'Ev*..' \")\n",
    "                             criteria=criteria_)\n",
    "filtered_result2 = result2[\"ids\", \"otype\"] # Keep only the columns we need\n",
    "result_df2 = filtered_result2.to_pandas() # Convert result to a Pandas DataFrame\n",
    "print(result_df2.shape)\n",
    "filtered_result2 = result_df2.dropna() # Remove rows with any NULL values\n",
    "data2 = filtered_result2.to_numpy() # Convert to numpy array\n",
    "\n",
    "# Combine the two arrays & save\n",
    "#otypes_agn = np.row_stack((data1, data2))\n",
    "#np.save(\"fullsky4catsunfiltered\", simbad_data)\n",
    "\n",
    "# without filtering\n",
    "otypes_star = pd.concat([result_df, result_df2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting coresponding gaia data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jcwin\\AppData\\Local\\Temp\\ipykernel_24756\\1499858909.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  simbad_data['gaia_id'] = simbad_data['gaia_id'].astype(str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jcwin\\AppData\\Local\\Temp\\ipykernel_24756\\1499858909.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  gaia_data['source_id'] = gaia_data['source_id'].astype(str)\n"
     ]
    }
   ],
   "source": [
    "# Concatenate the DataFrames\n",
    "simbad_data = otypes_star\n",
    "\n",
    "# Filter SIMBAD data to only include rows where 'ids' contains 'Gaia DR3'\n",
    "simbad_data['gaia_id'] = simbad_data['ids'].apply(lambda x: next((id for id in x.split('|') if id.startswith('Gaia DR3')), None))\n",
    "\n",
    "# Remove 'Gaia DR3' prefix and drop rows with NaN values in 'gaia_id'\n",
    "simbad_data['gaia_id'] = simbad_data['gaia_id'].str.lstrip('Gaia DR3')\n",
    "simbad_data = simbad_data.dropna(subset=['gaia_id'])\n",
    "\n",
    "# Ensure 'gaia_id' is a string\n",
    "simbad_data['gaia_id'] = simbad_data['gaia_id'].astype(str)\n",
    "\n",
    "# Split the Gaia DR3 source IDs into chunks of 30,000 IDs each\n",
    "GaiaDR3SourceIDs = ', '.join(simbad_data['gaia_id'].astype(str))\n",
    "chunks = split_ids_into_chunks(GaiaDR3SourceIDs)\n",
    "\n",
    "# Initialize an empty DataFrame to store all data\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "# Process each chunk and append the results to the combined DataFrame\n",
    "for chunk in chunks:\n",
    "    dfGaia = GetGAIAData(chunk)\n",
    "    combined_df = pd.concat([combined_df, dfGaia], ignore_index=True)\n",
    "\n",
    "# Convert the combined DataFrame to a NumPy array if needed\n",
    "combined_matrix = combined_df.to_numpy()\n",
    "\n",
    "# GAIA data\n",
    "gaia_data = combined_df[[\"source_id\", \"ra\", \"ra_error\", \"dec\", \"dec_error\", \"phot_g_mean_flux\", \"phot_g_mean_flux_error\", \"pmra\", \"pmra_error\", \"pmdec\", \"pmdec_error\", \"parallax\", \"parallax_error\", \"phot_bp_mean_flux\", \"phot_bp_mean_flux_error\", \"phot_rp_mean_flux\", \"phot_rp_mean_flux_error\"]]\n",
    "\n",
    "# Convert Gaia source_id to string\n",
    "gaia_data['source_id'] = gaia_data['source_id'].astype(str)\n",
    "\n",
    "# Merge Gaia and SIMBAD data on matching IDs\n",
    "star_data = pd.merge(gaia_data, simbad_data, left_on='source_id', right_on='gaia_id', how='inner')\n",
    "star_data.to_pickle(\"star_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1499508, 18)\n"
     ]
    }
   ],
   "source": [
    "with open('star_data.pkl', 'rb') as file:\n",
    "    star_data = pickle.load(file)\n",
    "star_data_high_otype = star_data.copy()\n",
    "star_data_high_otype = star_data_high_otype.assign(otype= \"STAR\")\n",
    "star_data_high_otype = star_data_high_otype[[\"source_id\", \"ra\", \"ra_error\", \"dec\", \"dec_error\", \"phot_g_mean_flux\", \"phot_g_mean_flux_error\", \"pmra\", \"pmra_error\", \"pmdec\", \"pmdec_error\", \"parallax\", \"parallax_error\", \"phot_bp_mean_flux\", \"phot_bp_mean_flux_error\", \"phot_rp_mean_flux\", \"phot_rp_mean_flux_error\", \"otype\"]]\n",
    "print(star_data_high_otype.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive way combining all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3267794, 18)\n"
     ]
    }
   ],
   "source": [
    "all_data_high_otype = pd.concat([agn_data_high_otype, gal_data_high_otype, bin_data_high_otype, star_data_high_otype])\n",
    "all_data_high_otype = all_data_high_otype.dropna()  \n",
    "np.save(\"all_data_high_otype\", all_data_high_otype.to_numpy())\n",
    "print(all_data_high_otype.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting equal amounts of data for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14071, 18)\n"
     ]
    }
   ],
   "source": [
    "n = min(agn_data_high_otype.dropna().shape[0], gal_data_high_otype.dropna().shape[0], bin_data_high_otype.dropna().shape[0], star_data_high_otype.dropna().shape[0])\n",
    "equal_data_high_otype = pd.concat([agn_data_high_otype.sample(n=n), gal_data_high_otype, bin_data_high_otype.sample(n=n), star_data_high_otype.sample(n=n)])\n",
    "equal_data_high_otype = equal_data_high_otype.dropna()\n",
    "np.save(\"equal_data_high_otype\", equal_data_high_otype.to_numpy())\n",
    "print(equal_data_high_otype.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using all except galaxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(949731, 18)\n"
     ]
    }
   ],
   "source": [
    "n = min(agn_data_high_otype.dropna().shape[0], bin_data_high_otype.dropna().shape[0], star_data_high_otype.dropna().shape[0])\n",
    "no_gal_high_otype = pd.concat([agn_data_high_otype.dropna().sample(n=n), bin_data_high_otype.dropna().sample(n=n), star_data_high_otype.dropna().sample(n=n)])\n",
    "#equal_data_high_otype = equal_data_high_otype.dropna()\n",
    "np.save(\"no_gal_high_otype\",no_gal_high_otype.to_numpy())\n",
    "print(no_gal_high_otype.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_gal_high_otype = pd.concat([agn_data_high_otype, bin_data_high_otype, star_data_high_otype])\n",
    "no_gal_high_otype = no_gal_high_otype.dropna()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'17'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# filter the rows that contain the substring\u001b[39;00m\n\u001b[0;32m      3\u001b[0m substring \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGAL\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28mfilter\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mdel_gal\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m17\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mcontains(substring)\n\u001b[0;32m      5\u001b[0m filtered_df \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;241m~\u001b[39m\u001b[38;5;28mfilter\u001b[39m]\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# display the filtered data frame\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\indexes\\range.py:417\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    415\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[1;32m--> 417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: '17'"
     ]
    }
   ],
   "source": [
    "del_gal = pd.DataFrame(all_data_high_otype)\n",
    "# filter the rows that contain the substring\n",
    "substring = 'GAL'\n",
    "filter = del_gal['17'].str.contains(substring)\n",
    "filtered_df = df[~filter]\n",
    "\n",
    "# display the filtered data frame\n",
    "print(f\"\\nData Frame after removing rows that contain '{substring}' in 'Name' column:\")\n",
    "print(filtered_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting LAMOST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matches: 40138\n",
      "out of  412025\n"
     ]
    }
   ],
   "source": [
    "# Load your AGN data and LAMOST catalog (assuming you have a local CSV or FITS file for LAMOST)\n",
    "agn_data = pd.read_pickle(\"agn_data.pkl\")  # Loaded AGN data\n",
    "lamost_catalog = pd.read_csv(\"dr9_v2.0_LRS_catalogue.csv\")  # Assuming CSV format for LAMOST catalog\n",
    "\n",
    "# Ensure that RA and Dec columns are numeric and have units\n",
    "agn_data['ra'] = pd.to_numeric(agn_data['ra'], errors='coerce')  # Coerce non-numeric to NaN\n",
    "agn_data['dec'] = pd.to_numeric(agn_data['dec'], errors='coerce')\n",
    "lamost_catalog['ra'] = pd.to_numeric(lamost_catalog['ra'], errors='coerce')\n",
    "lamost_catalog['dec'] = pd.to_numeric(lamost_catalog['dec'], errors='coerce')\n",
    "\n",
    "# Drop rows with NaN values in RA or Dec\n",
    "agn_data = agn_data.dropna(subset=['ra', 'dec'])\n",
    "lamost_catalog = lamost_catalog.dropna(subset=['ra', 'dec'])\n",
    "\n",
    "# Convert AGN and LAMOST data to SkyCoord objects for crossmatching\n",
    "agn_coords = SkyCoord(ra=agn_data['ra'].values*u.deg, dec=agn_data['dec'].values*u.deg)\n",
    "lamost_coords = SkyCoord(ra=lamost_catalog['ra'].values*u.deg, dec=lamost_catalog['dec'].values*u.deg)\n",
    "\n",
    "# Perform the crossmatch using astropy's match_to_catalog_sky function\n",
    "idx, d2d, _ = agn_coords.match_to_catalog_sky(lamost_coords)\n",
    "\n",
    "# Define a matching radius\n",
    "match_radius = 1 * u.arcsec\n",
    "matches = d2d < match_radius\n",
    "\n",
    "# Filter the matches\n",
    "matched_agn = agn_data.iloc[matches]\n",
    "matched_lamost = lamost_catalog.iloc[idx[matches]]\n",
    "\n",
    "# Combine matched data\n",
    "agn_lamost_data = pd.concat([matched_agn.reset_index(drop=True), matched_lamost.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Save the crossmatched data\n",
    "agn_lamost_data.to_pickle(\"agn_lamost_data.pkl\")\n",
    "\n",
    "print(f\"Number of matches: {agn_lamost_data.shape[0]}\")\n",
    "print(\"out of \", agn_data.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['source_id', 'ra', 'ra_error', 'dec', 'dec_error', 'phot_g_mean_flux',\n",
      "       'phot_g_mean_flux_error', 'pmra', 'pmra_error', 'pmdec', 'pmdec_error',\n",
      "       'parallax', 'parallax_error', 'phot_bp_mean_flux',\n",
      "       'phot_bp_mean_flux_error', 'phot_rp_mean_flux',\n",
      "       'phot_rp_mean_flux_error', 'ids', 'otype', 'gaia_id', 'obsid', 'uid',\n",
      "       'gp_id', 'designation', 'obsdate', 'lmjd', 'mjd', 'planid', 'spid',\n",
      "       'fiberid', 'ra_obs', 'dec_obs', 'snru', 'snrg', 'snrr', 'snri', 'snrz',\n",
      "       'class', 'subclass', 'z', 'z_err', 'ps_id', 'mag_ps_g', 'mag_ps_r',\n",
      "       'mag_ps_i', 'mag_ps_z', 'mag_ps_y', 'gaia_source_id', 'gaia_g_mean_mag',\n",
      "       'fibertype', 'offsets', 'offsets_v', 'ra', 'dec', 'fibermask',\n",
      "       'with_norm_flux'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(agn_lamost_data.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download 840001044.fits\n",
      "Failed to download 840004220.fits\n",
      "Failed to download 840003069.fits\n",
      "Failed to download 840015014.fits\n",
      "Failed to download 840015062.fits\n",
      "Failed to download 840015067.fits\n",
      "Failed to download 840014114.fits\n",
      "Failed to download 840016014.fits\n",
      "Failed to download 855610092.fits\n",
      "Downloaded 56111112.fits\n",
      "Downloaded 506201001.fits\n",
      "Downloaded 506202112.fits\n",
      "Downloaded 173401049.fits\n",
      "Downloaded 506201086.fits\n",
      "Downloaded 506201040.fits\n",
      "Downloaded 173401171.fits\n",
      "Downloaded 506201054.fits\n",
      "Downloaded 173410030.fits\n",
      "Downloaded 506210195.fits\n",
      "Downloaded 506210077.fits\n",
      "Downloaded 506207243.fits\n",
      "Downloaded 506207021.fits\n",
      "Downloaded 506207022.fits\n",
      "Downloaded 169907126.fits\n",
      "Downloaded 506207191.fits\n",
      "Downloaded 173401141.fits\n",
      "Downloaded 169901138.fits\n",
      "Downloaded 174606233.fits\n",
      "Downloaded 187606125.fits\n",
      "Downloaded 506206210.fits\n",
      "Downloaded 506206172.fits\n",
      "Downloaded 506206228.fits\n",
      "Downloaded 506206144.fits\n",
      "Downloaded 506206147.fits\n",
      "Downloaded 169908070.fits\n",
      "Downloaded 173406057.fits\n",
      "Downloaded 187613013.fits\n",
      "Downloaded 506205060.fits\n",
      "Downloaded 173408164.fits\n",
      "Downloaded 173404095.fits\n",
      "Downloaded 506208227.fits\n",
      "Downloaded 66204120.fits\n",
      "Downloaded 173403197.fits\n",
      "Downloaded 506210013.fits\n",
      "Downloaded 506204022.fits\n",
      "Downloaded 174604206.fits\n",
      "Failed to download 853706120.fits\n",
      "Downloaded 20604100.fits\n",
      "Downloaded 66208032.fits\n",
      "Downloaded 173413225.fits\n",
      "Downloaded 173413107.fits\n",
      "Downloaded 169912173.fits\n",
      "Downloaded 20606031.fits\n",
      "Downloaded 506212092.fits\n",
      "Downloaded 74311101.fits\n",
      "Failed to download 866807185.fits\n",
      "Failed to download 866806230.fits\n",
      "Failed to download 866810132.fits\n",
      "Failed to download 866808244.fits\n",
      "Failed to download 866804193.fits\n",
      "Failed to download 866813089.fits\n",
      "Failed to download 866813091.fits\n",
      "Failed to download 866813144.fits\n",
      "Failed to download 866813178.fits\n",
      "Failed to download 866809150.fits\n",
      "Failed to download 866809194.fits\n",
      "Failed to download 866812127.fits\n",
      "Failed to download 866811061.fits\n",
      "Failed to download 866811060.fits\n",
      "Failed to download 866811113.fits\n",
      "Failed to download 866811106.fits\n",
      "Failed to download 837310063.fits\n",
      "Failed to download 837310148.fits\n",
      "Downloaded 8703097.fits\n",
      "Downloaded 210064.fits\n",
      "Failed to download 845002233.fits\n",
      "Failed to download 845002211.fits\n",
      "Failed to download 837303218.fits\n",
      "Downloaded 210107.fits\n",
      "Downloaded 18115231.fits\n",
      "Failed to download 845002192.fits\n",
      "Downloaded 210147.fits\n",
      "Failed to download 837314226.fits\n",
      "Downloaded 8713102.fits\n",
      "Downloaded 18114225.fits\n",
      "Failed to download 837314111.fits\n",
      "Downloaded 18115059.fits\n",
      "Failed to download 837314076.fits\n",
      "Failed to download 837314084.fits\n",
      "Failed to download 845010109.fits\n",
      "Downloaded 18116081.fits\n",
      "Failed to download 837316179.fits\n",
      "Failed to download 837315141.fits\n",
      "Failed to download 837309174.fits\n",
      "Failed to download 837309115.fits\n",
      "Failed to download 845005248.fits\n",
      "Downloaded 214061.fits\n",
      "Downloaded 18115041.fits\n",
      "Downloaded 18115028.fits\n",
      "Downloaded 203145.fits\n",
      "Downloaded 203138.fits\n",
      "Downloaded 252907243.fits\n",
      "Downloaded 252907245.fits\n",
      "Downloaded 203107.fits\n",
      "Downloaded 18115083.fits\n",
      "Failed to download 837316152.fits\n",
      "Downloaded 21101185.fits\n",
      "Failed to download 783108135.fits\n",
      "Failed to download 837316001.fits\n",
      "Downloaded 21107224.fits\n",
      "Downloaded 21107211.fits\n",
      "Failed to download 837316183.fits\n",
      "Downloaded 18116210.fits\n",
      "Downloaded 8711187.fits\n",
      "Downloaded 483507166.fits\n",
      "Failed to download 855608120.fits\n",
      "Failed to download 855606198.fits\n",
      "Failed to download 855604187.fits\n",
      "Failed to download 855604017.fits\n",
      "Failed to download 855604025.fits\n",
      "Failed to download 855615235.fits\n",
      "Failed to download 855615100.fits\n",
      "Failed to download 855612179.fits\n",
      "Failed to download 855611231.fits\n",
      "Failed to download 855611056.fits\n",
      "Failed to download 866816027.fits\n",
      "Downloaded 500001033.fits\n",
      "Downloaded 500001190.fits\n",
      "Failed to download 866816030.fits\n",
      "Downloaded 500007231.fits\n",
      "Downloaded 500001141.fits\n",
      "Downloaded 500001139.fits\n",
      "Downloaded 500002203.fits\n",
      "Failed to download 855614107.fits\n",
      "Failed to download 855614011.fits\n",
      "Failed to download 780201006.fits\n",
      "Failed to download 855616065.fits\n",
      "Failed to download 780201054.fits\n",
      "Failed to download 780202056.fits\n",
      "Failed to download 855616242.fits\n",
      "Failed to download 780208119.fits\n",
      "Failed to download 780205144.fits\n",
      "Failed to download 780208194.fits\n",
      "Failed to download 780208174.fits\n",
      "Failed to download 780207133.fits\n",
      "Failed to download 855611048.fits\n",
      "Failed to download 780206224.fits\n",
      "Failed to download 780206012.fits\n",
      "Failed to download 780202250.fits\n",
      "Failed to download 780205201.fits\n",
      "Failed to download 780205205.fits\n",
      "Failed to download 780205086.fits\n",
      "Failed to download 780205199.fits\n",
      "Failed to download 780205047.fits\n",
      "Failed to download 780205040.fits\n",
      "Failed to download 780205167.fits\n",
      "Downloaded 169912177.fits\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "# Directory to store downloaded FITS files\n",
    "output_dir = 'lamost_fits_files'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Base URL for LAMOST FITS file downloads\n",
    "lamost_base_url = \"https://www.lamost.org/dr7/v2.0/spectrum/fits/\"\n",
    "\n",
    "# Iterate over the OBSIDs and download the FITS files\n",
    "for obsid in agn_lamost_data['obsid']:\n",
    "    fits_url = f\"{lamost_base_url}{obsid}\"\n",
    "    response = requests.get(fits_url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        file_path = os.path.join(output_dir, f'{obsid}.fits')\n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Downloaded {obsid}.fits\")\n",
    "    else:\n",
    "        print(f\"Failed to download {obsid}.fits\")\n",
    "\n",
    "print(\"Download complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
