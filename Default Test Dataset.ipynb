{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from astroquery.gaia import Gaia\n",
    "from astroquery.simbad import Simbad\n",
    "from astropy.coordinates import SkyCoord\n",
    "import astropy.units as u\n",
    "import numpy as np\n",
    "import pyvo as vo\n",
    "import pickle\n",
    "from astroquery.vizier import Vizier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetGAIAData(GaiaDR3SourceIDs):\n",
    "    try:\n",
    "        qry = f\"SELECT * FROM gaiadr3.gaia_source gs WHERE gs.source_id in ({GaiaDR3SourceIDs});\"\n",
    "        job = Gaia.launch_job_async(qry)\n",
    "        tblGaia = job.get_results()\n",
    "        dfGaia = tblGaia.to_pandas()\n",
    "        return dfGaia\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return pd.DataFrame()  # Return an empty DataFrame in case of error\n",
    "\n",
    "def split_ids_into_chunks(id_string, chunk_size=50000):\n",
    "    id_list = id_string.split(', ')\n",
    "    chunks = [', '.join(id_list[i:i + chunk_size]) for i in range(0, len(id_list), chunk_size)]\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting AGN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting SIMBAD labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(136855, 2)\n",
      "(819002, 2)\n"
     ]
    }
   ],
   "source": [
    "criteria_ = \"otype = 'AGN..' \"\n",
    "\n",
    "# Initialize SIMBAD\n",
    "simbad = Simbad()\n",
    "simbad.ROW_LIMIT = -1\n",
    "simbad.add_votable_fields(\"ids\", \"otype\")\n",
    "\n",
    "# Define coordinates (random)\n",
    "coordinates = SkyCoord(0, -90, unit=(\"deg\", \"deg\"))\n",
    "coordinates2 = SkyCoord(0, 90, unit=(\"deg\", \"deg\"))\n",
    "\n",
    "# Query region 1st half\n",
    "result = simbad.query_region(coordinates, radius=\"90d0m\",\n",
    "                           #  criteria=\"otype = 'Ma*..' OR otype = 'MS*..' OR otype = 'Y*O..' OR otype = 'Ev*..' \")\n",
    "                           criteria=criteria_)\n",
    "filtered_result = result[\"ids\", \"otype\"] # Keep only the columns we need\n",
    "result_df = filtered_result.to_pandas() # Convert result to a Pandas DataFrame\n",
    "print(result_df.shape)\n",
    "filtered_result = result_df.dropna() # Remove rows with any NULL values\n",
    "data1 = filtered_result.to_numpy() # Convert to numpy array\n",
    "\n",
    "\n",
    "# Query region 2nd half\n",
    "result2 = simbad.query_region(coordinates2, radius=\"90d0m\",\n",
    "                             #criteria=\"otype = 'Ma*..' OR otype = 'MS*..' OR otype = 'Y*O..' OR otype = 'Ev*..' \")\n",
    "                             criteria=criteria_)\n",
    "filtered_result2 = result2[\"ids\", \"otype\"] # Keep only the columns we need\n",
    "result_df2 = filtered_result2.to_pandas() # Convert result to a Pandas DataFrame\n",
    "print(result_df2.shape)\n",
    "filtered_result2 = result_df2.dropna() # Remove rows with any NULL values\n",
    "data2 = filtered_result2.to_numpy() # Convert to numpy array\n",
    "\n",
    "# Combine the two arrays & save\n",
    "#otypes_agn = np.row_stack((data1, data2))\n",
    "#np.save(\"fullsky4catsunfiltered\", simbad_data)\n",
    "\n",
    "# without filtering\n",
    "otypes_agn = pd.concat([result_df, result_df2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting coresponding Gaia data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jcwin\\AppData\\Local\\Temp\\ipykernel_10088\\1695916639.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  simbad_data['gaia_id'] = simbad_data['gaia_id'].astype(str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jcwin\\AppData\\Local\\Temp\\ipykernel_10088\\1695916639.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  gaia_data['source_id'] = gaia_data['source_id'].astype(str)\n"
     ]
    }
   ],
   "source": [
    "# Concatenate the DataFrames\n",
    "simbad_data = otypes_agn\n",
    "\n",
    "# Filter SIMBAD data to only include rows where 'ids' contains 'Gaia DR3'\n",
    "simbad_data['gaia_id'] = simbad_data['ids'].apply(lambda x: next((id for id in x.split('|') if id.startswith('Gaia DR3')), None))\n",
    "\n",
    "# Remove 'Gaia DR3' prefix and drop rows with NaN values in 'gaia_id'\n",
    "simbad_data['gaia_id'] = simbad_data['gaia_id'].str.lstrip('Gaia DR3')\n",
    "simbad_data = simbad_data.dropna(subset=['gaia_id'])\n",
    "\n",
    "# Ensure 'gaia_id' is a string\n",
    "simbad_data['gaia_id'] = simbad_data['gaia_id'].astype(str)\n",
    "\n",
    "# Split the Gaia DR3 source IDs into chunks of 30,000 IDs each\n",
    "GaiaDR3SourceIDs = ', '.join(simbad_data['gaia_id'].astype(str))\n",
    "chunks = split_ids_into_chunks(GaiaDR3SourceIDs)\n",
    "\n",
    "# Initialize an empty DataFrame to store all data\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "# Process each chunk and append the results to the combined DataFrame\n",
    "for chunk in chunks:\n",
    "    dfGaia = GetGAIAData(chunk)\n",
    "    combined_df = pd.concat([combined_df, dfGaia], ignore_index=True)\n",
    "\n",
    "# Convert the combined DataFrame to a NumPy array if needed\n",
    "combined_matrix = combined_df.to_numpy()\n",
    "\n",
    "# GAIA data\n",
    "gaia_data = combined_df[[\"source_id\", \"ra\", \"ra_error\", \"dec\", \"dec_error\", \"phot_g_mean_flux\", \"phot_g_mean_flux_error\", \"pmra\", \"pmra_error\", \"pmdec\", \"pmdec_error\", \"parallax\", \"parallax_error\", \"phot_bp_mean_flux\", \"phot_bp_mean_flux_error\", \"phot_rp_mean_flux\", \"phot_rp_mean_flux_error\"]]\n",
    "\n",
    "# Convert Gaia source_id to string\n",
    "gaia_data['source_id'] = gaia_data['source_id'].astype(str)\n",
    "\n",
    "# Merge Gaia and SIMBAD data on matching IDs\n",
    "agn_data = pd.merge(gaia_data, simbad_data, left_on='source_id', right_on='gaia_id', how='inner')\n",
    "agn_data.to_pickle(\"agn_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"agn_data.pkl\", \"rb\") as f:\n",
    "    agn_data = pickle.load(f)\n",
    "agn_data_high_otype = agn_data.copy()\n",
    "agn_data_high_otype = agn_data_high_otype.assign(otype= \"AGN\")\n",
    "agn_data_high_otype = agn_data_high_otype[[\"source_id\", \"ra\", \"ra_error\", \"dec\", \"dec_error\", \"phot_g_mean_flux\", \"phot_g_mean_flux_error\", \"pmra\", \"pmra_error\", \"pmdec\", \"pmdec_error\", \"parallax\", \"parallax_error\", \"phot_bp_mean_flux\", \"phot_bp_mean_flux_error\", \"phot_rp_mean_flux\", \"phot_rp_mean_flux_error\", \"otype\"]]\n",
    "agn_data_high_otype.to_pickle(\"agn_data_high_otype.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting non-AGN Galaxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57267, 2)\n",
      "(42307, 2)\n"
     ]
    }
   ],
   "source": [
    "criteria_ = \"otype = 'LSB..' OR otype = 'bCG..' OR otype = 'SBG..' OR otype = 'H2G..' OR otype = 'EmG..'\"\n",
    "\n",
    "# Initialize SIMBAD\n",
    "simbad = Simbad()\n",
    "simbad.ROW_LIMIT = -1\n",
    "simbad.add_votable_fields(\"ids\", \"otype\")\n",
    "\n",
    "# Define coordinates (random)\n",
    "coordinates = SkyCoord(0, -90, unit=(\"deg\", \"deg\"))\n",
    "coordinates2 = SkyCoord(0, 90, unit=(\"deg\", \"deg\"))\n",
    "\n",
    "# Query region 1st half\n",
    "result = simbad.query_region(coordinates, radius=\"90d0m\",\n",
    "                           #  criteria=\"otype = 'Ma*..' OR otype = 'MS*..' OR otype = 'Y*O..' OR otype = 'Ev*..' \")\n",
    "                           criteria=criteria_)\n",
    "filtered_result = result[\"ids\", \"otype\"] # Keep only the columns we need\n",
    "result_df = filtered_result.to_pandas() # Convert result to a Pandas DataFrame\n",
    "print(result_df.shape)\n",
    "filtered_result = result_df.dropna() # Remove rows with any NULL values\n",
    "data1 = filtered_result.to_numpy() # Convert to numpy array\n",
    "\n",
    "\n",
    "# Query region 2nd half\n",
    "result2 = simbad.query_region(coordinates2, radius=\"90d0m\",\n",
    "                             #criteria=\"otype = 'Ma*..' OR otype = 'MS*..' OR otype = 'Y*O..' OR otype = 'Ev*..' \")\n",
    "                             criteria=criteria_)\n",
    "filtered_result2 = result2[\"ids\", \"otype\"] # Keep only the columns we need\n",
    "result_df2 = filtered_result2.to_pandas() # Convert result to a Pandas DataFrame\n",
    "print(result_df2.shape)\n",
    "filtered_result2 = result_df2.dropna() # Remove rows with any NULL values\n",
    "data2 = filtered_result2.to_numpy() # Convert to numpy array\n",
    "\n",
    "# Combine the two arrays & save\n",
    "#otypes_agn = np.row_stack((data1, data2))\n",
    "#np.save(\"fullsky4catsunfiltered\", simbad_data)\n",
    "\n",
    "# without filtering\n",
    "otypes_gal = pd.concat([result_df, result_df2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting corresponding gaia data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jcwin\\AppData\\Local\\Temp\\ipykernel_10088\\1415820394.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  simbad_data['gaia_id'] = simbad_data['gaia_id'].astype(str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Query finished. [astroquery.utils.tap.core]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jcwin\\AppData\\Local\\Temp\\ipykernel_10088\\1415820394.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  gaia_data['source_id'] = gaia_data['source_id'].astype(str)\n"
     ]
    }
   ],
   "source": [
    "# Concatenate the DataFrames\n",
    "simbad_data = otypes_gal\n",
    "\n",
    "# Filter SIMBAD data to only include rows where 'ids' contains 'Gaia DR3'\n",
    "simbad_data['gaia_id'] = simbad_data['ids'].apply(lambda x: next((id for id in x.split('|') if id.startswith('Gaia DR3')), None))\n",
    "\n",
    "# Remove 'Gaia DR3' prefix and drop rows with NaN values in 'gaia_id'\n",
    "simbad_data['gaia_id'] = simbad_data['gaia_id'].str.lstrip('Gaia DR3')\n",
    "simbad_data = simbad_data.dropna(subset=['gaia_id'])\n",
    "\n",
    "# Ensure 'gaia_id' is a string\n",
    "simbad_data['gaia_id'] = simbad_data['gaia_id'].astype(str)\n",
    "\n",
    "# Split the Gaia DR3 source IDs into chunks of 30,000 IDs each\n",
    "GaiaDR3SourceIDs = ', '.join(simbad_data['gaia_id'].astype(str))\n",
    "chunks = split_ids_into_chunks(GaiaDR3SourceIDs)\n",
    "\n",
    "# Initialize an empty DataFrame to store all data\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "# Process each chunk and append the results to the combined DataFrame\n",
    "for chunk in chunks:\n",
    "    dfGaia = GetGAIAData(chunk)\n",
    "    combined_df = pd.concat([combined_df, dfGaia], ignore_index=True)\n",
    "\n",
    "# Convert the combined DataFrame to a NumPy array if needed\n",
    "combined_matrix = combined_df.to_numpy()\n",
    "\n",
    "# GAIA data\n",
    "gaia_data = combined_df[[\"source_id\", \"ra\", \"ra_error\", \"dec\", \"dec_error\", \"phot_g_mean_flux\", \"phot_g_mean_flux_error\", \"pmra\", \"pmra_error\", \"pmdec\", \"pmdec_error\", \"parallax\", \"parallax_error\", \"phot_bp_mean_flux\", \"phot_bp_mean_flux_error\", \"phot_rp_mean_flux\", \"phot_rp_mean_flux_error\"]]\n",
    "\n",
    "# Convert Gaia source_id to string\n",
    "gaia_data['source_id'] = gaia_data['source_id'].astype(str)\n",
    "\n",
    "# Merge Gaia and SIMBAD data on matching IDs\n",
    "gal_data = pd.merge(gaia_data, simbad_data, left_on='source_id', right_on='gaia_id', how='inner')\n",
    "gal_data.to_pickle(\"gal_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33531, 18)\n"
     ]
    }
   ],
   "source": [
    "with open(\"gal_data.pkl\", \"rb\") as f:\n",
    "    gal_data = pickle.load(f)\n",
    "gal_data_high_otype = gal_data.copy()\n",
    "gal_data_high_otype = gal_data_high_otype.assign(otype= \"GAL\") \n",
    "gal_data_high_otype = gal_data_high_otype[[\"source_id\", \"ra\", \"ra_error\", \"dec\", \"dec_error\", \"phot_g_mean_flux\", \"phot_g_mean_flux_error\", \"pmra\", \"pmra_error\", \"pmdec\", \"pmdec_error\", \"parallax\", \"parallax_error\", \"phot_bp_mean_flux\", \"phot_bp_mean_flux_error\", \"phot_rp_mean_flux\", \"phot_rp_mean_flux_error\", \"otype\"]]\n",
    "print(gal_data_high_otype.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Binaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1446685, 2)\n",
      "(917897, 2)\n"
     ]
    }
   ],
   "source": [
    "criteria_ = \"otype = '**..'\"\n",
    "\n",
    "# Initialize SIMBAD\n",
    "simbad = Simbad()\n",
    "simbad.ROW_LIMIT = -1\n",
    "simbad.add_votable_fields(\"ids\", \"otype\")\n",
    "\n",
    "# Define coordinates (random)\n",
    "coordinates = SkyCoord(0, -90, unit=(\"deg\", \"deg\"))\n",
    "coordinates2 = SkyCoord(0, 90, unit=(\"deg\", \"deg\"))\n",
    "\n",
    "# Query region 1st half\n",
    "result = simbad.query_region(coordinates, radius=\"90d0m\",\n",
    "                           #  criteria=\"otype = 'Ma*..' OR otype = 'MS*..' OR otype = 'Y*O..' OR otype = 'Ev*..' \")\n",
    "                           criteria=criteria_)\n",
    "filtered_result = result[\"ids\", \"otype\"] # Keep only the columns we need\n",
    "result_df = filtered_result.to_pandas() # Convert result to a Pandas DataFrame\n",
    "print(result_df.shape)\n",
    "filtered_result = result_df.dropna() # Remove rows with any NULL values\n",
    "data1 = filtered_result.to_numpy() # Convert to numpy array\n",
    "\n",
    "\n",
    "# Query region 2nd half\n",
    "result2 = simbad.query_region(coordinates2, radius=\"90d0m\",\n",
    "                             #criteria=\"otype = 'Ma*..' OR otype = 'MS*..' OR otype = 'Y*O..' OR otype = 'Ev*..' \")\n",
    "                             criteria=criteria_)\n",
    "filtered_result2 = result2[\"ids\", \"otype\"] # Keep only the columns we need\n",
    "result_df2 = filtered_result2.to_pandas() # Convert result to a Pandas DataFrame\n",
    "print(result_df2.shape)\n",
    "filtered_result2 = result_df2.dropna() # Remove rows with any NULL values\n",
    "data2 = filtered_result2.to_numpy() # Convert to numpy array\n",
    "\n",
    "# Combine the two arrays & save\n",
    "#otypes_agn = np.row_stack((data1, data2))\n",
    "#np.save(\"fullsky4catsunfiltered\", simbad_data)\n",
    "\n",
    "# without filtering\n",
    "otypes_bin = pd.concat([result_df, result_df2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting coresponding gaia data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jcwin\\AppData\\Local\\Temp\\ipykernel_10088\\249499439.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  simbad_data['gaia_id'] = simbad_data['gaia_id'].astype(str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jcwin\\AppData\\Local\\Temp\\ipykernel_10088\\249499439.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  gaia_data['source_id'] = gaia_data['source_id'].astype(str)\n"
     ]
    }
   ],
   "source": [
    "# Concatenate the DataFrames\n",
    "simbad_data = otypes_bin\n",
    "\n",
    "# Filter SIMBAD data to only include rows where 'ids' contains 'Gaia DR3'\n",
    "simbad_data['gaia_id'] = simbad_data['ids'].apply(lambda x: next((id for id in x.split('|') if id.startswith('Gaia DR3')), None))\n",
    "\n",
    "# Remove 'Gaia DR3' prefix and drop rows with NaN values in 'gaia_id'\n",
    "simbad_data['gaia_id'] = simbad_data['gaia_id'].str.lstrip('Gaia DR3')\n",
    "simbad_data = simbad_data.dropna(subset=['gaia_id'])\n",
    "\n",
    "# Ensure 'gaia_id' is a string\n",
    "simbad_data['gaia_id'] = simbad_data['gaia_id'].astype(str)\n",
    "\n",
    "# Split the Gaia DR3 source IDs into chunks of 30,000 IDs each\n",
    "GaiaDR3SourceIDs = ', '.join(simbad_data['gaia_id'].astype(str))\n",
    "chunks = split_ids_into_chunks(GaiaDR3SourceIDs)\n",
    "\n",
    "# Initialize an empty DataFrame to store all data\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "# Process each chunk and append the results to the combined DataFrame\n",
    "for chunk in chunks:\n",
    "    dfGaia = GetGAIAData(chunk)\n",
    "    combined_df = pd.concat([combined_df, dfGaia], ignore_index=True)\n",
    "\n",
    "# Convert the combined DataFrame to a NumPy array if needed\n",
    "combined_matrix = combined_df.to_numpy()\n",
    "\n",
    "# GAIA data\n",
    "gaia_data = combined_df[[\"source_id\", \"ra\", \"ra_error\", \"dec\", \"dec_error\", \"phot_g_mean_flux\", \"phot_g_mean_flux_error\", \"pmra\", \"pmra_error\", \"pmdec\", \"pmdec_error\", \"parallax\", \"parallax_error\", \"phot_bp_mean_flux\", \"phot_bp_mean_flux_error\", \"phot_rp_mean_flux\", \"phot_rp_mean_flux_error\"]]\n",
    "\n",
    "# Convert Gaia source_id to string\n",
    "gaia_data['source_id'] = gaia_data['source_id'].astype(str)\n",
    "\n",
    "# Merge Gaia and SIMBAD data on matching IDs\n",
    "bin_data = pd.merge(gaia_data, simbad_data, left_on='source_id', right_on='gaia_id', how='inner')\n",
    "bin_data.to_pickle(\"bin_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1700440, 18)\n"
     ]
    }
   ],
   "source": [
    "with open(\"bin_data.pkl\", \"rb\") as f:\n",
    "    bin_data = pickle.load(f)\n",
    "bin_data_high_otype = bin_data.copy()\n",
    "bin_data_high_otype = bin_data_high_otype.assign(otype= \"BIN\")\n",
    "bin_data_high_otype = bin_data_high_otype[[\"source_id\", \"ra\", \"ra_error\", \"dec\", \"dec_error\", \"phot_g_mean_flux\", \"phot_g_mean_flux_error\", \"pmra\", \"pmra_error\", \"pmdec\", \"pmdec_error\", \"parallax\", \"parallax_error\", \"phot_bp_mean_flux\", \"phot_bp_mean_flux_error\", \"phot_rp_mean_flux\", \"phot_rp_mean_flux_error\", \"otype\"]]\n",
    "print(bin_data_high_otype.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting data for single stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1237922, 2)\n",
      "(810162, 2)\n"
     ]
    }
   ],
   "source": [
    "criteria_ = \"otype = 'Ma*..' OR otype = 'MS*..' OR otype = 'Y*O..' OR otype = 'Ev*..' OR otype = 'Pe*..' OR otype = 'SN*' OR otype = 'LM*' OR otype = 'V*' OR otype = 'Em*' OR otype = 'PM*' OR otype = 'HV*'\"\n",
    "\n",
    "# Initialize SIMBAD\n",
    "simbad = Simbad()\n",
    "simbad.ROW_LIMIT = -1\n",
    "simbad.add_votable_fields(\"ids\", \"otype\")\n",
    "\n",
    "# Define coordinates (random)\n",
    "coordinates = SkyCoord(0, -90, unit=(\"deg\", \"deg\"))\n",
    "coordinates2 = SkyCoord(0, 90, unit=(\"deg\", \"deg\"))\n",
    "\n",
    "# Query region 1st half\n",
    "result = simbad.query_region(coordinates, radius=\"90d0m\",\n",
    "                           #  criteria=\"otype = 'Ma*..' OR otype = 'MS*..' OR otype = 'Y*O..' OR otype = 'Ev*..' \")\n",
    "                           criteria=criteria_)\n",
    "filtered_result = result[\"ids\", \"otype\"] # Keep only the columns we need\n",
    "result_df = filtered_result.to_pandas() # Convert result to a Pandas DataFrame\n",
    "print(result_df.shape)\n",
    "filtered_result = result_df.dropna() # Remove rows with any NULL values\n",
    "data1 = filtered_result.to_numpy() # Convert to numpy array\n",
    "\n",
    "\n",
    "# Query region 2nd half\n",
    "result2 = simbad.query_region(coordinates2, radius=\"90d0m\",\n",
    "                             #criteria=\"otype = 'Ma*..' OR otype = 'MS*..' OR otype = 'Y*O..' OR otype = 'Ev*..' \")\n",
    "                             criteria=criteria_)\n",
    "filtered_result2 = result2[\"ids\", \"otype\"] # Keep only the columns we need\n",
    "result_df2 = filtered_result2.to_pandas() # Convert result to a Pandas DataFrame\n",
    "print(result_df2.shape)\n",
    "filtered_result2 = result_df2.dropna() # Remove rows with any NULL values\n",
    "data2 = filtered_result2.to_numpy() # Convert to numpy array\n",
    "\n",
    "# Combine the two arrays & save\n",
    "#otypes_agn = np.row_stack((data1, data2))\n",
    "#np.save(\"fullsky4catsunfiltered\", simbad_data)\n",
    "\n",
    "# without filtering\n",
    "otypes_star = pd.concat([result_df, result_df2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting coresponding gaia data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jcwin\\AppData\\Local\\Temp\\ipykernel_24756\\1499858909.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  simbad_data['gaia_id'] = simbad_data['gaia_id'].astype(str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jcwin\\AppData\\Local\\Temp\\ipykernel_24756\\1499858909.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  gaia_data['source_id'] = gaia_data['source_id'].astype(str)\n"
     ]
    }
   ],
   "source": [
    "# Concatenate the DataFrames\n",
    "simbad_data = otypes_star\n",
    "\n",
    "# Filter SIMBAD data to only include rows where 'ids' contains 'Gaia DR3'\n",
    "simbad_data['gaia_id'] = simbad_data['ids'].apply(lambda x: next((id for id in x.split('|') if id.startswith('Gaia DR3')), None))\n",
    "\n",
    "# Remove 'Gaia DR3' prefix and drop rows with NaN values in 'gaia_id'\n",
    "simbad_data['gaia_id'] = simbad_data['gaia_id'].str.lstrip('Gaia DR3')\n",
    "simbad_data = simbad_data.dropna(subset=['gaia_id'])\n",
    "\n",
    "# Ensure 'gaia_id' is a string\n",
    "simbad_data['gaia_id'] = simbad_data['gaia_id'].astype(str)\n",
    "\n",
    "# Split the Gaia DR3 source IDs into chunks of 30,000 IDs each\n",
    "GaiaDR3SourceIDs = ', '.join(simbad_data['gaia_id'].astype(str))\n",
    "chunks = split_ids_into_chunks(GaiaDR3SourceIDs)\n",
    "\n",
    "# Initialize an empty DataFrame to store all data\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "# Process each chunk and append the results to the combined DataFrame\n",
    "for chunk in chunks:\n",
    "    dfGaia = GetGAIAData(chunk)\n",
    "    combined_df = pd.concat([combined_df, dfGaia], ignore_index=True)\n",
    "\n",
    "# Convert the combined DataFrame to a NumPy array if needed\n",
    "combined_matrix = combined_df.to_numpy()\n",
    "\n",
    "# GAIA data\n",
    "gaia_data = combined_df[[\"source_id\", \"ra\", \"ra_error\", \"dec\", \"dec_error\", \"phot_g_mean_flux\", \"phot_g_mean_flux_error\", \"pmra\", \"pmra_error\", \"pmdec\", \"pmdec_error\", \"parallax\", \"parallax_error\", \"phot_bp_mean_flux\", \"phot_bp_mean_flux_error\", \"phot_rp_mean_flux\", \"phot_rp_mean_flux_error\"]]\n",
    "\n",
    "# Convert Gaia source_id to string\n",
    "gaia_data['source_id'] = gaia_data['source_id'].astype(str)\n",
    "\n",
    "# Merge Gaia and SIMBAD data on matching IDs\n",
    "star_data = pd.merge(gaia_data, simbad_data, left_on='source_id', right_on='gaia_id', how='inner')\n",
    "star_data.to_pickle(\"star_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1499508, 18)\n"
     ]
    }
   ],
   "source": [
    "with open('star_data.pkl', 'rb') as file:\n",
    "    star_data = pickle.load(file)\n",
    "star_data_high_otype = star_data.copy()\n",
    "star_data_high_otype = star_data_high_otype.assign(otype= \"STAR\")\n",
    "star_data_high_otype = star_data_high_otype[[\"source_id\", \"ra\", \"ra_error\", \"dec\", \"dec_error\", \"phot_g_mean_flux\", \"phot_g_mean_flux_error\", \"pmra\", \"pmra_error\", \"pmdec\", \"pmdec_error\", \"parallax\", \"parallax_error\", \"phot_bp_mean_flux\", \"phot_bp_mean_flux_error\", \"phot_rp_mean_flux\", \"phot_rp_mean_flux_error\", \"otype\"]]\n",
    "print(star_data_high_otype.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive way combining all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3267794, 18)\n"
     ]
    }
   ],
   "source": [
    "all_data_high_otype = pd.concat([agn_data_high_otype, gal_data_high_otype, bin_data_high_otype, star_data_high_otype])\n",
    "all_data_high_otype = all_data_high_otype.dropna()  \n",
    "np.save(\"all_data_high_otype\", all_data_high_otype.to_numpy())\n",
    "print(all_data_high_otype.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting equal amounts of data for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14071, 18)\n"
     ]
    }
   ],
   "source": [
    "n = min(agn_data_high_otype.dropna().shape[0], gal_data_high_otype.dropna().shape[0], bin_data_high_otype.dropna().shape[0], star_data_high_otype.dropna().shape[0])\n",
    "equal_data_high_otype = pd.concat([agn_data_high_otype.sample(n=n), gal_data_high_otype, bin_data_high_otype.sample(n=n), star_data_high_otype.sample(n=n)])\n",
    "equal_data_high_otype = equal_data_high_otype.dropna()\n",
    "np.save(\"equal_data_high_otype\", equal_data_high_otype.to_numpy())\n",
    "print(equal_data_high_otype.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using all except galaxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(949731, 18)\n"
     ]
    }
   ],
   "source": [
    "n = min(agn_data_high_otype.dropna().shape[0], bin_data_high_otype.dropna().shape[0], star_data_high_otype.dropna().shape[0])\n",
    "no_gal_high_otype = pd.concat([agn_data_high_otype.dropna().sample(n=n), bin_data_high_otype.dropna().sample(n=n), star_data_high_otype.dropna().sample(n=n)])\n",
    "#equal_data_high_otype = equal_data_high_otype.dropna()\n",
    "np.save(\"no_gal_high_otype\",no_gal_high_otype.to_numpy())\n",
    "print(no_gal_high_otype.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_gal_high_otype = pd.concat([agn_data_high_otype, bin_data_high_otype, star_data_high_otype])\n",
    "no_gal_high_otype = no_gal_high_otype.dropna()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'17'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# filter the rows that contain the substring\u001b[39;00m\n\u001b[0;32m      3\u001b[0m substring \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGAL\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28mfilter\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mdel_gal\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m17\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mcontains(substring)\n\u001b[0;32m      5\u001b[0m filtered_df \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;241m~\u001b[39m\u001b[38;5;28mfilter\u001b[39m]\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# display the filtered data frame\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\indexes\\range.py:417\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    415\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[1;32m--> 417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: '17'"
     ]
    }
   ],
   "source": [
    "del_gal = pd.DataFrame(all_data_high_otype)\n",
    "# filter the rows that contain the substring\n",
    "substring = 'GAL'\n",
    "filter = del_gal['17'].str.contains(substring)\n",
    "filtered_df = df[~filter]\n",
    "\n",
    "# display the filtered data frame\n",
    "print(f\"\\nData Frame after removing rows that contain '{substring}' in 'Name' column:\")\n",
    "print(filtered_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting LAMOST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matches: 40138\n",
      "out of  412025\n"
     ]
    }
   ],
   "source": [
    "# Load your AGN data and LAMOST catalog (assuming you have a local CSV or FITS file for LAMOST)\n",
    "agn_data = pd.read_pickle(\"agn_data.pkl\")  # Loaded AGN data\n",
    "lamost_catalog = pd.read_csv(\"dr9_v2.0_LRS_catalogue.csv\")  # Assuming CSV format for LAMOST catalog\n",
    "\n",
    "# Ensure that RA and Dec columns are numeric and have units\n",
    "agn_data['ra'] = pd.to_numeric(agn_data['ra'], errors='coerce')  # Coerce non-numeric to NaN\n",
    "agn_data['dec'] = pd.to_numeric(agn_data['dec'], errors='coerce')\n",
    "lamost_catalog['ra'] = pd.to_numeric(lamost_catalog['ra'], errors='coerce')\n",
    "lamost_catalog['dec'] = pd.to_numeric(lamost_catalog['dec'], errors='coerce')\n",
    "\n",
    "# Drop rows with NaN values in RA or Dec\n",
    "agn_data = agn_data.dropna(subset=['ra', 'dec'])\n",
    "lamost_catalog = lamost_catalog.dropna(subset=['ra', 'dec'])\n",
    "\n",
    "# Convert AGN and LAMOST data to SkyCoord objects for crossmatching\n",
    "agn_coords = SkyCoord(ra=agn_data['ra'].values*u.deg, dec=agn_data['dec'].values*u.deg)\n",
    "lamost_coords = SkyCoord(ra=lamost_catalog['ra'].values*u.deg, dec=lamost_catalog['dec'].values*u.deg)\n",
    "\n",
    "# Perform the crossmatch using astropy's match_to_catalog_sky function\n",
    "idx, d2d, _ = agn_coords.match_to_catalog_sky(lamost_coords)\n",
    "\n",
    "# Define a matching radius\n",
    "match_radius = 1 * u.arcsec\n",
    "matches = d2d < match_radius\n",
    "\n",
    "# Filter the matches\n",
    "matched_agn = agn_data.iloc[matches]\n",
    "matched_lamost = lamost_catalog.iloc[idx[matches]]\n",
    "\n",
    "# Combine matched data\n",
    "agn_lamost_data = pd.concat([matched_agn.reset_index(drop=True), matched_lamost.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Save the crossmatched data\n",
    "agn_lamost_data.to_pickle(\"agn_lamost_data.pkl\")\n",
    "\n",
    "print(f\"Number of matches: {agn_lamost_data.shape[0]}\")\n",
    "print(\"out of \", agn_data.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
