{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Union\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from mambapy.mamba import MambaConfig, MambaBlock, RMSNorm\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "This file implements the Jamba architecture, as proposed by AI21labs (altough others have proposed blending Mamba & attention in the past).\n",
    "A Jamba model combines Mamba and attention layers, as well as MoE for the MLP blocks.\n",
    "\n",
    "This file closely follows the official Jamba implementation (https://huggingface.co/ai21labs/Jamba-v0.1).\n",
    "But it is quite shorter (800 lines vs 2100 lines), because it has been stripped of all optional features that come with transformers.\n",
    "It is thus easier to read, while keeping the same performances.\n",
    "It supports training (using official CUDA mamba backend or mamba.py) & inference.\n",
    "You can also load pretrained Jamba models from HF using the from_pretrained function.\n",
    "\n",
    "Architecture of the torch modules found in this file :\n",
    "- JambaLM: the final object, used for language modeling. has an embedding layer, an lm head...\n",
    "- Jamba: core model. inputs (B, L, D), outputs (B, L, D). (B=batch size, L=seq len, D=d_model).\n",
    "  It is composed of two types of layers : MambaLayer and AttentionLayer.\n",
    "- AttentionLayer: standard GQA attention layer + MoE MLP (the attn computations are located in the AttentionSDPA module)\n",
    "- MambaLayer : standard Mamba layer + MoE MLP. (the Mamba computations are located in the mamba.py file)\n",
    "- SparseMoEBlock and MLP : Moe MLP\n",
    "\n",
    "Notes :\n",
    "-if using use_cuda, you must train in float32. If not, the following error is triggered : \n",
    "\"Expected B.scalar_type() == (!is_variable_B ? weight_type : input_type) to be true, but got false.\"\n",
    "when calling the selective_scan_fn function. Not clear why this error shows up when in (b)float16. TODO: investigate.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "@dataclass\n",
    "class JambaLMConfig:\n",
    "    \n",
    "    d_model: int\n",
    "    n_layers: int\n",
    "    \n",
    "    mlp_size: int\n",
    "    \n",
    "    initializer_range: float = 0.02\n",
    "    rms_norm_eps: float = 1e-5\n",
    "\n",
    "    # mamba related\n",
    "    d_state: int = 16 # N in paper\n",
    "    expand_factor: int = 2 # N in paper\n",
    "    d_conv: int = 4\n",
    "    dt_rank: Union[int, str] = 'auto'\n",
    "\n",
    "    dt_min: float = 0.001\n",
    "    dt_max: float = 0.1\n",
    "    dt_init: str = \"random\" # \"random\" or \"constant\"\n",
    "    dt_scale: float = 1.0\n",
    "    dt_init_floor = 1e-4\n",
    "    bias: bool = False\n",
    "    conv_bias: bool = True\n",
    "    inner_layernorms: bool = True\n",
    "    use_cuda: bool = False\n",
    "    pscan: bool = True # use parallel scan mode or sequential mode when training\n",
    "\n",
    "    # attention related\n",
    "    num_attention_heads: int = 32\n",
    "    num_key_value_heads: int = 8 # GQA\n",
    "    attention_dropout: float = 0.\n",
    "\n",
    "    # MoE related\n",
    "    num_experts: int = 16\n",
    "    num_experts_per_tok: int = 2\n",
    "\n",
    "    # structure\n",
    "    attn_layer_offset: int = 4\n",
    "    attn_layer_period: int = 8\n",
    "    expert_layer_offset: int = 1\n",
    "    expert_layer_period: int = 2\n",
    "\n",
    "    # language modeling\n",
    "    vocab_size: int = 65536\n",
    "    pad_token_id: int = 0\n",
    "    tie_lm_weights: bool = True\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.d_inner = self.expand_factor * self.d_model # E*D = ED in comments\n",
    "\n",
    "        if self.dt_rank == 'auto':\n",
    "            self.dt_rank = math.ceil(self.d_model / 16)\n",
    "\n",
    "        self.mamba_config = MambaConfig(d_model=self.d_model, n_layers=0, dt_rank=self.dt_rank, d_state=self.d_state,\n",
    "                                        expand_factor=self.expand_factor, d_conv=self.d_conv, dt_min=self.dt_min, dt_max=self.dt_max,\n",
    "                                        dt_init=self.dt_init, dt_scale=self.dt_scale, rms_norm_eps=self.rms_norm_eps,\n",
    "                                        bias=self.bias, conv_bias=self.conv_bias, inner_layernorms=self.inner_layernorms,\n",
    "                                        pscan=self.pscan, use_cuda=self.use_cuda)\n",
    "\n",
    "def from_pretrained(name: str):\n",
    "    \"\"\"\n",
    "    Returns a model loaded with pretrained weights pulled from HuggingFace.\n",
    "    The model has to follow the same structure as the original Jamba model on HF (ai21labs/Jamba-v0.1).\n",
    "    You can easily adapt this function.\n",
    "\n",
    "    Args:\n",
    "        name: for example:\n",
    "            * 'TechxGenus/Mini-Jamba'\n",
    "            * 'ai21labs/Jamba-v0.1'\n",
    "\n",
    "    Returns:\n",
    "        model: a Jamba model configured with the proper parameters and initialized with the proper weights\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        from transformers import AutoModelForCausalLM\n",
    "    except ImportError:\n",
    "        print(\"The from_pretrained function pulls weights from HuggingFace and thus needs transformers to be installed (pip install transformers)\")\n",
    "        return\n",
    "\n",
    "    model_hf = AutoModelForCausalLM.from_pretrained(name, torch_dtype=torch.float32, use_mamba_kernels=False, device_map=\"auto\", trust_remote_code=True)\n",
    "        \n",
    "    # copy config data\n",
    "    config = JambaLMConfig(vocab_size=model_hf.config.vocab_size, d_model=model_hf.config.hidden_size, n_layers=model_hf.config.num_hidden_layers, \n",
    "                                rms_norm_eps=model_hf.config.rms_norm_eps, mlp_size=model_hf.config.intermediate_size, inner_layernorms=model_hf.config.mamba_inner_layernorms,\n",
    "                                expand_factor=model_hf.config.mamba_expand, dt_rank=model_hf.config.mamba_dt_rank, d_state=model_hf.config.mamba_d_state,\n",
    "                                d_conv=model_hf.config.mamba_d_conv, conv_bias=model_hf.config.mamba_conv_bias, initializer_range=model_hf.config.initializer_range,\n",
    "                                num_experts=model_hf.config.num_experts, num_experts_per_tok=model_hf.config.num_experts_per_tok, \n",
    "                                attn_layer_offset=model_hf.config.attn_layer_offset, attn_layer_period=model_hf.config.attn_layer_period, \n",
    "                                expert_layer_offset=model_hf.config.expert_layer_offset, expert_layer_period=model_hf.config.expert_layer_period,\n",
    "                                num_key_value_heads=model_hf.config.num_key_value_heads, num_attention_heads=model_hf.config.num_attention_heads,\n",
    "                                pad_token_id=model_hf.config.pad_token_id, bias=model_hf.config.mamba_proj_bias, attention_dropout=model_hf.config.attention_dropout,\n",
    "                                tie_lm_weights=model_hf.config.tie_word_embeddings)\n",
    "\n",
    "    model = JambaLM(config)\n",
    "\n",
    "    # copy weights\n",
    "    for name, param in model_hf.named_parameters():\n",
    "        name = name.replace(\"model.\", \"jamba.\")\n",
    "        \n",
    "        if \"embed_tokens\" in name:\n",
    "            name = \"embedding.weight\"\n",
    "        \n",
    "        if \"final_layernorm\" in name:\n",
    "            name = name.replace(\"jamba.\", \"\")\n",
    "\n",
    "        counterpart_param = model.get_parameter(name)\n",
    "        if counterpart_param is not None:\n",
    "            counterpart_param.data.copy_(param.data)\n",
    "\n",
    "    del model_hf\n",
    "\n",
    "    return model\n",
    "\n",
    "class JambaLM(nn.Module):\n",
    "    def __init__(self, config: JambaLMConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        self.padding_idx = config.pad_token_id\n",
    "        self.vocab_size = config.vocab_size\n",
    "\n",
    "        self.embedding = nn.Embedding(config.vocab_size, config.d_model, self.padding_idx)\n",
    "        self.jamba = Jamba(config)\n",
    "        self.final_layernorm = RMSNorm(config.d_model, config.rms_norm_eps)\n",
    "\n",
    "        self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
    "        if self.config.tie_lm_weights:\n",
    "            self.lm_head.weight = self.embedding.weight \n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        # tokens : (B, L)\n",
    "\n",
    "        # logits : (B, L, vocab_size)\n",
    "        # router_logits : (B*L, n_experts) if n_experts>1\n",
    "\n",
    "        x = self.embedding(tokens)\n",
    "\n",
    "        x, router_logits = self.jamba(x)\n",
    "        x = self.final_layernorm(x)\n",
    "\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if self.config.num_experts == 1:\n",
    "            return logits\n",
    "        else:\n",
    "            return logits, router_logits\n",
    "    \n",
    "    def step(self, tokens, caches):\n",
    "        # tokens : (B, L)\n",
    "\n",
    "        # logits : (B, L, vocab_size)\n",
    "\n",
    "        x = self.embedding(tokens)\n",
    "\n",
    "        x, caches = self.jamba.step(x, caches)\n",
    "        x = self.final_layernorm(x)\n",
    "\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        return logits, caches\n",
    "\n",
    "    # TODO process prompt in parallel, and pass in sequential mode when prompt is finished ?\n",
    "    def generate(self, tokenizer, prompt: str, max_tokens: int = 50, batch_size: int = 1, sample: bool = True, top_k: int = 40, temperature: float = 1.0):\n",
    "        self.eval()\n",
    "\n",
    "        input_ids = tokenizer(prompt, return_tensors='pt').input_ids.to(next(self.parameters()).device) # (1, num_tokens)\n",
    "        input_ids = input_ids.repeat(batch_size, 1)\n",
    "\n",
    "        # caches is a list of cache, one per layer\n",
    "        # cache is composed of : - if Mamba layer : the hidden state, and the last d_conv-1 inputs (see more in mamba_lm.py)\n",
    "        #                        - if Attention layer : the KV cache, ie 2 tensors of shape (B, num_kv_heads, L, head_dim)\n",
    "        caches = [self.jamba.layers[i].get_empty_cache(batch_size, input_ids.device) for i in range(self.config.n_layers)]\n",
    "\n",
    "        for i in range(input_ids.size(1) + max_tokens - 1):\n",
    "            with torch.no_grad():\n",
    "                # forward the new output, get new cache\n",
    "                next_token_logits, caches = self.step(input_ids[:, [i]], caches) # (batch_size, 1, vocab_size), caches\n",
    "                next_token_logits = next_token_logits.squeeze(1)\n",
    "\n",
    "            # sample (no sampling when the prompt is being processed)\n",
    "            if i+1 >= input_ids.size(1):\n",
    "                probs = F.softmax(next_token_logits / temperature, dim=-1) # (batch_size, vocab_size)\n",
    "\n",
    "                if top_k is not None:\n",
    "                    values, _ = torch.topk(probs, k=top_k) # (batch_size, k) ordered from lowest to biggest\n",
    "                    probs[probs < values[:, -1, None]] = 0\n",
    "                    probs = probs / probs.sum(axis=1, keepdims=True)\n",
    "\n",
    "                if sample:\n",
    "                    next_token = torch.multinomial(probs, num_samples=1).squeeze(1) # (batch_size)\n",
    "                else:\n",
    "                    next_token = torch.argmax(probs, dim=-1) # (batch_size)\n",
    "\n",
    "                input_ids = torch.cat([input_ids, next_token.unsqueeze(1)], dim=1)\n",
    "\n",
    "                if next_token.item() == tokenizer.eos_token_id:\n",
    "                    break\n",
    "\n",
    "        outputs = [tokenizer.decode(output.tolist(), skip_special_tokens=True) for output in input_ids[:, 1:]]\n",
    "\n",
    "        self.train()\n",
    "\n",
    "        if batch_size==1:\n",
    "            return outputs[0]\n",
    "        else:\n",
    "            return outputs\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        std = self.config.initializer_range\n",
    "\n",
    "        if isinstance(module, (nn.Linear, nn.Conv1d)):\n",
    "            module.weight.data.normal_(mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        \n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=std)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "\n",
    "class Jamba(nn.Module):\n",
    "    def __init__(self, config: JambaLMConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        # init each model layer, decide if it's mamba/attention and has experts or not\n",
    "        decoder_layers = []\n",
    "        for i in range(config.n_layers):\n",
    "            is_attn = True if (i - self.config.attn_layer_offset) % self.config.attn_layer_period == 0 else False\n",
    "            is_expert = True if (i - self.config.expert_layer_offset) % self.config.expert_layer_period == 0 else False\n",
    "\n",
    "            num_experts = self.config.num_experts if is_expert else 1\n",
    "\n",
    "            if is_attn:\n",
    "                decoder_layers.append(AttentionLayer(config, num_experts=num_experts))\n",
    "            else:\n",
    "                decoder_layers.append(MambaLayer(config, num_experts=num_experts))\n",
    "\n",
    "        self.layers = nn.ModuleList(decoder_layers)\n",
    "\n",
    "        # here you may want to init the weights in a particular manner if you don't use this jamba inside a JambaLM (see JambaLM)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, L, D)\n",
    "\n",
    "        # logits: (B, L, D)\n",
    "        # router_logits : (B*L, n_experts)\n",
    "\n",
    "        router_logits = []\n",
    "\n",
    "        for decoder_layer in self.layers:\n",
    "            layer_output, _ = decoder_layer(x)\n",
    "            x = layer_output[0]\n",
    "            router_logits.append(layer_output[1])\n",
    "\n",
    "        return x, router_logits\n",
    "    \n",
    "    def step(self, x, caches):\n",
    "        # x: (B, L, D)\n",
    "\n",
    "        # logits: (B, L, D)\n",
    "        # caches\n",
    "\n",
    "        for i, decoder_layer in enumerate(self.layers):\n",
    "            layer_output, caches[i] = decoder_layer(x, caches[i])\n",
    "            x = layer_output[0]\n",
    "\n",
    "        return x, caches\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, config: JambaLMConfig, num_experts: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attn = AttentionSDPA(config)\n",
    "\n",
    "        num_experts_per_tok = config.num_experts_per_tok if num_experts > 1 else 1\n",
    "        self.moe = SparseMoEBlock(config, num_experts=num_experts, num_experts_per_tok=num_experts_per_tok)\n",
    "        self.input_layernorm = RMSNorm(config.d_model, eps=config.rms_norm_eps)\n",
    "        self.pre_moe_layernorm = RMSNorm(config.d_model, eps=config.rms_norm_eps)\n",
    "\n",
    "    def forward(self, x, cache = None):\n",
    "        # x: (B, L, D)\n",
    "\n",
    "        # outputs: (B, L, D)\n",
    "        \n",
    "        # attention\n",
    "        residual = x\n",
    "        x = self.input_layernorm(x)\n",
    "        x, cache = self.self_attn(x, cache)\n",
    "        x = residual + x\n",
    "\n",
    "        # FFN\n",
    "        residual = x\n",
    "        x = self.pre_moe_layernorm(x)\n",
    "        x, router_logits = self.moe(x)\n",
    "        x = residual + x\n",
    "\n",
    "        outputs = (x, router_logits)\n",
    "        return outputs, cache\n",
    "\n",
    "    def get_empty_cache(self, batch_size, device):\n",
    "        return (None, None)\n",
    "\n",
    "class AttentionSDPA(nn.Module):\n",
    "    def __init__(self, config: JambaLMConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        self.hidden_size = config.d_model\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = self.hidden_size // self.num_heads\n",
    "        self.num_key_value_heads = config.num_key_value_heads\n",
    "        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n",
    "        self.attention_dropout = config.attention_dropout\n",
    "\n",
    "        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n",
    "\n",
    "    def forward(self, x, cache = None):\n",
    "        # x: (B, L, D)\n",
    "\n",
    "        # attn_output: (B, L, D)\n",
    "\n",
    "        B, L, _ = x.size()\n",
    "\n",
    "        queries = self.q_proj(x)\n",
    "        keys = self.k_proj(x)\n",
    "        values = self.v_proj(x)\n",
    "\n",
    "        queries = queries.view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        keys = keys.view(B, L, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "        values = values.view(B, L, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # kv cache implementation\n",
    "        if cache is not None:\n",
    "            past_keys, past_values = cache\n",
    "            \n",
    "            # not first in the sequence\n",
    "            if past_keys is not None:\n",
    "                keys = torch.cat([past_keys, keys], dim=2)\n",
    "                values = torch.cat([past_values, values], dim=2)\n",
    "            \n",
    "            cache = (keys, values) # prepare cache for next token\n",
    "\n",
    "        # GQA related\n",
    "        keys = repeat_kv(keys, self.num_key_value_groups)\n",
    "        values = repeat_kv(values, self.num_key_value_groups)\n",
    "\n",
    "        attn_output = F.scaled_dot_product_attention(queries, keys, values,\n",
    "                                                                       dropout_p=self.attention_dropout if self.training else 0.0,\n",
    "                                                                       is_causal=(cache is None))\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "        attn_output = attn_output.view(B, L, self.hidden_size)\n",
    "\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "\n",
    "        return attn_output, cache\n",
    "\n",
    "class MambaLayer(nn.Module):\n",
    "    def __init__(self, config: JambaLMConfig, num_experts: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "        self.mamba = MambaBlock(config=config.mamba_config)\n",
    "\n",
    "        num_experts_per_tok = config.num_experts_per_tok if num_experts > 1 else 1\n",
    "        self.moe = SparseMoEBlock(config, num_experts=num_experts, num_experts_per_tok=num_experts_per_tok)\n",
    "        self.input_layernorm = RMSNorm(config.d_model, eps=config.rms_norm_eps)\n",
    "        self.pre_moe_layernorm = RMSNorm(config.d_model, eps=config.rms_norm_eps)\n",
    "\n",
    "    def forward(self, x, cache = None):\n",
    "        # x: (B, L, D)\n",
    "\n",
    "        # outputs: (B, L, D)\n",
    "\n",
    "        # mamba\n",
    "        residual = x\n",
    "        x = self.input_layernorm(x)\n",
    "        if cache is None:\n",
    "            x = self.mamba(x)\n",
    "        else:\n",
    "            x, cache = self.mamba.step(x.squeeze(1), cache)\n",
    "            x = x.unsqueeze(1)\n",
    "        x = residual + x\n",
    "\n",
    "        # FFN\n",
    "        residual = x\n",
    "        x = self.pre_moe_layernorm(x)\n",
    "        x, router_logits = self.moe(x)\n",
    "        x = residual + x\n",
    "\n",
    "        outputs = (x, router_logits)\n",
    "\n",
    "        return outputs, cache\n",
    "    \n",
    "    def get_empty_cache(self, batch_size, device):\n",
    "        return (None, torch.zeros(batch_size, self.config.d_inner, self.config.d_conv-1, device=device))\n",
    "\n",
    "class SparseMoEBlock(nn.Module):\n",
    "    def __init__(self, config: JambaLMConfig, num_experts: int, num_experts_per_tok: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_dim = config.d_model\n",
    "        self.ffn_dim = config.mlp_size\n",
    "\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = num_experts_per_tok\n",
    "\n",
    "        if num_experts > 1:\n",
    "            self.router = nn.Linear(self.hidden_dim, self.num_experts, bias=False)\n",
    "        else:\n",
    "            self.router = None\n",
    "\n",
    "        self.experts = nn.ModuleList([MLP(config) for _ in range(self.num_experts)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, L, D)\n",
    "\n",
    "        # final_hidden_states: (B, L, D)\n",
    "        # router_logits: (B*L, n_experts)\n",
    "\n",
    "        #note : it is not clear why we work with shape (B*L, D) here.\n",
    "        #I copied this code from the official jamba imple, and did not have time to think it through.\n",
    "        \n",
    "        batch_size, sequence_length, hidden_dim = x.shape\n",
    "\n",
    "        # no routing\n",
    "        if self.num_experts == 1:\n",
    "            final_hidden_states = self.experts[0](x)\n",
    "            router_logits = torch.ones(\n",
    "                (batch_size * sequence_length, 1),\n",
    "                device=x.device,\n",
    "                dtype=x.dtype,\n",
    "                requires_grad=x.requires_grad,\n",
    "            )\n",
    "            return final_hidden_states, router_logits\n",
    "\n",
    "        # routing\n",
    "        x = x.view(-1, hidden_dim) # (B*L, D)\n",
    "\n",
    "        router_logits = self.router(x) # (B*L, n_experts)\n",
    "        routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n",
    "        routing_weights, selected_experts = torch.topk(routing_weights, self.top_k, dim=-1)\n",
    "        routing_weights = routing_weights.to(x.dtype)\n",
    "\n",
    "        final_hidden_states = torch.zeros((batch_size * sequence_length, hidden_dim), dtype=x.dtype, device=x.device)\n",
    "\n",
    "        # One hot encode the selected experts to create an expert mask\n",
    "        # this will be used to easily index which expert is going to be sollicitated\n",
    "        expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=self.num_experts).permute(2, 1, 0)\n",
    "\n",
    "        # loop over all available experts in the model and perform the computation on each expert\n",
    "        for expert_idx in range(self.num_experts):\n",
    "            expert_layer = self.experts[expert_idx]\n",
    "            idx, top_x = torch.where(expert_mask[expert_idx])\n",
    "\n",
    "            if top_x.shape[0] == 0:\n",
    "                continue\n",
    "\n",
    "            # in torch it is faster to index using lists than torch tensors\n",
    "            top_x_list = top_x.tolist()\n",
    "            idx_list = idx.tolist()\n",
    "\n",
    "            # Index the correct hidden states and compute the expert hidden state for\n",
    "            # the current expert. We need to make sure to multiply the output hidden\n",
    "            # states by `routing_weights` on the corresponding tokens (top-1 and top-2)\n",
    "            current_state = x[None, top_x_list].reshape(-1, hidden_dim)\n",
    "            current_hidden_states = expert_layer(current_state) * routing_weights[top_x_list, idx_list, None]\n",
    "\n",
    "            # However `index_add_` only support torch tensors for indexing so we'll use\n",
    "            # the `top_x` tensor here.\n",
    "            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(x.dtype))\n",
    "        final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n",
    "        \n",
    "        return final_hidden_states, router_logits\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config: JambaLMConfig):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_dim = config.d_model\n",
    "        self.ffn_dim = config.mlp_size\n",
    "\n",
    "        self.gate_proj = nn.Linear(self.hidden_dim, self.ffn_dim, bias=False)\n",
    "        self.down_proj = nn.Linear(self.ffn_dim, self.hidden_dim, bias=False)\n",
    "        self.up_proj = nn.Linear(self.hidden_dim, self.ffn_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x : (B, L, D)\n",
    "\n",
    "        # y : (B, L, D)\n",
    "\n",
    "        return self.down_proj(F.silu(self.gate_proj(x)) * self.up_proj(x))\n",
    "    \n",
    "def load_balancing_loss(router_logits, num_experts, num_experts_per_tok):\n",
    "    # router_logits: list of router_logit, one per layer, each (B*D, n_experts)\n",
    "\n",
    "    # moe_aux_loss : scalar\n",
    "\n",
    "    router_logits = torch.cat([r for r in router_logits if r.shape[1] > 1], dim=0)\n",
    "\n",
    "    routing_weights = torch.nn.functional.softmax(router_logits, dim=-1)\n",
    "    _, selected_experts = torch.topk(routing_weights, num_experts_per_tok, dim=-1)\n",
    "    expert_mask = torch.nn.functional.one_hot(selected_experts, num_experts)\n",
    "\n",
    "    # percentage of tokens routed to each experts\n",
    "    tokens_per_expert = torch.mean(expert_mask.float(), dim=0)\n",
    "\n",
    "    # average probability of routing to these experts\n",
    "    router_prob_per_expert = torch.mean(routing_weights, dim=0)\n",
    "\n",
    "    moe_aux_loss = torch.sum(tokens_per_expert * router_prob_per_expert.unsqueeze(0))\n",
    "    return moe_aux_loss * num_experts\n",
    "\n",
    "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n",
    "    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n",
    "    \"\"\"\n",
    "\n",
    "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
    "    if n_rep == 1:\n",
    "        return hidden_states\n",
    "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
    "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
