{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import wandb\n",
    "import gc\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import mambapy\n",
    "from mambapy.mamba import Mamba, MambaConfig\n",
    "from torch import nn, optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BalancedDataset(Dataset):\n",
    "    def __init__(self, X, y, limit_per_label=1600):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.limit_per_label = limit_per_label\n",
    "        self.classes = np.unique(y)\n",
    "        self.indices = self.balance_classes()\n",
    "\n",
    "    def balance_classes(self):\n",
    "        indices = []\n",
    "        for cls in self.classes:\n",
    "            cls_indices = np.where(self.y == cls)[0]\n",
    "            if len(cls_indices) > self.limit_per_label:\n",
    "                cls_indices = np.random.choice(cls_indices, self.limit_per_label, replace=False)\n",
    "            indices.extend(cls_indices)\n",
    "        np.random.shuffle(indices)\n",
    "        return indices\n",
    "\n",
    "    def re_sample(self):\n",
    "        self.indices = self.balance_classes()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        index = self.indices[idx]\n",
    "        return self.X[index], self.y[index]\n",
    "# Custom Dataset for validation with limit per class\n",
    "class BalancedValidationDataset(Dataset):\n",
    "    def __init__(self, X, y, limit_per_label=400):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.limit_per_label = limit_per_label\n",
    "        self.classes = np.unique(y)\n",
    "        self.indices = self.balance_classes()\n",
    "\n",
    "    def balance_classes(self):\n",
    "        indices = []\n",
    "        for cls in self.classes:\n",
    "            cls_indices = np.where(self.y == cls)[0]\n",
    "            if len(cls_indices) > self.limit_per_label:\n",
    "                cls_indices = np.random.choice(cls_indices, self.limit_per_label, replace=False)\n",
    "            indices.extend(cls_indices)\n",
    "        np.random.shuffle(indices)\n",
    "        return indices\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        index = self.indices[idx]\n",
    "        return self.X[index], self.y[index]\n",
    "    \n",
    "def train_model_mamba(\n",
    "    model, train_loader, val_loader, test_loader, \n",
    "    num_epochs=500, lr=1e-4, max_patience=20, device='cuda'\n",
    "):\n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Define optimizer, scheduler, and loss function\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=int(max_patience / 3), verbose=True\n",
    "    )\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience = max_patience\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Resample training and validation data\n",
    "        train_loader.dataset.re_sample()\n",
    "        val_loader.dataset.balance_classes()\n",
    "\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss, train_accuracy = 0.0, 0.0\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * X_batch.size(0)\n",
    "            train_accuracy += (outputs.argmax(dim=1) == y_batch).float().mean().item()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss, val_accuracy = 0.0, 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_val, y_val in val_loader:\n",
    "                X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "                outputs = model(X_val)\n",
    "                loss = criterion(outputs, y_val)\n",
    "\n",
    "                val_loss += loss.item() * X_val.size(0)\n",
    "                val_accuracy += (outputs.argmax(dim=1) == y_val).float().mean().item()\n",
    "\n",
    "        # Test phase and metric collection\n",
    "        test_loss, test_accuracy = 0.0, 0.0\n",
    "        y_true, y_pred = [], []\n",
    "        with torch.no_grad():\n",
    "            for X_test, y_test in test_loader:\n",
    "                X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "                outputs = model(X_test)\n",
    "                loss = criterion(outputs, y_test)\n",
    "\n",
    "                test_loss += loss.item() * X_test.size(0)\n",
    "                test_accuracy += (outputs.argmax(dim=1) == y_test).float().mean().item()\n",
    "                y_true.extend(y_test.cpu().numpy())\n",
    "                y_pred.extend(outputs.argmax(dim=1).cpu().numpy())\n",
    "\n",
    "        # Update scheduler\n",
    "        scheduler.step(val_loss / len(val_loader.dataset))\n",
    "\n",
    "        # Log metrics to WandB\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": train_loss / len(train_loader.dataset),\n",
    "            \"val_loss\": val_loss / len(val_loader.dataset),\n",
    "            \"train_accuracy\": train_accuracy / len(train_loader),\n",
    "            \"val_accuracy\": val_accuracy / len(val_loader),\n",
    "            \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "            \"test_loss\": test_loss / len(test_loader.dataset),\n",
    "            \"test_accuracy\": test_accuracy / len(test_loader),\n",
    "            \"confusion_matrix\": wandb.plot.confusion_matrix(\n",
    "                probs=None, y_true=y_true, preds=y_pred, class_names=np.unique(y_true)\n",
    "            ),\n",
    "            \"classification_report\": classification_report(\n",
    "                y_true, y_pred, target_names=[str(i) for i in range(len(np.unique(y_true)))]\n",
    "            )\n",
    "        })\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience = max_patience\n",
    "            best_model = model.state_dict()\n",
    "        else:\n",
    "            patience -= 1\n",
    "            if patience <= 0:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    # Load the best model weights\n",
    "    model.load_state_dict(best_model)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and preprocess your data (example from original script)\n",
    "    # Load and preprocess data\n",
    "    X = pd.read_pickle(\"Pickles/trainv2.pkl\")\n",
    "    y = X[\"label\"]\n",
    "    label_mapping = {'star': 0, 'binary_star': 1, 'galaxy': 2, 'agn': 3}\n",
    "    y = y.map(label_mapping).values\n",
    "    X = X.drop([\"parallax\", \"ra\", \"dec\", \"ra_error\", \"dec_error\", \"parallax_error\", \"pmra\", \"pmdec\", \"pmra_error\", \"pmdec_error\", \n",
    "                \"phot_g_mean_flux\", \"flagnopllx\", \"phot_g_mean_flux_error\", \"phot_bp_mean_flux\", \"phot_rp_mean_flux\", \n",
    "                \"phot_bp_mean_flux_error\", \"phot_rp_mean_flux_error\", \"label\", \"obsid\"], axis=1).values\n",
    "    \n",
    "    # Read test data\n",
    "    X_test = pd.read_pickle(\"Pickles/testv2.pkl\")\n",
    "    y_test = X_test[\"label\"].map(label_mapping).values\n",
    "    X_test = X_test.drop([\"parallax\", \"ra\", \"dec\", \"ra_error\", \"dec_error\", \"parallax_error\", \"pmra\", \"pmdec\", \"pmra_error\", \"pmdec_error\", \n",
    "                \"phot_g_mean_flux\", \"flagnopllx\", \"phot_g_mean_flux_error\", \"phot_bp_mean_flux\", \"phot_rp_mean_flux\", \n",
    "                \"phot_bp_mean_flux_error\", \"phot_rp_mean_flux_error\", \"label\", \"obsid\"], axis=1).values\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Clear memory\n",
    "    del X, y\n",
    "    gc.collect()\n",
    "\n",
    "    # Convert to torch tensors and create datasets\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32).unsqueeze(1)\n",
    "    X_val = torch.tensor(X_val, dtype=torch.float32).unsqueeze(1)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "    y_val = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "    train_dataset = BalancedDataset(X_train, y_train)\n",
    "    val_dataset = BalancedValidationDataset(X_val, y_val)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(BalancedValidationDataset(torch.tensor(X_test, dtype=torch.float32).unsqueeze(1),\n",
    "                                                    torch.tensor(y_test, dtype=torch.long)), batch_size=batch_size, shuffle=False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StarClassifierMAMBA(nn.Module):\n",
    "    def __init__(self, d_model, num_classes, d_state=64, d_conv=4, input_dim=17, n_layers=6):\n",
    "        super(StarClassifierMAMBA, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # MAMBA layer initialization\n",
    "        config = MambaConfig(\n",
    "            d_model=d_model,\n",
    "            d_state=d_state,\n",
    "            d_conv=d_conv,\n",
    "            n_layers=n_layers\n",
    "\n",
    "        )\n",
    "        self.mamba_layer = Mamba(config)\n",
    "\n",
    "        # Input projection to match the MAMBA layer dimension\n",
    "        self.input_projection = nn.Linear(input_dim, d_model)\n",
    "\n",
    "        # Fully connected classifier head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, num_classes)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_projection(x)  # Ensure the input has the correct dimension\n",
    "        #x = x.unsqueeze(1)  # Adds a sequence dimension (L=1).\n",
    "        x = self.mamba_layer(x)\n",
    "        x = x.mean(dim=1)  # Pooling operation for classification\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\jcwin\\OneDrive - University of Southampton\\_Southampton\\2024-25\\Star-Classifier\\wandb\\run-20241127_215106-1vsdawdz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/joaoc-university-of-southampton/lamost-mamba-test/runs/1vsdawdz' target=\"_blank\">classic-lion-15</a></strong> to <a href='https://wandb.ai/joaoc-university-of-southampton/lamost-mamba-test' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/joaoc-university-of-southampton/lamost-mamba-test' target=\"_blank\">https://wandb.ai/joaoc-university-of-southampton/lamost-mamba-test</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/joaoc-university-of-southampton/lamost-mamba-test/runs/1vsdawdz' target=\"_blank\">https://wandb.ai/joaoc-university-of-southampton/lamost-mamba-test/runs/1vsdawdz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StarClassifierMAMBA(\n",
      "  (mamba_layer): Mamba(\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x ResidualBlock(\n",
      "        (mixer): MambaBlock(\n",
      "          (in_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (conv1d): Conv1d(4096, 4096, kernel_size=(4,), stride=(1,), padding=(3,), groups=4096)\n",
      "          (x_proj): Linear(in_features=4096, out_features=256, bias=False)\n",
      "          (dt_proj): Linear(in_features=128, out_features=4096, bias=True)\n",
      "          (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n",
      "        )\n",
      "        (norm): RMSNorm()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (input_projection): Linear(in_features=3748, out_features=2048, bias=True)\n",
      "  (classifier): Sequential(\n",
      "    (0): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): Linear(in_features=2048, out_features=4, bias=True)\n",
      "  )\n",
      ")\n",
      "mamba_layer.layers.0.mixer.A_log 262144\n",
      "mamba_layer.layers.0.mixer.D 4096\n",
      "mamba_layer.layers.0.mixer.in_proj.weight 16777216\n",
      "mamba_layer.layers.0.mixer.conv1d.weight 16384\n",
      "mamba_layer.layers.0.mixer.conv1d.bias 4096\n",
      "mamba_layer.layers.0.mixer.x_proj.weight 1048576\n",
      "mamba_layer.layers.0.mixer.dt_proj.weight 524288\n",
      "mamba_layer.layers.0.mixer.dt_proj.bias 4096\n",
      "mamba_layer.layers.0.mixer.out_proj.weight 8388608\n",
      "mamba_layer.layers.0.norm.weight 2048\n",
      "mamba_layer.layers.1.mixer.A_log 262144\n",
      "mamba_layer.layers.1.mixer.D 4096\n",
      "mamba_layer.layers.1.mixer.in_proj.weight 16777216\n",
      "mamba_layer.layers.1.mixer.conv1d.weight 16384\n",
      "mamba_layer.layers.1.mixer.conv1d.bias 4096\n",
      "mamba_layer.layers.1.mixer.x_proj.weight 1048576\n",
      "mamba_layer.layers.1.mixer.dt_proj.weight 524288\n",
      "mamba_layer.layers.1.mixer.dt_proj.bias 4096\n",
      "mamba_layer.layers.1.mixer.out_proj.weight 8388608\n",
      "mamba_layer.layers.1.norm.weight 2048\n",
      "mamba_layer.layers.2.mixer.A_log 262144\n",
      "mamba_layer.layers.2.mixer.D 4096\n",
      "mamba_layer.layers.2.mixer.in_proj.weight 16777216\n",
      "mamba_layer.layers.2.mixer.conv1d.weight 16384\n",
      "mamba_layer.layers.2.mixer.conv1d.bias 4096\n",
      "mamba_layer.layers.2.mixer.x_proj.weight 1048576\n",
      "mamba_layer.layers.2.mixer.dt_proj.weight 524288\n",
      "mamba_layer.layers.2.mixer.dt_proj.bias 4096\n",
      "mamba_layer.layers.2.mixer.out_proj.weight 8388608\n",
      "mamba_layer.layers.2.norm.weight 2048\n",
      "mamba_layer.layers.3.mixer.A_log 262144\n",
      "mamba_layer.layers.3.mixer.D 4096\n",
      "mamba_layer.layers.3.mixer.in_proj.weight 16777216\n",
      "mamba_layer.layers.3.mixer.conv1d.weight 16384\n",
      "mamba_layer.layers.3.mixer.conv1d.bias 4096\n",
      "mamba_layer.layers.3.mixer.x_proj.weight 1048576\n",
      "mamba_layer.layers.3.mixer.dt_proj.weight 524288\n",
      "mamba_layer.layers.3.mixer.dt_proj.bias 4096\n",
      "mamba_layer.layers.3.mixer.out_proj.weight 8388608\n",
      "mamba_layer.layers.3.norm.weight 2048\n",
      "mamba_layer.layers.4.mixer.A_log 262144\n",
      "mamba_layer.layers.4.mixer.D 4096\n",
      "mamba_layer.layers.4.mixer.in_proj.weight 16777216\n",
      "mamba_layer.layers.4.mixer.conv1d.weight 16384\n",
      "mamba_layer.layers.4.mixer.conv1d.bias 4096\n",
      "mamba_layer.layers.4.mixer.x_proj.weight 1048576\n",
      "mamba_layer.layers.4.mixer.dt_proj.weight 524288\n",
      "mamba_layer.layers.4.mixer.dt_proj.bias 4096\n",
      "mamba_layer.layers.4.mixer.out_proj.weight 8388608\n",
      "mamba_layer.layers.4.norm.weight 2048\n",
      "mamba_layer.layers.5.mixer.A_log 262144\n",
      "mamba_layer.layers.5.mixer.D 4096\n",
      "mamba_layer.layers.5.mixer.in_proj.weight 16777216\n",
      "mamba_layer.layers.5.mixer.conv1d.weight 16384\n",
      "mamba_layer.layers.5.mixer.conv1d.bias 4096\n",
      "mamba_layer.layers.5.mixer.x_proj.weight 1048576\n",
      "mamba_layer.layers.5.mixer.dt_proj.weight 524288\n",
      "mamba_layer.layers.5.mixer.dt_proj.bias 4096\n",
      "mamba_layer.layers.5.mixer.out_proj.weight 8388608\n",
      "mamba_layer.layers.5.norm.weight 2048\n",
      "input_projection.weight 7675904\n",
      "input_projection.bias 2048\n",
      "classifier.0.weight 2048\n",
      "classifier.0.bias 2048\n",
      "classifier.1.weight 8192\n",
      "classifier.1.bias 4\n",
      "Total number of parameters: 169879556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jcwin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb5f9e41971b4e19a829e2366bcdf126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.516 MB of 0.516 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>learning_rate</td><td>███████████████████████████████████▃▃▃▃▁</td></tr><tr><td>test_accuracy</td><td>▁▃▄▄▄▅▆▅▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██████</td></tr><tr><td>test_loss</td><td>█▇▆▅▅▄▄▄▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_accuracy</td><td>▁▁▃▅▆▆▆▆▆▆▆▆▆▆▆▇▆▆▇▇▇▇▇▇▇▇▇▇▇▇██▇▇█▇████</td></tr><tr><td>train_loss</td><td>█▇▆▆▆▅▅▄▄▄▄▃▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▁▂▃▃▄▅▅▆▆▆▆▇▆▇▇▇▇▇▇▇▇▇▇█████████▇██████</td></tr><tr><td>val_loss</td><td>█▇▅▅▅▄▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>classification_report</td><td>              precis...</td></tr><tr><td>epoch</td><td>384</td></tr><tr><td>learning_rate</td><td>0.0</td></tr><tr><td>test_accuracy</td><td>0.76964</td></tr><tr><td>test_loss</td><td>0.55254</td></tr><tr><td>train_accuracy</td><td>0.78883</td></tr><tr><td>train_loss</td><td>0.4609</td></tr><tr><td>val_accuracy</td><td>0.73679</td></tr><tr><td>val_loss</td><td>0.56997</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">classic-lion-15</strong> at: <a href='https://wandb.ai/joaoc-university-of-southampton/lamost-mamba-test/runs/1vsdawdz' target=\"_blank\">https://wandb.ai/joaoc-university-of-southampton/lamost-mamba-test/runs/1vsdawdz</a><br/> View project at: <a href='https://wandb.ai/joaoc-university-of-southampton/lamost-mamba-test' target=\"_blank\">https://wandb.ai/joaoc-university-of-southampton/lamost-mamba-test</a><br/>Synced 5 W&B file(s), 0 media file(s), 766 artifact file(s) and 385 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241127_215106-1vsdawdz\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the model with your parameters\n",
    "d_model = 2048 # Embedding dimension\n",
    "num_classes = 4  # Star classification categories\n",
    "\n",
    "# Define the training parameters\n",
    "num_epochs = 500\n",
    "lr = 2e-7\n",
    "patience = 50   \n",
    "depth = 6\n",
    "\n",
    "# Define the config dictionary object\n",
    "config = {\"num_classes\": num_classes, \"batch_size\": batch_size, \"lr\": lr, \"patience\": patience, \"num_epochs\": num_epochs, \"d_model\": d_model, \"depth\": depth}\n",
    "\n",
    "# Initialize WandB project\n",
    "wandb.init(project=\"lamost-mamba-test\", entity=\"joaoc-university-of-southampton\", config=config)\n",
    "# Initialize and train the model\n",
    "# Train the model using your `train_model_vit` or an adjusted training loop\n",
    "model_mamba = StarClassifierMAMBA(d_model=d_model, num_classes=num_classes, input_dim=3748, n_layers=depth)\n",
    "print(model_mamba)\n",
    "# print number of parameters per layer\n",
    "for name, param in model_mamba.named_parameters():\n",
    "    print(name, param.numel())\n",
    "print(\"Total number of parameters:\", sum(p.numel() for p in model_mamba.parameters() if p.requires_grad))\n",
    "\n",
    "# Move the model to device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model_mamba = model_mamba.to(device)\n",
    "\n",
    "# Train the model using your `train_model_vit` or an adjusted training loop\n",
    "trained_model = train_model_mamba(\n",
    "    model=model_mamba,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    num_epochs=num_epochs,\n",
    "    lr=lr,\n",
    "    max_patience=patience,\n",
    "    device=device\n",
    ")\n",
    "# Save the model and finish WandB session\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wandb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 27\u001b[0m\n\u001b[0;32m     23\u001b[0m         trained_model \u001b[38;5;241m=\u001b[39m train_model_mamba(model_mamba, train_loader, val_loader, test_loader, num_epochs\u001b[38;5;241m=\u001b[39mnum_epochs, lr\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlr, max_patience\u001b[38;5;241m=\u001b[39mpatience, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Start sweep\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m sweep_id \u001b[38;5;241m=\u001b[39m \u001b[43mwandb\u001b[49m\u001b[38;5;241m.\u001b[39msweep(sweep_config, project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspectra-mamba-sweep\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     28\u001b[0m wandb\u001b[38;5;241m.\u001b[39magent(sweep_id, function\u001b[38;5;241m=\u001b[39msweep_train, count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'wandb' is not defined"
     ]
    }
   ],
   "source": [
    "# Define the hyperparameters\n",
    "num_classes = 4\n",
    "num_epochs = 500\n",
    "patience = 30\n",
    "\n",
    "# Define sweep config\n",
    "sweep_config = {\n",
    "    \"method\": \"random\",\n",
    "    \"metric\": {\"name\": \"test_accuracy\", \"goal\": \"maximize\"},\n",
    "    \"parameters\": {\n",
    "        \"lr\": {\"values\": [1e-4, 2e-6, 1e-5]},\n",
    "        \"dim\": {\"values\": [64, 256, 512, 1024, 2048]},\n",
    "        \"depth\": {\"values\": [3, 6, 12]}\n",
    "    }\n",
    "}\n",
    "\n",
    "def sweep_train():\n",
    "    with wandb.init() as run:\n",
    "        config = run.config\n",
    "        model_mamba = StarClassifierMAMBA(d_model=config.dim, num_classes=num_classes, input_dim=3748, n_layers=config.depth)\n",
    "        \n",
    "        # Pass config.num_epochs explicitly\n",
    "        trained_model = train_model_mamba(model_mamba, train_loader, val_loader, test_loader, num_epochs=num_epochs, lr=config.lr, max_patience=patience, device='cuda')\n",
    "\n",
    "\n",
    "# Start sweep\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"spectra-mamba-sweep\")\n",
    "wandb.agent(sweep_id, function=sweep_train, count=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: joaoc (joaoc-university-of-southampton). Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\jcwin\\OneDrive - University of Southampton\\_Southampton\\2024-25\\Star-Classifier\\wandb\\run-20241127_161828-4w8y2kb7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/joaoc-university-of-southampton/lamost-jamba-test/runs/4w8y2kb7' target=\"_blank\">fancy-haze-1</a></strong> to <a href='https://wandb.ai/joaoc-university-of-southampton/lamost-jamba-test' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/joaoc-university-of-southampton/lamost-jamba-test' target=\"_blank\">https://wandb.ai/joaoc-university-of-southampton/lamost-jamba-test</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/joaoc-university-of-southampton/lamost-jamba-test/runs/4w8y2kb7' target=\"_blank\">https://wandb.ai/joaoc-university-of-southampton/lamost-jamba-test/runs/4w8y2kb7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jamba(\n",
      "  (layers): ModuleList(\n",
      "    (0-3): 4 x JambaBlock(\n",
      "      (mamba_layer): MambaBlock(\n",
      "        (in_proj): Linear(in_features=3748, out_features=14992, bias=False)\n",
      "        (conv1d): Conv1d(7496, 7496, kernel_size=(128,), stride=(1,), padding=(127,), groups=7496)\n",
      "        (x_proj): Linear(in_features=7496, out_features=491, bias=False)\n",
      "        (dt_proj): Linear(in_features=235, out_features=7496, bias=True)\n",
      "        (out_proj): Linear(in_features=7496, out_features=3748, bias=False)\n",
      "      )\n",
      "      (mamba_moe_layer): MambaMoELayer(\n",
      "        (mamba): MambaBlock(\n",
      "          (in_proj): Linear(in_features=3748, out_features=14992, bias=False)\n",
      "          (conv1d): Conv1d(7496, 7496, kernel_size=(128,), stride=(1,), padding=(127,), groups=7496)\n",
      "          (x_proj): Linear(in_features=7496, out_features=491, bias=False)\n",
      "          (dt_proj): Linear(in_features=235, out_features=7496, bias=True)\n",
      "          (out_proj): Linear(in_features=7496, out_features=3748, bias=False)\n",
      "        )\n",
      "        (moe): MoE(\n",
      "          (gate): Top2Gating()\n",
      "          (experts): Experts(\n",
      "            (act): ReLU()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (transformer): TransformerBlock(\n",
      "        (attn): MultiQueryAttention(\n",
      "          (Wqkv): Linear(in_features=3748, out_features=4684, bias=True)\n",
      "          (out_proj): Linear(in_features=3748, out_features=3748, bias=True)\n",
      "        )\n",
      "        (ffn): FeedForward(\n",
      "          (ff): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): Linear(in_features=3748, out_features=14992, bias=True)\n",
      "              (1): SiLU()\n",
      "            )\n",
      "            (1): LayerNorm((14992,), eps=1e-05, elementwise_affine=True)\n",
      "            (2): Dropout(p=0.0, inplace=False)\n",
      "            (3): Linear(in_features=14992, out_features=3748, bias=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (embed): Embedding(100, 3748)\n",
      "  (norm): Identity()\n",
      ")\n",
      "layers.0.mamba_layer.A_log 959488\n",
      "layers.0.mamba_layer.D 7496\n",
      "layers.0.mamba_layer.in_proj.weight 56190016\n",
      "layers.0.mamba_layer.conv1d.weight 959488\n",
      "layers.0.mamba_layer.conv1d.bias 7496\n",
      "layers.0.mamba_layer.x_proj.weight 3680536\n",
      "layers.0.mamba_layer.dt_proj.weight 1761560\n",
      "layers.0.mamba_layer.dt_proj.bias 7496\n",
      "layers.0.mamba_layer.out_proj.weight 28095008\n",
      "layers.0.mamba_moe_layer.mamba.A_log 959488\n",
      "layers.0.mamba_moe_layer.mamba.D 7496\n",
      "layers.0.mamba_moe_layer.mamba.in_proj.weight 56190016\n",
      "layers.0.mamba_moe_layer.mamba.conv1d.weight 959488\n",
      "layers.0.mamba_moe_layer.mamba.conv1d.bias 7496\n",
      "layers.0.mamba_moe_layer.mamba.x_proj.weight 3680536\n",
      "layers.0.mamba_moe_layer.mamba.dt_proj.weight 1761560\n",
      "layers.0.mamba_moe_layer.mamba.dt_proj.bias 7496\n",
      "layers.0.mamba_moe_layer.mamba.out_proj.weight 28095008\n",
      "layers.0.mamba_moe_layer.moe.gate.w_gating 29984\n",
      "layers.0.mamba_moe_layer.moe.experts.w1 449520128\n",
      "layers.0.mamba_moe_layer.moe.experts.w2 449520128\n",
      "layers.0.transformer.attn.Wqkv.weight 17555632\n",
      "layers.0.transformer.attn.Wqkv.bias 4684\n",
      "layers.0.transformer.attn.out_proj.weight 14047504\n",
      "layers.0.transformer.attn.out_proj.bias 3748\n",
      "layers.0.transformer.ffn.ff.0.0.weight 56190016\n",
      "layers.0.transformer.ffn.ff.0.0.bias 14992\n",
      "layers.0.transformer.ffn.ff.1.weight 14992\n",
      "layers.0.transformer.ffn.ff.1.bias 14992\n",
      "layers.0.transformer.ffn.ff.3.weight 56190016\n",
      "layers.1.mamba_layer.A_log 959488\n",
      "layers.1.mamba_layer.D 7496\n",
      "layers.1.mamba_layer.in_proj.weight 56190016\n",
      "layers.1.mamba_layer.conv1d.weight 959488\n",
      "layers.1.mamba_layer.conv1d.bias 7496\n",
      "layers.1.mamba_layer.x_proj.weight 3680536\n",
      "layers.1.mamba_layer.dt_proj.weight 1761560\n",
      "layers.1.mamba_layer.dt_proj.bias 7496\n",
      "layers.1.mamba_layer.out_proj.weight 28095008\n",
      "layers.1.mamba_moe_layer.mamba.A_log 959488\n",
      "layers.1.mamba_moe_layer.mamba.D 7496\n",
      "layers.1.mamba_moe_layer.mamba.in_proj.weight 56190016\n",
      "layers.1.mamba_moe_layer.mamba.conv1d.weight 959488\n",
      "layers.1.mamba_moe_layer.mamba.conv1d.bias 7496\n",
      "layers.1.mamba_moe_layer.mamba.x_proj.weight 3680536\n",
      "layers.1.mamba_moe_layer.mamba.dt_proj.weight 1761560\n",
      "layers.1.mamba_moe_layer.mamba.dt_proj.bias 7496\n",
      "layers.1.mamba_moe_layer.mamba.out_proj.weight 28095008\n",
      "layers.1.mamba_moe_layer.moe.gate.w_gating 29984\n",
      "layers.1.mamba_moe_layer.moe.experts.w1 449520128\n",
      "layers.1.mamba_moe_layer.moe.experts.w2 449520128\n",
      "layers.1.transformer.attn.Wqkv.weight 17555632\n",
      "layers.1.transformer.attn.Wqkv.bias 4684\n",
      "layers.1.transformer.attn.out_proj.weight 14047504\n",
      "layers.1.transformer.attn.out_proj.bias 3748\n",
      "layers.1.transformer.ffn.ff.0.0.weight 56190016\n",
      "layers.1.transformer.ffn.ff.0.0.bias 14992\n",
      "layers.1.transformer.ffn.ff.1.weight 14992\n",
      "layers.1.transformer.ffn.ff.1.bias 14992\n",
      "layers.1.transformer.ffn.ff.3.weight 56190016\n",
      "layers.2.mamba_layer.A_log 959488\n",
      "layers.2.mamba_layer.D 7496\n",
      "layers.2.mamba_layer.in_proj.weight 56190016\n",
      "layers.2.mamba_layer.conv1d.weight 959488\n",
      "layers.2.mamba_layer.conv1d.bias 7496\n",
      "layers.2.mamba_layer.x_proj.weight 3680536\n",
      "layers.2.mamba_layer.dt_proj.weight 1761560\n",
      "layers.2.mamba_layer.dt_proj.bias 7496\n",
      "layers.2.mamba_layer.out_proj.weight 28095008\n",
      "layers.2.mamba_moe_layer.mamba.A_log 959488\n",
      "layers.2.mamba_moe_layer.mamba.D 7496\n",
      "layers.2.mamba_moe_layer.mamba.in_proj.weight 56190016\n",
      "layers.2.mamba_moe_layer.mamba.conv1d.weight 959488\n",
      "layers.2.mamba_moe_layer.mamba.conv1d.bias 7496\n",
      "layers.2.mamba_moe_layer.mamba.x_proj.weight 3680536\n",
      "layers.2.mamba_moe_layer.mamba.dt_proj.weight 1761560\n",
      "layers.2.mamba_moe_layer.mamba.dt_proj.bias 7496\n",
      "layers.2.mamba_moe_layer.mamba.out_proj.weight 28095008\n",
      "layers.2.mamba_moe_layer.moe.gate.w_gating 29984\n",
      "layers.2.mamba_moe_layer.moe.experts.w1 449520128\n",
      "layers.2.mamba_moe_layer.moe.experts.w2 449520128\n",
      "layers.2.transformer.attn.Wqkv.weight 17555632\n",
      "layers.2.transformer.attn.Wqkv.bias 4684\n",
      "layers.2.transformer.attn.out_proj.weight 14047504\n",
      "layers.2.transformer.attn.out_proj.bias 3748\n",
      "layers.2.transformer.ffn.ff.0.0.weight 56190016\n",
      "layers.2.transformer.ffn.ff.0.0.bias 14992\n",
      "layers.2.transformer.ffn.ff.1.weight 14992\n",
      "layers.2.transformer.ffn.ff.1.bias 14992\n",
      "layers.2.transformer.ffn.ff.3.weight 56190016\n",
      "layers.3.mamba_layer.A_log 959488\n",
      "layers.3.mamba_layer.D 7496\n",
      "layers.3.mamba_layer.in_proj.weight 56190016\n",
      "layers.3.mamba_layer.conv1d.weight 959488\n",
      "layers.3.mamba_layer.conv1d.bias 7496\n",
      "layers.3.mamba_layer.x_proj.weight 3680536\n",
      "layers.3.mamba_layer.dt_proj.weight 1761560\n",
      "layers.3.mamba_layer.dt_proj.bias 7496\n",
      "layers.3.mamba_layer.out_proj.weight 28095008\n",
      "layers.3.mamba_moe_layer.mamba.A_log 959488\n",
      "layers.3.mamba_moe_layer.mamba.D 7496\n",
      "layers.3.mamba_moe_layer.mamba.in_proj.weight 56190016\n",
      "layers.3.mamba_moe_layer.mamba.conv1d.weight 959488\n",
      "layers.3.mamba_moe_layer.mamba.conv1d.bias 7496\n",
      "layers.3.mamba_moe_layer.mamba.x_proj.weight 3680536\n",
      "layers.3.mamba_moe_layer.mamba.dt_proj.weight 1761560\n",
      "layers.3.mamba_moe_layer.mamba.dt_proj.bias 7496\n",
      "layers.3.mamba_moe_layer.mamba.out_proj.weight 28095008\n",
      "layers.3.mamba_moe_layer.moe.gate.w_gating 29984\n",
      "layers.3.mamba_moe_layer.moe.experts.w1 449520128\n",
      "layers.3.mamba_moe_layer.moe.experts.w2 449520128\n",
      "layers.3.transformer.attn.Wqkv.weight 17555632\n",
      "layers.3.transformer.attn.Wqkv.bias 4684\n",
      "layers.3.transformer.attn.out_proj.weight 14047504\n",
      "layers.3.transformer.attn.out_proj.bias 3748\n",
      "layers.3.transformer.ffn.ff.0.0.weight 56190016\n",
      "layers.3.transformer.ffn.ff.0.0.bias 14992\n",
      "layers.3.transformer.ffn.ff.1.weight 14992\n",
      "layers.3.transformer.ffn.ff.1.bias 14992\n",
      "layers.3.transformer.ffn.ff.3.weight 56190016\n",
      "embed.weight 374800\n",
      "Total number of parameters: 4906150736\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 47\u001b[0m\n\u001b[0;32m     44\u001b[0m model_mamba \u001b[38;5;241m=\u001b[39m model_mamba\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model_mamba\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_mamba\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_patience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Save the model and finish WandB session\u001b[39;00m\n\u001b[0;32m     59\u001b[0m wandb\u001b[38;5;241m.\u001b[39mfinish()\n",
      "Cell \u001b[1;32mIn[6], line 26\u001b[0m, in \u001b[0;36mtrain_model_mamba\u001b[1;34m(model, train_loader, val_loader, test_loader, num_epochs, lr, max_patience, device)\u001b[0m\n\u001b[0;32m     24\u001b[0m X_batch, y_batch \u001b[38;5;241m=\u001b[39m X_batch\u001b[38;5;241m.\u001b[39mto(device), y_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 26\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, y_batch)\n\u001b[0;32m     28\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\jamba\\model.py:357\u001b[0m, in \u001b[0;36mJamba.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;124;03mForward pass of the Jamba model.\u001b[39;00m\n\u001b[0;32m    347\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    353\u001b[0m \n\u001b[0;32m    354\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;66;03m# Embed the input tensor to transform\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;66;03m# From tokens -> tensors\u001b[39;00m\n\u001b[1;32m--> 357\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[38;5;66;03m# Normalize the embeddings\u001b[39;00m\n\u001b[0;32m    360\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\sparse.py:164\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\functional.py:2267\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2261\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2262\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2263\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2264\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2265\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2266\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "from jamba.model import Jamba\n",
    "\n",
    "# Define the model with your parameters\n",
    "d_model = 128 # Embedding dimension\n",
    "num_classes = 4  # Star classification categories\n",
    "\n",
    "# Define the training parameters\n",
    "num_epochs = 500\n",
    "lr = 1e-4\n",
    "patience = 30   \n",
    "depth = 10\n",
    "\n",
    "# Define the config dictionary object\n",
    "config = {\"num_classes\": num_classes, \"batch_size\": batch_size, \"lr\": lr, \"patience\": patience, \"num_epochs\": num_epochs, \"d_model\": d_model, \"depth\": depth}\n",
    "\n",
    "# Initialize WandB project\n",
    "wandb.init(project=\"lamost-jamba-test\", entity=\"joaoc-university-of-southampton\", config=config)\n",
    "\n",
    "# Define hyperparameters\n",
    "num_epochs = 100\n",
    "lr = 1e-3\n",
    "patience = 10\n",
    "\n",
    "# Initialize the Jamba model\n",
    "model_mamba = Jamba(\n",
    "    dim=3748,                # Input dimensionality\n",
    "    depth=4,                # Number of layers\n",
    "    num_tokens=100,         # Token size (adapt to your case)\n",
    "    d_state=d_model,            # Hidden state dimensionality\n",
    "    d_conv=128,             # Convolutional layers dimensionality\n",
    "    heads=8,                # Number of attention heads\n",
    "    num_experts=8,          # Number of expert networks\n",
    "    num_experts_per_token=2 # Experts per token\n",
    ")\n",
    "\n",
    "print(model_mamba)\n",
    "# Print number of parameters per layer\n",
    "for name, param in model_mamba.named_parameters():\n",
    "    print(name, param.numel())\n",
    "print(\"Total number of parameters:\", sum(p.numel() for p in model_mamba.parameters() if p.requires_grad))\n",
    "\n",
    "# Move the model to device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model_mamba = model_mamba.to(device)\n",
    "\n",
    "# Train the model\n",
    "trained_model = train_model_mamba(\n",
    "    model=model_mamba,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    num_epochs=num_epochs,\n",
    "    lr=lr,\n",
    "    max_patience=patience,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Save the model and finish WandB session\n",
    "wandb.finish()\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
