{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import wandb\n",
    "import torch\n",
    "import mambapy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import nn, optim\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as metrics\n",
    "from mambapy.mamba import Mamba, MambaConfig\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, hamming_loss, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BalancedMultiLabelDataset(Dataset):\n",
    "    def __init__(self, X, y, limit_per_label=201):\n",
    "        \"\"\"\n",
    "        Multilabel version of the BalancedDataset.\n",
    "        \n",
    "        Args:\n",
    "        - X (array-like): Input features.\n",
    "        - y (array-like): Multi-hot encoded labels (2D array, each row is a multi-hot vector).\n",
    "        - limit_per_label (int): Target number of samples per label.\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.limit_per_label = limit_per_label\n",
    "        self.num_classes = y.shape[1]  # Number of possible classes\n",
    "        self.indices = self.balance_classes()\n",
    "\n",
    "    def balance_classes(self):\n",
    "        indices = []\n",
    "        class_counts = torch.sum(self.y, axis=0)  # Total occurrences of each class\n",
    "        for cls in range(self.num_classes):\n",
    "            cls_indices = np.where(self.y[:, cls] == 1)[0]  # Indices where this label is active\n",
    "            if len(cls_indices) < self.limit_per_label:  # Upsample minority classes\n",
    "                if len(cls_indices) == 0:\n",
    "                    #print(f\"No samples found for class {cls}. Skipping.\")\n",
    "                    continue  # Skip this class if there are no samples for it\n",
    "                extra_indices = np.random.choice(cls_indices, self.limit_per_label - len(cls_indices), replace=True)\n",
    "                cls_indices = np.concatenate([cls_indices, extra_indices])\n",
    "            elif len(cls_indices) > self.limit_per_label:  # Downsample majority classes\n",
    "                cls_indices = np.random.choice(cls_indices, self.limit_per_label, replace=False)\n",
    "            indices.extend(cls_indices)\n",
    "        indices = np.unique(indices)  # Remove duplicate indices\n",
    "        np.random.shuffle(indices)\n",
    "        return indices\n",
    "\n",
    "    def re_sample(self):\n",
    "        \"\"\"Rebalance the dataset if needed, for example, after changes to the dataset.\"\"\"\n",
    "        self.indices = self.balance_classes()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        index = self.indices[idx]\n",
    "        return self.X[index], self.y[index]\n",
    "\n",
    "    \n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    metrics = {\n",
    "        \"micro_f1\": f1_score(y_true, y_pred, average='micro'),\n",
    "        \"macro_f1\": f1_score(y_true, y_pred, average='macro'),\n",
    "        \"weighted_f1\": f1_score(y_true, y_pred, average='weighted'),\n",
    "        \"micro_precision\": precision_score(y_true, y_pred, average='micro', zero_division=1),\n",
    "        \"macro_precision\": precision_score(y_true, y_pred, average='macro', zero_division=1),\n",
    "        \"weighted_precision\": precision_score(y_true, y_pred, average='weighted', zero_division=1),\n",
    "        \"micro_recall\": recall_score(y_true, y_pred, average='micro'),\n",
    "        \"macro_recall\": recall_score(y_true, y_pred, average='macro'),\n",
    "        \"weighted_recall\": recall_score(y_true, y_pred, average='weighted'),\n",
    "        \"hamming_loss\": hamming_loss(y_true, y_pred)\n",
    "    }\n",
    "    \n",
    "    # Check if there are at least two classes present in y_true\n",
    "    #if len(np.unique(y_true)) > 1:\n",
    "        #metrics[\"roc_auc\"] = roc_auc_score(y_true, y_pred, average='macro', multi_class='ovr')\n",
    "    #else:\n",
    "       # metrics[\"roc_auc\"] = None  # or you can set it to a default value or message\n",
    "    \n",
    "    return metrics\n",
    "    \n",
    "def calculate_class_weights(y):\n",
    "    if y.ndim > 1:  # Check if y is 2D (multi-hot encoded)\n",
    "        class_counts = np.sum(y, axis=0)  # Count how many times each class appears\n",
    "    else:\n",
    "        class_counts = np.bincount(y)  # For a 1D array, use bincount \n",
    "    total_samples = y.shape[0] if y.ndim > 1 else len(y)\n",
    "    class_weights = np.where(class_counts > 0, total_samples / (len(class_counts) * class_counts), 0)\n",
    "    return class_weights\n",
    "\n",
    "def calculate_class_weights(y):\n",
    "    if y.ndim > 1:  \n",
    "        class_counts = np.sum(y, axis=0)  \n",
    "    else:\n",
    "        class_counts = np.bincount(y)\n",
    "\n",
    "    total_samples = y.shape[0] if y.ndim > 1 else len(y)\n",
    "    class_counts = np.where(class_counts == 0, 1, class_counts)  # Prevent division by zero\n",
    "    class_weights = total_samples / (len(class_counts) * class_counts)\n",
    "    \n",
    "    return class_weights\n",
    "\n",
    "    \n",
    "def train_model_mamba(\n",
    "    model, train_loader, val_loader, test_loader, \n",
    "    num_epochs=500, lr=1e-4, max_patience=20, device='cuda'\n",
    "):\n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Define optimizer, scheduler, and loss function\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=int(max_patience / 5)\n",
    "    )\n",
    "    all_labels = []\n",
    "\n",
    "    for _, y_batch in train_loader:\n",
    "        all_labels.extend(y_batch.cpu().numpy())    \n",
    "        \n",
    "    class_weights = calculate_class_weights(np.array(all_labels))\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "    #print(\"Class weights:\", class_weights)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience = max_patience\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Resample training and validation data\n",
    "        train_loader.dataset.re_sample()\n",
    "        #val_loader.dataset.balance_classes() should remane the same?\n",
    "\n",
    "        # Class weights\n",
    "        all_labels = []\n",
    "        for _, y_batch in train_loader:\n",
    "            all_labels.extend(y_batch.cpu().numpy())\n",
    "        class_weights = calculate_class_weights(np.array(all_labels))\n",
    "        class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights)\n",
    "\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss, train_accuracy = 0.0, 0.0\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * X_batch.size(0)\n",
    "            # Convert outputs to binary predictions\n",
    "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "\n",
    "\n",
    "            # Calculate accuracy for each batch\n",
    "            correct = (predicted == y_batch).float()\n",
    "            train_accuracy += correct.mean(dim=1).mean().item()  # Mean across classes and samples\n",
    "\n",
    "            \n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss, val_accuracy = 0.0, 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_val, y_val in val_loader:\n",
    "                X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "                outputs = model(X_val)\n",
    "                loss = criterion(outputs, y_val)\n",
    "\n",
    "                val_loss += loss.item() * X_val.size(0)\n",
    "                predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                correct = (predicted == y_val).float()\n",
    "                val_accuracy += correct.mean(dim=1).mean().item()\n",
    "\n",
    "        # Test phase\n",
    "        test_loss, test_accuracy = 0.0, 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_test, y_test in test_loader:\n",
    "                X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "                outputs = model(X_test)\n",
    "                loss = criterion(outputs, y_test)\n",
    "\n",
    "                test_loss += loss.item() * X_test.size(0)\n",
    "                predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                correct = (predicted == y_test).float()\n",
    "                test_accuracy += correct.mean(dim=1).mean().item()\n",
    "\n",
    "        # Test phase and metric collection\n",
    "        # Inside your test phase\n",
    "        y_true, y_pred = [], []\n",
    "        with torch.no_grad():\n",
    "            for X_test, y_test in test_loader:\n",
    "                X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "                outputs = model(X_test)\n",
    "                predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                y_true.extend(y_test.cpu().numpy())\n",
    "                y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "        metrics = calculate_metrics(np.array(y_true), np.array(y_pred))\n",
    "        wandb.log(metrics)\n",
    "\n",
    "        # Update scheduler\n",
    "        scheduler.step(val_loss / len(val_loader.dataset))\n",
    "\n",
    "        # Log metrics to WandB\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": train_loss / len(train_loader.dataset),\n",
    "            \"val_loss\": val_loss / len(val_loader.dataset),\n",
    "            \"train_accuracy\": train_accuracy / len(train_loader),\n",
    "            \"val_accuracy\": val_accuracy / len(val_loader),\n",
    "            \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "            \"test_loss\": test_loss / len(test_loader.dataset),\n",
    "            \"test_accuracy\": test_accuracy / len(test_loader),\n",
    "            #\"confusion_matrix\": wandb.plot.confusion_matrix(\n",
    "            #   probs=None, y_true=y_true, preds=y_pred, class_names=np.unique(y_true)\n",
    "            #), remove for now as it is not multilabel\n",
    "            #\"classification_report\": classification_report(\n",
    "            #    y_true, y_pred, target_names=[str(i) for i in range(len(np.unique(y_true)))]\n",
    "            #)\n",
    "        })\n",
    "\n",
    "        #for name, param in model.named_parameters():\n",
    "          #  if param.grad is None:\n",
    "          #      print(f\"Warning: {name} has no gradient!\")\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience = max_patience\n",
    "            best_model = model.state_dict()\n",
    "        else:\n",
    "            patience -= 1\n",
    "            if patience <= 0:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    # Load the best model weights\n",
    "    model.load_state_dict(best_model)\n",
    "    return model\n",
    "\n",
    "class StarClassifierMAMBA(nn.Module):\n",
    "    def __init__(self, d_model, num_classes, d_state=64, d_conv=4, input_dim=17, n_layers=6):\n",
    "        super(StarClassifierMAMBA, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # MAMBA layer initialization\n",
    "        config = MambaConfig(\n",
    "            d_model=d_model,\n",
    "            d_state=d_state,\n",
    "            d_conv=d_conv,\n",
    "            n_layers=n_layers\n",
    "\n",
    "        )\n",
    "        self.mamba_layer = Mamba(config)\n",
    "\n",
    "        # Input projection to match the MAMBA layer dimension\n",
    "        self.input_projection = nn.Linear(input_dim, d_model)\n",
    "\n",
    "        # Fully connected classifier head with sigmoid activation for multi-label classification\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, num_classes)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_projection(x)  # Ensure correct embedding dimension\n",
    "\n",
    "        # Reshape to (batch_size, sequence_length, d_model) for Mamba\n",
    "        x = x.unsqueeze(1)  # Adding a sequence dimension, making it (batch_size, 1, d_model)\n",
    "\n",
    "        x = self.mamba_layer(x)  # Now the input shape is correct\n",
    "        x = x.mean(dim=1)  # Pooling operation for classification\n",
    "        x = self.classifier(x)  # Classification head\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (108918, 3722)\n",
      "Data shape: (108918, 73)\n",
      "parallax                   float64\n",
      "ra                         float64\n",
      "dec                        float64\n",
      "ra_error                   float32\n",
      "dec_error                  float32\n",
      "parallax_error             float32\n",
      "pmra                       float64\n",
      "pmdec                      float64\n",
      "pmra_error                 float32\n",
      "pmdec_error                float32\n",
      "phot_g_mean_flux           float64\n",
      "flagnopllx                 float64\n",
      "phot_g_mean_flux_error     float32\n",
      "phot_bp_mean_flux          float64\n",
      "phot_rp_mean_flux          float64\n",
      "phot_bp_mean_flux_error    float32\n",
      "phot_rp_mean_flux_error    float32\n",
      "flagnoflux                 float64\n",
      "dtype: object\n",
      "Train dataset shape: torch.Size([87134, 18])\n",
      "Validation dataset shape: torch.Size([21784, 18])\n",
      "Test dataset shape: torch.Size([27237, 18])\n",
      "Train labels shape: torch.Size([87134, 55])\n",
      "Validation labels shape: torch.Size([21784, 55])\n",
      "Test labels shape: torch.Size([27237, 55])\n"
     ]
    }
   ],
   "source": [
    "# If X exists, delete it\n",
    "if 'X' in locals():   \n",
    "    del X, y\n",
    "gc.collect()\n",
    "\n",
    "batch_size = 512\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and preprocess your data (example from original script)\n",
    "    X = pd.read_pickle(\"Pickles/train_data_transformed2.pkl\")\n",
    "    classes = pd.read_pickle(\"Pickles/Updated_list_of_Classes.pkl\")\n",
    "\n",
    "    # Get labels and set them as y, drop them from X\n",
    "    y = X[classes]\n",
    "\n",
    "        # Print the shape of the data\n",
    "    print(f\"Data shape: {X.shape}\")\n",
    "\n",
    "    # Use Gaia data\n",
    "    X = X[[\"parallax\", \"ra\", \"dec\", \"ra_error\", \"dec_error\", \"parallax_error\", \"pmra\", \"pmdec\", \"pmra_error\", \"pmdec_error\",\n",
    "            \"phot_g_mean_flux\", \"flagnopllx\", \"phot_g_mean_flux_error\", \"phot_bp_mean_flux\", \"phot_rp_mean_flux\",\n",
    "            \"phot_bp_mean_flux_error\", \"phot_rp_mean_flux_error\", \"flagnoflux\", 'RS*', \n",
    "            '**', 'El*', 'Y*O', 's*b', 'cC*', 'HB*', 'dS*', 'Or*', 'LP*', 'BS*', 'Ae*', 'WV*', 'HS*', 'Ev*', 'AB*', 'sg*', 's*r', 'Ce*', 'gD*', 'OH*', 'HXB', 'Pu*', 'RV*', 'Sy*', 'V*', 'TT*', 'SN*', 'Be*', 'SB*', 'Em*', 'Er*', 'PM*', 'HV*', 'pA*', 'C*', 'BY*', 'Ro*', 'XB*', 'Ma*', 'Pe*', 'CV*', 'bC*', 'RR*', 'Mi*', 'SX*', 'RG*', 'LM*', 'WD*', 'S*', 'MS*', 'Ir*', 'a2*', 'PN', 'EB*']]\n",
    "\n",
    "    # Print the shape of the data\n",
    "    print(f\"Data shape: {X.shape}\")\n",
    "    \n",
    "    # Drop labels\n",
    "    X.drop(classes, axis=1, inplace=True)\n",
    "    \n",
    "    # Read test data\n",
    "    X_test = pd.read_pickle(\"Pickles/test_data_transformed.pkl\")\n",
    "\n",
    "    # Get labels and set them as y, drop them from X\n",
    "    y_test = X_test[classes]\n",
    "\n",
    "    # Drop gaia data\n",
    "    X_test = X_test[[\"parallax\", \"ra\", \"dec\", \"ra_error\", \"dec_error\", \"parallax_error\", \"pmra\", \"pmdec\", \"pmra_error\", \"pmdec_error\", \n",
    "                \"phot_g_mean_flux\", \"flagnopllx\", \"phot_g_mean_flux_error\", \"phot_bp_mean_flux\", \"phot_rp_mean_flux\", \n",
    "                \"phot_bp_mean_flux_error\", \"phot_rp_mean_flux_error\", \"flagnoflux\",\n",
    "                'RS*', '**', 'El*', 'Y*O', 's*b', 'cC*', 'HB*', 'dS*', 'Or*', 'LP*', 'BS*', 'Ae*', 'WV*', 'HS*', 'Ev*', 'AB*', 'sg*', 's*r', 'Ce*', 'gD*', 'OH*', 'HXB', 'Pu*', 'RV*', 'Sy*', 'V*', 'TT*', 'SN*', 'Be*', 'SB*', 'Em*', 'Er*', 'PM*', 'HV*', 'pA*', 'C*', 'BY*', 'Ro*', 'XB*', 'Ma*', 'Pe*', 'CV*', 'bC*', 'RR*', 'Mi*', 'SX*', 'RG*', 'LM*', 'WD*', 'S*', 'MS*', 'Ir*', 'a2*', 'PN', 'EB*']]\n",
    "\n",
    "    # Drop labels\n",
    "    X_test.drop(classes, axis=1, inplace=True)\n",
    "    \n",
    "    # Split validation data\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Clear memory\n",
    "    del X, y\n",
    "    gc.collect()\n",
    "\n",
    "    # Convert to torch tensors and create datasets\n",
    "    print(X_train.dtypes)\n",
    "\n",
    "    X_train = torch.tensor(X_train.values, dtype=torch.float32) # Convert DataFrame to numpy array\n",
    "    X_val = torch.tensor(X_val.values, dtype=torch.float32)     # Convert DataFrame to numpy array    \n",
    "    X_test = torch.tensor(X_test.values, dtype=torch.float32)   # Convert DataFrame to numpy array\n",
    "    y_train = torch.tensor(y_train.values, dtype=torch.float32)  # Convert DataFrame to numpy array and float32\n",
    "    y_val = torch.tensor(y_val.values, dtype=torch.float32)      # Convert DataFrame to numpy array and float32\n",
    "    y_test = torch.tensor(y_test.values, dtype=torch.float32)    # Convert DataFrame to numpy array and float32\n",
    "\n",
    "    train_dataset = BalancedMultiLabelDataset(X_train, y_train)\n",
    "    val_dataset = BalancedMultiLabelDataset(X_val, y_val)\n",
    "    test_dataset = BalancedMultiLabelDataset(X_test, y_test)\n",
    "\n",
    "\n",
    "    #train_dataset = BalancedDataset(X_train, y_train)\n",
    "    #val_dataset = BalancedValidationDataset(X_val, y_val)\n",
    "    #test_dataset = BalancedValidationDataset(X_test, y_test, limit_per_label=10000)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)   \n",
    "\n",
    "\n",
    "    # Print the shapes of the datasets\n",
    "    print(f\"Train dataset shape: {X_train.shape}\")\n",
    "    print(f\"Validation dataset shape: {X_val.shape}\")\n",
    "    print(f\"Test dataset shape: {X_test.shape}\")\n",
    "    print(f\"Train labels shape: {y_train.shape}\")\n",
    "    print(f\"Validation labels shape: {y_val.shape}\")\n",
    "    print(f\"Test labels shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: joaoc (joaoc-university-of-southampton). Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\jcwin\\OneDrive - University of Southampton\\_Southampton\\2024-25\\Star-Classifier\\wandb\\run-20250117_111311-12zdrcnm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/joaoc-university-of-southampton/ALLSTARS%2A%2A%2Agaia-mamba-test/runs/12zdrcnm' target=\"_blank\">proud-thunder-4</a></strong> to <a href='https://wandb.ai/joaoc-university-of-southampton/ALLSTARS%2A%2A%2Agaia-mamba-test' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/joaoc-university-of-southampton/ALLSTARS%2A%2A%2Agaia-mamba-test' target=\"_blank\">https://wandb.ai/joaoc-university-of-southampton/ALLSTARS%2A%2A%2Agaia-mamba-test</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/joaoc-university-of-southampton/ALLSTARS%2A%2A%2Agaia-mamba-test/runs/12zdrcnm' target=\"_blank\">https://wandb.ai/joaoc-university-of-southampton/ALLSTARS%2A%2A%2Agaia-mamba-test/runs/12zdrcnm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StarClassifierMAMBA(\n",
      "  (mamba_layer): Mamba(\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x ResidualBlock(\n",
      "        (mixer): MambaBlock(\n",
      "          (in_proj): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "          (conv1d): Conv1d(2048, 2048, kernel_size=(4,), stride=(1,), padding=(3,), groups=2048)\n",
      "          (x_proj): Linear(in_features=2048, out_features=192, bias=False)\n",
      "          (dt_proj): Linear(in_features=64, out_features=2048, bias=True)\n",
      "          (out_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "        )\n",
      "        (norm): RMSNorm()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (input_projection): Linear(in_features=18, out_features=1024, bias=True)\n",
      "  (classifier): Sequential(\n",
      "    (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): Linear(in_features=1024, out_features=55, bias=True)\n",
      "  )\n",
      ")\n",
      "mamba_layer.layers.0.mixer.A_log 131072\n",
      "mamba_layer.layers.0.mixer.D 2048\n",
      "mamba_layer.layers.0.mixer.in_proj.weight 4194304\n",
      "mamba_layer.layers.0.mixer.conv1d.weight 8192\n",
      "mamba_layer.layers.0.mixer.conv1d.bias 2048\n",
      "mamba_layer.layers.0.mixer.x_proj.weight 393216\n",
      "mamba_layer.layers.0.mixer.dt_proj.weight 131072\n",
      "mamba_layer.layers.0.mixer.dt_proj.bias 2048\n",
      "mamba_layer.layers.0.mixer.out_proj.weight 2097152\n",
      "mamba_layer.layers.0.norm.weight 1024\n",
      "mamba_layer.layers.1.mixer.A_log 131072\n",
      "mamba_layer.layers.1.mixer.D 2048\n",
      "mamba_layer.layers.1.mixer.in_proj.weight 4194304\n",
      "mamba_layer.layers.1.mixer.conv1d.weight 8192\n",
      "mamba_layer.layers.1.mixer.conv1d.bias 2048\n",
      "mamba_layer.layers.1.mixer.x_proj.weight 393216\n",
      "mamba_layer.layers.1.mixer.dt_proj.weight 131072\n",
      "mamba_layer.layers.1.mixer.dt_proj.bias 2048\n",
      "mamba_layer.layers.1.mixer.out_proj.weight 2097152\n",
      "mamba_layer.layers.1.norm.weight 1024\n",
      "mamba_layer.layers.2.mixer.A_log 131072\n",
      "mamba_layer.layers.2.mixer.D 2048\n",
      "mamba_layer.layers.2.mixer.in_proj.weight 4194304\n",
      "mamba_layer.layers.2.mixer.conv1d.weight 8192\n",
      "mamba_layer.layers.2.mixer.conv1d.bias 2048\n",
      "mamba_layer.layers.2.mixer.x_proj.weight 393216\n",
      "mamba_layer.layers.2.mixer.dt_proj.weight 131072\n",
      "mamba_layer.layers.2.mixer.dt_proj.bias 2048\n",
      "mamba_layer.layers.2.mixer.out_proj.weight 2097152\n",
      "mamba_layer.layers.2.norm.weight 1024\n",
      "mamba_layer.layers.3.mixer.A_log 131072\n",
      "mamba_layer.layers.3.mixer.D 2048\n",
      "mamba_layer.layers.3.mixer.in_proj.weight 4194304\n",
      "mamba_layer.layers.3.mixer.conv1d.weight 8192\n",
      "mamba_layer.layers.3.mixer.conv1d.bias 2048\n",
      "mamba_layer.layers.3.mixer.x_proj.weight 393216\n",
      "mamba_layer.layers.3.mixer.dt_proj.weight 131072\n",
      "mamba_layer.layers.3.mixer.dt_proj.bias 2048\n",
      "mamba_layer.layers.3.mixer.out_proj.weight 2097152\n",
      "mamba_layer.layers.3.norm.weight 1024\n",
      "mamba_layer.layers.4.mixer.A_log 131072\n",
      "mamba_layer.layers.4.mixer.D 2048\n",
      "mamba_layer.layers.4.mixer.in_proj.weight 4194304\n",
      "mamba_layer.layers.4.mixer.conv1d.weight 8192\n",
      "mamba_layer.layers.4.mixer.conv1d.bias 2048\n",
      "mamba_layer.layers.4.mixer.x_proj.weight 393216\n",
      "mamba_layer.layers.4.mixer.dt_proj.weight 131072\n",
      "mamba_layer.layers.4.mixer.dt_proj.bias 2048\n",
      "mamba_layer.layers.4.mixer.out_proj.weight 2097152\n",
      "mamba_layer.layers.4.norm.weight 1024\n",
      "mamba_layer.layers.5.mixer.A_log 131072\n",
      "mamba_layer.layers.5.mixer.D 2048\n",
      "mamba_layer.layers.5.mixer.in_proj.weight 4194304\n",
      "mamba_layer.layers.5.mixer.conv1d.weight 8192\n",
      "mamba_layer.layers.5.mixer.conv1d.bias 2048\n",
      "mamba_layer.layers.5.mixer.x_proj.weight 393216\n",
      "mamba_layer.layers.5.mixer.dt_proj.weight 131072\n",
      "mamba_layer.layers.5.mixer.dt_proj.bias 2048\n",
      "mamba_layer.layers.5.mixer.out_proj.weight 2097152\n",
      "mamba_layer.layers.5.norm.weight 1024\n",
      "mamba_layer.layers.6.mixer.A_log 131072\n",
      "mamba_layer.layers.6.mixer.D 2048\n",
      "mamba_layer.layers.6.mixer.in_proj.weight 4194304\n",
      "mamba_layer.layers.6.mixer.conv1d.weight 8192\n",
      "mamba_layer.layers.6.mixer.conv1d.bias 2048\n",
      "mamba_layer.layers.6.mixer.x_proj.weight 393216\n",
      "mamba_layer.layers.6.mixer.dt_proj.weight 131072\n",
      "mamba_layer.layers.6.mixer.dt_proj.bias 2048\n",
      "mamba_layer.layers.6.mixer.out_proj.weight 2097152\n",
      "mamba_layer.layers.6.norm.weight 1024\n",
      "mamba_layer.layers.7.mixer.A_log 131072\n",
      "mamba_layer.layers.7.mixer.D 2048\n",
      "mamba_layer.layers.7.mixer.in_proj.weight 4194304\n",
      "mamba_layer.layers.7.mixer.conv1d.weight 8192\n",
      "mamba_layer.layers.7.mixer.conv1d.bias 2048\n",
      "mamba_layer.layers.7.mixer.x_proj.weight 393216\n",
      "mamba_layer.layers.7.mixer.dt_proj.weight 131072\n",
      "mamba_layer.layers.7.mixer.dt_proj.bias 2048\n",
      "mamba_layer.layers.7.mixer.out_proj.weight 2097152\n",
      "mamba_layer.layers.7.norm.weight 1024\n",
      "mamba_layer.layers.8.mixer.A_log 131072\n",
      "mamba_layer.layers.8.mixer.D 2048\n",
      "mamba_layer.layers.8.mixer.in_proj.weight 4194304\n",
      "mamba_layer.layers.8.mixer.conv1d.weight 8192\n",
      "mamba_layer.layers.8.mixer.conv1d.bias 2048\n",
      "mamba_layer.layers.8.mixer.x_proj.weight 393216\n",
      "mamba_layer.layers.8.mixer.dt_proj.weight 131072\n",
      "mamba_layer.layers.8.mixer.dt_proj.bias 2048\n",
      "mamba_layer.layers.8.mixer.out_proj.weight 2097152\n",
      "mamba_layer.layers.8.norm.weight 1024\n",
      "mamba_layer.layers.9.mixer.A_log 131072\n",
      "mamba_layer.layers.9.mixer.D 2048\n",
      "mamba_layer.layers.9.mixer.in_proj.weight 4194304\n",
      "mamba_layer.layers.9.mixer.conv1d.weight 8192\n",
      "mamba_layer.layers.9.mixer.conv1d.bias 2048\n",
      "mamba_layer.layers.9.mixer.x_proj.weight 393216\n",
      "mamba_layer.layers.9.mixer.dt_proj.weight 131072\n",
      "mamba_layer.layers.9.mixer.dt_proj.bias 2048\n",
      "mamba_layer.layers.9.mixer.out_proj.weight 2097152\n",
      "mamba_layer.layers.9.norm.weight 1024\n",
      "mamba_layer.layers.10.mixer.A_log 131072\n",
      "mamba_layer.layers.10.mixer.D 2048\n",
      "mamba_layer.layers.10.mixer.in_proj.weight 4194304\n",
      "mamba_layer.layers.10.mixer.conv1d.weight 8192\n",
      "mamba_layer.layers.10.mixer.conv1d.bias 2048\n",
      "mamba_layer.layers.10.mixer.x_proj.weight 393216\n",
      "mamba_layer.layers.10.mixer.dt_proj.weight 131072\n",
      "mamba_layer.layers.10.mixer.dt_proj.bias 2048\n",
      "mamba_layer.layers.10.mixer.out_proj.weight 2097152\n",
      "mamba_layer.layers.10.norm.weight 1024\n",
      "mamba_layer.layers.11.mixer.A_log 131072\n",
      "mamba_layer.layers.11.mixer.D 2048\n",
      "mamba_layer.layers.11.mixer.in_proj.weight 4194304\n",
      "mamba_layer.layers.11.mixer.conv1d.weight 8192\n",
      "mamba_layer.layers.11.mixer.conv1d.bias 2048\n",
      "mamba_layer.layers.11.mixer.x_proj.weight 393216\n",
      "mamba_layer.layers.11.mixer.dt_proj.weight 131072\n",
      "mamba_layer.layers.11.mixer.dt_proj.bias 2048\n",
      "mamba_layer.layers.11.mixer.out_proj.weight 2097152\n",
      "mamba_layer.layers.11.norm.weight 1024\n",
      "input_projection.weight 18432\n",
      "input_projection.bias 1024\n",
      "classifier.0.weight 1024\n",
      "classifier.0.bias 1024\n",
      "classifier.1.weight 56320\n",
      "classifier.1.bias 55\n",
      "Total number of parameters: 83623991\n"
     ]
    }
   ],
   "source": [
    "# Same, but now with class weights\n",
    "# Define the model with your parameters\n",
    "d_model = 1024 # Embedding dimension\n",
    "num_classes = 55  # Star classification categories\n",
    "input_dim = 18 # Number of spectra points\n",
    "\n",
    "# Define the training parameters\n",
    "num_epochs = 2000\n",
    "lr = 1e-4\n",
    "patience = 100\n",
    "depth = 12\n",
    "\n",
    "# Define the config dictionary object\n",
    "config = {\"num_classes\": num_classes, \"batch_size\": batch_size, \"lr\": lr, \"patience\": patience, \"num_epochs\": num_epochs, \"d_model\": d_model, \"depth\": depth}\n",
    "\n",
    "# Initialize WandB project\n",
    "wandb.init(project=\"ALLSTARS***gaia-mamba-test\", entity=\"joaoc-university-of-southampton\", config=config)\n",
    "# Initialize and train the model\n",
    "# Train the model using your `train_model_vit` or an adjusted training loop\n",
    "model_mamba = StarClassifierMAMBA(d_model=d_model, num_classes=num_classes, input_dim=input_dim, n_layers=depth)\n",
    "print(model_mamba)\n",
    "# print number of parameters per layer\n",
    "for name, param in model_mamba.named_parameters():\n",
    "    print(name, param.numel())\n",
    "print(\"Total number of parameters:\", sum(p.numel() for p in model_mamba.parameters() if p.requires_grad))\n",
    "\n",
    "# Move the model to device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#device = 'cpu'\n",
    "model_mamba = model_mamba.to(device)\n",
    "\n",
    "# Train the model using your `train_model_vit` or an adjusted training loop\n",
    "trained_model = train_model_mamba(\n",
    "    model=model_mamba,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    num_epochs=num_epochs,\n",
    "    lr=lr,\n",
    "    max_patience=patience,\n",
    "    device=device\n",
    ")\n",
    "# Save the model and finish WandB session\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(trained_model.state_dict(), \"mamba_gaia_star_classifier_proud-thunder-4.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code can work with specific classes or all classes\n",
    "def process_star_data(model_path, data_path, classes_path, d_model=1024, num_classes=55, input_dim=3647, depth=12, class_to_plot=\"AllStars***lamost\"):\n",
    "    # Load the data\n",
    "    X = pd.read_pickle(data_path)\n",
    "    classes = pd.read_pickle(classes_path)\n",
    "\n",
    "    # Load the trained model\n",
    "    model = StarClassifierMAMBA(d_model=d_model, num_classes=num_classes, input_dim=input_dim, n_layers=depth)\n",
    "\n",
    "    # Load the state dictionary\n",
    "    state_dict = torch.load(model_path)\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "    # Get the spectral data for all stars with the label multihot encoded\n",
    "    y = X[classes]\n",
    "\n",
    "    # Drop Gaia data\n",
    "    X.drop([\"parallax\", \"ra\", \"dec\", \"ra_error\", \"dec_error\", \"parallax_error\", \"pmra\", \"pmdec\", \"pmra_error\", \"pmdec_error\", \n",
    "            \"phot_g_mean_flux\", \"flagnopllx\", \"phot_g_mean_flux_error\", \"phot_bp_mean_flux\", \"phot_rp_mean_flux\", \n",
    "            \"phot_bp_mean_flux_error\", \"phot_rp_mean_flux_error\", \"obsid\", \"flagnoflux\", \"otype\"], axis=1, inplace=True)\n",
    "    print(f\"X shape after Gaia data drop: {X.shape}\")\n",
    "    print(f\"y shape: {y.shape}\")\n",
    "\n",
    "    if class_to_plot != \"AllStars***lamost\":\n",
    "        # Filter for a specific class\n",
    "        X = X[y[class_to_plot] == 1]\n",
    "        y = y[y[class_to_plot] == 1]\n",
    "\n",
    "        print(f\"X shape after filtering for {class_to_plot}: {X.shape}\")\n",
    "        print(f\"y shape after filtering for {class_to_plot}: {y.shape}\")\n",
    "\n",
    "    # Drop label columns\n",
    "    X.drop(classes, axis=1, inplace=True)\n",
    "\n",
    "    # Convert to tensors\n",
    "    X = torch.tensor(X.values, dtype=torch.float32)\n",
    "    y = torch.tensor(y.values, dtype=torch.float32)\n",
    "\n",
    "    # Create DataLoader\n",
    "    class BalancedMultiLabelDataset(Dataset):\n",
    "        def __init__(self, X, y):\n",
    "            self.X = X\n",
    "            self.y = y\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.X)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return self.X[idx], self.y[idx]\n",
    "\n",
    "    dataset = BalancedMultiLabelDataset(X, y)\n",
    "    loader = DataLoader(dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_predicted = []\n",
    "    all_y = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (X_batch, y_batch) in enumerate(loader):\n",
    "            # Move batch to device\n",
    "            X_batch = X_batch.to(device)  # Add channel dimension\n",
    "            y_batch = y_batch.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(X_batch)\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            \n",
    "            # Store predictions and labels\n",
    "            all_predicted.append(predicted.cpu().numpy())\n",
    "            all_y.append(y_batch.cpu().numpy())\n",
    "\n",
    "            # Free GPU memory\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # Concatenate all predictions and labels\n",
    "    y_cpu = np.concatenate(all_y, axis=0)\n",
    "    predicted_cpu = np.concatenate(all_predicted, axis=0)\n",
    "\n",
    "    return y_cpu, predicted_cpu\n",
    "\n",
    "# Example usage\n",
    "model_path = \"\n",
    "data_path = \"Pickles/test_data_transformed.pkl\"\n",
    "classes_path = \"Pickles/Updated_list_of_Classes.pkl\"\n",
    "y_cpu, predicted_cpu = process_star_data(model_path, data_path, classes_path)\n",
    "\n",
    "# Save the predictions\n",
    "np.save(\"mamba_lamost_v1_y_cpu.npy\", y_cpu)\n",
    "np.save(\"mamba_lamost_v1_predicted_cpu.npy\", predicted_cpu)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
