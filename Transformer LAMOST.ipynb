{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import gc\n",
    "import psutil\n",
    "import GPUtil\n",
    "\n",
    "# Enable MLflow autologging for PyTorch\n",
    "mlflow.pytorch.autolog()\n",
    "mlflow.set_tracking_uri(uri=\"file:///C:/Users/jcwin/OneDrive - University of Southampton/_Southampton/2024-25/Star-Classifier/mlflow\")\n",
    "mlflow.set_experiment(\"LAMOST_ConVnet\")\n",
    "\n",
    "# Custom Dataset for handling balanced data\n",
    "class BalancedDataset(Dataset):\n",
    "    def __init__(self, X, y, limit_per_label=1600):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.limit_per_label = limit_per_label\n",
    "        self.classes = np.unique(y)\n",
    "        self.indices = self.balance_classes()\n",
    "\n",
    "    def balance_classes(self):\n",
    "        indices = []\n",
    "        for cls in self.classes:\n",
    "            cls_indices = np.where(self.y == cls)[0]\n",
    "            if len(cls_indices) > self.limit_per_label:\n",
    "                cls_indices = np.random.choice(cls_indices, self.limit_per_label, replace=False)\n",
    "            indices.extend(cls_indices)\n",
    "        np.random.shuffle(indices)\n",
    "        return indices\n",
    "\n",
    "    def re_sample(self):\n",
    "        self.indices = self.balance_classes()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        index = self.indices[idx]\n",
    "        return self.X[index], self.y[index]\n",
    "# Custom Dataset for validation with limit per class\n",
    "class BalancedValidationDataset(Dataset):\n",
    "    def __init__(self, X, y, limit_per_label=400):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.limit_per_label = limit_per_label\n",
    "        self.classes = np.unique(y)\n",
    "        self.indices = self.balance_classes()\n",
    "\n",
    "    def balance_classes(self):\n",
    "        indices = []\n",
    "        for cls in self.classes:\n",
    "            cls_indices = np.where(self.y == cls)[0]\n",
    "            if len(cls_indices) > self.limit_per_label:\n",
    "                cls_indices = np.random.choice(cls_indices, self.limit_per_label, replace=False)\n",
    "            indices.extend(cls_indices)\n",
    "        np.random.shuffle(indices)\n",
    "        return indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        index = self.indices[idx]\n",
    "        return self.X[index], self.y[index]\n",
    "\n",
    "# Define the Conv1D PyTorch model\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, input_shape, num_classes, \n",
    "                 num_filters=[128, 128, 128, 128, 128, 128, 128, 128], \n",
    "                 kernel_size=9,\n",
    "                 dense_units=[256, 256, 256, 128, 128, 128, 64, 64, 64],\n",
    "                 dropout_rate=0.2, padding='same'):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        self.pool_layers = nn.ModuleList()\n",
    "        in_channels = 1  # Since it's a 1D input\n",
    "        \n",
    "        # Add convolutional layers\n",
    "        for filters in num_filters:\n",
    "            conv_layer = nn.Conv1d(in_channels=in_channels, out_channels=filters, kernel_size=kernel_size, padding=kernel_size//2)\n",
    "            self.conv_layers.append(conv_layer)\n",
    "            self.pool_layers.append(nn.MaxPool1d(kernel_size=2))\n",
    "            in_channels = filters\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # Compute the flattened output size (based on input shape and pooling)\n",
    "        # Assumption: input_shape[0] is the sequence length\n",
    "        final_seq_len = input_shape[0] // (2 ** len(num_filters))  # After all pooling layers\n",
    "        \n",
    "        # Add dense layers\n",
    "        dense_input_units = num_filters[-1] * final_seq_len\n",
    "        self.dense_layers = nn.ModuleList()\n",
    "        for units in dense_units:\n",
    "            self.dense_layers.append(nn.Linear(dense_input_units, units))\n",
    "            dense_input_units = units\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(dense_input_units, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for conv_layer, pool_layer in zip(self.conv_layers, self.pool_layers):\n",
    "            x = pool_layer(torch.relu(conv_layer(x)))\n",
    "            x = self.dropout(x)\n",
    "        x = self.flatten(x)\n",
    "        for dense_layer in self.dense_layers:\n",
    "            x = torch.relu(dense_layer(x))\n",
    "            x = self.dropout(x)\n",
    "        x = self.output_layer(x)\n",
    "        return torch.softmax(x, dim=1)\n",
    "\n",
    "\n",
    "def log_system_metrics(epoch):\n",
    "    cpu_usage = psutil.cpu_percent()\n",
    "    memory_info = psutil.virtual_memory()\n",
    "    gpus = GPUtil.getGPUs()\n",
    "    \n",
    "    if gpus:\n",
    "        for i, gpu in enumerate(gpus):\n",
    "            mlflow.log_metric(f\"gpu_{i}_usage\", gpu.load * 100, step=epoch)\n",
    "            mlflow.log_metric(f\"gpu_{i}_memory_used\", gpu.memoryUsed, step=epoch)\n",
    "            mlflow.log_metric(f\"gpu_{i}_memory_total\", gpu.memoryTotal, step=epoch)\n",
    "    \n",
    "    mlflow.log_metric(\"cpu_usage\", cpu_usage, step=epoch)\n",
    "    mlflow.log_metric(\"memory_usage\", memory_info.percent, step=epoch)\n",
    "\n",
    "# Updated train_model function with system metrics logging\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-4, patience=5, device='cuda'):\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    early_stopping_counter = 0\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Re-sample the training dataset at the start of each epoch\n",
    "        train_loader.dataset.re_sample()\n",
    "        \n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_accuracy = (outputs.argmax(dim=1) == y_batch).float().mean()\n",
    "            train_loss += loss.item() * X_batch.size(0)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_val, y_val in val_loader:\n",
    "                X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "                outputs = model(X_val)\n",
    "                val_accuracy = (outputs.argmax(dim=1) == y_val).float().mean()\n",
    "                loss = criterion(outputs, y_val)\n",
    "                val_loss += loss.item() * X_val.size(0)\n",
    "        \n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "\n",
    "        # Log metrics to MLflow\n",
    "        mlflow.log_metric(\"train_loss\", train_loss, step=epoch)\n",
    "        mlflow.log_metric(\"val_loss\", val_loss, step=epoch)\n",
    "        mlflow.log_metric(\"val_accuracy\", val_accuracy.item(), step=epoch)\n",
    "        mlflow.log_metric(\"train_accuracy\", train_accuracy.item(), step=epoch)\n",
    "        \n",
    "        # Log system metrics\n",
    "        log_system_metrics(epoch)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            early_stopping_counter = 0\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "            if early_stopping_counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "    \n",
    "    return model\n",
    "# Confusion matrix and classification report\n",
    "def print_confusion_matrix(model, val_loader, device='cuda'):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            preds = model(X_batch).cpu().numpy()\n",
    "            all_preds.extend(np.argmax(preds, axis=1))\n",
    "            all_labels.extend(y_batch.numpy())\n",
    "\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(all_labels, all_preds))\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "                xticklabels=['Star', 'Binary Star', 'Galaxy', 'AGN'], \n",
    "                yticklabels=['Star', 'Binary Star', 'Galaxy', 'AGN'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "# Main script to load data and train the model\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and preprocess data\n",
    "    X = pd.read_pickle(\"Pickles/fusionv0/train.pkl\")\n",
    "    y = X[\"label\"]\n",
    "    label_mapping = {'star': 0, 'binary_star': 1, 'galaxy': 2, 'agn': 3}\n",
    "    y = y.map(label_mapping).values\n",
    "    \n",
    "    X = X.drop([\"parallax\", \"ra\", \"dec\", \"ra_error\", \"dec_error\", \"parallax_error\", \"pmra\", \"pmdec\", \"pmra_error\", \"pmdec_error\", \n",
    "                \"phot_g_mean_flux\", \"flagnopllx\", \"phot_g_mean_flux_error\", \"phot_bp_mean_flux\", \"phot_rp_mean_flux\", \n",
    "                \"phot_bp_mean_flux_error\", \"phot_rp_mean_flux_error\", \"label\"], axis=1).values\n",
    "    \n",
    "    # Read test data\n",
    "    X_test = pd.read_pickle(\"Pickles/fusionv0/test.pkl\")\n",
    "    y_test = X_test[\"label\"].map(label_mapping).values\n",
    "    X_test = X_test.drop([\"parallax\", \"ra\", \"dec\", \"ra_error\", \"dec_error\", \"parallax_error\", \"pmra\", \"pmdec\", \"pmra_error\", \"pmdec_error\", \n",
    "                \"phot_g_mean_flux\", \"flagnopllx\", \"phot_g_mean_flux_error\", \"phot_bp_mean_flux\", \"phot_rp_mean_flux\", \n",
    "                \"phot_bp_mean_flux_error\", \"phot_rp_mean_flux_error\", \"label\"], axis=1).values\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Clear memory\n",
    "    del X, y\n",
    "    gc.collect()\n",
    "\n",
    "    # Convert to torch tensors and create datasets\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32).unsqueeze(1)\n",
    "    X_val = torch.tensor(X_val, dtype=torch.float32).unsqueeze(1)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "    y_val = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "    train_dataset = BalancedDataset(X_train, y_train)\n",
    "    val_dataset = BalancedValidationDataset(X_val, y_val)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "# Initialize model, train, and evaluate\n",
    "filters=[128, 128, 128, 256, 256, 256, 512, 512, 512, 512, 512]\n",
    "dense=[512, 256, 64]\n",
    "model = ConvNet(input_shape=(3748,), num_classes=4, num_filters=filters, kernel_size=9, dense_units=dense, dropout_rate=0.2)\n",
    "\n",
    "\n",
    "\n",
    "# model summary\n",
    "print(model)\n",
    "\n",
    "# Print number of parameters\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
    "num_epochs = 200\n",
    "lr = 1e-4\n",
    "patience = num_epochs\n",
    "batch_size = 512\n",
    "dropout_rate = 0.2\n",
    "kernel_size = 9\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run(log_system_metrics=True):\n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"num_epochs\", num_epochs)\n",
    "    mlflow.log_param(\"lr\", lr)\n",
    "    mlflow.log_param(\"patience\", patience)\n",
    "    mlflow.log_param(\"batch_size\", batch_size)\n",
    "    mlflow.log_param(\"num_filters\", filters)\n",
    "    mlflow.log_param(\"dense_units\", dense)\n",
    "    mlflow.log_param(\"dropout_rate\", dropout_rate)\n",
    "    mlflow.log_param(\"kernel_size\", kernel_size)\n",
    "\n",
    "    # Train the model\n",
    "    trained_model = train_model(model, train_loader, val_loader, num_epochs=num_epochs, lr=lr, patience=patience)\n",
    "\n",
    "    # Evaluate the model\n",
    "    print_confusion_matrix(trained_model, val_loader)\n",
    "\n",
    "    # Save the model\n",
    "    mlflow.pytorch.log_model(trained_model, \"model\")\n",
    "# Evaluate with confusion matrix and classification report\n",
    "test_loader = DataLoader(BalancedValidationDataset(torch.tensor(X_test, dtype=torch.float32).unsqueeze(1),\n",
    "                                                    torch.tensor(y_test, dtype=torch.long)),\n",
    "                         batch_size=512, shuffle=False)\n",
    "print_confusion_matrix(trained_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/25 12:41:45 WARNING mlflow.utils.autologging_utils: MLflow pytorch autologging is known to be compatible with 1.9.0 <= torch <= 2.4.1, but the installed version is 2.4.1+cu124. If you encounter errors during autologging, try upgrading / downgrading torch to a compatible version, or try upgrading MLflow.\n",
      "C:\\Users\\jcwin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[512, 234, 16]' is invalid for input of size 1918976",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 227\u001b[0m\n\u001b[0;32m    225\u001b[0m mlflow\u001b[38;5;241m.\u001b[39mlog_param(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_epochs)\n\u001b[0;32m    226\u001b[0m mlflow\u001b[38;5;241m.\u001b[39mlog_param(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m, lr)\n\u001b[1;32m--> 227\u001b[0m trained_model_vit \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model_vit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_vit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    228\u001b[0m print_confusion_matrix_vit(trained_model_vit, val_loader)\n",
      "Cell \u001b[1;32mIn[3], line 132\u001b[0m, in \u001b[0;36mtrain_model_vit\u001b[1;34m(model, train_loader, val_loader, num_epochs, lr, patience, device)\u001b[0m\n\u001b[0;32m    130\u001b[0m X_batch, y_batch \u001b[38;5;241m=\u001b[39m X_batch\u001b[38;5;241m.\u001b[39mto(device), y_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    131\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 132\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    133\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, y_batch)\n\u001b[0;32m    134\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 51\u001b[0m, in \u001b[0;36mVisionTransformer1D.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;66;03m# Convert input into patches and embed them\u001b[39;00m\n\u001b[0;32m     50\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 51\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_patches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_embed(x) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embedding\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;66;03m# Transformer forward pass\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[512, 234, 16]' is invalid for input of size 1918976"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import gc\n",
    "from sklearn.model_selection import train_test_split\n",
    "import psutil\n",
    "import GPUtil\n",
    "\n",
    "# Enable MLflow autologging\n",
    "mlflow.pytorch.autolog()\n",
    "\n",
    "# Define the Vision Transformer model\n",
    "class VisionTransformer1D(nn.Module):\n",
    "    def __init__(self, input_size=3748, num_classes=4, patch_size=16, dim=128, depth=6, heads=8, mlp_dim=256, dropout=0.1):\n",
    "        super(VisionTransformer1D, self).__init__()\n",
    "        \n",
    "        # Parameters\n",
    "        self.num_patches = input_size // patch_size\n",
    "        self.patch_size = patch_size\n",
    "        self.dim = dim\n",
    "        \n",
    "        # Patch Embedding layer\n",
    "        self.patch_embed = nn.Linear(patch_size, dim)\n",
    "        \n",
    "        # Positional Encoding\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_patches, dim))\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(dim, heads, mlp_dim, dropout),\n",
    "            depth\n",
    "        )\n",
    "        \n",
    "        # MLP Head\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convert input into patches and embed them\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.view(batch_size, self.num_patches, self.patch_size)\n",
    "        x = self.patch_embed(x) + self.pos_embedding\n",
    "        \n",
    "        # Transformer forward pass\n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        # Classify based on the first token representation\n",
    "        x = self.fc(x[:, 0])\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create dataset classes (using your BalancedDataset approach) and training function\n",
    "class BalancedDataset(Dataset):\n",
    "    def __init__(self, X, y, limit_per_label=1600):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.limit_per_label = limit_per_label\n",
    "        self.classes = np.unique(y)\n",
    "        self.indices = self.balance_classes()\n",
    "\n",
    "    def balance_classes(self):\n",
    "        indices = []\n",
    "        for cls in self.classes:\n",
    "            cls_indices = np.where(self.y == cls)[0]\n",
    "            if len(cls_indices) > self.limit_per_label:\n",
    "                cls_indices = np.random.choice(cls_indices, self.limit_per_label, replace=False)\n",
    "            indices.extend(cls_indices)\n",
    "        np.random.shuffle(indices)\n",
    "        return indices\n",
    "\n",
    "    def re_sample(self):\n",
    "        self.indices = self.balance_classes()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        index = self.indices[idx]\n",
    "        return self.X[index], self.y[index]\n",
    "# Custom Dataset for validation with limit per class\n",
    "class BalancedValidationDataset(Dataset):\n",
    "    def __init__(self, X, y, limit_per_label=400):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.limit_per_label = limit_per_label\n",
    "        self.classes = np.unique(y)\n",
    "        self.indices = self.balance_classes()\n",
    "\n",
    "    def balance_classes(self):\n",
    "        indices = []\n",
    "        for cls in self.classes:\n",
    "            cls_indices = np.where(self.y == cls)[0]\n",
    "            if len(cls_indices) > self.limit_per_label:\n",
    "                cls_indices = np.random.choice(cls_indices, self.limit_per_label, replace=False)\n",
    "            indices.extend(cls_indices)\n",
    "        np.random.shuffle(indices)\n",
    "        return indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        index = self.indices[idx]\n",
    "        return self.X[index], self.y[index]\n",
    "\n",
    "# Training function (using a similar approach to your ConvNet setup)\n",
    "def train_model_vit(model, train_loader, val_loader, num_epochs=10, lr=1e-4, patience=5, device='cuda'):\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Re-sample training data at the start of each epoch\n",
    "        train_loader.dataset.re_sample()\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * X_batch.size(0)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_val, y_val in val_loader:\n",
    "                X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "                outputs = model(X_val)\n",
    "                loss = criterion(outputs, y_val)\n",
    "                val_loss += loss.item() * X_val.size(0)\n",
    "        \n",
    "        # Log metrics\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        mlflow.log_metric(\"train_loss\", train_loss, step=epoch)\n",
    "        mlflow.log_metric(\"val_loss\", val_loss, step=epoch)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "        else:\n",
    "            patience -= 1\n",
    "            if patience <= 0:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    return model\n",
    "\n",
    "# Use your confusion matrix function to evaluate the model\n",
    "def print_confusion_matrix_vit(model, val_loader, device='cuda'):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            preds = model(X_batch).cpu().numpy()\n",
    "            all_preds.extend(np.argmax(preds, axis=1))\n",
    "            all_labels.extend(y_batch.numpy())\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "    print(\"Confusion Matrix:\", conf_matrix)\n",
    "    print(\"Classification Report:\", classification_report(all_labels, all_preds))\n",
    "\n",
    "# Main training script\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and preprocess your data\n",
    "    X = pd.read_pickle(\"Pickles/fusionv0/train.pkl\")\n",
    "    y = X[\"label\"]\n",
    "    label_mapping = {'star': 0, 'binary_star': 1, 'galaxy': 2, 'agn': 3}\n",
    "    y = y.map(label_mapping).values\n",
    "    \n",
    "    X = X.drop([\"parallax\", \"ra\", \"dec\", \"ra_error\", \"dec_error\", \"parallax_error\", \"pmra\", \"pmdec\", \"pmra_error\", \"pmdec_error\", \n",
    "                \"phot_g_mean_flux\", \"flagnopllx\", \"phot_g_mean_flux_error\", \"phot_bp_mean_flux\", \"phot_rp_mean_flux\", \n",
    "                \"phot_bp_mean_flux_error\", \"phot_rp_mean_flux_error\", \"label\"], axis=1).values\n",
    "    \n",
    "    # Read test data\n",
    "    X_test = pd.read_pickle(\"Pickles/fusionv0/test.pkl\")\n",
    "    y_test = X_test[\"label\"].map(label_mapping).values\n",
    "    X_test = X_test.drop([\"parallax\", \"ra\", \"dec\", \"ra_error\", \"dec_error\", \"parallax_error\", \"pmra\", \"pmdec\", \"pmra_error\", \"pmdec_error\", \n",
    "                \"phot_g_mean_flux\", \"flagnopllx\", \"phot_g_mean_flux_error\", \"phot_bp_mean_flux\", \"phot_rp_mean_flux\", \n",
    "                \"phot_bp_mean_flux_error\", \"phot_rp_mean_flux_error\", \"label\"], axis=1).values\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Clear memory\n",
    "    del X, y\n",
    "    gc.collect()\n",
    "\n",
    "    # Convert to torch tensors and create datasets\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32).unsqueeze(1)\n",
    "    X_val = torch.tensor(X_val, dtype=torch.float32).unsqueeze(1)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "    y_val = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "    train_dataset = BalancedDataset(X_train, y_train)\n",
    "    val_dataset = BalancedValidationDataset(X_val, y_val)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "    # Initialize Vision Transformer\n",
    "    model_vit = VisionTransformer1D()\n",
    "    num_epochs = 200\n",
    "    lr = 1e-4\n",
    "    patience = 10\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_param(\"num_epochs\", num_epochs)\n",
    "        mlflow.log_param(\"learning_rate\", lr)\n",
    "        trained_model_vit = train_model_vit(model_vit, train_loader, val_loader, num_epochs, lr, patience)\n",
    "        print_confusion_matrix_vit(trained_model_vit, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:0w5hzc3k) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">daily-bee-2</strong> at: <a href='https://wandb.ai/joaoc-university-of-southampton/spectra-classification-vit/runs/0w5hzc3k' target=\"_blank\">https://wandb.ai/joaoc-university-of-southampton/spectra-classification-vit/runs/0w5hzc3k</a><br/> View project at: <a href='https://wandb.ai/joaoc-university-of-southampton/spectra-classification-vit' target=\"_blank\">https://wandb.ai/joaoc-university-of-southampton/spectra-classification-vit</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241025_124837-0w5hzc3k\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:0w5hzc3k). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\jcwin\\OneDrive - University of Southampton\\_Southampton\\2024-25\\Star-Classifier\\wandb\\run-20241025_125048-8nljxcee</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/joaoc-university-of-southampton/spectra-classification-vit/runs/8nljxcee' target=\"_blank\">wandering-galaxy-3</a></strong> to <a href='https://wandb.ai/joaoc-university-of-southampton/spectra-classification-vit' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/joaoc-university-of-southampton/spectra-classification-vit' target=\"_blank\">https://wandb.ai/joaoc-university-of-southampton/spectra-classification-vit</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/joaoc-university-of-southampton/spectra-classification-vit/runs/8nljxcee' target=\"_blank\">https://wandb.ai/joaoc-university-of-southampton/spectra-classification-vit/runs/8nljxcee</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jcwin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 172\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# Initialize and train the model\u001b[39;00m\n\u001b[0;32m    171\u001b[0m model_vit \u001b[38;5;241m=\u001b[39m VisionTransformer1D()\n\u001b[1;32m--> 172\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model_vit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_vit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;66;03m# Save the model and finish WandB session\u001b[39;00m\n\u001b[0;32m    175\u001b[0m wandb\u001b[38;5;241m.\u001b[39mfinish()\n",
      "Cell \u001b[1;32mIn[7], line 73\u001b[0m, in \u001b[0;36mtrain_model_vit\u001b[1;34m(model, train_loader, val_loader, num_epochs, lr, patience, device)\u001b[0m\n\u001b[0;32m     71\u001b[0m X_batch, y_batch \u001b[38;5;241m=\u001b[39m X_batch\u001b[38;5;241m.\u001b[39mto(device), y_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     72\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 73\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, y_batch)\n\u001b[0;32m     75\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 133\u001b[0m, in \u001b[0;36mVisionTransformer1D.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;66;03m# Calculate required padding for divisibility by patch_size and pad input\u001b[39;00m\n\u001b[1;32m--> 133\u001b[0m     batch_size, seq_len \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    134\u001b[0m     pad_length \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_size \u001b[38;5;241m-\u001b[39m (seq_len \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_size)) \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_size\n\u001b[0;32m    135\u001b[0m     x \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mpad(x, (\u001b[38;5;241m0\u001b[39m, pad_length))\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wandb\n",
    "import gc\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Initialize WandB project\n",
    "wandb.init(project=\"spectra-classification-vit\", entity=\"joaoc-university-of-southampton\")\n",
    "\n",
    "# Define the Vision Transformer model\n",
    "class VisionTransformer1D(nn.Module):\n",
    "    def __init__(self, input_size=3748, num_classes=4, patch_size=17, dim=128, depth=6, heads=8, mlp_dim=256, dropout=0.1):\n",
    "        super(VisionTransformer1D, self).__init__()\n",
    "        \n",
    "        # Parameters\n",
    "        self.num_patches = input_size // patch_size\n",
    "        self.patch_size = patch_size\n",
    "        self.dim = dim\n",
    "        \n",
    "        # Patch Embedding layer\n",
    "        self.patch_embed = nn.Linear(patch_size, dim)\n",
    "        \n",
    "        # Positional Encoding\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_patches, dim))\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(dim, heads, mlp_dim, dropout),\n",
    "            depth\n",
    "        )\n",
    "        \n",
    "        # MLP Head\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convert input into patches and embed them\n",
    "        batch_size = x.shape[0]\n",
    "        x = x[:, :self.num_patches * self.patch_size].view(batch_size, self.num_patches, self.patch_size)\n",
    "        x = self.patch_embed(x) + self.pos_embedding\n",
    "        \n",
    "        # Transformer forward pass\n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        # Classify based on the first token representation\n",
    "        x = self.fc(x[:, 0])\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Training function (similar to your ConvNet setup but using WandB)\n",
    "def train_model_vit(model, train_loader, val_loader, num_epochs=10, lr=1e-4, patience=5, device='cuda'):\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Re-sample training data at the start of each epoch\n",
    "        train_loader.dataset.re_sample()\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * X_batch.size(0)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_val, y_val in val_loader:\n",
    "                X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "                outputs = model(X_val)\n",
    "                loss = criterion(outputs, y_val)\n",
    "                val_loss += loss.item() * X_val.size(0)\n",
    "        \n",
    "        # Log metrics to WandB\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        wandb.log({\"train_loss\": train_loss, \"val_loss\": val_loss, \"epoch\": epoch})\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "        else:\n",
    "            patience -= 1\n",
    "            if patience <= 0:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    return model\n",
    "class VisionTransformer1D(nn.Module):\n",
    "    def __init__(self, input_size=3748, num_classes=4, patch_size=17, dim=128, depth=6, heads=8, mlp_dim=256, dropout=0.1):\n",
    "        super(VisionTransformer1D, self).__init__()\n",
    "        \n",
    "        # Store patch size and dimensionality for embedding\n",
    "        self.patch_size = patch_size\n",
    "        self.dim = dim\n",
    "\n",
    "        # Patch Embedding layer\n",
    "        self.patch_embed = nn.Linear(patch_size, dim)\n",
    "        \n",
    "        # Positional Encoding (initialize to a reasonable size, but we’ll adjust it dynamically)\n",
    "        max_patches = (input_size + patch_size - 1) // patch_size  # Approximate max patches\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, max_patches, dim))\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(dim, heads, mlp_dim, dropout),\n",
    "            depth\n",
    "        )\n",
    "        \n",
    "        # MLP Head\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Calculate required padding for divisibility by patch_size and pad input\n",
    "        batch_size, seq_len = x.shape\n",
    "        pad_length = (self.patch_size - (seq_len % self.patch_size)) % self.patch_size\n",
    "        x = nn.functional.pad(x, (0, pad_length))\n",
    "        \n",
    "        # Dynamically calculate number of patches after padding\n",
    "        num_patches = x.size(1) // self.patch_size\n",
    "        x = x.view(batch_size, num_patches, self.patch_size)  # Reshape to patches\n",
    "        \n",
    "        # Embed patches and add positional encoding (resize pos_embedding if needed)\n",
    "        if self.pos_embedding.size(1) != num_patches:\n",
    "            self.pos_embedding = nn.Parameter(self.pos_embedding[:, :num_patches, :])\n",
    "        x = self.patch_embed(x) + self.pos_embedding\n",
    "        \n",
    "        # Transformer forward pass\n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        # Classify based on the first token representation\n",
    "        x = self.fc(x[:, 0])\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and preprocess your data (example from original script)\n",
    "    X = pd.read_pickle(\"Pickles/fusionv0/train.pkl\")\n",
    "    y = X[\"label\"].map({'star': 0, 'binary_star': 1, 'galaxy': 2, 'agn': 3}).values\n",
    "    X = X.drop(columns=[\"parallax\", \"ra\", \"dec\", \"label\"]).values\n",
    "\n",
    "    # Convert data to tensors and split for training/validation\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    X_train, X_val = torch.tensor(X_train, dtype=torch.float32).unsqueeze(1), torch.tensor(X_val, dtype=torch.float32).unsqueeze(1)\n",
    "    y_train, y_val = torch.tensor(y_train, dtype=torch.long), torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "    train_loader = DataLoader(BalancedDataset(X_train, y_train), batch_size=512, shuffle=True)\n",
    "    val_loader = DataLoader(BalancedValidationDataset(X_val, y_val), batch_size=512, shuffle=False)\n",
    "\n",
    "    # Initialize and train the model\n",
    "    model_vit = VisionTransformer1D()\n",
    "    trained_model = train_model_vit(model_vit, train_loader, val_loader, num_epochs=200, lr=1e-4, patience=10)\n",
    "    \n",
    "    # Save the model and finish WandB session\n",
    "    wandb.finish()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
