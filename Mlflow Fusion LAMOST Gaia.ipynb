{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/18 17:23:37 WARNING mlflow.utils.autologging_utils: MLflow pytorch autologging is known to be compatible with 1.9.0 <= torch <= 2.4.1, but the installed version is 2.4.1+cu124. If you encounter errors during autologging, try upgrading / downgrading torch to a compatible version, or try upgrading MLflow.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location=('file:///C:/Users/jcwin/OneDrive - University of '\n",
       " 'Southampton/_Southampton/2024-25/Star-Classifier/mlflow/286740436428343516'), creation_time=1729267230861, experiment_id='286740436428343516', last_update_time=1729267230861, lifecycle_stage='active', name='Fusion_Gaia_LAMOST', tags={}>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import gc\n",
    "import psutil\n",
    "import GPUtil\n",
    "import mlflow.pytorch\n",
    "import mlflow\n",
    "\n",
    "# Enable MLflow autologging for PyTorch\n",
    "mlflow.pytorch.autolog()\n",
    "mlflow.set_tracking_uri(uri=\"file:///C:/Users/jcwin/OneDrive - University of Southampton/_Southampton/2024-25/Star-Classifier/mlflow\")\n",
    "mlflow.set_experiment(\"Fusion_Gaia_LAMOST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNetFusion(nn.Module):\n",
    "    def __init__(self, input_shape, num_classes, gaia_input_size, \n",
    "                 num_filters=[128, 128, 128, 128, 128, 128, 128, 128], \n",
    "                 kernel_size=9,\n",
    "                 dense_units=[256, 256, 256, 128, 128, 128, 64, 64, 64],\n",
    "                 dropout_rate=0.2, \n",
    "                 gaia_fusion_units=512,\n",
    "                 padding='same'):\n",
    "        super(ConvNetFusion, self).__init__()\n",
    "        \n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        self.pool_layers = nn.ModuleList()\n",
    "        in_channels = 1  # Since it's a 1D input\n",
    "        \n",
    "        # Add convolutional layers\n",
    "        for filters in num_filters:\n",
    "            conv_layer = nn.Conv1d(in_channels=in_channels, out_channels=filters, kernel_size=kernel_size, padding=kernel_size//2)\n",
    "            self.conv_layers.append(conv_layer)\n",
    "            self.pool_layers.append(nn.MaxPool1d(kernel_size=2))\n",
    "            in_channels = filters\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # Compute the flattened output size (based on input shape and pooling)\n",
    "        final_seq_len = input_shape[0] // (2 ** len(num_filters))  # After all pooling layers\n",
    "        \n",
    "        # Add Gaia features to the input\n",
    "        self.gaia_input_layer = nn.Linear(gaia_input_size, gaia_fusion_units)\n",
    "\n",
    "        # Add dense layers\n",
    "        dense_input_units = num_filters[-1] * final_seq_len + gaia_fusion_units  # Add Gaia features to the dense input\n",
    "        self.dense_layers = nn.ModuleList()\n",
    "        for units in dense_units:\n",
    "            self.dense_layers.append(nn.Linear(dense_input_units, units))\n",
    "            dense_input_units = units\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(dense_input_units, num_classes)\n",
    "    \n",
    "    def forward(self, x_conv, x_gaia):\n",
    "        # Pass through convolutional layers\n",
    "        for conv_layer, pool_layer in zip(self.conv_layers, self.pool_layers):\n",
    "            x_conv = pool_layer(torch.relu(conv_layer(x_conv)))\n",
    "            x_conv = self.dropout(x_conv)\n",
    "        # Flatten the conv output\n",
    "        x_conv = self.flatten(x_conv)\n",
    "\n",
    "        # Connect Gaia features to a separate dense layer and connect after to the main network\n",
    "        x_gaia = torch.relu(self.gaia_input_layer(x_gaia))\n",
    "\n",
    "        \n",
    "        # Concatenate Gaia features\n",
    "        x = torch.cat((x_conv, x_gaia), dim=1)\n",
    "        \n",
    "        # Pass through dense layers\n",
    "        for dense_layer in self.dense_layers:\n",
    "            x = torch.relu(dense_layer(x))\n",
    "            x = self.dropout(x)\n",
    "        \n",
    "        # Output layer\n",
    "        x = self.output_layer(x)\n",
    "        return torch.softmax(x, dim=1)\n",
    "# Create Datasets\n",
    "class BalancedDatasetFusion(Dataset):\n",
    "    def __init__(self, X_conv, X_gaia, y, limit_per_label=1600):\n",
    "        self.X_conv = X_conv\n",
    "        self.X_gaia = X_gaia\n",
    "        self.y = y\n",
    "        self.limit_per_label = limit_per_label\n",
    "        self.classes = np.unique(y)\n",
    "        self.indices = self.balance_classes()\n",
    "\n",
    "    def balance_classes(self):\n",
    "        indices = []\n",
    "        for cls in self.classes:\n",
    "            cls_indices = np.where(self.y == cls)[0]\n",
    "            if len(cls_indices) > self.limit_per_label:\n",
    "                cls_indices = np.random.choice(cls_indices, self.limit_per_label, replace=False)\n",
    "            indices.extend(cls_indices)\n",
    "        np.random.shuffle(indices)\n",
    "        return indices\n",
    "\n",
    "    def re_sample(self):\n",
    "        self.indices = self.balance_classes()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        index = self.indices[idx]\n",
    "        return self.X_conv[index], self.X_gaia[index], self.y[index]\n",
    "\n",
    "# Define a function to train the model\n",
    "def train_model(model, train_loader, val_loader, num_epochs=200, lr=1e-4, patience=10):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss, correct_train, total_train = 0.0, 0, 0\n",
    "\n",
    "        # Training loop\n",
    "        for X_conv, X_gaia, y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_conv, X_gaia)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * X_conv.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_train += (predicted == y).sum().item()\n",
    "            total_train += y.size(0)\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_acc = correct_train / total_train\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_loss, correct_val, total_val = 0.0, 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X_conv, X_gaia, y in val_loader:\n",
    "                outputs = model(X_conv, X_gaia)\n",
    "                loss = criterion(outputs, y)\n",
    "\n",
    "                val_loss += loss.item() * X_conv.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct_val += (predicted == y).sum().item()\n",
    "                total_val += y.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_acc = correct_val / total_val\n",
    "\n",
    "        # Log metrics for MLflow\n",
    "        mlflow.log_metric(\"train_loss\", train_loss, step=epoch)\n",
    "        mlflow.log_metric(\"train_acc\", train_acc, step=epoch)\n",
    "        mlflow.log_metric(\"val_loss\", val_loss, step=epoch)\n",
    "        mlflow.log_metric(\"val_acc\", val_acc, step=epoch)\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = model.state_dict()\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # Load the best model weights before returning\n",
    "    model.load_state_dict(best_model)\n",
    "    return model\n",
    "\n",
    "# Function to print confusion matrix and log it with MLflow\n",
    "def print_confusion_matrix(model, val_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_conv, X_gaia, y in val_loader:\n",
    "            outputs = model(X_conv, X_gaia)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Log confusion matrix\n",
    "    mlflow.log_artifact(confusion_matrix_to_image(cm), artifact_path=\"confusion_matrix\")\n",
    "\n",
    "def confusion_matrix_to_image(cm):\n",
    "    # Convert confusion matrix to an image (optional)\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import io\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "\n",
    "    # Save the figure to a buffer\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    buf.seek(0)\n",
    "    \n",
    "    # Return the buffer\n",
    "    return buf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "X = pd.read_pickle(\"Pickles/fusionv0/train.pkl\")\n",
    "gaia_features = [\"parallax\", \"ra\", \"dec\", \"ra_error\", \"dec_error\", \"parallax_error\", \"pmra\", \"pmdec\", \"pmra_error\", \"pmdec_error\", \n",
    "           \"phot_g_mean_flux\", \"flagnopllx\", \"phot_g_mean_flux_error\", \"phot_bp_mean_flux\", \"phot_rp_mean_flux\", \n",
    "           \"phot_bp_mean_flux_error\", \"phot_rp_mean_flux_error\"]\n",
    "\n",
    "# Extract Gaia data\n",
    "X_gaia = X[gaia_features].values\n",
    "X_conv = X.drop(gaia_features + [\"label\"], axis=1).values\n",
    "y = X[\"label\"]\n",
    "\n",
    "# Mapping labels to integers\n",
    "label_mapping = {'star': 0, 'binary_star': 1, 'galaxy': 2, 'agn': 3}\n",
    "y = y.map(label_mapping).values\n",
    "\n",
    "# Split data into train and validation\n",
    "X_train_conv, X_val_conv, X_train_gaia, X_val_gaia, y_train, y_val = train_test_split(X_conv, X_gaia, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_conv = torch.tensor(X_train_conv, dtype=torch.float32).unsqueeze(1)\n",
    "X_val_conv = torch.tensor(X_val_conv, dtype=torch.float32).unsqueeze(1)\n",
    "X_train_gaia = torch.tensor(X_train_gaia, dtype=torch.float32)\n",
    "X_val_gaia = torch.tensor(X_val_gaia, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "y_val = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = BalancedDatasetFusion(X_train_conv, X_train_gaia, y_train)\n",
    "val_dataset = BalancedDatasetFusion(X_val_conv, X_val_gaia, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=512, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaia input size: 17\n",
      "ConvNetFusion(\n",
      "  (conv_layers): ModuleList(\n",
      "    (0): Conv1d(1, 128, kernel_size=(9,), stride=(1,), padding=(4,))\n",
      "    (1-2): 2 x Conv1d(128, 128, kernel_size=(9,), stride=(1,), padding=(4,))\n",
      "    (3): Conv1d(128, 256, kernel_size=(9,), stride=(1,), padding=(4,))\n",
      "    (4-5): 2 x Conv1d(256, 256, kernel_size=(9,), stride=(1,), padding=(4,))\n",
      "    (6): Conv1d(256, 512, kernel_size=(9,), stride=(1,), padding=(4,))\n",
      "    (7-10): 4 x Conv1d(512, 512, kernel_size=(9,), stride=(1,), padding=(4,))\n",
      "  )\n",
      "  (pool_layers): ModuleList(\n",
      "    (0-10): 11 x MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (gaia_input_layer): Linear(in_features=17, out_features=512, bias=True)\n",
      "  (dense_layers): ModuleList(\n",
      "    (0-1): 2 x Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (3): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (4): Linear(in_features=256, out_features=64, bias=True)\n",
      "  )\n",
      "  (output_layer): Linear(in_features=64, out_features=4, bias=True)\n",
      ")\n",
      "Number of parameters: 15172420\n"
     ]
    }
   ],
   "source": [
    "# Initialize model with Gaia input size\n",
    "gaia_input_size = X_train_gaia.shape[1]\n",
    "print(f\"Gaia input size: {gaia_input_size}\")\n",
    "filters=[128, 128, 128, 256, 256, 256, 512, 512, 512, 512, 512]\n",
    "dense=[1024, 1024, 512, 256, 64]\n",
    "\n",
    "model = ConvNetFusion(input_shape=(3748,), num_classes=4, gaia_input_size=gaia_input_size, \n",
    "                      num_filters=filters, kernel_size=9, dense_units=dense, dropout_rate=0.2, gaia_fusion_units=512)\n",
    "print(model)\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNetFusion(\n",
      "  (conv_layers): ModuleList(\n",
      "    (0): Conv1d(1, 32, kernel_size=(9,), stride=(1,), padding=(4,))\n",
      "    (1): Conv1d(32, 64, kernel_size=(9,), stride=(1,), padding=(4,))\n",
      "    (2-3): 2 x Conv1d(64, 64, kernel_size=(9,), stride=(1,), padding=(4,))\n",
      "    (4): Conv1d(64, 128, kernel_size=(9,), stride=(1,), padding=(4,))\n",
      "    (5-6): 2 x Conv1d(128, 128, kernel_size=(9,), stride=(1,), padding=(4,))\n",
      "    (7): Conv1d(128, 256, kernel_size=(9,), stride=(1,), padding=(4,))\n",
      "    (8-9): 2 x Conv1d(256, 256, kernel_size=(9,), stride=(1,), padding=(4,))\n",
      "  )\n",
      "  (pool_layers): ModuleList(\n",
      "    (0-9): 10 x MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (gaia_input_layer): Linear(in_features=17, out_features=256, bias=True)\n",
      "  (dense_layers): ModuleList(\n",
      "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "  )\n",
      "  (output_layer): Linear(in_features=128, out_features=4, bias=True)\n",
      ")\n",
      "Number of parameters: 2959620\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "gaia_input_size = X_train_gaia.shape[1]\n",
    "filters = [128, 128, 128, 256, 256, 256, 512, 512, 512, 512, 512]\n",
    "dense = [1024, 1024, 512, 256, 64]\n",
    "\n",
    "# smaller model\n",
    "filters = [32, 64,64,64, 128, 128, 128, 256, 256, 256]\n",
    "dense = [512, 512, 256, 256, 128]\n",
    "\n",
    "\n",
    "model = ConvNetFusion(input_shape=(3748,), num_classes=4, gaia_input_size=gaia_input_size, \n",
    "                      num_filters=filters, kernel_size=9, dense_units=dense, dropout_rate=0.2, gaia_fusion_units=256)\n",
    "print(model)\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/18 17:35:52 INFO mlflow.system_metrics.system_metrics_monitor: Started monitoring system metrics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Train Loss: 1.3845, Train Acc: 0.2655, Val Loss: 1.3791, Val Acc: 0.3178\n",
      "Epoch [2/200], Train Loss: 1.3766, Train Acc: 0.3881, Val Loss: 1.3599, Val Acc: 0.6179\n",
      "Epoch [3/200], Train Loss: 1.3386, Train Acc: 0.4141, Val Loss: 1.2968, Val Acc: 0.5848\n",
      "Epoch [4/200], Train Loss: 1.2362, Train Acc: 0.5119, Val Loss: 1.1819, Val Acc: 0.6002\n",
      "Epoch [5/200], Train Loss: 1.1394, Train Acc: 0.6165, Val Loss: 1.1294, Val Acc: 0.6481\n",
      "Epoch [6/200], Train Loss: 1.0797, Train Acc: 0.6674, Val Loss: 1.1051, Val Acc: 0.6648\n",
      "Epoch [7/200], Train Loss: 1.0563, Train Acc: 0.6876, Val Loss: 1.0900, Val Acc: 0.6682\n",
      "Epoch [8/200], Train Loss: 1.0423, Train Acc: 0.6974, Val Loss: 1.1101, Val Acc: 0.6631\n",
      "Epoch [9/200], Train Loss: 1.0268, Train Acc: 0.7125, Val Loss: 1.0964, Val Acc: 0.6730\n",
      "Epoch [10/200], Train Loss: 1.0200, Train Acc: 0.7239, Val Loss: 1.0903, Val Acc: 0.6746\n",
      "Epoch [11/200], Train Loss: 1.0120, Train Acc: 0.7298, Val Loss: 1.0754, Val Acc: 0.6769\n",
      "Epoch [12/200], Train Loss: 1.0123, Train Acc: 0.7258, Val Loss: 1.0830, Val Acc: 0.6793\n",
      "Epoch [13/200], Train Loss: 1.0101, Train Acc: 0.7296, Val Loss: 1.0755, Val Acc: 0.6796\n",
      "Epoch [14/200], Train Loss: 1.0061, Train Acc: 0.7322, Val Loss: 1.0560, Val Acc: 0.6888\n",
      "Epoch [15/200], Train Loss: 1.0037, Train Acc: 0.7355, Val Loss: 1.0543, Val Acc: 0.6909\n",
      "Epoch [16/200], Train Loss: 0.9985, Train Acc: 0.7393, Val Loss: 1.0623, Val Acc: 0.6849\n",
      "Epoch [17/200], Train Loss: 0.9925, Train Acc: 0.7445, Val Loss: 1.0567, Val Acc: 0.7036\n",
      "Epoch [18/200], Train Loss: 0.9926, Train Acc: 0.7485, Val Loss: 1.0607, Val Acc: 0.7003\n",
      "Epoch [19/200], Train Loss: 0.9876, Train Acc: 0.7526, Val Loss: 1.0420, Val Acc: 0.7153\n",
      "Epoch [20/200], Train Loss: 0.9829, Train Acc: 0.7560, Val Loss: 1.0240, Val Acc: 0.7309\n",
      "Epoch [21/200], Train Loss: 0.9724, Train Acc: 0.7711, Val Loss: 1.0177, Val Acc: 0.7295\n",
      "Epoch [22/200], Train Loss: 0.9703, Train Acc: 0.7708, Val Loss: 1.0119, Val Acc: 0.7375\n",
      "Epoch [23/200], Train Loss: 0.9619, Train Acc: 0.7779, Val Loss: 1.0253, Val Acc: 0.7229\n",
      "Epoch [24/200], Train Loss: 0.9605, Train Acc: 0.7821, Val Loss: 0.9906, Val Acc: 0.7540\n",
      "Epoch [25/200], Train Loss: 0.9563, Train Acc: 0.7828, Val Loss: 1.0180, Val Acc: 0.7264\n",
      "Epoch [26/200], Train Loss: 0.9480, Train Acc: 0.7936, Val Loss: 1.0118, Val Acc: 0.7287\n",
      "Epoch [27/200], Train Loss: 0.9478, Train Acc: 0.7931, Val Loss: 1.0579, Val Acc: 0.6929\n",
      "Epoch [28/200], Train Loss: 0.9436, Train Acc: 0.7987, Val Loss: 1.0001, Val Acc: 0.7367\n",
      "Epoch [29/200], Train Loss: 0.9369, Train Acc: 0.8053, Val Loss: 0.9958, Val Acc: 0.7416\n",
      "Epoch [30/200], Train Loss: 0.9376, Train Acc: 0.8063, Val Loss: 1.0432, Val Acc: 0.7040\n",
      "Epoch [31/200], Train Loss: 0.9398, Train Acc: 0.8025, Val Loss: 0.9969, Val Acc: 0.7389\n",
      "Epoch [32/200], Train Loss: 0.9326, Train Acc: 0.8104, Val Loss: 1.0098, Val Acc: 0.7316\n",
      "Epoch [33/200], Train Loss: 0.9269, Train Acc: 0.8155, Val Loss: 0.9973, Val Acc: 0.7406\n",
      "Epoch [34/200], Train Loss: 0.9303, Train Acc: 0.8127, Val Loss: 0.9843, Val Acc: 0.7583\n",
      "Epoch [35/200], Train Loss: 0.9258, Train Acc: 0.8173, Val Loss: 1.0148, Val Acc: 0.7254\n",
      "Epoch [36/200], Train Loss: 0.9310, Train Acc: 0.8097, Val Loss: 0.9896, Val Acc: 0.7490\n",
      "Epoch [37/200], Train Loss: 0.9268, Train Acc: 0.8137, Val Loss: 0.9872, Val Acc: 0.7540\n",
      "Epoch [38/200], Train Loss: 0.9236, Train Acc: 0.8207, Val Loss: 1.0124, Val Acc: 0.7309\n",
      "Epoch [39/200], Train Loss: 0.9237, Train Acc: 0.8224, Val Loss: 1.0224, Val Acc: 0.7217\n",
      "Epoch [40/200], Train Loss: 0.9235, Train Acc: 0.8204, Val Loss: 0.9811, Val Acc: 0.7583\n",
      "Epoch [41/200], Train Loss: 0.9191, Train Acc: 0.8212, Val Loss: 0.9974, Val Acc: 0.7445\n",
      "Epoch [42/200], Train Loss: 0.9173, Train Acc: 0.8256, Val Loss: 0.9954, Val Acc: 0.7470\n",
      "Epoch [43/200], Train Loss: 0.9157, Train Acc: 0.8263, Val Loss: 0.9769, Val Acc: 0.7607\n",
      "Epoch [44/200], Train Loss: 0.9122, Train Acc: 0.8304, Val Loss: 0.9822, Val Acc: 0.7575\n",
      "Epoch [45/200], Train Loss: 0.9140, Train Acc: 0.8286, Val Loss: 0.9720, Val Acc: 0.7667\n",
      "Epoch [46/200], Train Loss: 0.9133, Train Acc: 0.8297, Val Loss: 0.9817, Val Acc: 0.7574\n",
      "Epoch [47/200], Train Loss: 0.9112, Train Acc: 0.8320, Val Loss: 0.9868, Val Acc: 0.7533\n",
      "Epoch [48/200], Train Loss: 0.9095, Train Acc: 0.8340, Val Loss: 0.9969, Val Acc: 0.7449\n",
      "Epoch [49/200], Train Loss: 0.9119, Train Acc: 0.8284, Val Loss: 0.9703, Val Acc: 0.7677\n",
      "Epoch [50/200], Train Loss: 0.9077, Train Acc: 0.8329, Val Loss: 0.9730, Val Acc: 0.7659\n",
      "Epoch [51/200], Train Loss: 0.9059, Train Acc: 0.8362, Val Loss: 0.9875, Val Acc: 0.7531\n",
      "Epoch [52/200], Train Loss: 0.9068, Train Acc: 0.8366, Val Loss: 0.9950, Val Acc: 0.7461\n",
      "Epoch [53/200], Train Loss: 0.9078, Train Acc: 0.8340, Val Loss: 0.9758, Val Acc: 0.7614\n",
      "Epoch [54/200], Train Loss: 0.9028, Train Acc: 0.8414, Val Loss: 0.9673, Val Acc: 0.7704\n",
      "Epoch [55/200], Train Loss: 0.9093, Train Acc: 0.8339, Val Loss: 0.9716, Val Acc: 0.7683\n",
      "Epoch [56/200], Train Loss: 0.9100, Train Acc: 0.8311, Val Loss: 0.9852, Val Acc: 0.7546\n",
      "Epoch [57/200], Train Loss: 0.9035, Train Acc: 0.8376, Val Loss: 0.9663, Val Acc: 0.7745\n",
      "Epoch [58/200], Train Loss: 0.9014, Train Acc: 0.8426, Val Loss: 0.9669, Val Acc: 0.7712\n",
      "Epoch [59/200], Train Loss: 0.9024, Train Acc: 0.8414, Val Loss: 0.9603, Val Acc: 0.7784\n",
      "Epoch [60/200], Train Loss: 0.9031, Train Acc: 0.8394, Val Loss: 0.9615, Val Acc: 0.7794\n",
      "Epoch [61/200], Train Loss: 0.9004, Train Acc: 0.8424, Val Loss: 0.9737, Val Acc: 0.7655\n",
      "Epoch [62/200], Train Loss: 0.9013, Train Acc: 0.8411, Val Loss: 0.9861, Val Acc: 0.7556\n",
      "Epoch [63/200], Train Loss: 0.9013, Train Acc: 0.8407, Val Loss: 0.9566, Val Acc: 0.7838\n",
      "Epoch [64/200], Train Loss: 0.9027, Train Acc: 0.8388, Val Loss: 0.9666, Val Acc: 0.7739\n",
      "Epoch [65/200], Train Loss: 0.8978, Train Acc: 0.8452, Val Loss: 0.9639, Val Acc: 0.7755\n",
      "Epoch [66/200], Train Loss: 0.9050, Train Acc: 0.8368, Val Loss: 0.9417, Val Acc: 0.7981\n",
      "Epoch [67/200], Train Loss: 0.9058, Train Acc: 0.8355, Val Loss: 0.9573, Val Acc: 0.7821\n",
      "Epoch [68/200], Train Loss: 0.9041, Train Acc: 0.8407, Val Loss: 0.9604, Val Acc: 0.7784\n",
      "Epoch [69/200], Train Loss: 0.9006, Train Acc: 0.8419, Val Loss: 0.9650, Val Acc: 0.7751\n",
      "Epoch [70/200], Train Loss: 0.8995, Train Acc: 0.8444, Val Loss: 0.9700, Val Acc: 0.7688\n",
      "Epoch [71/200], Train Loss: 0.8963, Train Acc: 0.8470, Val Loss: 0.9488, Val Acc: 0.7893\n",
      "Epoch [72/200], Train Loss: 0.8986, Train Acc: 0.8449, Val Loss: 0.9469, Val Acc: 0.7918\n",
      "Epoch [73/200], Train Loss: 0.8961, Train Acc: 0.8468, Val Loss: 0.9404, Val Acc: 0.8002\n",
      "Epoch [74/200], Train Loss: 0.8955, Train Acc: 0.8495, Val Loss: 0.9500, Val Acc: 0.7901\n",
      "Epoch [75/200], Train Loss: 0.8935, Train Acc: 0.8498, Val Loss: 0.9524, Val Acc: 0.7860\n",
      "Epoch [76/200], Train Loss: 0.8933, Train Acc: 0.8478, Val Loss: 0.9531, Val Acc: 0.7870\n",
      "Epoch [77/200], Train Loss: 0.8905, Train Acc: 0.8542, Val Loss: 0.9495, Val Acc: 0.7889\n",
      "Epoch [78/200], Train Loss: 0.8893, Train Acc: 0.8532, Val Loss: 0.9528, Val Acc: 0.7862\n",
      "Epoch [79/200], Train Loss: 0.8931, Train Acc: 0.8496, Val Loss: 0.9533, Val Acc: 0.7862\n",
      "Epoch [80/200], Train Loss: 0.8931, Train Acc: 0.8501, Val Loss: 0.9504, Val Acc: 0.7907\n",
      "Epoch [81/200], Train Loss: 0.8899, Train Acc: 0.8539, Val Loss: 0.9467, Val Acc: 0.7918\n",
      "Epoch [82/200], Train Loss: 0.8895, Train Acc: 0.8526, Val Loss: 0.9459, Val Acc: 0.7940\n",
      "Early stopping at epoch 83\n",
      "Confusion Matrix:\n",
      "[[1358  216    2   24]\n",
      " [ 551 1028    4   17]\n",
      " [   1    0  330    4]\n",
      " [   4    3  236 1357]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/18 18:44:11 INFO mlflow.system_metrics.system_metrics_monitor: Stopping system metrics monitoring...\n",
      "2024/10/18 18:44:11 INFO mlflow.system_metrics.system_metrics_monitor: Successfully terminated system metrics monitoring!\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "stat: path should be string, bytes, os.PathLike or integer, not BytesIO",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m train_model(model, train_loader, val_loader, num_epochs\u001b[38;5;241m=\u001b[39mnum_epochs, lr\u001b[38;5;241m=\u001b[39mlr, patience\u001b[38;5;241m=\u001b[39mpatience)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[43mprint_confusion_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrained_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Save the model in MLflow\u001b[39;00m\n\u001b[0;32m     25\u001b[0m mlflow\u001b[38;5;241m.\u001b[39mpytorch\u001b[38;5;241m.\u001b[39mlog_model(trained_model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[18], line 184\u001b[0m, in \u001b[0;36mprint_confusion_matrix\u001b[1;34m(model, val_loader)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28mprint\u001b[39m(cm)\n\u001b[0;32m    183\u001b[0m \u001b[38;5;66;03m# Log confusion matrix\u001b[39;00m\n\u001b[1;32m--> 184\u001b[0m \u001b[43mmlflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_artifact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfusion_matrix_to_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcm\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martifact_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfusion_matrix\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\mlflow\\tracking\\fluent.py:1130\u001b[0m, in \u001b[0;36mlog_artifact\u001b[1;34m(local_path, artifact_path, run_id)\u001b[0m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1101\u001b[0m \u001b[38;5;124;03mLog a local file or directory as an artifact of the currently active run. If no run is\u001b[39;00m\n\u001b[0;32m   1102\u001b[0m \u001b[38;5;124;03mactive, this method will create a new active run.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;124;03m            mlflow.log_artifact(path)\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1129\u001b[0m run_id \u001b[38;5;241m=\u001b[39m run_id \u001b[38;5;129;01mor\u001b[39;00m _get_or_start_run()\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mrun_id\n\u001b[1;32m-> 1130\u001b[0m \u001b[43mMlflowClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_artifact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martifact_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\mlflow\\tracking\\client.py:1928\u001b[0m, in \u001b[0;36mMlflowClient.log_artifact\u001b[1;34m(self, run_id, local_path, artifact_path)\u001b[0m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_id\u001b[38;5;241m.\u001b[39mstartswith(TRACE_REQUEST_ID_PREFIX):\n\u001b[0;32m   1925\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MlflowException(\n\u001b[0;32m   1926\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid run id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. `log_artifact` run id must map to a valid run.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1927\u001b[0m     )\n\u001b[1;32m-> 1928\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tracking_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_artifact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martifact_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\mlflow\\tracking\\_tracking_service\\client.py:834\u001b[0m, in \u001b[0;36mTrackingServiceClient.log_artifact\u001b[1;34m(self, run_id, local_path, artifact_path)\u001b[0m\n\u001b[0;32m    825\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;124;03mWrite a local file or directory to the remote ``artifact_uri``.\u001b[39;00m\n\u001b[0;32m    827\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    831\u001b[0m \u001b[38;5;124;03m    artifact_path: If provided, the directory in ``artifact_uri`` to write to.\u001b[39;00m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    833\u001b[0m artifact_repo \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_artifact_repo(run_id)\n\u001b[1;32m--> 834\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_path\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    835\u001b[0m     dir_name \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mnormpath(local_path))\n\u001b[0;32m    836\u001b[0m     path_name \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    837\u001b[0m         os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(artifact_path, dir_name) \u001b[38;5;28;01mif\u001b[39;00m artifact_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m dir_name\n\u001b[0;32m    838\u001b[0m     )\n",
      "File \u001b[1;32m<frozen genericpath>:42\u001b[0m, in \u001b[0;36misdir\u001b[1;34m(s)\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: stat: path should be string, bytes, os.PathLike or integer, not BytesIO"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training parameters\n",
    "num_epochs = 200\n",
    "lr = 1e-4\n",
    "patience = 10\n",
    "batch_size = 512\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run(log_system_metrics=True):\n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"num_epochs\", num_epochs)\n",
    "    mlflow.log_param(\"lr\", lr)\n",
    "    mlflow.log_param(\"patience\", patience)\n",
    "    mlflow.log_param(\"batch_size\", batch_size)\n",
    "    mlflow.log_param(\"num_filters\", filters)\n",
    "    mlflow.log_param(\"dense_units\", dense)\n",
    "    mlflow.log_param(\"dropout_rate\", 0.2)\n",
    "    mlflow.log_param(\"kernel_size\", 9)\n",
    "\n",
    "    # Train the model\n",
    "    trained_model = train_model(model, train_loader, val_loader, num_epochs=num_epochs, lr=lr, patience=patience)\n",
    "\n",
    "    # Evaluate the model\n",
    "    print_confusion_matrix(trained_model, val_loader)\n",
    "    # Save the model in MLflow\n",
    "    mlflow.pytorch.log_model(trained_model, \"model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
