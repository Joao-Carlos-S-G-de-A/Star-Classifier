{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def fusion_convnet(input_shape1, input_shape2, num_classes, \n",
    "                   num_filters=[128, 128, 128, 128, 128, 128, 128, 128], \n",
    "                   kernel_size=(9,),\n",
    "                   dense_units1=128, \n",
    "                   dense_units2=64,\n",
    "                   dense_units3=32,\n",
    "                   dense_units4=16,\n",
    "                   dropout_rate=0.2,\n",
    "                   padding='same'):\n",
    "    \n",
    "    # Input 1: The original Conv1D input\n",
    "    input1 = tf.keras.layers.Input(shape=input_shape1)\n",
    "    \n",
    "    # First convolutional layer\n",
    "    x = tf.keras.layers.Conv1D(filters=num_filters[0], kernel_size=kernel_size, \n",
    "                               activation='relu', padding=padding)(input1)\n",
    "    x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)\n",
    "    \n",
    "    # Additional convolutional layers\n",
    "    for filters in num_filters[1:]:\n",
    "        x = tf.keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, \n",
    "                                   activation='relu', padding=padding)(x)\n",
    "        x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)\n",
    "        x = tf.keras.layers.Dropout(rate=dropout_rate)(x)\n",
    "    \n",
    "    # Flatten the output from the convolutional layers\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    \n",
    "    # Input 2: The second input, GAIA data\n",
    "    input2 = tf.keras.layers.Input(shape=input_shape2)\n",
    "    input2_flattened = tf.keras.layers.Flatten()(input2)\n",
    "\n",
    "    # Add a dense layer to the second input\n",
    "    input2_flattened = tf.keras.layers.Dense(units=1024, activation='relu')(input2_flattened)\n",
    "    input2_flattened = tf.keras.layers.Dropout(rate=dropout_rate)(input2_flattened)\n",
    "\n",
    "    \n",
    "    \n",
    "    # Concatenate the output of the convolutional layers with the second input\n",
    "    combined = tf.keras.layers.Concatenate()([x, input2_flattened])\n",
    "    \n",
    "    # Adding a dense layer\n",
    "    x = tf.keras.layers.Dense(units=dense_units1, activation='relu')(combined)\n",
    "    x = tf.keras.layers.Dropout(rate=dropout_rate)(x)\n",
    "    \n",
    "    # Adding another dense layer\n",
    "    if dense_units2:\n",
    "        x = tf.keras.layers.Dense(units=dense_units2, activation='relu')(x)\n",
    "        x = tf.keras.layers.Dropout(rate=dropout_rate)(x)\n",
    "    \n",
    "    # Adding another dense layer\n",
    "    if dense_units3:\n",
    "        x = tf.keras.layers.Dense(units=dense_units3, activation='relu')(x)\n",
    "        x = tf.keras.layers.Dropout(rate=dropout_rate)(x)\n",
    "    \n",
    "    # Adding another dense layer\n",
    "    if dense_units4:\n",
    "        x = tf.keras.layers.Dense(units=dense_units4, activation='relu')(x)\n",
    "        x = tf.keras.layers.Dropout(rate=dropout_rate)(x)\n",
    "    \n",
    "    # Output layer\n",
    "    output = tf.keras.layers.Dense(units=num_classes, activation='softmax')(x)\n",
    "    \n",
    "    # Model with two inputs\n",
    "    model = tf.keras.models.Model(inputs=[input1, input2], outputs=output)\n",
    "    \n",
    "    # Optimizer and loss function\n",
    "    optimizer_ = tf.keras.optimizers.AdamW(learning_rate=1e-4)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizer_, \n",
    "                  loss='sparse_categorical_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "def generate_file_list_from_directories(base_dirs, npy_base_dirs, limit_per_dir=10000):\n",
    "    \"\"\"Generates a list of FITS files and corresponding npy files and their labels.\"\"\"\n",
    "    spectra_dirs = {\n",
    "        \"gal_spectra\": 0,  # Label 0 for galaxies\n",
    "        \"star_spectra\": 1,  # Label 1 for stars\n",
    "        \"agn_spectra\": 2,   # Label 2 for AGNs\n",
    "        \"bin_spectra\": 3    # Label 3 for binary stars\n",
    "    }\n",
    "\n",
    "    fits_file_list = []\n",
    "    npy_file_list = []\n",
    "    labels = []\n",
    "\n",
    "    print(\"Gathering FITS and npy files from pre-separated directories...\")\n",
    "    for dir_name, label in spectra_dirs.items():\n",
    "        for base_dir, npy_base_dir in zip(base_dirs, npy_base_dirs):\n",
    "            # FITS file paths\n",
    "            fits_dir_path = os.path.join(base_dir, dir_name)\n",
    "            fits_dir_files = []\n",
    "            # npy file paths\n",
    "            npy_dir_path = os.path.join(npy_base_dir, dir_name)\n",
    "            npy_dir_files = []\n",
    "\n",
    "            # Collect all FITS files in the directory\n",
    "            for root, dirs, files in os.walk(fits_dir_path):\n",
    "                for file in files:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    fits_dir_files.append(file_path)\n",
    "\n",
    "            # Collect all npy files in the directory\n",
    "            for root, dirs, files in os.walk(npy_dir_path):\n",
    "                for file in files:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    npy_dir_files.append(file_path)\n",
    "\n",
    "            # Ensure that both FITS and npy files are paired\n",
    "            fits_dir_files.sort()  # Sorting ensures that corresponding files match\n",
    "            npy_dir_files.sort()\n",
    "\n",
    "            print(f\"Found {len(fits_dir_files)} FITS files and {len(npy_dir_files)} npy files for {dir_name} in {base_dir}\")\n",
    "\n",
    "            # Randomly select files up to the limit\n",
    "            if len(fits_dir_files) > limit_per_dir:\n",
    "                selected_fits_files = random.sample(fits_dir_files, limit_per_dir)\n",
    "                selected_npy_files = random.sample(npy_dir_files, limit_per_dir)\n",
    "            else:\n",
    "                selected_fits_files = fits_dir_files\n",
    "                selected_npy_files = npy_dir_files\n",
    "\n",
    "            # Append selected FITS and npy files and their labels\n",
    "            fits_file_list.extend(selected_fits_files)\n",
    "            npy_file_list.extend(selected_npy_files)\n",
    "            labels.extend([label] * len(selected_fits_files))\n",
    "\n",
    "    print(f\"Total spectra files collected: {len(fits_file_list)}\")\n",
    "    return fits_file_list, npy_file_list, labels\n",
    "\n",
    "def create_dataset(fits_file_list, npy_file_list, labels, batch_size=32, target_length=3748):\n",
    "    \"\"\"Create TensorFlow dataset with both FITS and npy files as inputs.\"\"\"\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((fits_file_list, npy_file_list, labels))\n",
    "\n",
    "    def load_and_preprocess(fits_file_path, npy_file_path, label):\n",
    "        # Load the FITS spectra\n",
    "        fits_spectra = tf_load_single_spectrum(fits_file_path, target_length)\n",
    "        # Load the npy array\n",
    "        npy_spectra = tf.py_function(np.load, [npy_file_path], tf.float32)\n",
    "        npy_spectra.set_shape([None])  # Set the shape explicitly for TensorFlow to optimize\n",
    "        return (fits_spectra, npy_spectra), label\n",
    "\n",
    "    # Apply parallelism and optimizations\n",
    "    dataset = dataset.map(load_and_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.cache()  # Cache the dataset after loading it once\n",
    "    dataset = dataset.shuffle(buffer_size=len(fits_file_list))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)  # Prefetch to overlap data loading and training\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "# Load the validation dataset once and keep it in memory\n",
    "def load_validation_dataset(limit_per_label=2000):\n",
    "    val_dataset = generate_datasets_from_preseparated(limit_per_dir=limit_per_label)[1]\n",
    "    return val_dataset\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import random\n",
    "\n",
    "def generate_datasets_from_preseparated(fits_limit_per_dir=10000, npy_limit_per_dir=10000):\n",
    "    \"\"\"Generates training and validation datasets from both FITS and npy files.\"\"\"\n",
    "    \n",
    "    # Directories for FITS and npy files\n",
    "    train_base_dir = os.path.join(os.getcwd(), \"lamost_train_set\")\n",
    "    val_base_dir = os.path.join(os.getcwd(), \"lamost_val_set\")\n",
    "    \n",
    "    npy_train_base_dir = os.path.join(os.getcwd(), \"gaia_training_set\")\n",
    "    npy_val_base_dir = os.path.join(os.getcwd(), \"gaia_validation_set\")\n",
    "\n",
    "    # Generate file lists for both FITS and npy files\n",
    "    train_fits_files, train_npy_files, train_labels = generate_file_list_from_directories([train_base_dir], [npy_train_base_dir], fits_limit_per_dir)\n",
    "    val_fits_files, val_npy_files, val_labels = generate_file_list_from_directories([val_base_dir], [npy_val_base_dir], npy_limit_per_dir)\n",
    "\n",
    "    # Create TensorFlow datasets for training and validation\n",
    "    train_dataset = create_dataset(train_fits_files, train_npy_files, train_labels)\n",
    "    val_dataset = create_dataset(val_fits_files, val_npy_files, val_labels)\n",
    "\n",
    "    return train_dataset, val_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_file_list_from_directories(base_dirs, npy_base_dirs, limit_per_dir=10000):\n",
    "    \"\"\"Generates a list of FITS files and corresponding npy files and their labels.\"\"\"\n",
    "    spectra_dirs = {\n",
    "        \"gal_data\": 0,  # Label 0 for galaxies\n",
    "        \"star_data\": 1,  # Label 1 for stars\n",
    "        \"agn_data\": 2,   # Label 2 for AGNs (update this if needed)\n",
    "        \"bin_data\": 3    # Label 3 for binary stars (update this if needed)\n",
    "    }\n",
    "\n",
    "    fits_file_list = []\n",
    "    npy_file_list = []\n",
    "    labels = []\n",
    "\n",
    "    print(\"Gathering FITS and npy files from pre-separated directories...\")\n",
    "    for dir_name, label in spectra_dirs.items():\n",
    "        for base_dir, npy_base_dir in zip(base_dirs, npy_base_dirs):\n",
    "            # FITS file paths\n",
    "            fits_dir_path = os.path.join(base_dir, dir_name)\n",
    "            fits_dir_files = []\n",
    "            # npy file paths\n",
    "            npy_dir_path = os.path.join(npy_base_dir, dir_name)\n",
    "            npy_dir_files = []\n",
    "\n",
    "            # Collect all FITS files in the directory\n",
    "            for root, dirs, files in os.walk(fits_dir_path):\n",
    "                for file in files:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    fits_dir_files.append(file_path)\n",
    "\n",
    "            # Collect all npy files in the directory\n",
    "            for root, dirs, files in os.walk(npy_dir_path):\n",
    "                for file in files:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    npy_dir_files.append(file_path)\n",
    "\n",
    "            # Ensure that both FITS and npy files are paired\n",
    "            fits_dir_files.sort()  # Sorting ensures that corresponding files match\n",
    "            npy_dir_files.sort()\n",
    "\n",
    "            print(f\"Found {len(fits_dir_files)} FITS files and {len(npy_dir_files)} npy files for {dir_name} in {base_dir}\")\n",
    "\n",
    "            # Randomly select files up to the limit\n",
    "            if len(fits_dir_files) > limit_per_dir:\n",
    "                selected_fits_files = random.sample(fits_dir_files, limit_per_dir)\n",
    "                selected_npy_files = random.sample(npy_dir_files, limit_per_dir)\n",
    "            else:\n",
    "                selected_fits_files = fits_dir_files\n",
    "                selected_npy_files = npy_dir_files\n",
    "\n",
    "            # Append selected FITS and npy files and their labels\n",
    "            fits_file_list.extend(selected_fits_files)\n",
    "            npy_file_list.extend(selected_npy_files)\n",
    "            labels.extend([label] * len(selected_fits_files))\n",
    "\n",
    "    print(f\"Total spectra files collected: {len(fits_file_list)}\")\n",
    "    return fits_file_list, npy_file_list, labels\n",
    "def tf_load_single_spectrum(file_path, target_length=3748):\n",
    "    \"\"\"TensorFlow wrapper for loading a single spectrum using py_function.\"\"\"\n",
    "    spectra = tf.py_function(load_single_spectrum, [file_path, target_length], tf.float32)\n",
    "    spectra.set_shape([target_length])  # Set the shape explicitly for TensorFlow to optimize\n",
    "    return spectra\n",
    "def load_single_spectrum(file_path, target_length=3748):\n",
    "    \"\"\"Load and normalize a single spectrum from a FITS file, truncating or padding to target_length.\"\"\"\n",
    "    try:\n",
    "        with fits.open(file_path) as hdul:\n",
    "            spectra = hdul[0].data[0]\n",
    "            spectra = normalize_spectra(spectra)\n",
    "            \n",
    "            # Truncate or pad spectra to ensure uniform length\n",
    "            if len(spectra) > target_length:\n",
    "                spectra = spectra[:target_length]  # Truncate\n",
    "            else:\n",
    "                spectra = np.pad(spectra, (0, max(0, target_length - len(spectra))), mode='constant')  # Pad with zeros\n",
    "            \n",
    "            return spectra\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return None  # Return None if there's an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gathering FITS and npy files from pre-separated directories...\n",
      "Found 1699 FITS files and 1699 npy files for gal_data in /home/jcwind/Star Classifier/Star-Classifier/lamost_train_set\n",
      "Found 86037 FITS files and 86037 npy files for star_data in /home/jcwind/Star Classifier/Star-Classifier/lamost_train_set\n",
      "Found 35936 FITS files and 35936 npy files for agn_data in /home/jcwind/Star Classifier/Star-Classifier/lamost_train_set\n",
      "Found 40676 FITS files and 40676 npy files for bin_data in /home/jcwind/Star Classifier/Star-Classifier/lamost_train_set\n",
      "Total spectra files collected: 31699\n",
      "Gathering FITS and npy files from pre-separated directories...\n",
      "Found 400 FITS files and 400 npy files for gal_data in /home/jcwind/Star Classifier/Star-Classifier/lamost_val_set\n",
      "Found 400 FITS files and 400 npy files for star_data in /home/jcwind/Star Classifier/Star-Classifier/lamost_val_set\n",
      "Found 400 FITS files and 400 npy files for agn_data in /home/jcwind/Star Classifier/Star-Classifier/lamost_val_set\n",
      "Found 400 FITS files and 400 npy files for bin_data in /home/jcwind/Star Classifier/Star-Classifier/lamost_val_set\n",
      "Total spectra files collected: 1600\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'filters_20' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m train_dataset, val_dataset \u001b[38;5;241m=\u001b[39m generate_datasets_from_preseparated()\n\u001b[0;32m----> 2\u001b[0m model_20 \u001b[38;5;241m=\u001b[39m fusion_convnet(input_shape1\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3748\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m1\u001b[39m), input_shape2\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m1\u001b[39m), num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, num_filters\u001b[38;5;241m=\u001b[39m\u001b[43mfilters_20\u001b[49m, kernel_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m20\u001b[39m,), dense_units1\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2048\u001b[39m, dense_units2\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, dense_units3\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, dense_units4\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, dropout_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)\n\u001b[1;32m      3\u001b[0m histories \u001b[38;5;241m=\u001b[39m train_convnet_many_times(model_20, val_dataset, epochs_per_run\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, num_runs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'filters_20' is not defined"
     ]
    }
   ],
   "source": [
    "train_dataset, val_dataset = generate_datasets_from_preseparated()\n",
    "model_20 = fusion_convnet(input_shape1=(3748-10, 1), input_shape2=(10, 1), num_classes=4, num_filters=filters_20, kernel_size=(20,), dense_units1=2048, dense_units2=512, dense_units3=128, dense_units4=64, dropout_rate=0.2)\n",
    "histories = train_convnet_many_times(model_20, val_dataset, epochs_per_run=1, batch_size=32, num_runs=20)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
