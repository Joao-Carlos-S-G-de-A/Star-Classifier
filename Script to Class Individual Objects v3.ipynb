{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import pyvo as vo\n",
    "import torch\n",
    "from astroquery.gaia import Gaia\n",
    "from astropy.coordinates import SkyCoord\n",
    "import astropy.units as u\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "from tqdm import tqdm\n",
    "from astropy.io import fits\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.preprocessing import MinMaxScaler, PowerTransformer\n",
    "import torch.nn as nn\n",
    "from mambapy.mamba import Mamba, MambaConfig\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import seaborn as sns\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_star_labels(gaia_ids, model_path, lamost_catalogue, gaia_transformer_path):\n",
    "    \"\"\"\n",
    "    Given a list of Gaia DR3 IDs, this function:\n",
    "    1) Queries Gaia for star parameters.\n",
    "    2) Cross-matches with LAMOST spectra.\n",
    "    3) Downloads and processes LAMOST spectra.\n",
    "    4) Normalizes both Gaia and LAMOST data.\n",
    "    5) Applies a trained StarClassifierFusion model to predict labels.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with Gaia IDs and predicted multi-label classifications.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\nüöÄ Step 1: Querying Gaia data...\")\n",
    "    df_gaia = query_gaia_data(gaia_ids)\n",
    "    if df_gaia.empty:\n",
    "        print(\"‚ö†Ô∏è No Gaia data found. Exiting.\")\n",
    "        return None\n",
    "\n",
    "    print(\"\\nüîÑ Step 2: Cross-matching with LAMOST catalog...\")\n",
    "    df_matched = crossmatch_lamost(df_gaia, lamost_catalogue)\n",
    "    if df_matched.empty:\n",
    "        print(\"‚ö†Ô∏è No LAMOST matches found. Exiting.\")\n",
    "        return None\n",
    "\n",
    "    print(\"\\nüì• Step 3: Downloading LAMOST spectra (if needed)...\")\n",
    "    obsids = df_matched[\"obsid\"].unique()\n",
    "    spectra_folder = \"lamost_spectra_uniques\"\n",
    "    download_lamost_spectra(obsids, save_folder=spectra_folder, num_workers=50)\n",
    "\n",
    "    print(\"\\nüîß Step 4: Converting from FITS LAMOST spectra...\")\n",
    "    process_lamost_fits_files(folder_path=spectra_folder, output_file=\"Pickles/lamost_data.csv\")\n",
    "\n",
    "    print(\"\\nüìä Step 5: Extracting and saving flux & frequency values...\")\n",
    "    extract_flux_frequency_from_csv(csv_path=\"Pickles/lamost_data.csv\")\n",
    "\n",
    "    print(\"\\nüìä Step 6: Interpolating and normalizing LAMOST spectra...\")\n",
    "    nan_files = interpolate_spectrum(\"Pickles/flux_values.pkl\", \"Pickles/freq_values.pkl\", \"Pickles/lamost_data_interpolated.pkl\")\n",
    "    spectrum_interpolated = pd.read_pickle(\"Pickles/lamost_data_interpolated.pkl\")\n",
    "    spectrum_normalized = normalize_lamost_spectra(spectrum_interpolated)\n",
    "\n",
    "    if spectrum_normalized.empty:\n",
    "        print(\"‚ö†Ô∏è No processed LAMOST spectra found. Exiting.\")\n",
    "        return None\n",
    "\n",
    "    print(\"\\nüìä Step 7: Normalizing Gaia data...\")\n",
    "    with open(gaia_transformer_path, \"rb\") as f:\n",
    "        gaia_transformers = pickle.load(f)   # Dict of {col_name: fitted PowerTransformer}\n",
    "    gaia_normalized = apply_gaia_transforms(df_gaia, gaia_transformers)\n",
    "\n",
    "    print(\"\\nüîó Step 8: Merging Gaia and LAMOST data...\")\n",
    "    gaia_lamost_match = df_matched[[\"source_id\", \"obsid\"]]\n",
    "    spectrum_normalized[\"source_id\"] = spectrum_normalized[\"obsid\"].astype(int).map(gaia_lamost_match.set_index(\"obsid\")[\"source_id\"])\n",
    "    gaia_lamost_merged = pd.merge(gaia_normalized, spectrum_normalized, on=\"source_id\", how=\"inner\")\n",
    "\n",
    "    if gaia_lamost_merged.empty:\n",
    "        print(\"‚ö†Ô∏è No valid data after merging. Exiting.\")\n",
    "        return None\n",
    "\n",
    "    print(\"\\nü§ñ Step 9: Predicting labels using the trained model...\")\n",
    "    predictions = process_star_data_fusion(model_path, gaia_lamost_merged, \"Pickles/Updated_list_of_Classes.pkl\", sigmoid_constant=0.5)\n",
    "\n",
    "    print(\"\\nüíæ Step 10: Saving predictions...\")\n",
    "    df_predictions = pd.DataFrame(predictions, columns=pd.read_pickle(\"Pickles/Updated_list_of_Classes.pkl\"))\n",
    "    df_predictions[\"source_id\"] = gaia_lamost_merged[\"source_id\"].values\n",
    "\n",
    "    return df_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_gaia_data(gaia_id_list):\n",
    "    \"\"\"\n",
    "    Given a list of Gaia DR3 source IDs, queries the Gaia archive\n",
    "    for the relevant columns used during training.\n",
    "    Returns a concatenated DataFrame of results.\n",
    "    \"\"\"\n",
    "    # Columns you actually need (adapt to match your pipeline!)\n",
    "    # e.g. ra, dec, pmra, pmdec, phot_g_mean_flux, ...\n",
    "    desired_cols = [\n",
    "        \"source_id\", \"ra\", \"ra_error\", \"dec\", \"dec_error\",\n",
    "        \"pmra\", \"pmra_error\", \"pmdec\", \"pmdec_error\",\n",
    "        \"parallax\", \"parallax_error\",\n",
    "        \"phot_g_mean_flux\", \"phot_g_mean_flux_error\",\n",
    "        \"phot_bp_mean_flux\", \"phot_bp_mean_flux_error\",\n",
    "        \"phot_rp_mean_flux\", \"phot_rp_mean_flux_error\"\n",
    "    ]\n",
    "\n",
    "    all_dfs = []\n",
    "    chunks = split_ids_into_chunks(gaia_id_list, chunk_size=30000)\n",
    "    for chunk in chunks:\n",
    "        query = f\"\"\"\n",
    "        SELECT {', '.join(desired_cols)}\n",
    "        FROM gaiadr3.gaia_source\n",
    "        WHERE source_id IN ({chunk})\n",
    "        \"\"\"\n",
    "        job = Gaia.launch_job_async(query)\n",
    "        tbl = job.get_results()\n",
    "        df_tmp = tbl.to_pandas()\n",
    "        all_dfs.append(df_tmp)\n",
    "\n",
    "    # Print a warning if some IDs were not found\n",
    "    all_ids = pd.concat(all_dfs)[\"source_id\"].values\n",
    "    missing_ids = set(gaia_id_list) - set(all_ids)\n",
    "    if missing_ids:\n",
    "        print(f\"Warning: {len(missing_ids)} IDs not found in Gaia DR3.\")\n",
    "        print(f\"Missing IDs: {missing_ids}\")\n",
    "\n",
    "    if not all_dfs:\n",
    "        return pd.DataFrame(columns=desired_cols)\n",
    "    else:\n",
    "        return pd.concat(all_dfs, ignore_index=True)\n",
    "    \n",
    "def split_ids_into_chunks(gaia_id_list, chunk_size=50000):\n",
    "    \"\"\"\n",
    "    Takes a Python list of Gaia IDs (strings or ints),\n",
    "    returns a list of comma-joined strings, each containing up to `chunk_size` IDs.\n",
    "    \"\"\"\n",
    "    # Convert everything to string for the SQL query\n",
    "    gaia_id_list = [str(x) for x in gaia_id_list]\n",
    "    chunks = []\n",
    "    for i in range(0, len(gaia_id_list), chunk_size):\n",
    "        chunk = \", \".join(gaia_id_list[i:i+chunk_size])\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "def crossmatch_lamost(gaia_df, lamost_df, match_radius=3*u.arcsec):\n",
    "    \"\"\"\n",
    "    Cross-matches Gaia sources with a local LAMOST catalogue.\n",
    "    Returns a merged DataFrame of matched objects.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure RA/Dec are numeric\n",
    "    gaia_df['ra'] = pd.to_numeric(gaia_df['ra'], errors='coerce')\n",
    "    gaia_df['dec'] = pd.to_numeric(gaia_df['dec'], errors='coerce')\n",
    "    lamost_df['ra'] = pd.to_numeric(lamost_df['ra'], errors='coerce')\n",
    "    lamost_df['dec'] = pd.to_numeric(lamost_df['dec'], errors='coerce')\n",
    "\n",
    "    # Drop NaN values\n",
    "    gaia_df = gaia_df.dropna(subset=['ra', 'dec'])\n",
    "    lamost_df = lamost_df.dropna(subset=['ra', 'dec'])\n",
    "\n",
    "    print(f\"After NaN removal: Gaia={gaia_df.shape}, LAMOST={lamost_df.shape}\")\n",
    "\n",
    "    # Check if LAMOST coordinates are in arcseconds (convert if necessary)\n",
    "    if lamost_df['ra'].max() > 360:  # RA should not exceed 360 degrees\n",
    "        print(\"‚ö†Ô∏è LAMOST RA/Dec seem to be in arcseconds. Converting to degrees.\")\n",
    "        lamost_df['ra'] /= 3600\n",
    "        lamost_df['dec'] /= 3600\n",
    "\n",
    "    # Convert to SkyCoord objects (ensuring same frame)\n",
    "    gaia_coords = SkyCoord(ra=gaia_df['ra'].values*u.deg,\n",
    "                           dec=gaia_df['dec'].values*u.deg,\n",
    "                           frame='icrs')\n",
    "\n",
    "    lamost_coords = SkyCoord(ra=lamost_df['ra'].values*u.deg,\n",
    "                             dec=lamost_df['dec'].values*u.deg,\n",
    "                             frame='icrs')\n",
    "\n",
    "    # Perform crossmatch\n",
    "    idx, d2d, _ = gaia_coords.match_to_catalog_sky(lamost_coords)\n",
    "\n",
    "    # Apply matching radius filter\n",
    "    matches = d2d < match_radius\n",
    "    print(f\"Maximum match distance (arcsec): {matches.to(u.arcsec).max()}\")\n",
    "    print(f\"Mean match distance (arcsec): {matches.to(u.arcsec).mean()}\")\n",
    "\n",
    "    if matches.sum() == 0:\n",
    "        print(\"‚ö†Ô∏è No matches found! Try increasing `match_radius`.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Extract matched rows correctly\n",
    "    gaia_matched = gaia_df.iloc[matches].copy().reset_index(drop=True)\n",
    "    lamost_matched = lamost_df.iloc[idx[matches]].copy().reset_index(drop=True)\n",
    "\n",
    "    print(f\"Matched Gaia Objects: {gaia_matched.shape}\")\n",
    "    print(f\"Matched LAMOST Objects: {lamost_matched.shape}\")\n",
    "\n",
    "    # Merge matches into final DataFrame\n",
    "    final = pd.concat([gaia_matched, lamost_matched], axis=1)\n",
    "\n",
    "    return final\n",
    "\n",
    "def download_lamost_spectra(obsid_list, save_folder=\"star_spectra\", num_workers=10):\n",
    "    \"\"\"\n",
    "    Downloads LAMOST spectra by obsid in parallel.\n",
    "    \n",
    "    :param obsid_list: List of obsids to download\n",
    "    :param save_folder: Folder where spectra will be saved\n",
    "    :param num_workers: Number of parallel download threads\n",
    "    \"\"\"\n",
    "    if not os.path.exists(save_folder):\n",
    "        os.makedirs(save_folder)\n",
    "\n",
    "    # Create a requests Session with Retry to handle transient errors\n",
    "    retries = Retry(total=5, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])\n",
    "    session = requests.Session()\n",
    "    session.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
    "\n",
    "    # Use ThreadPoolExecutor to download in parallel\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        future_to_obsid = {\n",
    "            executor.submit(download_one_spectrum, obsid, session, save_folder): obsid \n",
    "            for obsid in obsid_list\n",
    "        }\n",
    "\n",
    "        # Wrap with tqdm for progress bar\n",
    "        for future in tqdm(as_completed(future_to_obsid), total=len(future_to_obsid), desc=\"Downloading Spectra\"):\n",
    "            obsid = future_to_obsid[future]\n",
    "            try:\n",
    "                obsid, success, error_msg = future.result()\n",
    "                results.append((obsid, success, error_msg))\n",
    "            except Exception as e:\n",
    "                results.append((obsid, False, str(e)))\n",
    "\n",
    "    # Print any failures\n",
    "    failures = [r for r in results if not r[1]]\n",
    "    if failures:\n",
    "        print(f\"‚ùå Failed to download {len(failures)} spectra:\")\n",
    "        for (obsid, _, err) in failures[:10]:  # show first 10 errors\n",
    "            print(f\"  obsid={obsid} => Error: {err}\")\n",
    "\n",
    "    # Return list of successfully downloaded obsids for reference\n",
    "    downloaded_obsids = [r[0] for r in results if r[1]]\n",
    "    return downloaded_obsids\n",
    "\n",
    "def download_one_spectrum(obsid, session, save_folder):\n",
    "    \"\"\"\n",
    "    Helper function to download one spectrum file given an obsid.\n",
    "    Uses the same session to get the file and saves it locally.\n",
    "    \"\"\"\n",
    "    url = f\"https://www.lamost.org/dr7/v2.0/spectrum/fits/{obsid}\"\n",
    "    local_path = os.path.join(save_folder, str(obsid))\n",
    "\n",
    "    # If already downloaded, skip\n",
    "    if os.path.exists(local_path):\n",
    "        return obsid, True, None\n",
    "\n",
    "    try:\n",
    "        resp = session.get(url, timeout=30)\n",
    "        resp.raise_for_status()\n",
    "        with open(local_path, \"wb\") as f:\n",
    "            f.write(resp.content)\n",
    "        return obsid, True, None\n",
    "    except Exception as e:\n",
    "        return obsid, False, str(e)\n",
    "    \n",
    "def process_lamost_fits_files(folder_path=\"lamost_spectra_uniques\", output_file=\"Pickles/lamost_data.csv\", batch_size=10000):\n",
    "    \"\"\"\n",
    "    Processes LAMOST FITS spectra by extracting flux and frequency data.\n",
    "    Saves data in a CSV file with batching to optimize memory usage.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing FITS files.\n",
    "        output_file (str): Path to the output CSV file.\n",
    "        batch_size (int): Number of records to process before writing to the CSV.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\nüìÇ Processing LAMOST FITS files...\")\n",
    "\n",
    "    # Create output folder if necessary and delete existing file\n",
    "    if not os.path.exists(os.path.dirname(output_file)):\n",
    "        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "    else:\n",
    "        if os.path.exists(output_file):\n",
    "            os.remove(output_file)\n",
    "            os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "\n",
    "    # Define column headers\n",
    "    columns = [f'col_{i}' for i in range(3748)] + ['file_name', 'row']\n",
    "\n",
    "    # Initialize the CSV file with headers\n",
    "    with open(output_file, 'w') as f:\n",
    "        pd.DataFrame(columns=columns).to_csv(f, index=False)\n",
    "        f.close()\n",
    "\n",
    "    # Count total files for progress tracking\n",
    "    total_files = sum([len(files) for _, _, files in os.walk(folder_path)])\n",
    "\n",
    "    batch_list = []\n",
    "\n",
    "    # Process FITS files\n",
    "    with tqdm(total=total_files, desc='Processing FITS files') as pbar:\n",
    "        for filename in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            \n",
    "            try:\n",
    "                with fits.open(file_path) as hdul:\n",
    "                    data = hdul[0].data[:3, :3748]  # Extract first 3 rows and 3748 columns\n",
    "                    \n",
    "                    for i, row_data in enumerate(data):\n",
    "                        data_dict = {f'col_{j}': value for j, value in enumerate(row_data)}\n",
    "                        data_dict['file_name'] = filename\n",
    "                        data_dict['row'] = i  # Track which row from the FITS file\n",
    "                        batch_list.append(data_dict)\n",
    "                \n",
    "                # Write batch to CSV\n",
    "                if len(batch_list) >= batch_size:\n",
    "                    pd.DataFrame(batch_list).to_csv(output_file, mode='a', header=False, index=False)\n",
    "                    batch_list.clear()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error processing {filename}: {e}\")\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "        # Write any remaining data\n",
    "        if batch_list:\n",
    "            pd.DataFrame(batch_list).to_csv(output_file, mode='a', header=False, index=False)\n",
    "\n",
    "\n",
    "def extract_flux_frequency_from_csv(csv_path=\"Pickles/lamost_data.csv\", flux_pickle=\"Pickles/flux_values.pkl\", freq_pickle=\"Pickles/freq_values.pkl\", chunk_size=10000):\n",
    "    \"\"\"\n",
    "    Extracts flux and frequency data from a CSV file and saves them as separate pickle files.\n",
    "\n",
    "    Args:\n",
    "        csv_path (str): Path to the CSV file containing processed spectra.\n",
    "        flux_pickle (str): Path to save extracted flux values.\n",
    "        freq_pickle (str): Path to save extracted frequency values.\n",
    "        chunk_size (int): Number of rows to process per chunk.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\nüìä Extracting flux and frequency values...\")\n",
    "\n",
    "    flux_values = pd.DataFrame()\n",
    "    freq_values = pd.DataFrame()\n",
    "\n",
    "    # Count total rows for progress tracking\n",
    "    total_rows = sum(1 for _ in open(csv_path)) - 1  # Subtract header row\n",
    "\n",
    "    for chunk in tqdm(pd.read_csv(csv_path, chunksize=chunk_size), total=total_rows // chunk_size):\n",
    "        flux_mask = chunk['row'] == 0  # Select only flux values\n",
    "        freq_mask = chunk['row'] == 2  # Select only frequency values\n",
    "\n",
    "        flux_values = pd.concat([flux_values, chunk[flux_mask].drop(columns=['row'])])\n",
    "        freq_values = pd.concat([freq_values, chunk[freq_mask].drop(columns=['row'])])\n",
    "\n",
    "    print(f\"‚úÖ Flux values shape: {flux_values.shape}, Frequency values shape: {freq_values.shape}\")\n",
    "\n",
    "    # Save extracted values\n",
    "    flux_values.to_pickle(flux_pickle)\n",
    "    freq_values.to_pickle(freq_pickle)\n",
    "\n",
    "    \n",
    "\n",
    "def normalize_lamost_spectra(spectra_df):\n",
    "    \"\"\"\n",
    "    Reads LAMOST FITS spectra, applies interpolation, normalization, and transformation.\n",
    "    Returns a DataFrame of final spectral features (one row per spectrum).\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    spectra = spectra_df.iloc[:, 100:-1].values  # Exclude the last column (file_name)\n",
    "\n",
    "    #print(f\"Shape of the spectra array: {spectra.shape}\")\n",
    "\n",
    "    # Normalize the spectra between 0 and 1\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    spectra_normalized = min_max_scaler.fit_transform(spectra.T).T\n",
    "\n",
    "    #print(f\"Shape of the normalized spectra array: {spectra_normalized.shape}\")\n",
    "\n",
    "    # Apply the Yeo-Johnson transformation to the spectra\n",
    "    pt = PowerTransformer(method='yeo-johnson', standardize=True)\n",
    "    spectra_transformed = pt.fit_transform(spectra_normalized.T).T\n",
    "\n",
    "    # Create a new DataFrame with the transformed spectra\n",
    "    df_transformed = pd.DataFrame(spectra_transformed, columns=spectra_df.columns[100:-1]) # Exclude the first 100+3 columns and the last column\n",
    "\n",
    "    #print(f\"Shape of the transformed spectra array: {spectra_transformed.shape}\")\n",
    "\n",
    "    # Add the file_name column back to the DataFrame\n",
    "    #print(f\"Available columns in spectra_df: {spectra_df.columns}\")\n",
    "    df_transformed['obsid'] = spectra_df['file_name']\n",
    "\n",
    "    return df_transformed\n",
    "\n",
    "def apply_gaia_transforms(gaia_df, transformers_dict):\n",
    "    \"\"\"\n",
    "    Applies the same Yeo-Johnson (or other) transformations used in training\n",
    "    to the relevant Gaia columns. \n",
    "    \"\"\"\n",
    "    # Fill the same NaN values or set the same flags as in training\n",
    "    # e.g. if you flagged parallax=NaN => set parallax=0, error=10\n",
    "    # do that here too, to keep consistent with your training pipeline\n",
    "    #\n",
    "    # Example based on your code:\n",
    "    gaia_df['flagnopllx'] = np.where(gaia_df['parallax'].isna(), 1, 0)\n",
    "    gaia_df['parallax']       = gaia_df['parallax'].fillna(0)\n",
    "    gaia_df['parallax_error'] = gaia_df['parallax_error'].fillna(10)\n",
    "    gaia_df['pmra']           = gaia_df['pmra'].fillna(0)\n",
    "    gaia_df['pmra_error']     = gaia_df['pmra_error'].fillna(10)\n",
    "    gaia_df['pmdec']          = gaia_df['pmdec'].fillna(0)\n",
    "    gaia_df['pmdec_error']    = gaia_df['pmdec_error'].fillna(10)\n",
    "\n",
    "    gaia_df['flagnoflux'] = 0\n",
    "    # If G or BP or RP is missing\n",
    "    missing_flux = gaia_df['phot_g_mean_flux'].isna() | gaia_df['phot_bp_mean_flux'].isna() \n",
    "    gaia_df.loc[missing_flux, 'flagnoflux'] = 1\n",
    "\n",
    "    # fill flux with 0 and error with large number\n",
    "    gaia_df['phot_g_mean_flux']       = gaia_df['phot_g_mean_flux'].fillna(0)\n",
    "    gaia_df['phot_g_mean_flux_error'] = gaia_df['phot_g_mean_flux_error'].fillna(50000)\n",
    "    gaia_df['phot_bp_mean_flux']      = gaia_df['phot_bp_mean_flux'].fillna(0)\n",
    "    gaia_df['phot_bp_mean_flux_error']= gaia_df['phot_bp_mean_flux_error'].fillna(50000)\n",
    "    gaia_df['phot_rp_mean_flux']      = gaia_df['phot_rp_mean_flux'].fillna(0)\n",
    "    gaia_df['phot_rp_mean_flux_error']= gaia_df['phot_rp_mean_flux_error'].fillna(50000)\n",
    "\n",
    "    # Drop any rows that are incomplete, if that was your final approach:\n",
    "    gaia_df.dropna(axis=0, inplace=True)\n",
    "    print(f\"Dropped {len(gaia_df) - len(gaia_df.dropna())} rows with NaN values.\")\n",
    "\n",
    "    # Remove source_id and other columns not to be transformed to be added back later\n",
    "    source_id = gaia_df['source_id']\n",
    "    gaia_df = gaia_df.drop(columns=[\"source_id\"])\n",
    "\n",
    "    # Now apply the stored transformations:\n",
    "    for col, transformer in transformers_dict.items():\n",
    "        if col in gaia_df.columns:\n",
    "            #print(f\"Transforming column: {col}\")\n",
    "            gaia_df[col] = transformer.transform(gaia_df[[col]])\n",
    "            #print(f\"Transformed column: {col}\")\n",
    "        else:\n",
    "            # If the column didn't exist, maybe set to 0 or skip?\n",
    "            print(f\"Warning: column {col} not found in new data, skipping transform.\")\n",
    "\n",
    "    # Add back the source_id column\n",
    "    gaia_df['source_id'] = source_id\n",
    "    return gaia_df\n",
    "\n",
    "def interpolate_spectrum(fluxes_loc, frequencies_loc, output_dir, limit=10, edge_limit=20):\n",
    "    \"\"\"Interpolates the flux values to fill in missing data points.\"\"\"\n",
    "    # Load the data from the pickle file    \n",
    "    df_freq = pd.read_pickle(frequencies_loc).reset_index(drop=True)      \n",
    "    df_flux = pd.read_pickle(fluxes_loc).reset_index(drop=True)  # Reset index for zero-based iteration\n",
    "\n",
    "    # Initialize an empty list to store the results before concatenating into a DataFrame\n",
    "    results_list = []\n",
    "\n",
    "    # Initialize lists to store problematic file_names\n",
    "    nan_files = []  \n",
    "\n",
    "    # Count the number of successful interpolations\n",
    "    cnt_success = 0\n",
    "\n",
    "    # Debugging counters\n",
    "    cnt_total_skipped = 0\n",
    "    cnt_nan_skipped = 0\n",
    "    cnt_zero_skipped = 0\n",
    "\n",
    "    # Overwrite the output file at the beginning\n",
    "    if os.path.exists(output_dir):\n",
    "        os.remove(output_dir)\n",
    "\n",
    "    # Loop through each row in the DataFrame (each row is a spectrum) with tqdm for progress bar\n",
    "    for index, row in tqdm(df_flux.iterrows(), total=len(df_flux), desc='Interpolating spectra'):\n",
    "\n",
    "        # Extract the fluxes (assuming they start at column 0 and continue to the last column)\n",
    "        fluxes = row[:-2].values  # Exclude the last columns (file_name, label)\n",
    "\n",
    "        # Extract the frequencies\n",
    "        frequencies = df_freq.iloc[int(index), :-2].values  # Exclude the last columns (file_name, label)\n",
    "\n",
    "        # Count the number of NaN and 0 values in the fluxes and frequencies\n",
    "        fluxes = pd.to_numeric(row[:-2], errors='coerce').values  # Exclude and convert to numeric\n",
    "        frequencies = pd.to_numeric(df_freq.iloc[index, :-2], errors='coerce').values  # Same for frequencies\n",
    "        num_nan = np.isnan(fluxes).sum() + np.isnan(frequencies).sum()  # Count NaN values\n",
    "        num_zero = (fluxes == 0).sum() + (frequencies == 0).sum()  # Count zero values\n",
    "        num_freq_nan = np.isnan(frequencies).sum() + (frequencies == 0).sum()\n",
    "        if num_freq_nan > 0:\n",
    "            print(f\"Number of NaN or zero frequency values: {num_freq_nan}\")\n",
    "        # Special handling for NaN values, counting nans in sequence, except for the first and last 10\n",
    "        if num_nan > limit and index > edge_limit and index < len(fluxes)-edge_limit:\n",
    "            cnt_nan_skipped += 1  # Debug: count NaN-skipped rows\n",
    "            nan_files.append(row['file_name'])\n",
    "            continue\n",
    "        \n",
    "        if num_zero > limit and index > edge_limit and index < len(fluxes)-edge_limit:\n",
    "            cnt_zero_skipped += 1  # Debug: count zero-skipped rows\n",
    "            nan_files.append(row['file_name'])\n",
    "            continue\n",
    "\n",
    "        # Deal with NaN values\n",
    "        fluxes = fluxes[~np.isnan(fluxes)]\n",
    "        frequencies = frequencies[~np.isnan(fluxes)]\n",
    "\n",
    "        # Interpolate to fill in missing values\n",
    "        f = interp1d(frequencies, fluxes, kind='linear', fill_value=\"extrapolate\")\n",
    "        new_frequencies = np.linspace(frequencies.min(), frequencies.max(), len(row[:-2].values))\n",
    "\n",
    "        # Interpolated flux values\n",
    "        interpolated_fluxes = f(new_frequencies)\n",
    "\n",
    "        # Store the interpolated data along with labels and other metadata\n",
    "        # Create a dictionary for the interpolated spectrum\n",
    "        interpolated_data = {f'flux_{i}': value for i, value in enumerate(interpolated_fluxes)}\n",
    "\n",
    "        # Add the original metadata back (e.g., file_name, label, row)\n",
    "        interpolated_data['file_name'] = row['file_name']\n",
    "                \n",
    "        # Append the interpolated data to the results list\n",
    "        results_list.append(interpolated_data)\n",
    "\n",
    "        if index % 2000 == 0:  # Save every 5000 rows\n",
    "            if os.path.exists(output_dir):\n",
    "                existing_df = pd.read_pickle(output_dir)  # Load existing data\n",
    "                new_df = pd.DataFrame(results_list)\n",
    "                # Concatenate existing and new data\n",
    "                combined_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "                combined_df.to_pickle(output_dir)  # Save combined DataFrame\n",
    "            else:\n",
    "                # If the file doesn't exist, create a new DataFrame and save\n",
    "                pd.DataFrame(results_list).to_pickle(output_dir)\n",
    "            cnt_success += len(results_list)  # Increment the count of successful interpolations\n",
    "            results_list = []  # Clear list to free memory\n",
    "\n",
    "    print(f\"Initial number of rows: {len(df_flux)}\")\n",
    "\n",
    "    # After the loop, save any remaining results\n",
    "    if results_list:\n",
    "        if os.path.exists(output_dir):\n",
    "            existing_df = pd.read_pickle(output_dir)\n",
    "            new_df = pd.DataFrame(results_list)\n",
    "            combined_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "            combined_df.to_pickle(output_dir)\n",
    "        else:\n",
    "            pd.DataFrame(results_list).to_pickle(output_dir)\n",
    "        cnt_success += len(results_list)\n",
    "\n",
    "    # Debugging information\n",
    "    cnt_total_skipped = len(nan_files)\n",
    "    print(f\"Total successful interpolations: {cnt_success}\")\n",
    "    #print(f\"Total skipped due to NaNs: {cnt_nan_skipped}\")\n",
    "    #print(f\"Total skipped due to zeros: {cnt_zero_skipped}\")\n",
    "    print(f\"Total skipped rows (NaNs + zeros): {cnt_total_skipped}\")\n",
    "    print(f\"Final check: len(df_flux) == cnt_success + len(nan_files)? {len(df_flux) == cnt_success + cnt_total_skipped}\")\n",
    "\n",
    "    return nan_files\n",
    "\n",
    "def process_star_data_fusion(\n",
    "    model_path, \n",
    "    X, \n",
    "    classes_path, \n",
    "    d_model_spectra=1024, \n",
    "    d_model_gaia=1024, \n",
    "    num_classes=55, \n",
    "    input_dim_spectra=3647, \n",
    "    input_dim_gaia=18, \n",
    "    depth=12, \n",
    "    sigmoid_constant=0.5,\n",
    "    class_to_plot=\"AllStars***lamost\"\n",
    "):\n",
    "    \"\"\"Processes star data using the fused StarClassifierFusion model.\"\"\"\n",
    "\n",
    "    # Load the data\n",
    "    classes = pd.read_pickle(classes_path)\n",
    "\n",
    "    # Load the trained fusion model\n",
    "    model = StarClassifierFusion(\n",
    "        d_model_spectra=d_model_spectra,\n",
    "        d_model_gaia=d_model_gaia,\n",
    "        num_classes=num_classes,\n",
    "        input_dim_spectra=input_dim_spectra,\n",
    "        input_dim_gaia=input_dim_gaia,\n",
    "        n_layers=depth,\n",
    "        use_cross_attention=True,  # Change to False for late fusion\n",
    "        n_cross_attn_heads=8\n",
    "    )\n",
    "\n",
    "    # Load the state dictionary\n",
    "    state_dict = torch.load(model_path)\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "    # Get multi-hot encoded labels\n",
    "    #y = X[classes]\n",
    "\n",
    "    # Define Gaia columns\n",
    "    gaia_columns = [\n",
    "        \"parallax\", \"ra\", \"dec\", \"ra_error\", \"dec_error\", \"parallax_error\", \"pmra\", \"pmdec\",\n",
    "        \"pmra_error\", \"pmdec_error\", \"phot_g_mean_flux\", \"flagnopllx\", \"phot_g_mean_flux_error\",\n",
    "        \"phot_bp_mean_flux\", \"phot_rp_mean_flux\", \"phot_bp_mean_flux_error\", \"phot_rp_mean_flux_error\",\n",
    "        \"flagnoflux\"\n",
    "    ]\n",
    "\n",
    "    # Separate Gaia and Spectra features\n",
    "    X_spectra = X.drop(columns={\"obsid\",\"source_id\", *gaia_columns})\n",
    "    X_gaia = X[gaia_columns]\n",
    "\n",
    "    print(f\"X_spectra shape: {X_spectra.shape}\")\n",
    "    print(f\"X_gaia shape: {X_gaia.shape}\")\n",
    "    #print(f\"y shape: {y.shape}\")\n",
    "\n",
    "    if class_to_plot != \"AllStars***lamost\":\n",
    "        # Filter for a specific class\n",
    "        X_spectra = X_spectra[y[class_to_plot] == 1]\n",
    "        X_gaia = X_gaia[y[class_to_plot] == 1]\n",
    "        #y = y[y[class_to_plot] == 1]\n",
    "\n",
    "        print(f\"X_spectra shape after filtering for {class_to_plot}: {X_spectra.shape}\")\n",
    "        print(f\"X_gaia shape after filtering for {class_to_plot}: {X_gaia.shape}\")\n",
    "       # print(f\"y shape after filtering for {class_to_plot}: {y.shape}\")\n",
    "\n",
    "    # Drop label columns from spectra\n",
    "    #X_spectra.drop(classes, axis=1, inplace=True)\n",
    "\n",
    "    # Convert to tensors\n",
    "    X_spectra = torch.tensor(X_spectra.values, dtype=torch.float32)\n",
    "    X_gaia = torch.tensor(X_gaia.values, dtype=torch.float32)\n",
    "    #y = torch.tensor(y.values, dtype=torch.float32)\n",
    "\n",
    "    # Create DataLoader\n",
    "    class MultiModalDataset(Dataset):\n",
    "        def __init__(self, X_spectra, X_gaia):\n",
    "            self.X_spectra = X_spectra\n",
    "            self.X_gaia = X_gaia\n",
    "            #self.y = y\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.X_spectra)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return self.X_spectra[idx], self.X_gaia[idx]\n",
    "\n",
    "    dataset = MultiModalDataset(X_spectra, X_gaia)\n",
    "    loader = DataLoader(dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "    # Move model to device\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_predicted = []\n",
    "    all_y = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_spc, X_ga in loader:\n",
    "            # Move data to device\n",
    "            X_spc, X_ga = X_spc.to(device), X_ga.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(X_spc, X_ga)\n",
    "            predicted = (torch.sigmoid(outputs) > sigmoid_constant).float()\n",
    "\n",
    "            # Store predictions and labels\n",
    "            all_predicted.append(predicted.cpu().numpy())\n",
    "            #all_y.append(y_batch.cpu().numpy())\n",
    "\n",
    "            # Free GPU memory\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # Concatenate all predictions and labels\n",
    "    #y_cpu = np.concatenate(all_y, axis=0)\n",
    "    predicted_cpu = np.concatenate(all_predicted, axis=0)\n",
    "\n",
    "    return predicted_cpu\n",
    "\n",
    "class StarClassifierFusion(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model_spectra,\n",
    "        d_model_gaia,\n",
    "        input_dim_spectra,\n",
    "        input_dim_gaia,\n",
    "        num_classes=55,\n",
    "        n_layers=6,\n",
    "        use_cross_attention=False,\n",
    "        n_cross_attn_heads=8\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model_spectra (int): embedding dimension for the spectra MAMBA\n",
    "            d_model_gaia (int): embedding dimension for the gaia MAMBA\n",
    "            num_classes (int): multi-label classification\n",
    "            input_dim_spectra (int): # of features for spectra\n",
    "            input_dim_gaia (int): # of features for gaia\n",
    "            n_layers (int): depth for each MAMBA\n",
    "            use_cross_attention (bool): whether to use cross-attention\n",
    "            n_cross_attn_heads (int): number of heads for cross-attention\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # --- MAMBA for spectra ---\n",
    "        config_spectra = MambaConfig(\n",
    "            d_model=d_model_spectra,\n",
    "            d_state=64,\n",
    "            d_conv=4,\n",
    "            n_layers=n_layers,\n",
    "        )\n",
    "        self.mamba_spectra = Mamba(config_spectra)\n",
    "        self.input_proj_spectra = nn.Linear(input_dim_spectra, d_model_spectra)\n",
    "\n",
    "        # --- MAMBA for gaia ---\n",
    "        config_gaia = MambaConfig(\n",
    "            d_model=d_model_gaia,\n",
    "            d_state=64,\n",
    "            d_conv=4,\n",
    "            n_layers=n_layers\n",
    "        )\n",
    "        self.mamba_gaia = Mamba(config_gaia)\n",
    "        self.input_proj_gaia = nn.Linear(input_dim_gaia, d_model_gaia)\n",
    "\n",
    "        # --- Cross Attention (Optional) ---\n",
    "        self.use_cross_attention = use_cross_attention\n",
    "        if use_cross_attention:\n",
    "            # We'll do cross-attn in both directions or just one‚Äîhere is an example with 2 blocks\n",
    "            self.cross_attn_block_spectra = CrossAttentionBlock(d_model_spectra, n_heads=n_cross_attn_heads)\n",
    "            self.cross_attn_block_gaia = CrossAttentionBlock(d_model_gaia, n_heads=n_cross_attn_heads)\n",
    "\n",
    "        # --- Final Classifier ---\n",
    "        # If you do late fusion by concatenation, the dimension is d_model_spectra + d_model_gaia\n",
    "        # If you do average fusion, it is max(d_model_spectra, d_model_gaia) (or keep them separate).\n",
    "        fusion_dim = d_model_spectra + d_model_gaia\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(fusion_dim),\n",
    "            nn.Linear(fusion_dim, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x_spectra, x_gaia):\n",
    "        \"\"\"\n",
    "        x_spectra : (batch_size, input_dim_spectra) or potentially (batch_size, seq_len_spectra, input_dim_spectra)\n",
    "        x_gaia    : (batch_size, input_dim_gaia) or (batch_size, seq_len_gaia, input_dim_gaia)\n",
    "        \"\"\"\n",
    "        # For MAMBA, we expect shape: (B, seq_len, d_model). \n",
    "        # If your input is just (B, d_in), we turn it into (B, 1, d_in).\n",
    "        \n",
    "        # --- Project to d_model and add sequence dimension (seq_len=1) ---\n",
    "        x_spectra = self.input_proj_spectra(x_spectra)  # (B, d_model_spectra)\n",
    "        x_spectra = x_spectra.unsqueeze(1)             # (B, 1, d_model_spectra)\n",
    "\n",
    "        x_gaia = self.input_proj_gaia(x_gaia)          # (B, d_model_gaia)\n",
    "        x_gaia = x_gaia.unsqueeze(1)                   # (B, 1, d_model_gaia)\n",
    "\n",
    "        # --- MAMBA encoding (each modality separately) ---\n",
    "        x_spectra = self.mamba_spectra(x_spectra)  # (B, 1, d_model_spectra)\n",
    "        x_gaia = self.mamba_gaia(x_gaia)          # (B, 1, d_model_gaia)\n",
    "\n",
    "        # Optionally, use cross-attention to fuse the representations\n",
    "        if self.use_cross_attention:\n",
    "            # Cross-attention from spectra -> gaia\n",
    "            x_spectra_fused = self.cross_attn_block_spectra(x_spectra, x_gaia)\n",
    "            # Cross-attention from gaia -> spectra\n",
    "            x_gaia_fused = self.cross_attn_block_gaia(x_gaia, x_spectra)\n",
    "            \n",
    "            # Update x_spectra and x_gaia\n",
    "            x_spectra = x_spectra_fused\n",
    "            x_gaia = x_gaia_fused\n",
    "        \n",
    "        # --- Pool across sequence dimension (since our seq_len=1, just squeeze) ---\n",
    "        x_spectra = x_spectra.mean(dim=1)  # (B, d_model_spectra)\n",
    "        x_gaia = x_gaia.mean(dim=1)        # (B, d_model_gaia)\n",
    "\n",
    "        # --- Late Fusion by Concatenation ---\n",
    "        x_fused = torch.cat([x_spectra, x_gaia], dim=-1)  # (B, d_model_spectra + d_model_gaia)\n",
    "\n",
    "        # --- Final classification ---\n",
    "        logits = self.classifier(x_fused)  # (B, num_classes)\n",
    "        return logits\n",
    "class CrossAttentionBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple cross-attention block with a feed-forward sub-layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads=8):\n",
    "        super().__init__()\n",
    "        self.cross_attn = nn.MultiheadAttention(\n",
    "            embed_dim=d_model, \n",
    "            num_heads=n_heads, \n",
    "            batch_first=True\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * d_model, d_model)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x_q, x_kv):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x_q  : (batch_size, seq_len_q, d_model)\n",
    "            x_kv : (batch_size, seq_len_kv, d_model)\n",
    "        \"\"\"\n",
    "        # Cross-attention\n",
    "        attn_output, _ = self.cross_attn(query=x_q, key=x_kv, value=x_kv)\n",
    "        x = self.norm1(x_q + attn_output)\n",
    "\n",
    "        # Feed forward\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = self.norm2(x + ffn_out)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example usage with eclipsing binaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "‚úÖ Retrieved 2184477 eclipsing binary sources from Gaia DR3.\n",
      "         source_id\n",
      "0   46626164993792\n",
      "1  104487964717440\n",
      "2  138577120584320\n",
      "3  160219460292352\n",
      "4  203551385701760\n"
     ]
    }
   ],
   "source": [
    "# Define ADQL query to fetch source IDs of eclipsing binaries\n",
    "query = \"\"\"\n",
    "SELECT source_id\n",
    "FROM gaiadr3.vari_eclipsing_binary\n",
    "\"\"\"\n",
    "\n",
    "# Run the query asynchronously\n",
    "job = Gaia.launch_job_async(query)\n",
    "results = job.get_results()\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "gaia_ids = results.to_pandas()\n",
    "\n",
    "print(f\"‚úÖ Retrieved {len(gaia_ids)} eclipsing binary sources from Gaia DR3.\")\n",
    "print(gaia_ids.head())  # Display first few rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaia_ids = gaia_ids[\"source_id\"].values[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making lamost catalogue minimal (only needs to be done once)\n",
    "#lamost_catalogue = pd.read_csv(\"lamost/dr9_v2.0_LRS_catalogue.csv\")  # Load LAMOST catalog before passing it\n",
    "#lamost_catalogue = lamost_catalogue[[\"obsid\", \"ra\", \"dec\"]]\n",
    "#lamost_catalogue.to_csv(\"lamost/minimal.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the LAMOST catalog to cross-match with Gaia as csv\n",
    "lamost_catalogue = pd.read_csv(\"lamost/minimal.csv\")  # Load LAMOST catalog before passing it\n",
    "label_cols = pd.read_pickle(\"Pickles/Updated_list_of_Classes.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Step 1: Querying Gaia data...\n",
      "INFO: Query finished. [astroquery.utils.tap.core]\n",
      "\n",
      "üîÑ Step 2: Cross-matching with LAMOST catalog...\n",
      "After NaN removal: Gaia=(100, 17), LAMOST=(10809336, 3)\n",
      "Maximum match distance (arcsec): 400.93942087048276 arcsec\n",
      "Mean match distance (arcsec): 57.5302727075882 arcsec\n",
      "Matched Gaia Objects: (38, 17)\n",
      "Matched LAMOST Objects: (38, 3)\n",
      "\n",
      "üì• Step 3: Downloading LAMOST spectra (if needed)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Spectra: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 38/38 [00:34<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Failed to download 2 spectra:\n",
      "  obsid=845015157 => Error: HTTPSConnectionPool(host='www.lamost.org', port=443): Max retries exceeded with url: /dr7/v2.0/spectrum/fits/845015157 (Caused by ResponseError('too many 500 error responses'))\n",
      "  obsid=879312226 => Error: HTTPSConnectionPool(host='www.lamost.org', port=443): Max retries exceeded with url: /dr7/v2.0/spectrum/fits/879312226 (Caused by ResponseError('too many 500 error responses'))\n",
      "\n",
      "üîß Step 4: Converting from FITS LAMOST spectra...\n",
      "\n",
      "üìÇ Processing LAMOST FITS files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing FITS files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30996/30996 [09:43<00:00, 53.15it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Step 5: Extracting and saving flux & frequency values...\n",
      "\n",
      "üìä Extracting flux and frequency values...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "model_path = \"models/model_fusion_mamba_v2.pth\"\n",
    "gaia_transformers = \"transforms/gaia_transformers.pkl\"\n",
    "\n",
    "df_predictions = predict_star_labels(gaia_ids, model_path, lamost_catalogue, gaia_transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the predictions and class labels\n",
    "y_pred = np.load(\"example_eclipsing_bin_y_pred_cpu.npy\")\n",
    "classes = pd.read_pickle(\"Pickles/Updated_list_of_Classes.pkl\")\n",
    "\n",
    "# Generate the expected y_true for eclipsing binaries\n",
    "y_true = np.zeros_like(y_pred)\n",
    "y_true[:, -1] = 1  # \"EB*\" column (last column)\n",
    "y_true[:, 1] = 1   # \"**\" column (second column)\n",
    "\n",
    "# Compute precision, recall, and F1-score for each class\n",
    "precision = precision_score(y_true, y_pred, average=None)\n",
    "recall = recall_score(y_true, y_pred, average=None)\n",
    "f1 = f1_score(y_true, y_pred, average=None)\n",
    "\n",
    "# Create a DataFrame to store metrics per class\n",
    "metrics_df = pd.DataFrame({\n",
    "    \"Class\": classes,\n",
    "    \"Precision\": precision,\n",
    "    \"Recall\": recall,\n",
    "    \"F1 Score\": f1\n",
    "})\n",
    "\n",
    "# Identify incorrectly classified samples (False Positives and False Negatives)\n",
    "incorrect_predictions = (y_pred != y_true).any(axis=1)\n",
    "incorrect_gaia_ids = df_predictions.loc[incorrect_predictions, \"source_id\"]\n",
    "\n",
    "# Visualizing Precision, Recall, and F1 Score\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "axes[0].barh(metrics_df[\"Class\"], metrics_df[\"Precision\"], color=\"steelblue\")\n",
    "axes[0].set_xlabel(\"Precision\")\n",
    "axes[0].set_title(\"Precision per Class\")\n",
    "\n",
    "axes[1].barh(metrics_df[\"Class\"], metrics_df[\"Recall\"], color=\"darkorange\")\n",
    "axes[1].set_xlabel(\"Recall\")\n",
    "axes[1].set_title(\"Recall per Class\")\n",
    "\n",
    "axes[2].barh(metrics_df[\"Class\"], metrics_df[\"F1 Score\"], color=\"forestgreen\")\n",
    "axes[2].set_xlabel(\"F1 Score\")\n",
    "axes[2].set_title(\"F1 Score per Class\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display incorrectly classified Gaia IDs\n",
    "import ace_tools as tools\n",
    "tools.display_dataframe_to_user(name=\"Incorrectly Classified Gaia IDs\", dataframe=pd.DataFrame({\"source_id\": incorrect_gaia_ids}))\n",
    "\n",
    "# Display the performance metrics\n",
    "tools.display_dataframe_to_user(name=\"Performance Metrics\", dataframe=metrics_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
