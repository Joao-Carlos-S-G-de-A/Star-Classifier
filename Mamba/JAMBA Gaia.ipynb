{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor, nn\n",
    "from zeta import MambaBlock\n",
    "from zeta.nn import FeedForward\n",
    "from zeta import MultiQueryAttention\n",
    "from zeta.nn.modules.simple_rmsnorm import SimpleRMSNorm\n",
    "from jamba.moe import MoE\n",
    "from zeta.nn import OutputHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BalancedDataset(Dataset):\n",
    "    def __init__(self, X, y, limit_per_label=1600):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.limit_per_label = limit_per_label\n",
    "        self.classes = np.unique(y)\n",
    "        self.indices = self.balance_classes()\n",
    "\n",
    "    def balance_classes(self):\n",
    "        indices = []\n",
    "        for cls in self.classes:\n",
    "            cls_indices = np.where(self.y == cls)[0]\n",
    "            if len(cls_indices) > self.limit_per_label:\n",
    "                cls_indices = np.random.choice(cls_indices, self.limit_per_label, replace=False)\n",
    "            indices.extend(cls_indices)\n",
    "        np.random.shuffle(indices)\n",
    "        return indices\n",
    "\n",
    "    def re_sample(self):\n",
    "        self.indices = self.balance_classes()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        index = self.indices[idx]\n",
    "        return self.X[index], self.y[index]\n",
    "# Custom Dataset for validation with limit per class\n",
    "class BalancedValidationDataset(Dataset):\n",
    "    def __init__(self, X, y, limit_per_label=400):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.limit_per_label = limit_per_label\n",
    "        self.classes = np.unique(y)\n",
    "        self.indices = self.balance_classes()\n",
    "\n",
    "    def balance_classes(self):\n",
    "        indices = []\n",
    "        for cls in self.classes:\n",
    "            cls_indices = np.where(self.y == cls)[0]\n",
    "            if len(cls_indices) > self.limit_per_label:\n",
    "                cls_indices = np.random.choice(cls_indices, self.limit_per_label, replace=False)\n",
    "            indices.extend(cls_indices)\n",
    "        np.random.shuffle(indices)\n",
    "        return indices\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        index = self.indices[idx]\n",
    "        return self.X[index], self.y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_mamba(\n",
    "    model, train_loader, val_loader, test_loader, \n",
    "    num_epochs=500, lr=1e-4, max_patience=20, device='cuda'\n",
    "):\n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Define optimizer, scheduler, and loss function\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=int(max_patience / 3), verbose=True\n",
    "    )\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience = max_patience\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss, train_accuracy = 0.0, 0.0\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * X_batch.size(0)\n",
    "            train_accuracy += (outputs.argmax(dim=1) == y_batch).float().mean().item()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss, val_accuracy = 0.0, 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_val, y_val in val_loader:\n",
    "                X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "                outputs = model(X_val)\n",
    "                loss = criterion(outputs, y_val)\n",
    "\n",
    "                val_loss += loss.item() * X_val.size(0)\n",
    "                val_accuracy += (outputs.argmax(dim=1) == y_val).float().mean().item()\n",
    "\n",
    "        # Test phase and metric collection\n",
    "        test_loss, test_accuracy = 0.0, 0.0\n",
    "        y_true, y_pred = [], []\n",
    "        with torch.no_grad():\n",
    "            for X_test, y_test in test_loader:\n",
    "                X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "                outputs = model(X_test)\n",
    "                loss = criterion(outputs, y_test)\n",
    "\n",
    "                test_loss += loss.item() * X_test.size(0)\n",
    "                test_accuracy += (outputs.argmax(dim=1) == y_test).float().mean().item()\n",
    "                y_true.extend(y_test.cpu().numpy())\n",
    "                y_pred.extend(outputs.argmax(dim=1).cpu().numpy())\n",
    "\n",
    "        # Update scheduler\n",
    "        scheduler.step(val_loss / len(val_loader.dataset))\n",
    "\n",
    "        # Log metrics to WandB\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": train_loss / len(train_loader.dataset),\n",
    "            \"val_loss\": val_loss / len(val_loader.dataset),\n",
    "            \"train_accuracy\": train_accuracy / len(train_loader),\n",
    "            \"val_accuracy\": val_accuracy / len(val_loader),\n",
    "            \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "            \"test_loss\": test_loss / len(test_loader.dataset),\n",
    "            \"test_accuracy\": test_accuracy / len(test_loader),\n",
    "            \"confusion_matrix\": wandb.plot.confusion_matrix(\n",
    "                probs=None, y_true=y_true, preds=y_pred, class_names=np.unique(y_true)\n",
    "            ),\n",
    "            \"classification_report\": classification_report(\n",
    "                y_true, y_pred, target_names=[str(i) for i in range(len(np.unique(y_true)))]\n",
    "            )\n",
    "        })\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience = max_patience\n",
    "            best_model = model.state_dict()\n",
    "        else:\n",
    "            patience -= 1\n",
    "            if patience <= 0:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    # Load the best model weights\n",
    "    model.load_state_dict(best_model)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerMoEBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        heads: int,\n",
    "        num_experts: int,\n",
    "        num_experts_per_token: int,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes a TransformerMoEBlock.\n",
    "\n",
    "        Args:\n",
    "            dim (int): The dimension of the input tensor.\n",
    "            heads (int): The number of attention heads.\n",
    "            num_experts (int): The total number of experts.\n",
    "            num_experts_per_token (int): The number of experts per token.\n",
    "            *args: Variable length argument list.\n",
    "            **kwargs: Arbitrary keyword arguments.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.heads = heads\n",
    "        self.num_experts = num_experts\n",
    "        self.num_experts_per_tok = num_experts_per_token\n",
    "\n",
    "        self.attn = MultiQueryAttention(dim, heads)\n",
    "        self.moe = MoE(\n",
    "            dim,\n",
    "            num_experts=num_experts,\n",
    "            hidden_dim=dim * 4,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the TransformerMoEBlock.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor after applying the TransformerMoEBlock.\n",
    "        \"\"\"\n",
    "        skip = x\n",
    "        x = SimpleRMSNorm(self.dim)(x)\n",
    "        x, _, _ = self.attn(x) + x\n",
    "\n",
    "        x = SimpleRMSNorm(self.dim)(x)\n",
    "        moe_out, _ = self.moe(x)\n",
    "        x = moe_out + skip\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        heads: int,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes a TransformerBlock.\n",
    "\n",
    "        Args:\n",
    "            dim (int): Dimension of the input tensor.\n",
    "            heads (int): Number of attention heads.\n",
    "            num_experts (int): Number of experts.\n",
    "            num_experts_per_token (int): Number of experts per token.\n",
    "            *args: Variable length argument list.\n",
    "            **kwargs: Arbitrary keyword arguments.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.heads = heads\n",
    "\n",
    "        self.attn = MultiQueryAttention(dim, heads)\n",
    "        self.ffn = FeedForward(\n",
    "            dim,\n",
    "            dim,\n",
    "            4,\n",
    "            swish=True,\n",
    "            post_act_ln=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the TransformerBlock.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor after applying the TransformerBlock.\n",
    "        \"\"\"\n",
    "        skip = x\n",
    "        x = SimpleRMSNorm(self.dim)(x)\n",
    "        x, _, _ = self.attn(x)\n",
    "        x += skip\n",
    "\n",
    "        skip_two = x\n",
    "\n",
    "        x = SimpleRMSNorm(self.dim)(x)\n",
    "        x = self.ffn(x) + skip_two\n",
    "        return x\n",
    "\n",
    "\n",
    "class MambaMoELayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        d_state: int,\n",
    "        d_conv: int,\n",
    "        num_experts: int = 8,\n",
    "        num_experts_per_token: int = 2,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the MambaMoELayer.\n",
    "\n",
    "        Args:\n",
    "            dim (int): Dimension of the input tensor.\n",
    "            d_state (int): Dimension of the state tensor.\n",
    "            d_conv (int): Dimension of the convolutional tensor.\n",
    "            num_experts (int, optional): Number of experts. Defaults to 8.\n",
    "            num_experts_per_token (int, optional): Number of experts per token. Defaults to 2.\n",
    "            *args: Variable length argument list.\n",
    "            **kwargs: Arbitrary keyword arguments.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.d_state = d_state\n",
    "        self.d_conv = d_conv\n",
    "        self.num_experts = num_experts\n",
    "        self.num_experts_per_tok = num_experts_per_token\n",
    "\n",
    "        # Mamba\n",
    "        self.mamba = MambaBlock(\n",
    "            dim,\n",
    "            depth=1,\n",
    "            d_state=d_state,\n",
    "            d_conv=d_conv,\n",
    "        )\n",
    "\n",
    "        # MoE\n",
    "        self.moe = MoE(\n",
    "            dim,\n",
    "            num_experts=num_experts,\n",
    "            hidden_dim=dim * 4,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass of the MambaMoELayer.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor after applying the MambaMoELayer.\n",
    "        \"\"\"\n",
    "        skip = x\n",
    "\n",
    "        x = SimpleRMSNorm(self.dim)(x)\n",
    "        x = self.mamba(x) + x\n",
    "\n",
    "        x = SimpleRMSNorm(self.dim)(x)\n",
    "        moe_out, _ = self.moe(x)\n",
    "        x = moe_out + skip\n",
    "        return x\n",
    "\n",
    "\n",
    "class JambaBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    JambaBlock is a module that combines MambaBlock, MambaMoELayer, and TransformerBlock\n",
    "    to process input tensors.\n",
    "\n",
    "    Args:\n",
    "        dim (int): The input dimension.\n",
    "        d_state (int): The dimension of the state in MambaBlock and MambaMoELayer.\n",
    "        d_conv (int): The dimension of the convolutional output in MambaBlock and MambaMoELayer.\n",
    "        heads (int): The number of attention heads in TransformerBlock.\n",
    "        num_experts (int, optional): The number of experts in MambaMoELayer. Defaults to 8.\n",
    "        num_experts_per_token (int, optional): The number of experts per token in MambaMoELayer. Defaults to 2.\n",
    "\n",
    "    Attributes:\n",
    "        dim (int): The input dimension.\n",
    "        d_state (int): The dimension of the state in MambaBlock and MambaMoELayer.\n",
    "        d_conv (int): The dimension of the convolutional output in MambaBlock and MambaMoELayer.\n",
    "        heads (int): The number of attention heads in TransformerBlock.\n",
    "        num_experts (int): The number of experts in MambaMoELayer.\n",
    "        num_experts_per_tok (int): The number of experts per token in MambaMoELayer.\n",
    "        mamba_layer (MambaBlock): The MambaBlock layer.\n",
    "        mamba_moe_layer (MambaMoELayer): The MambaMoELayer layer.\n",
    "        transformer (TransformerBlock): The TransformerBlock layer.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        d_state: int,\n",
    "        d_conv: int,\n",
    "        heads: int,\n",
    "        num_experts: int = 8,\n",
    "        num_experts_per_token: int = 2,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.d_state = d_state\n",
    "        self.d_conv = d_conv\n",
    "        self.heads = heads\n",
    "        self.num_experts = num_experts\n",
    "        self.num_experts_per_tok = num_experts_per_token\n",
    "\n",
    "        # Mamba\n",
    "        self.mamba_layer = MambaBlock(\n",
    "            dim,\n",
    "            depth=1,\n",
    "            d_state=d_state,\n",
    "            d_conv=d_conv,\n",
    "        )\n",
    "\n",
    "        # Mamba MoE layer\n",
    "        self.mamba_moe_layer = MambaMoELayer(\n",
    "            dim,\n",
    "            d_state,\n",
    "            d_conv,\n",
    "            num_experts,\n",
    "            num_experts_per_token,\n",
    "        )\n",
    "\n",
    "        # Transformer\n",
    "        self.transformer = TransformerBlock(\n",
    "            dim,\n",
    "            heads,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.mamba_layer(x)\n",
    "        x = self.mamba_moe_layer(x)\n",
    "        x = self.transformer(x)\n",
    "        x = self.mamba_moe_layer(x)\n",
    "        x = self.mamba_layer(x)\n",
    "        x = self.mamba_moe_layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Jamba(nn.Module):\n",
    "    \"\"\"\n",
    "    Jamba model implementation.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Dimension of the model.\n",
    "        depth (int): Depth of the model.\n",
    "        num_tokens (int): Number of tokens.\n",
    "        max_seq_len (int): Maximum sequence length.\n",
    "        d_state (int): State dimension.\n",
    "        d_conv (int): Convolutional dimension.\n",
    "        heads (int): Number of attention heads.\n",
    "        num_experts (int, optional): Number of experts. Defaults to 8.\n",
    "        num_experts_per_token (int, optional): Number of experts per token. Defaults to 2.\n",
    "        pre_emb_norm (bool, optional): Whether to normalize the embeddings. Defaults to False.\n",
    "        return_embeddings (bool, optional): Whether to return the embeddings. Defaults to False.\n",
    "\n",
    "    Attributes:\n",
    "        dim (int): Dimension of the model.\n",
    "        depth (int): Depth of the model.\n",
    "        d_state (int): State dimension.\n",
    "        d_conv (int): Convolutional dimension.\n",
    "        heads (int): Number of attention heads.\n",
    "        num_experts (int): Number of experts.\n",
    "        num_experts_per_tok (int): Number of experts per token.\n",
    "        pre_emb_norm (bool): Whether to normalize the embeddings.\n",
    "        return_embeddings (bool): Whether to return the embeddings.\n",
    "        layers (nn.ModuleList): List of JambaBlock layers.\n",
    "        embed (nn.Embedding): Embedding layer.\n",
    "        norm (nn.LayerNorm or nn.Identity): Normalization layer.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        depth: int,\n",
    "        num_tokens: int,\n",
    "        d_state: int,\n",
    "        d_conv: int,\n",
    "        heads: int,\n",
    "        num_experts: int = 8,\n",
    "        num_experts_per_token: int = 2,\n",
    "        pre_emb_norm: bool = False,\n",
    "        return_embeddings: bool = False,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.depth = depth\n",
    "        self.d_state = d_state\n",
    "        self.d_conv = d_conv\n",
    "        self.heads = heads\n",
    "        self.num_experts = num_experts\n",
    "        self.num_experts_per_tok = num_experts_per_token\n",
    "        self.pre_emb_norm = pre_emb_norm\n",
    "        self.return_embeddings = return_embeddings\n",
    "\n",
    "        # Layers\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                JambaBlock(\n",
    "                    dim,\n",
    "                    d_state,\n",
    "                    d_conv,\n",
    "                    heads,\n",
    "                    num_experts,\n",
    "                    num_experts_per_token,\n",
    "                )\n",
    "                for _ in range(depth)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Pre Emb\n",
    "        self.embed = nn.Embedding(num_tokens, dim)\n",
    "\n",
    "        # Embedding Norm\n",
    "        self.norm = (\n",
    "            nn.LayerNorm(dim) if pre_emb_norm else nn.Identity()\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the Jamba model.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor.\n",
    "\n",
    "        \"\"\"\n",
    "        # Embed the input tensor to transform\n",
    "        # From tokens -> tensors\n",
    "        x = self.embed(x)\n",
    "\n",
    "        # Normalize the embeddings\n",
    "        x = self.norm(x)\n",
    "\n",
    "        # Apply the layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        if self.return_embeddings:\n",
    "            return x\n",
    "        else:\n",
    "            # return the logits\n",
    "            return OutputHead(self.dim, -1)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Load and preprocess your data (example from original script)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Load and preprocess data\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_pickle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPickles/trainv2.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m     y \u001b[38;5;241m=\u001b[39m X[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      9\u001b[0m     label_mapping \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstar\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_star\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgalaxy\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124magn\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m3\u001b[39m}\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "batch_size = 512\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and preprocess your data (example from original script)\n",
    "    # Load and preprocess data\n",
    "    X = pd.read_pickle(\"Pickles/trainv2.pkl\")\n",
    "    y = X[\"label\"]\n",
    "    label_mapping = {'star': 0, 'binary_star': 1, 'galaxy': 2, 'agn': 3}\n",
    "    y = y.map(label_mapping).values\n",
    "    X = X.drop([\"parallax\", \"ra\", \"dec\", \"ra_error\", \"dec_error\", \"parallax_error\", \"pmra\", \"pmdec\", \"pmra_error\", \"pmdec_error\", \n",
    "                \"phot_g_mean_flux\", \"flagnopllx\", \"phot_g_mean_flux_error\", \"phot_bp_mean_flux\", \"phot_rp_mean_flux\", \n",
    "                \"phot_bp_mean_flux_error\", \"phot_rp_mean_flux_error\", \"label\", \"obsid\"], axis=1).values\n",
    "    \n",
    "    # Read test data\n",
    "    X_test = pd.read_pickle(\"Pickles/testv2.pkl\")\n",
    "    y_test = X_test[\"label\"].map(label_mapping).values\n",
    "    X_test = X_test.drop([\"parallax\", \"ra\", \"dec\", \"ra_error\", \"dec_error\", \"parallax_error\", \"pmra\", \"pmdec\", \"pmra_error\", \"pmdec_error\", \n",
    "                \"phot_g_mean_flux\", \"flagnopllx\", \"phot_g_mean_flux_error\", \"phot_bp_mean_flux\", \"phot_rp_mean_flux\", \n",
    "                \"phot_bp_mean_flux_error\", \"phot_rp_mean_flux_error\", \"label\", \"obsid\"], axis=1).values\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Clear memory\n",
    "    del X, y\n",
    "    gc.collect()\n",
    "\n",
    "    # Convert to torch tensors and create datasets\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32).unsqueeze(1)\n",
    "    X_val = torch.tensor(X_val, dtype=torch.float32).unsqueeze(1)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "    y_val = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "    train_dataset = BalancedDataset(X_train, y_train)\n",
    "    val_dataset = BalancedValidationDataset(X_val, y_val)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(BalancedValidationDataset(torch.tensor(X_test, dtype=torch.float32).unsqueeze(1),\n",
    "                                                    torch.tensor(y_test, dtype=torch.long)), batch_size=batch_size, shuffle=False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m depth \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Define the config dictionary object\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m config \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_classes\u001b[39m\u001b[38;5;124m\"\u001b[39m: num_classes, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mbatch_size\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m: lr, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpatience\u001b[39m\u001b[38;5;124m\"\u001b[39m: patience, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m: num_epochs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124md_model\u001b[39m\u001b[38;5;124m\"\u001b[39m: d_model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdepth\u001b[39m\u001b[38;5;124m\"\u001b[39m: depth}\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Initialize WandB project\u001b[39;00m\n\u001b[0;32m     17\u001b[0m wandb\u001b[38;5;241m.\u001b[39minit(project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlamost-jamba-test\u001b[39m\u001b[38;5;124m\"\u001b[39m, entity\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjoaoc-university-of-southampton\u001b[39m\u001b[38;5;124m\"\u001b[39m, config\u001b[38;5;241m=\u001b[39mconfig)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'batch_size' is not defined"
     ]
    }
   ],
   "source": [
    "from jamba.model import Jamba\n",
    "\n",
    "# Define the model with your parameters\n",
    "d_model = 128 # Embedding dimension\n",
    "num_classes = 4  # Star classification categories\n",
    "\n",
    "# Define the training parameters\n",
    "num_epochs = 500\n",
    "lr = 1e-4\n",
    "patience = 30   \n",
    "depth = 10\n",
    "\n",
    "# Define the config dictionary object\n",
    "config = {\"num_classes\": num_classes, \"batch_size\": batch_size, \"lr\": lr, \"patience\": patience, \"num_epochs\": num_epochs, \"d_model\": d_model, \"depth\": depth}\n",
    "\n",
    "# Initialize WandB project\n",
    "wandb.init(project=\"lamost-jamba-test\", entity=\"joaoc-university-of-southampton\", config=config)\n",
    "\n",
    "# Define hyperparameters\n",
    "num_epochs = 100\n",
    "lr = 1e-3\n",
    "patience = 10\n",
    "\n",
    "# Initialize the Jamba model\n",
    "model_mamba = Jamba(\n",
    "    dim=3748,                # Input dimensionality\n",
    "    depth=4,                # Number of layers\n",
    "    num_tokens=100,         # Token size (adapt to your case)\n",
    "    d_state=d_model,            # Hidden state dimensionality\n",
    "    d_conv=128,             # Convolutional layers dimensionality\n",
    "    heads=8,                # Number of attention heads\n",
    "    num_experts=8,          # Number of expert networks\n",
    "    num_experts_per_token=2 # Experts per token\n",
    ")\n",
    "\n",
    "print(model_mamba)\n",
    "# Print number of parameters per layer\n",
    "for name, param in model_mamba.named_parameters():\n",
    "    print(name, param.numel())\n",
    "print(\"Total number of parameters:\", sum(p.numel() for p in model_mamba.parameters() if p.requires_grad))\n",
    "\n",
    "# Move the model to device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model_mamba = model_mamba.to(device)\n",
    "\n",
    "# Train the model\n",
    "trained_model = train_model_mamba(\n",
    "    model=model_mamba,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    num_epochs=num_epochs,\n",
    "    lr=lr,\n",
    "    max_patience=patience,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Save the model and finish WandB session\n",
    "wandb.finish()\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
