{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import wandb\n",
    "import gc\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import mambapy\n",
    "from mambapy.mamba import Mamba, MambaConfig\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BalancedDataset(Dataset):\n",
    "    def __init__(self, X, y, limit_per_label=1600):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.limit_per_label = limit_per_label\n",
    "        self.classes = np.unique(y)\n",
    "        self.indices = self.balance_classes()\n",
    "\n",
    "    def balance_classes(self):\n",
    "        indices = []\n",
    "        for cls in self.classes:\n",
    "            cls_indices = np.where(self.y == cls)[0]\n",
    "            if len(cls_indices) > self.limit_per_label:\n",
    "                cls_indices = np.random.choice(cls_indices, self.limit_per_label, replace=False)\n",
    "            indices.extend(cls_indices)\n",
    "        np.random.shuffle(indices)\n",
    "        return indices\n",
    "\n",
    "    def re_sample(self):\n",
    "        self.indices = self.balance_classes()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        index = self.indices[idx]\n",
    "        return self.X[index], self.y[index]\n",
    "# Custom Dataset for validation with limit per class\n",
    "class BalancedValidationDataset(Dataset):\n",
    "    def __init__(self, X, y, limit_per_label=400):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.limit_per_label = limit_per_label\n",
    "        self.classes = np.unique(y)\n",
    "        self.indices = self.balance_classes()\n",
    "\n",
    "    def balance_classes(self):\n",
    "        indices = []\n",
    "        for cls in self.classes:\n",
    "            cls_indices = np.where(self.y == cls)[0]\n",
    "            if len(cls_indices) > self.limit_per_label:\n",
    "                cls_indices = np.random.choice(cls_indices, self.limit_per_label, replace=False)\n",
    "            indices.extend(cls_indices)\n",
    "        np.random.shuffle(indices)\n",
    "        return indices\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        index = self.indices[idx]\n",
    "        return self.X[index], self.y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2048\n",
    "\n",
    "\n",
    "# Main script to load data and train the model\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and preprocess data\n",
    "    X = pd.read_pickle(\"Pickles/train.pkl\")\n",
    "    y = X[\"label\"]\n",
    "    label_mapping = {'star': 0, 'binary_star': 1, 'galaxy': 2, 'agn': 3}\n",
    "    y = y.map(label_mapping).values\n",
    "    columns = [\"parallax\", \"ra\", \"dec\", \"ra_error\", \"dec_error\", \"parallax_error\", \"pmra\", \"pmdec\", \"pmra_error\", \"pmdec_error\", \n",
    "           \"phot_g_mean_flux\", \"flagnopllx\", \"phot_g_mean_flux_error\", \"phot_bp_mean_flux\", \"phot_rp_mean_flux\", \n",
    "           \"phot_bp_mean_flux_error\", \"phot_rp_mean_flux_error\"]\n",
    "    X = X[columns]\n",
    "    \n",
    "    # Read test data\n",
    "    X_test = pd.read_pickle(\"Pickles/test.pkl\")\n",
    "    y_test = X_test[\"label\"].map(label_mapping).values\n",
    "    X_test = X_test[columns]\n",
    "\n",
    "    # Convert test data to torch tensors\n",
    "    X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)  # Convert DataFrame to NumPy array first\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "    # Create the test dataset without the unsqueeze\n",
    "    test_dataset = BalancedValidationDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "    # Create the DataLoader\n",
    "    test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Clear memory\n",
    "    del X, y\n",
    "    gc.collect()\n",
    "\n",
    "    # Convert to torch tensors and create datasets\n",
    "    X_train = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "    X_val = torch.tensor(X_val.values, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "    y_val = torch.tensor(y_val, dtype=torch.long)\n",
    "    train_dataset = BalancedDataset(X_train, y_train)\n",
    "    val_dataset = BalancedValidationDataset(X_val, y_val)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StarClassifierMAMBA(nn.Module):\n",
    "    def __init__(self, d_model, num_classes, d_state=64, d_conv=4, input_dim=17, n_layers=6):\n",
    "        super(StarClassifierMAMBA, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # MAMBA layer initialization\n",
    "        config = MambaConfig(\n",
    "            d_model=d_model,\n",
    "            d_state=d_state,\n",
    "            d_conv=d_conv,\n",
    "            n_layers=n_layers\n",
    "\n",
    "        )\n",
    "        self.mamba_layer = Mamba(config)\n",
    "\n",
    "        # Input projection to match the MAMBA layer dimension\n",
    "        self.input_projection = nn.Linear(input_dim, d_model)\n",
    "\n",
    "        # Fully connected classifier head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, num_classes)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_projection(x)  # Ensure the input has the correct dimension\n",
    "        x = x.unsqueeze(1)  # Adds a sequence dimension (L=1).\n",
    "        x = self.mamba_layer(x)\n",
    "        x = x.mean(dim=1)  # Pooling operation for classification\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "\n",
    "def train_model_mamba(\n",
    "    model, train_loader, val_loader, test_loader, \n",
    "    num_epochs=500, lr=1e-4, max_patience=20, device='cuda'\n",
    "):\n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Define optimizer, scheduler, and loss function\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=int(max_patience / 3), verbose=True\n",
    "    )\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience = max_patience\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Resample the training dataset\n",
    "        train_loader.dataset.re_sample()\n",
    "\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss, train_accuracy = 0.0, 0.0\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * X_batch.size(0)\n",
    "            train_accuracy += (outputs.argmax(dim=1) == y_batch).float().mean().item()\n",
    "\n",
    "        # Validation phase\n",
    "        # Resample the validation dataset\n",
    "        val_loader.dataset.indices = val_loader.dataset.balance_classes()\n",
    "        model.eval()\n",
    "        val_loss, val_accuracy = 0.0, 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_val, y_val in val_loader:\n",
    "                X_val, y_val = X_val.to(device), y_val.to(device)\n",
    "                outputs = model(X_val)\n",
    "                loss = criterion(outputs, y_val)\n",
    "\n",
    "                val_loss += loss.item() * X_val.size(0)\n",
    "                val_accuracy += (outputs.argmax(dim=1) == y_val).float().mean().item()\n",
    "\n",
    "        # Test phase and metric collection\n",
    "        test_loss, test_accuracy = 0.0, 0.0\n",
    "        y_true, y_pred = [], []\n",
    "        with torch.no_grad():\n",
    "            for X_test, y_test in test_loader:\n",
    "                X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "                outputs = model(X_test)\n",
    "                loss = criterion(outputs, y_test)\n",
    "\n",
    "                test_loss += loss.item() * X_test.size(0)\n",
    "                test_accuracy += (outputs.argmax(dim=1) == y_test).float().mean().item()\n",
    "                y_true.extend(y_test.cpu().numpy())\n",
    "                y_pred.extend(outputs.argmax(dim=1).cpu().numpy())\n",
    "\n",
    "        # Update scheduler\n",
    "        scheduler.step(val_loss / len(val_loader.dataset))\n",
    "\n",
    "        # Log metrics to WandB\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": train_loss / len(train_loader.dataset),\n",
    "            \"val_loss\": val_loss / len(val_loader.dataset),\n",
    "            \"train_accuracy\": train_accuracy / len(train_loader),\n",
    "            \"val_accuracy\": val_accuracy / len(val_loader),\n",
    "            \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "            \"test_loss\": test_loss / len(test_loader.dataset),\n",
    "            \"test_accuracy\": test_accuracy / len(test_loader),\n",
    "            \"confusion_matrix\": wandb.plot.confusion_matrix(\n",
    "                probs=None, y_true=y_true, preds=y_pred, class_names=np.unique(y_true)\n",
    "            ),\n",
    "            \"classification_report\": classification_report(\n",
    "                y_true, y_pred, target_names=[str(i) for i in range(len(np.unique(y_true)))]\n",
    "            )\n",
    "        })\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience = max_patience\n",
    "            best_model = model.state_dict()\n",
    "        else:\n",
    "            patience -= 1\n",
    "            if patience <= 0:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    # Load the best model weights\n",
    "    model.load_state_dict(best_model)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\jcwin\\OneDrive - University of Southampton\\_Southampton\\2024-25\\Star-Classifier\\wandb\\run-20241127_163343-cvrm6ro4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/joaoc-university-of-southampton/gaia-mamba-test/runs/cvrm6ro4' target=\"_blank\">worldly-energy-18</a></strong> to <a href='https://wandb.ai/joaoc-university-of-southampton/gaia-mamba-test' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/joaoc-university-of-southampton/gaia-mamba-test' target=\"_blank\">https://wandb.ai/joaoc-university-of-southampton/gaia-mamba-test</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/joaoc-university-of-southampton/gaia-mamba-test/runs/cvrm6ro4' target=\"_blank\">https://wandb.ai/joaoc-university-of-southampton/gaia-mamba-test/runs/cvrm6ro4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StarClassifierMAMBA(\n",
      "  (mamba_layer): Mamba(\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x ResidualBlock(\n",
      "        (mixer): MambaBlock(\n",
      "          (in_proj): Linear(in_features=512, out_features=2048, bias=False)\n",
      "          (conv1d): Conv1d(1024, 1024, kernel_size=(4,), stride=(1,), padding=(3,), groups=1024)\n",
      "          (x_proj): Linear(in_features=1024, out_features=160, bias=False)\n",
      "          (dt_proj): Linear(in_features=32, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=512, bias=False)\n",
      "        )\n",
      "        (norm): RMSNorm()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (input_projection): Linear(in_features=17, out_features=512, bias=True)\n",
      "  (classifier): Sequential(\n",
      "    (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): Linear(in_features=512, out_features=4, bias=True)\n",
      "  )\n",
      ")\n",
      "mamba_layer.layers.0.mixer.A_log 65536\n",
      "mamba_layer.layers.0.mixer.D 1024\n",
      "mamba_layer.layers.0.mixer.in_proj.weight 1048576\n",
      "mamba_layer.layers.0.mixer.conv1d.weight 4096\n",
      "mamba_layer.layers.0.mixer.conv1d.bias 1024\n",
      "mamba_layer.layers.0.mixer.x_proj.weight 163840\n",
      "mamba_layer.layers.0.mixer.dt_proj.weight 32768\n",
      "mamba_layer.layers.0.mixer.dt_proj.bias 1024\n",
      "mamba_layer.layers.0.mixer.out_proj.weight 524288\n",
      "mamba_layer.layers.0.norm.weight 512\n",
      "mamba_layer.layers.1.mixer.A_log 65536\n",
      "mamba_layer.layers.1.mixer.D 1024\n",
      "mamba_layer.layers.1.mixer.in_proj.weight 1048576\n",
      "mamba_layer.layers.1.mixer.conv1d.weight 4096\n",
      "mamba_layer.layers.1.mixer.conv1d.bias 1024\n",
      "mamba_layer.layers.1.mixer.x_proj.weight 163840\n",
      "mamba_layer.layers.1.mixer.dt_proj.weight 32768\n",
      "mamba_layer.layers.1.mixer.dt_proj.bias 1024\n",
      "mamba_layer.layers.1.mixer.out_proj.weight 524288\n",
      "mamba_layer.layers.1.norm.weight 512\n",
      "mamba_layer.layers.2.mixer.A_log 65536\n",
      "mamba_layer.layers.2.mixer.D 1024\n",
      "mamba_layer.layers.2.mixer.in_proj.weight 1048576\n",
      "mamba_layer.layers.2.mixer.conv1d.weight 4096\n",
      "mamba_layer.layers.2.mixer.conv1d.bias 1024\n",
      "mamba_layer.layers.2.mixer.x_proj.weight 163840\n",
      "mamba_layer.layers.2.mixer.dt_proj.weight 32768\n",
      "mamba_layer.layers.2.mixer.dt_proj.bias 1024\n",
      "mamba_layer.layers.2.mixer.out_proj.weight 524288\n",
      "mamba_layer.layers.2.norm.weight 512\n",
      "mamba_layer.layers.3.mixer.A_log 65536\n",
      "mamba_layer.layers.3.mixer.D 1024\n",
      "mamba_layer.layers.3.mixer.in_proj.weight 1048576\n",
      "mamba_layer.layers.3.mixer.conv1d.weight 4096\n",
      "mamba_layer.layers.3.mixer.conv1d.bias 1024\n",
      "mamba_layer.layers.3.mixer.x_proj.weight 163840\n",
      "mamba_layer.layers.3.mixer.dt_proj.weight 32768\n",
      "mamba_layer.layers.3.mixer.dt_proj.bias 1024\n",
      "mamba_layer.layers.3.mixer.out_proj.weight 524288\n",
      "mamba_layer.layers.3.norm.weight 512\n",
      "mamba_layer.layers.4.mixer.A_log 65536\n",
      "mamba_layer.layers.4.mixer.D 1024\n",
      "mamba_layer.layers.4.mixer.in_proj.weight 1048576\n",
      "mamba_layer.layers.4.mixer.conv1d.weight 4096\n",
      "mamba_layer.layers.4.mixer.conv1d.bias 1024\n",
      "mamba_layer.layers.4.mixer.x_proj.weight 163840\n",
      "mamba_layer.layers.4.mixer.dt_proj.weight 32768\n",
      "mamba_layer.layers.4.mixer.dt_proj.bias 1024\n",
      "mamba_layer.layers.4.mixer.out_proj.weight 524288\n",
      "mamba_layer.layers.4.norm.weight 512\n",
      "mamba_layer.layers.5.mixer.A_log 65536\n",
      "mamba_layer.layers.5.mixer.D 1024\n",
      "mamba_layer.layers.5.mixer.in_proj.weight 1048576\n",
      "mamba_layer.layers.5.mixer.conv1d.weight 4096\n",
      "mamba_layer.layers.5.mixer.conv1d.bias 1024\n",
      "mamba_layer.layers.5.mixer.x_proj.weight 163840\n",
      "mamba_layer.layers.5.mixer.dt_proj.weight 32768\n",
      "mamba_layer.layers.5.mixer.dt_proj.bias 1024\n",
      "mamba_layer.layers.5.mixer.out_proj.weight 524288\n",
      "mamba_layer.layers.5.norm.weight 512\n",
      "input_projection.weight 8704\n",
      "input_projection.bias 512\n",
      "classifier.0.weight 512\n",
      "classifier.0.bias 512\n",
      "classifier.1.weight 2048\n",
      "classifier.1.bias 4\n",
      "Total number of parameters: 11068420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jcwin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e93d081777d49c5bb9a239f40d1b9ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.248 MB of 0.248 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇██</td></tr><tr><td>learning_rate</td><td>████████████████████▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>test_accuracy</td><td>▁▃▅▇▇██▇▇█▇▇███▇▇█████▇██████▇▇▇▇▇██▇▇█▇</td></tr><tr><td>test_loss</td><td>█▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_accuracy</td><td>▁▄▇▇▇█▇█████████████████████████████████</td></tr><tr><td>train_loss</td><td>█▅▄▄▄▄▃▃▃▃▃▂▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▅▅▅▄▅▆▄▆▆▆▇▆▆▇▇▇██▇▇▆▇▇▇▇▇█▆▇▇▇▇▇▇█▇▇▇▇</td></tr><tr><td>val_loss</td><td>█▃▃▃▃▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>classification_report</td><td>              precis...</td></tr><tr><td>epoch</td><td>182</td></tr><tr><td>learning_rate</td><td>0.0</td></tr><tr><td>test_accuracy</td><td>0.88379</td></tr><tr><td>test_loss</td><td>0.2824</td></tr><tr><td>train_accuracy</td><td>0.89161</td></tr><tr><td>train_loss</td><td>0.27085</td></tr><tr><td>val_accuracy</td><td>0.88925</td></tr><tr><td>val_loss</td><td>0.29224</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">worldly-energy-18</strong> at: <a href='https://wandb.ai/joaoc-university-of-southampton/gaia-mamba-test/runs/cvrm6ro4' target=\"_blank\">https://wandb.ai/joaoc-university-of-southampton/gaia-mamba-test/runs/cvrm6ro4</a><br/> View project at: <a href='https://wandb.ai/joaoc-university-of-southampton/gaia-mamba-test' target=\"_blank\">https://wandb.ai/joaoc-university-of-southampton/gaia-mamba-test</a><br/>Synced 5 W&B file(s), 0 media file(s), 356 artifact file(s) and 183 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241127_163343-cvrm6ro4\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the model with your parameters\n",
    "d_model = 512 # Embedding dimension\n",
    "num_classes = 4  # Star classification categories\n",
    "\n",
    "# Define the training parameters\n",
    "num_epochs = 500\n",
    "lr = 1e-4\n",
    "patience = 50   \n",
    "depth = 6\n",
    "\n",
    "# Define the config dictionary object\n",
    "config = {\"num_classes\": num_classes, \"batch_size\": batch_size, \"lr\": lr, \"patience\": patience, \"num_epochs\": num_epochs, \"d_model\": d_model, \"depth\": depth}\n",
    "\n",
    "# Initialize WandB project\n",
    "wandb.init(project=\"gaia-mamba-test\", entity=\"joaoc-university-of-southampton\", config=config)\n",
    "# Initialize and train the model\n",
    "# Train the model using your `train_model_vit` or an adjusted training loop\n",
    "model_mamba = StarClassifierMAMBA(d_model=d_model, num_classes=num_classes, input_dim=17, n_layers=depth)\n",
    "print(model_mamba)\n",
    "# print number of parameters per layer\n",
    "for name, param in model_mamba.named_parameters():\n",
    "    print(name, param.numel())\n",
    "print(\"Total number of parameters:\", sum(p.numel() for p in model_mamba.parameters() if p.requires_grad))\n",
    "\n",
    "# Move the model to device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model_mamba = model_mamba.to(device)\n",
    "\n",
    "# Train the model using your `train_model_vit` or an adjusted training loop\n",
    "trained_model = train_model_mamba(\n",
    "    model=model_mamba,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    num_epochs=num_epochs,\n",
    "    lr=lr,\n",
    "    max_patience=patience,\n",
    "    device=device\n",
    ")\n",
    "# Save the model and finish WandB session\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model with your parameters\n",
    "d_model = 512  # Embedding dimension\n",
    "num_classes = 4  # Star classification categories\n",
    "\n",
    "# Define the training parameters\n",
    "num_epochs = 500\n",
    "lr = 1e-4\n",
    "patience = 50   \n",
    "depth = 10\n",
    "\n",
    "# Define the config dictionary object\n",
    "config = {\"num_classes\": num_classes, \"batch_size\": batch_size, \"lr\": lr, \"patience\": patience, \"num_epochs\": num_epochs, \"d_model\": d_model, \"depth\": depth}\n",
    "\n",
    "# Initialize WandB project\n",
    "wandb.init(project=\"gaia-mamba-test\", entity=\"joaoc-university-of-southampton\", config=config)\n",
    "# Initialize and train the model\n",
    "# Train the model using your `train_model_vit` or an adjusted training loop\n",
    "model_mamba = StarClassifierMAMBA(d_model=d_model, num_classes=num_classes, input_dim=17, n_layers=depth)\n",
    "print(model_mamba)\n",
    "# print number of parameters per layer\n",
    "for name, param in model_mamba.named_parameters():\n",
    "    print(name, param.numel())\n",
    "print(\"Total number of parameters:\", sum(p.numel() for p in model_mamba.parameters() if p.requires_grad))\n",
    "\n",
    "# Move the model to device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model_mamba = model_mamba.to(device)\n",
    "\n",
    "# Train the model using your `train_model_vit` or an adjusted training loop\n",
    "trained_model = train_model_mamba(\n",
    "    model=model_mamba,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    num_epochs=num_epochs,\n",
    "    lr=lr,\n",
    "    max_patience=patience,\n",
    "    device=device\n",
    ")\n",
    "# Save the model and finish WandB session\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        star       0.84      0.78      0.81       400\n",
      " binary_star       0.80      0.86      0.83       400\n",
      "      galaxy       0.86      0.99      0.92       376\n",
      "         agn       0.98      0.84      0.90       400\n",
      "\n",
      "    accuracy                           0.86      1576\n",
      "   macro avg       0.87      0.87      0.86      1576\n",
      "weighted avg       0.87      0.86      0.86      1576\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the classification report\n",
    "print(\"Classification report:\")\n",
    "y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for X_test, y_test in test_loader:\n",
    "        X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "        outputs = trained_model(X_test)\n",
    "        y_true.extend(y_test.cpu().numpy())\n",
    "        y_pred.extend(outputs.argmax(dim=1).cpu().numpy())\n",
    "print(classification_report(y_true, y_pred, target_names=label_mapping.keys()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
