{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import wandb\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, hamming_loss, roc_auc_score\n",
    "import gc\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mamba_ssm import Mamba2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttentionBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple cross-attention block with a feed-forward sub-layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads=8):\n",
    "        super().__init__()\n",
    "        self.cross_attn = nn.MultiheadAttention(\n",
    "            embed_dim=d_model, \n",
    "            num_heads=n_heads, \n",
    "            batch_first=True\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * d_model, d_model)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x_q, x_kv):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x_q  : (batch_size, seq_len_q, d_model)\n",
    "            x_kv : (batch_size, seq_len_kv, d_model)\n",
    "        \"\"\"\n",
    "        # Cross-attention\n",
    "        attn_output, _ = self.cross_attn(query=x_q, key=x_kv, value=x_kv)\n",
    "        x = self.norm1(x_q + attn_output)\n",
    "\n",
    "        # Feed forward\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = self.norm2(x + ffn_out)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class StarClassifierFusion(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model_spectra,\n",
    "        d_model_gaia,\n",
    "        num_classes,\n",
    "        input_dim_spectra,\n",
    "        input_dim_gaia,\n",
    "        n_layers=6,\n",
    "        use_cross_attention=True,\n",
    "        n_cross_attn_heads=8,\n",
    "        d_state=256,\n",
    "        d_conv=4,\n",
    "        expand=2,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model_spectra (int): embedding dimension for the spectra MAMBA\n",
    "            d_model_gaia (int): embedding dimension for the gaia MAMBA\n",
    "            num_classes (int): multi-label classification\n",
    "            input_dim_spectra (int): # of features for spectra\n",
    "            input_dim_gaia (int): # of features for gaia\n",
    "            n_layers (int): depth for each MAMBA\n",
    "            use_cross_attention (bool): whether to use cross-attention\n",
    "            n_cross_attn_heads (int): number of heads for cross-attention\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "\n",
    "        # --- MAMBA 2 for spectra ---\n",
    "        self.mamba_spectra = nn.Sequential(\n",
    "            *[Mamba2(\n",
    "                d_model=d_model_spectra,\n",
    "                d_state=d_state,\n",
    "                d_conv=d_conv,\n",
    "                expand=expand,\n",
    "            ) for _ in range(n_layers)]\n",
    "        )\n",
    "        self.input_proj_spectra = nn.Linear(input_dim_spectra, d_model_spectra)\n",
    "\n",
    "\n",
    "        # --- MAMBA 2 for gaia ---\n",
    "        self.mamba_gaia = nn.Sequential(\n",
    "            *[Mamba2(\n",
    "                d_model=d_model_gaia,\n",
    "                d_state=d_state,\n",
    "                d_conv=d_conv,\n",
    "                expand=expand,\n",
    "            ) for _ in range(n_layers)]\n",
    "        )\n",
    "        self.input_proj_gaia = nn.Linear(input_dim_gaia, d_model_gaia)\n",
    "\n",
    "        # --- Cross Attention (Optional) ---\n",
    "        self.use_cross_attention = use_cross_attention\n",
    "        if use_cross_attention:\n",
    "            # We'll do cross-attn in both directions or just one—here is an example with 2 blocks\n",
    "            self.cross_attn_block_spectra = CrossAttentionBlock(d_model_spectra, n_heads=n_cross_attn_heads)\n",
    "            self.cross_attn_block_gaia = CrossAttentionBlock(d_model_gaia, n_heads=n_cross_attn_heads)\n",
    "\n",
    "        # --- Final Classifier ---\n",
    "        # If you do late fusion by concatenation, the dimension is d_model_spectra + d_model_gaia\n",
    "        # If you do average fusion, it is max(d_model_spectra, d_model_gaia) (or keep them separate).\n",
    "        fusion_dim = d_model_spectra + d_model_gaia\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(fusion_dim),\n",
    "            nn.Linear(fusion_dim, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x_spectra, x_gaia):\n",
    "        \"\"\"\n",
    "        x_spectra : (batch_size, input_dim_spectra) or potentially (batch_size, seq_len_spectra, input_dim_spectra)\n",
    "        x_gaia    : (batch_size, input_dim_gaia) or (batch_size, seq_len_gaia, input_dim_gaia)\n",
    "        \"\"\"\n",
    "        # For MAMBA, we expect shape: (B, seq_len, d_model). \n",
    "        # If your input is just (B, d_in), we turn it into (B, 1, d_in).\n",
    "        \n",
    "        # --- Project to d_model and add sequence dimension (seq_len=1) ---\n",
    "        x_spectra = self.input_proj_spectra(x_spectra)  # (B, d_model_spectra)\n",
    "        x_spectra = x_spectra.unsqueeze(1)             # (B, 1, d_model_spectra)\n",
    "\n",
    "        x_gaia = self.input_proj_gaia(x_gaia)          # (B, d_model_gaia)\n",
    "        x_gaia = x_gaia.unsqueeze(1)                   # (B, 1, d_model_gaia)\n",
    "\n",
    "        # --- MAMBA encoding (each modality separately) ---\n",
    "        x_spectra = self.mamba_spectra(x_spectra)  # (B, 1, d_model_spectra)\n",
    "        x_gaia = self.mamba_gaia(x_gaia)          # (B, 1, d_model_gaia)\n",
    "\n",
    "        # Optionally, use cross-attention to fuse the representations\n",
    "        if self.use_cross_attention:\n",
    "            # Cross-attention from spectra -> gaia\n",
    "            x_spectra_fused = self.cross_attn_block_spectra(x_spectra, x_gaia)\n",
    "            # Cross-attention from gaia -> spectra\n",
    "            x_gaia_fused = self.cross_attn_block_gaia(x_gaia, x_spectra)\n",
    "            \n",
    "            # Update x_spectra and x_gaia\n",
    "            x_spectra = x_spectra_fused\n",
    "            x_gaia = x_gaia_fused\n",
    "        \n",
    "        # --- Pool across sequence dimension (since our seq_len=1, just squeeze) ---\n",
    "        x_spectra = x_spectra.mean(dim=1)  # (B, d_model_spectra)\n",
    "        x_gaia = x_gaia.mean(dim=1)        # (B, d_model_gaia)\n",
    "\n",
    "        # --- Late Fusion by Concatenation ---\n",
    "        x_fused = torch.cat([x_spectra, x_gaia], dim=-1)  # (B, d_model_spectra + d_model_gaia)\n",
    "\n",
    "        # --- Final classification ---\n",
    "        logits = self.classifier(x_fused)  # (B, num_classes)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_fusion(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    test_loader,\n",
    "    num_epochs=100,\n",
    "    lr=1e-4,\n",
    "    max_patience=20,\n",
    "    device='cuda'\n",
    "):\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=int(max_patience / 5)\n",
    "    )\n",
    "\n",
    "    # We assume the datasets are MultiModalBalancedMultiLabelDataset\n",
    "    # that returns (X_spectra, X_gaia, y).\n",
    "    # You can keep the class weighting logic as in train_model_mamba.\n",
    "    all_labels = []\n",
    "    for _, _, y_batch in train_loader:\n",
    "        all_labels.extend(y_batch.cpu().numpy())\n",
    "    \n",
    "    class_weights = calculate_class_weights(np.array(all_labels))\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience = max_patience\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Resample training data\n",
    "        train_loader.dataset.re_sample()\n",
    "\n",
    "        # Recompute class weights if needed\n",
    "        all_labels = []\n",
    "        for _, _, y_batch in train_loader:\n",
    "            all_labels.extend(y_batch.cpu().numpy())\n",
    "        class_weights = calculate_class_weights(np.array(all_labels))\n",
    "        class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights)\n",
    "\n",
    "        # --- Training ---\n",
    "        model.train()\n",
    "        train_loss, train_acc = 0.0, 0.0\n",
    "        for X_spc, X_ga, y_batch in train_loader:\n",
    "            X_spc, X_ga, y_batch = X_spc.to(device), X_ga.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_spc, X_ga)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * X_spc.size(0)\n",
    "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            correct = (predicted == y_batch).float()\n",
    "            train_acc += correct.mean(dim=1).mean().item()\n",
    "\n",
    "        # --- Validation ---\n",
    "        model.eval()\n",
    "        val_loss, val_acc = 0.0, 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_spc, X_ga, y_batch in val_loader:\n",
    "                X_spc, X_ga, y_batch = X_spc.to(device), X_ga.to(device), y_batch.to(device)\n",
    "                outputs = model(X_spc, X_ga)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_loss += loss.item() * X_spc.size(0)\n",
    "                predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                correct = (predicted == y_batch).float()\n",
    "                val_acc += correct.mean(dim=1).mean().item()\n",
    "\n",
    "        # --- Test metrics (optional or do after training) ---\n",
    "        test_loss, test_acc = 0.0, 0.0\n",
    "        y_true, y_pred = [], []\n",
    "        with torch.no_grad():\n",
    "            for X_spc, X_ga, y_batch in test_loader:\n",
    "                X_spc, X_ga, y_batch = X_spc.to(device), X_ga.to(device), y_batch.to(device)\n",
    "                outputs = model(X_spc, X_ga)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                test_loss += loss.item() * X_spc.size(0)\n",
    "                \n",
    "                predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                correct = (predicted == y_batch).float()\n",
    "                test_acc += correct.mean(dim=1).mean().item()\n",
    "\n",
    "                y_true.extend(y_batch.cpu().numpy())\n",
    "                y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "        # Compute multi-label metrics as before\n",
    "        all_metrics = calculate_metrics(np.array(y_true), np.array(y_pred))\n",
    "\n",
    "        # Logging example\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": train_loss / len(train_loader.dataset),\n",
    "            \"val_loss\": val_loss / len(val_loader.dataset),\n",
    "            \"train_acc\": train_acc / len(train_loader),\n",
    "            \"val_acc\": val_acc / len(val_loader),\n",
    "            \"test_loss\": test_loss / len(test_loader.dataset),\n",
    "            \"test_acc\": test_acc / len(test_loader),\n",
    "            **all_metrics\n",
    "        })\n",
    "\n",
    "        # Scheduler\n",
    "        scheduler.step(val_loss / len(val_loader.dataset))\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience = max_patience\n",
    "            best_model = model.state_dict()\n",
    "        else:\n",
    "            patience -= 1\n",
    "            if patience <= 0:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(best_model)\n",
    "    return model\n",
    "\n",
    "class MultiModalBalancedMultiLabelDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A balanced multi-label dataset that returns (X_spectra, X_gaia, y).\n",
    "    It uses the same balancing strategy as `BalancedMultiLabelDataset`.\n",
    "    \"\"\"\n",
    "    def __init__(self, X_spectra, X_gaia, y, limit_per_label=201):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X_spectra (torch.Tensor): [num_samples, num_spectra_features]\n",
    "            X_gaia (torch.Tensor): [num_samples, num_gaia_features]\n",
    "            y (torch.Tensor): [num_samples, num_classes], multi-hot labels\n",
    "            limit_per_label (int): limit or target number of samples per label\n",
    "        \"\"\"\n",
    "        self.X_spectra = X_spectra\n",
    "        self.X_gaia = X_gaia\n",
    "        self.y = y\n",
    "        self.limit_per_label = limit_per_label\n",
    "        self.num_classes = y.shape[1]\n",
    "        self.indices = self.balance_classes()\n",
    "        \n",
    "    def balance_classes(self):\n",
    "        indices = []\n",
    "        class_counts = torch.sum(self.y, axis=0)\n",
    "        for cls in range(self.num_classes):\n",
    "            cls_indices = np.where(self.y[:, cls] == 1)[0]\n",
    "            if len(cls_indices) < self.limit_per_label:\n",
    "                if len(cls_indices) == 0:\n",
    "                    # No samples for this class\n",
    "                    continue\n",
    "                extra_indices = np.random.choice(\n",
    "                    cls_indices, self.limit_per_label - len(cls_indices), replace=True\n",
    "                )\n",
    "                cls_indices = np.concatenate([cls_indices, extra_indices])\n",
    "            elif len(cls_indices) > self.limit_per_label:\n",
    "                cls_indices = np.random.choice(cls_indices, self.limit_per_label, replace=False)\n",
    "            indices.extend(cls_indices)\n",
    "        indices = np.unique(indices)\n",
    "        np.random.shuffle(indices)\n",
    "        return indices\n",
    "\n",
    "    def re_sample(self):\n",
    "        self.indices = self.balance_classes()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        index = self.indices[idx]\n",
    "        return (\n",
    "            self.X_spectra[index],  # spectra features\n",
    "            self.X_gaia[index],     # gaia features\n",
    "            self.y[index],          # multi-hot labels\n",
    "        )\n",
    "def calculate_class_weights(y):\n",
    "    if y.ndim > 1:  \n",
    "        class_counts = np.sum(y, axis=0)  \n",
    "    else:\n",
    "        class_counts = np.bincount(y)\n",
    "\n",
    "    total_samples = y.shape[0] if y.ndim > 1 else len(y)\n",
    "    class_counts = np.where(class_counts == 0, 1, class_counts)  # Prevent division by zero\n",
    "    class_weights = total_samples / (len(class_counts) * class_counts)\n",
    "    \n",
    "    return class_weights\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    metrics = {\n",
    "        \"micro_f1\": f1_score(y_true, y_pred, average='micro'),\n",
    "        \"macro_f1\": f1_score(y_true, y_pred, average='macro'),\n",
    "        \"weighted_f1\": f1_score(y_true, y_pred, average='weighted'),\n",
    "        \"micro_precision\": precision_score(y_true, y_pred, average='micro', zero_division=1),\n",
    "        \"macro_precision\": precision_score(y_true, y_pred, average='macro', zero_division=1),\n",
    "        \"weighted_precision\": precision_score(y_true, y_pred, average='weighted', zero_division=1),\n",
    "        \"micro_recall\": recall_score(y_true, y_pred, average='micro'),\n",
    "        \"macro_recall\": recall_score(y_true, y_pred, average='macro'),\n",
    "        \"weighted_recall\": recall_score(y_true, y_pred, average='weighted'),\n",
    "        \"hamming_loss\": hamming_loss(y_true, y_pred)\n",
    "    }\n",
    "    \n",
    "    # Check if there are at least two classes present in y_true\n",
    "    #if len(np.unique(y_true)) > 1:\n",
    "        #metrics[\"roc_auc\"] = roc_auc_score(y_true, y_pred, average='macro', multi_class='ovr')\n",
    "    #else:\n",
    "       # metrics[\"roc_auc\"] = None  # or you can set it to a default value or message\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parallax                   0\n",
      "ra                         0\n",
      "dec                        0\n",
      "ra_error                   0\n",
      "dec_error                  0\n",
      "parallax_error             0\n",
      "pmra                       0\n",
      "pmdec                      0\n",
      "pmra_error                 0\n",
      "pmdec_error                0\n",
      "phot_g_mean_flux           0\n",
      "flagnopllx                 0\n",
      "phot_g_mean_flux_error     0\n",
      "phot_bp_mean_flux          0\n",
      "phot_rp_mean_flux          0\n",
      "phot_bp_mean_flux_error    0\n",
      "phot_rp_mean_flux_error    0\n",
      "flagnoflux                 0\n",
      "dtype: int64\n",
      "parallax                   0\n",
      "ra                         0\n",
      "dec                        0\n",
      "ra_error                   0\n",
      "dec_error                  0\n",
      "parallax_error             0\n",
      "pmra                       0\n",
      "pmdec                      0\n",
      "pmra_error                 0\n",
      "pmdec_error                0\n",
      "phot_g_mean_flux           0\n",
      "flagnopllx                 0\n",
      "phot_g_mean_flux_error     0\n",
      "phot_bp_mean_flux          0\n",
      "phot_rp_mean_flux          0\n",
      "phot_bp_mean_flux_error    0\n",
      "phot_rp_mean_flux_error    0\n",
      "flagnoflux                 0\n",
      "dtype: int64\n",
      "X_train_spectra shape: torch.Size([87134, 3647])\n",
      "X_val_spectra shape: torch.Size([21784, 3647])\n",
      "X_test_spectra shape: torch.Size([27237, 3647])\n",
      "X_train_gaia shape: torch.Size([87134, 18])\n",
      "X_val_gaia shape: torch.Size([21784, 18])\n",
      "X_test_gaia shape: torch.Size([27237, 18])\n",
      "y_train shape: torch.Size([87134, 55])\n",
      "y_val shape: torch.Size([21784, 55])\n",
      "y_test shape: torch.Size([27237, 55])\n",
      "Train dataset: 1860 samples\n",
      "Validation dataset: 1507 samples\n",
      "Test dataset: 1580 samples\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "batch_limit = int(batch_size / 2.5)\n",
    "\n",
    "\n",
    "# Load datasets\n",
    "import pickle\n",
    "# Open them in a cross-platform way\n",
    "with open(\"Pickles/Updated_List_of_Classes_ubuntu.pkl\", \"rb\") as f:\n",
    "    classes = pickle.load(f)  # This reads the actual data\n",
    "with open(\"Pickles/train_data_transformed_ubuntu.pkl\", \"rb\") as f:\n",
    "    X_train_full = pickle.load(f)\n",
    "with open(\"Pickles/test_data_transformed_ubuntu.pkl\", \"rb\") as f:\n",
    "    X_test_full = pickle.load(f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Extract labels\n",
    "y_train_full = X_train_full[classes]\n",
    "y_test = X_test_full[classes]\n",
    "\n",
    "# Drop labels from both datasets\n",
    "X_train_full.drop(classes, axis=1, inplace=True)\n",
    "X_test_full.drop(classes, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Columns for spectral data (assuming all remaining columns after removing Gaia are spectra)\n",
    "gaia_columns = [\"parallax\", \"ra\", \"dec\", \"ra_error\", \"dec_error\", \"parallax_error\", \"pmra\", \"pmdec\", \n",
    "                \"pmra_error\", \"pmdec_error\", \"phot_g_mean_flux\", \"flagnopllx\", \"phot_g_mean_flux_error\", \n",
    "                \"phot_bp_mean_flux\", \"phot_rp_mean_flux\", \"phot_bp_mean_flux_error\", \"phot_rp_mean_flux_error\", \n",
    "                \"flagnoflux\"]\n",
    "\n",
    "# Spectra data (everything that is not Gaia-related) and the column 'otype'\n",
    "X_train_spectra = X_train_full.drop(columns={\"otype\", \"obsid\", *gaia_columns})\n",
    "X_test_spectra = X_test_full.drop(columns={\"otype\", \"obsid\", *gaia_columns})\n",
    "\n",
    "# Gaia data (only the selected columns)\n",
    "X_train_gaia = X_train_full[gaia_columns]\n",
    "X_test_gaia = X_test_full[gaia_columns]\n",
    "\n",
    "# Count nans and infs in x_train_gaia\n",
    "print(X_train_gaia.isnull().sum())\n",
    "print(X_train_gaia.isin([np.inf, -np.inf]).sum())\n",
    "\n",
    "\n",
    "# Free up memory\n",
    "del X_train_full, X_test_full\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "# Split training set into training and validation\n",
    "X_train_spectra, X_val_spectra, X_train_gaia, X_val_gaia, y_train, y_val = train_test_split(\n",
    "    X_train_spectra, X_train_gaia, y_train_full, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Free memory\n",
    "del y_train_full\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "# Convert spectra and Gaia data into PyTorch tensors\n",
    "X_train_spectra = torch.tensor(X_train_spectra.values, dtype=torch.float32)\n",
    "X_val_spectra = torch.tensor(X_val_spectra.values, dtype=torch.float32)\n",
    "X_test_spectra = torch.tensor(X_test_spectra.values, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "X_train_gaia = torch.tensor(X_train_gaia.values, dtype=torch.float32)\n",
    "X_val_gaia = torch.tensor(X_val_gaia.values, dtype=torch.float32)\n",
    "X_test_gaia = torch.tensor(X_test_gaia.values, dtype=torch.float32)\n",
    "\n",
    "y_train = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "# Print dataset shapes\n",
    "print(f\"X_train_spectra shape: {X_train_spectra.shape}\")\n",
    "print(f\"X_val_spectra shape: {X_val_spectra.shape}\")\n",
    "print(f\"X_test_spectra shape: {X_test_spectra.shape}\")\n",
    "\n",
    "print(f\"X_train_gaia shape: {X_train_gaia.shape}\")\n",
    "print(f\"X_val_gaia shape: {X_val_gaia.shape}\")\n",
    "print(f\"X_test_gaia shape: {X_test_gaia.shape}\")\n",
    "\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_val shape: {y_val.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "\n",
    "train_dataset = MultiModalBalancedMultiLabelDataset(X_train_spectra, X_train_gaia, y_train, limit_per_label=batch_limit)\n",
    "val_dataset = MultiModalBalancedMultiLabelDataset(X_val_spectra, X_val_gaia, y_val, limit_per_label=batch_limit)\n",
    "test_dataset = MultiModalBalancedMultiLabelDataset(X_test_spectra, X_test_gaia, y_test, limit_per_label=batch_limit)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# print the number of samples in each dataset\n",
    "print(f\"Train dataset: {len(train_dataset)} samples\")\n",
    "print(f\"Validation dataset: {len(val_dataset)} samples\")\n",
    "print(f\"Test dataset: {len(test_dataset)} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleStarClassifier:\n",
    "    \"\"\"\n",
    "    Ensemble of StarClassifierFusion models with uncertainty quantification.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_class,\n",
    "        model_args,\n",
    "        num_models=5,\n",
    "        device='cuda'\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model_class: The model class to use (StarClassifierFusion)\n",
    "            model_args: Dictionary of arguments to pass to the model constructor\n",
    "            num_models: Number of models in the ensemble\n",
    "            device: Device to use for computation\n",
    "        \"\"\"\n",
    "        self.num_models = num_models\n",
    "        self.device = device\n",
    "        self.models = []\n",
    "        \n",
    "        # Initialize models with different random initializations\n",
    "        for i in range(num_models):\n",
    "            model = model_class(**model_args)\n",
    "            model.to(device)\n",
    "            self.models.append(model)\n",
    "            \n",
    "    def train(\n",
    "        self,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        test_loader,\n",
    "        train_function,\n",
    "        train_args,\n",
    "        bootstrap=True,\n",
    "        random_seed_offset=0\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Train each model in the ensemble.\n",
    "        \n",
    "        Args:\n",
    "            train_loader: DataLoader for training data\n",
    "            val_loader: DataLoader for validation data\n",
    "            test_loader: DataLoader for test data\n",
    "            train_function: Function to train a single model\n",
    "            train_args: Dictionary of arguments to pass to train_function\n",
    "            bootstrap: Whether to use bootstrapping for training\n",
    "            random_seed_offset: Offset for random seeds\n",
    "        \"\"\"\n",
    "        trained_models = []\n",
    "        \n",
    "        for i in range(self.num_models):\n",
    "            print(f\"Training model {i+1}/{self.num_models}\")\n",
    "            \n",
    "            # Set different random seed for each model\n",
    "            seed = random_seed_offset + i\n",
    "            torch.manual_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "            \n",
    "            if bootstrap:\n",
    "                # Create bootstrapped dataset\n",
    "                bootstrap_train_loader = self._create_bootstrap_loader(train_loader)\n",
    "                curr_train_loader = bootstrap_train_loader\n",
    "            else:\n",
    "                curr_train_loader = train_loader\n",
    "            \n",
    "            # Initialize a new model for this ensemble member\n",
    "            model = copy.deepcopy(self.models[i])\n",
    "            \n",
    "            # Create a new wandb run for this model\n",
    "            run_name = f\"ensemble_member_{i+1}\"\n",
    "            wandb.init(project=\"ALLSTARS_ensemble\", name=run_name, group=\"ensemble_training\", reinit=True)\n",
    "            \n",
    "            # Log ensemble member info\n",
    "            wandb.config.update({\n",
    "                \"ensemble_member\": i+1,\n",
    "                \"num_models\": self.num_models,\n",
    "                \"bootstrap\": bootstrap,\n",
    "                \"random_seed\": seed\n",
    "            })\n",
    "            \n",
    "            # Train the model\n",
    "            trained_model = train_function(\n",
    "                model=model,\n",
    "                train_loader=curr_train_loader,\n",
    "                val_loader=val_loader,\n",
    "                test_loader=test_loader,\n",
    "                **train_args\n",
    "            )\n",
    "            \n",
    "            # Save the trained model\n",
    "            trained_models.append(trained_model)\n",
    "            \n",
    "            # Save model checkpoint\n",
    "            torch.save(trained_model.state_dict(), f\"ensemble_model_{i+1}.pth\")\n",
    "            \n",
    "            # Finish wandb run\n",
    "            wandb.finish()\n",
    "        \n",
    "        self.models = trained_models\n",
    "        return trained_models\n",
    "    \n",
    "    def _create_bootstrap_loader(self, dataloader):\n",
    "        \"\"\"\n",
    "        Create a bootstrapped version of a dataloader.\n",
    "        \n",
    "        Args:\n",
    "            dataloader: Original DataLoader\n",
    "            \n",
    "        Returns:\n",
    "            DataLoader with bootstrapped samples\n",
    "        \"\"\"\n",
    "        dataset = dataloader.dataset\n",
    "        n_samples = len(dataset)\n",
    "        \n",
    "        # Generate bootstrap indices (sampling with replacement)\n",
    "        bootstrap_indices = np.random.choice(n_samples, size=n_samples, replace=True)\n",
    "        \n",
    "        # Create a subset dataset with the bootstrapped indices\n",
    "        bootstrap_dataset = torch.utils.data.Subset(dataset, bootstrap_indices)\n",
    "        \n",
    "        # Create a new dataloader with the bootstrapped dataset\n",
    "        bootstrap_loader = DataLoader(\n",
    "            bootstrap_dataset,\n",
    "            batch_size=dataloader.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=dataloader.num_workers if hasattr(dataloader, 'num_workers') else 0\n",
    "        )\n",
    "        \n",
    "        return bootstrap_loader\n",
    "    \n",
    "    def predict(self, X_spectra, X_gaia, return_individual=False):\n",
    "        \"\"\"\n",
    "        Generate predictions from the ensemble.\n",
    "        \n",
    "        Args:\n",
    "            X_spectra: Spectral features tensor\n",
    "            X_gaia: Gaia features tensor\n",
    "            return_individual: Whether to return individual model predictions\n",
    "            \n",
    "        Returns:\n",
    "            mean_probs: Mean probabilities across ensemble\n",
    "            std_probs: Standard deviation of probabilities (uncertainty)\n",
    "            individual_probs: Individual model probabilities (if return_individual=True)\n",
    "        \"\"\"\n",
    "        # Ensure inputs are on the correct device\n",
    "        X_spectra = X_spectra.to(self.device)\n",
    "        X_gaia = X_gaia.to(self.device)\n",
    "        \n",
    "        all_probs = []\n",
    "        \n",
    "        # Get predictions from each model\n",
    "        for model in self.models:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                logits = model(X_spectra, X_gaia)\n",
    "                probs = torch.sigmoid(logits)\n",
    "                all_probs.append(probs.cpu().numpy())\n",
    "        \n",
    "        # Stack predictions\n",
    "        all_probs = np.stack(all_probs)\n",
    "        \n",
    "        # Calculate mean and standard deviation\n",
    "        mean_probs = np.mean(all_probs, axis=0)\n",
    "        std_probs = np.std(all_probs, axis=0)\n",
    "        \n",
    "        if return_individual:\n",
    "            return mean_probs, std_probs, all_probs\n",
    "        else:\n",
    "            return mean_probs, std_probs\n",
    "    \n",
    "    def evaluate(self, test_loader, threshold=0.5, return_predictions=False):\n",
    "        \"\"\"\n",
    "        Evaluate the ensemble on a test set.\n",
    "        \n",
    "        Args:\n",
    "            test_loader: DataLoader for test data\n",
    "            threshold: Classification threshold\n",
    "            return_predictions: Whether to return predictions\n",
    "            \n",
    "        Returns:\n",
    "            metrics: Dictionary of evaluation metrics\n",
    "            mean_probs: Mean probabilities (if return_predictions=True)\n",
    "            std_probs: Standard deviation of probabilities (if return_predictions=True)\n",
    "            y_true: True labels (if return_predictions=True)\n",
    "        \"\"\"\n",
    "        all_mean_probs = []\n",
    "        all_std_probs = []\n",
    "        all_y_true = []\n",
    "        \n",
    "        # Generate predictions for each batch\n",
    "        for X_spectra, X_gaia, y_batch in test_loader:\n",
    "            X_spectra, X_gaia = X_spectra.to(self.device), X_gaia.to(self.device)\n",
    "            \n",
    "            # Get ensemble predictions\n",
    "            mean_probs, std_probs = self.predict(X_spectra, X_gaia)\n",
    "            \n",
    "            all_mean_probs.extend(mean_probs)\n",
    "            all_std_probs.extend(std_probs)\n",
    "            all_y_true.extend(y_batch.cpu().numpy())\n",
    "        \n",
    "        # Convert to numpy arrays\n",
    "        mean_probs = np.array(all_mean_probs)\n",
    "        std_probs = np.array(all_std_probs)\n",
    "        y_true = np.array(all_y_true)\n",
    "        \n",
    "        # Make binary predictions\n",
    "        y_pred = (mean_probs > threshold).astype(float)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            \"micro_f1\": f1_score(y_true, y_pred, average='micro'),\n",
    "            \"macro_f1\": f1_score(y_true, y_pred, average='macro'),\n",
    "            \"weighted_f1\": f1_score(y_true, y_pred, average='weighted'),\n",
    "            \"micro_precision\": precision_score(y_true, y_pred, average='micro', zero_division=1),\n",
    "            \"macro_precision\": precision_score(y_true, y_pred, average='macro', zero_division=1),\n",
    "            \"weighted_precision\": precision_score(y_true, y_pred, average='weighted', zero_division=1),\n",
    "            \"micro_recall\": recall_score(y_true, y_pred, average='micro'),\n",
    "            \"macro_recall\": recall_score(y_true, y_pred, average='macro'),\n",
    "            \"weighted_recall\": recall_score(y_true, y_pred, average='weighted'),\n",
    "            \"hamming_loss\": hamming_loss(y_true, y_pred),\n",
    "            \"mean_uncertainty\": np.mean(std_probs),\n",
    "            \"median_uncertainty\": np.median(std_probs),\n",
    "            \"max_uncertainty\": np.max(std_probs)\n",
    "        }\n",
    "        \n",
    "        # Try to calculate ROC AUC if possible\n",
    "        try:\n",
    "            metrics[\"roc_auc\"] = roc_auc_score(y_true, mean_probs, average='macro', multi_class='ovr')\n",
    "        except:\n",
    "            metrics[\"roc_auc\"] = None\n",
    "        \n",
    "        if return_predictions:\n",
    "            return metrics, mean_probs, std_probs, y_true\n",
    "        else:\n",
    "            return metrics\n",
    "    \n",
    "    def visualize_uncertainty(self, mean_probs, std_probs, y_true, num_classes=10, class_names=None):\n",
    "        \"\"\"\n",
    "        Visualize uncertainty for selected classes.\n",
    "        \n",
    "        Args:\n",
    "            mean_probs: Mean probabilities from ensemble\n",
    "            std_probs: Standard deviation of probabilities (uncertainty)\n",
    "            y_true: True labels\n",
    "            num_classes: Number of classes to visualize\n",
    "            class_names: List of class names\n",
    "        \"\"\"\n",
    "        n_classes = y_true.shape[1]\n",
    "        \n",
    "        # Select a subset of classes to visualize\n",
    "        classes_to_plot = np.random.choice(n_classes, min(num_classes, n_classes), replace=False)\n",
    "        \n",
    "        # Create figure\n",
    "        fig, axes = plt.subplots(len(classes_to_plot), 1, figsize=(10, 3 * len(classes_to_plot)))\n",
    "        \n",
    "        if len(classes_to_plot) == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for i, class_idx in enumerate(classes_to_plot):\n",
    "            ax = axes[i]\n",
    "            \n",
    "            # Get probabilities, uncertainties, and true labels for this class\n",
    "            probs = mean_probs[:, class_idx]\n",
    "            uncertainties = std_probs[:, class_idx]\n",
    "            true_labels = y_true[:, class_idx]\n",
    "            \n",
    "            # Create scatter plot\n",
    "            scatter = ax.scatter(probs, uncertainties, c=true_labels, cmap='coolwarm', alpha=0.6)\n",
    "            \n",
    "            # Add colorbar\n",
    "            cbar = plt.colorbar(scatter, ax=ax)\n",
    "            cbar.set_label('True Label')\n",
    "            \n",
    "            # Set class label\n",
    "            if class_names is not None:\n",
    "                class_label = class_names[class_idx]\n",
    "            else:\n",
    "                class_label = f\"Class {class_idx}\"\n",
    "            \n",
    "            ax.set_xlabel('Predicted Probability')\n",
    "            ax.set_ylabel('Uncertainty (Std. Dev.)')\n",
    "            ax.set_title(f'Uncertainty vs. Prediction for {class_label}')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add threshold line\n",
    "            ax.axvline(x=0.5, color='gray', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "    \n",
    "    def analyze_errors(self, mean_probs, std_probs, y_true, threshold=0.5):\n",
    "        \"\"\"\n",
    "        Analyze relationship between prediction errors and uncertainty.\n",
    "        \n",
    "        Args:\n",
    "            mean_probs: Mean probabilities from ensemble\n",
    "            std_probs: Standard deviation of probabilities (uncertainty)\n",
    "            y_true: True labels\n",
    "            threshold: Classification threshold\n",
    "            \n",
    "        Returns:\n",
    "            fig: Matplotlib figure\n",
    "        \"\"\"\n",
    "        # Make binary predictions\n",
    "        y_pred = (mean_probs > threshold).astype(float)\n",
    "        \n",
    "        # Calculate error\n",
    "        errors = np.abs(y_true - mean_probs)\n",
    "        \n",
    "        # Flatten arrays\n",
    "        flat_errors = errors.flatten()\n",
    "        flat_uncertainty = std_probs.flatten()\n",
    "        \n",
    "        # Create bins for uncertainty\n",
    "        n_bins = 20\n",
    "        bins = np.linspace(np.min(flat_uncertainty), np.max(flat_uncertainty), n_bins+1)\n",
    "        bin_indices = np.digitize(flat_uncertainty, bins) - 1\n",
    "        \n",
    "        # Calculate mean error for each bin\n",
    "        bin_mean_errors = np.zeros(n_bins)\n",
    "        bin_counts = np.zeros(n_bins)\n",
    "        \n",
    "        for i in range(len(flat_errors)):\n",
    "            bin_idx = bin_indices[i]\n",
    "            if bin_idx >= 0 and bin_idx < n_bins:\n",
    "                bin_mean_errors[bin_idx] += flat_errors[i]\n",
    "                bin_counts[bin_idx] += 1\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        valid_bins = bin_counts > 0\n",
    "        bin_mean_errors[valid_bins] /= bin_counts[valid_bins]\n",
    "        \n",
    "        # Create figure\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        \n",
    "        # Plot mean error vs. uncertainty\n",
    "        bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "        ax.plot(bin_centers, bin_mean_errors, 'o-', markersize=8)\n",
    "        \n",
    "        # Fit linear regression\n",
    "        valid_x = bin_centers[valid_bins]\n",
    "        valid_y = bin_mean_errors[valid_bins]\n",
    "        \n",
    "        if len(valid_x) > 1:\n",
    "            from sklearn.linear_model import LinearRegression\n",
    "            reg = LinearRegression().fit(valid_x.reshape(-1, 1), valid_y)\n",
    "            x_range = np.linspace(np.min(valid_x), np.max(valid_x), 100)\n",
    "            y_pred = reg.predict(x_range.reshape(-1, 1))\n",
    "            ax.plot(x_range, y_pred, 'r--', linewidth=2, \n",
    "                    label=f'Slope: {reg.coef_[0]:.4f}, R²: {reg.score(valid_x.reshape(-1, 1), valid_y):.4f}')\n",
    "            ax.legend()\n",
    "        \n",
    "        ax.set_xlabel('Uncertainty (Std. Dev.)')\n",
    "        ax.set_ylabel('Mean Absolute Error')\n",
    "        ax.set_title('Relationship Between Uncertainty and Prediction Error')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        return fig\n",
    "    \n",
    "    def calibration_curve(self, mean_probs, std_probs, y_true, n_bins=10):\n",
    "        \"\"\"\n",
    "        Plot calibration curve to analyze if predicted probabilities match observed frequencies.\n",
    "        \n",
    "        Args:\n",
    "            mean_probs: Mean probabilities from ensemble\n",
    "            std_probs: Standard deviation of probabilities (uncertainty)\n",
    "            y_true: True labels\n",
    "            n_bins: Number of bins for calibration curve\n",
    "            \n",
    "        Returns:\n",
    "            fig: Matplotlib figure\n",
    "        \"\"\"\n",
    "        # Flatten arrays\n",
    "        flat_probs = mean_probs.flatten()\n",
    "        flat_true = y_true.flatten()\n",
    "        flat_uncertainty = std_probs.flatten()\n",
    "        \n",
    "        # Create bins for probabilities\n",
    "        bins = np.linspace(0, 1, n_bins+1)\n",
    "        bin_indices = np.digitize(flat_probs, bins) - 1\n",
    "        \n",
    "        # Calculate observed frequency and mean predicted probability for each bin\n",
    "        bin_obs_freq = np.zeros(n_bins)\n",
    "        bin_pred_prob = np.zeros(n_bins)\n",
    "        bin_uncertainty = np.zeros(n_bins)\n",
    "        bin_counts = np.zeros(n_bins)\n",
    "        \n",
    "        for i in range(len(flat_probs)):\n",
    "            bin_idx = bin_indices[i]\n",
    "            if bin_idx >= 0 and bin_idx < n_bins:\n",
    "                bin_obs_freq[bin_idx] += flat_true[i]\n",
    "                bin_pred_prob[bin_idx] += flat_probs[i]\n",
    "                bin_uncertainty[bin_idx] += flat_uncertainty[i]\n",
    "                bin_counts[bin_idx] += 1\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        valid_bins = bin_counts > 0\n",
    "        bin_obs_freq[valid_bins] /= bin_counts[valid_bins]\n",
    "        bin_pred_prob[valid_bins] /= bin_counts[valid_bins]\n",
    "        bin_uncertainty[valid_bins] /= bin_counts[valid_bins]\n",
    "        \n",
    "        # Create figure\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        \n",
    "        # Plot calibration curve\n",
    "        bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "        ax.plot(bin_centers, bin_obs_freq, 'o-', markersize=8, label='Calibration Curve')\n",
    "        \n",
    "        # Plot perfect calibration\n",
    "        ax.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')\n",
    "        \n",
    "        # Plot uncertainties\n",
    "        ax2 = ax.twinx()\n",
    "        ax2.bar(bin_centers, bin_uncertainty, alpha=0.2, width=1/n_bins, color='r', label='Mean Uncertainty')\n",
    "        ax2.set_ylabel('Mean Uncertainty (Std. Dev.)', color='r')\n",
    "        ax2.tick_params(axis='y', labelcolor='r')\n",
    "        \n",
    "        # Add labels\n",
    "        ax.set_xlabel('Mean Predicted Probability')\n",
    "        ax.set_ylabel('Observed Frequency')\n",
    "        ax.set_title('Calibration Curve with Uncertainty')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add legends\n",
    "        lines1, labels1 = ax.get_legend_handles_labels()\n",
    "        lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "        ax.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "        \n",
    "        return fig\n",
    "    \n",
    "    def uncertainty_threshold(self, mean_probs, std_probs, y_true, threshold=0.5, uncertainty_percentiles=None):\n",
    "        \"\"\"\n",
    "        Analyze performance at different uncertainty thresholds.\n",
    "        \n",
    "        Args:\n",
    "            mean_probs: Mean probabilities from ensemble\n",
    "            std_probs: Standard deviation of probabilities (uncertainty)\n",
    "            y_true: True labels\n",
    "            threshold: Classification threshold\n",
    "            uncertainty_percentiles: List of uncertainty percentiles to evaluate\n",
    "            \n",
    "        Returns:\n",
    "            fig: Matplotlib figure\n",
    "            metrics: Dictionary of metrics at different uncertainty thresholds\n",
    "        \"\"\"\n",
    "        if uncertainty_percentiles is None:\n",
    "            uncertainty_percentiles = [0, 25, 50, 75, 90, 95]\n",
    "        \n",
    "        # Flatten arrays for uncertainty analysis\n",
    "        flat_uncertainty = std_probs.flatten()\n",
    "        \n",
    "        # Calculate uncertainty thresholds\n",
    "        uncertainty_thresholds = [np.percentile(flat_uncertainty, p) for p in uncertainty_percentiles]\n",
    "        \n",
    "        # Calculate metrics at different uncertainty thresholds\n",
    "        metrics = []\n",
    "        coverage = []\n",
    "        \n",
    "        for unc_thresh in uncertainty_thresholds:\n",
    "            # Create mask for samples below uncertainty threshold\n",
    "            mask = np.max(std_probs, axis=1) <= unc_thresh\n",
    "            \n",
    "            # Skip if no samples meet the criteria\n",
    "            if np.sum(mask) == 0:\n",
    "                metrics.append(None)\n",
    "                coverage.append(0)\n",
    "                continue\n",
    "            \n",
    "            # Filter predictions and true labels\n",
    "            filtered_probs = mean_probs[mask]\n",
    "            filtered_true = y_true[mask]\n",
    "            \n",
    "            # Make binary predictions\n",
    "            filtered_pred = (filtered_probs > threshold).astype(float)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            current_metrics = {\n",
    "                \"micro_f1\": f1_score(filtered_true, filtered_pred, average='micro'),\n",
    "                \"macro_f1\": f1_score(filtered_true, filtered_pred, average='macro'),\n",
    "                \"weighted_f1\": f1_score(filtered_true, filtered_pred, average='weighted'),\n",
    "                \"hamming_loss\": hamming_loss(filtered_true, filtered_pred),\n",
    "            }\n",
    "            \n",
    "            metrics.append(current_metrics)\n",
    "            coverage.append(np.mean(mask))\n",
    "        \n",
    "        # Create figure\n",
    "        fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "        \n",
    "        # Plot F1 score\n",
    "        f1_scores = [m[\"micro_f1\"] if m is not None else 0 for m in metrics]\n",
    "        ax1.plot(uncertainty_percentiles, f1_scores, 'bo-', label='Micro F1 Score')\n",
    "        \n",
    "        # Plot macro F1 score\n",
    "        macro_f1_scores = [m[\"macro_f1\"] if m is not None else 0 for m in metrics]\n",
    "        ax1.plot(uncertainty_percentiles, macro_f1_scores, 'go-', label='Macro F1 Score')\n",
    "        \n",
    "        # Plot coverage\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.plot(uncertainty_percentiles, coverage, 'r--', label='Data Coverage')\n",
    "        ax2.set_ylabel('Data Coverage', color='r')\n",
    "        ax2.tick_params(axis='y', labelcolor='r')\n",
    "        \n",
    "        # Add labels\n",
    "        ax1.set_xlabel('Uncertainty Percentile Threshold')\n",
    "        ax1.set_ylabel('F1 Score')\n",
    "        ax1.set_title('Performance vs. Uncertainty Threshold')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add legend\n",
    "        lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "        lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "        ax1.legend(lines1 + lines2, labels1 + labels2, loc='center right')\n",
    "        \n",
    "        return fig, {\"metrics\": metrics, \"coverage\": coverage, \"percentiles\": uncertainty_percentiles}\n",
    "    \n",
    "    def selective_prediction(self, mean_probs, std_probs, y_true, threshold=0.5):\n",
    "        \"\"\"\n",
    "        Perform selective prediction analysis.\n",
    "        \n",
    "        Args:\n",
    "            mean_probs: Mean probabilities from ensemble\n",
    "            std_probs: Standard deviation of probabilities (uncertainty)\n",
    "            y_true: True labels\n",
    "            threshold: Classification threshold\n",
    "            \n",
    "        Returns:\n",
    "            fig: Matplotlib figure\n",
    "        \"\"\"\n",
    "        # Calculate max uncertainty for each sample\n",
    "        max_uncertainties = np.max(std_probs, axis=1)\n",
    "        \n",
    "        # Sort samples by uncertainty\n",
    "        sorted_indices = np.argsort(max_uncertainties)\n",
    "        \n",
    "        # Initialize lists for storing results\n",
    "        coverages = []\n",
    "        f1_scores = []\n",
    "        \n",
    "        # Calculate metrics at different coverage levels\n",
    "        coverage_steps = np.linspace(0.1, 1.0, 10)\n",
    "        \n",
    "        for coverage in coverage_steps:\n",
    "            # Select top-k% most certain predictions\n",
    "            k = int(len(sorted_indices) * coverage)\n",
    "            selected_indices = sorted_indices[:k]\n",
    "            \n",
    "            # Filter predictions and true labels\n",
    "            selected_probs = mean_probs[selected_indices]\n",
    "            selected_true = y_true[selected_indices]\n",
    "            \n",
    "            # Make binary predictions\n",
    "            selected_pred = (selected_probs > threshold).astype(float)\n",
    "            \n",
    "            # Calculate F1 score\n",
    "            f1 = f1_score(selected_true, selected_pred, average='micro')\n",
    "            \n",
    "            coverages.append(coverage)\n",
    "            f1_scores.append(f1)\n",
    "        \n",
    "        # Create figure\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        \n",
    "        # Plot F1 score vs. coverage\n",
    "        ax.plot(coverages, f1_scores, 'bo-', markersize=8)\n",
    "        \n",
    "        # Add area under curve\n",
    "        ax.fill_between(coverages, 0, f1_scores, alpha=0.2)\n",
    "        \n",
    "        # Add labels\n",
    "        ax.set_xlabel('Coverage (Fraction of Data)')\n",
    "        ax.set_ylabel('Micro F1 Score')\n",
    "        ax.set_title('Selective Prediction: Performance vs. Coverage')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add area under the curve value\n",
    "        auc = np.trapz(f1_scores, coverages)\n",
    "        ax.text(0.05, 0.95, f'AUC: {auc:.4f}', transform=ax.transAxes, \n",
    "                fontsize=12, verticalalignment='top', bbox=dict(boxstyle='round', alpha=0.1))\n",
    "        \n",
    "        return fig\n",
    "\n",
    "# Function to run ensemble training and evaluation\n",
    "def train_and_evaluate_ensemble(\n",
    "    model_class,\n",
    "    model_args,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    test_loader,\n",
    "    train_function,\n",
    "    train_args,\n",
    "    num_models=5,\n",
    "    bootstrap=True,\n",
    "    device='cuda',\n",
    "    class_names=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Train and evaluate an ensemble model.\n",
    "    \n",
    "    Args:\n",
    "        model_class: The model class to use\n",
    "        model_args: Arguments for model initialization\n",
    "        train_loader: DataLoader for training data\n",
    "        val_loader: DataLoader for validation data\n",
    "        test_loader: DataLoader for test data\n",
    "        train_function: Function to train a single model\n",
    "        train_args: Arguments for the training function\n",
    "        num_models: Number of models in the ensemble\n",
    "        bootstrap: Whether to use bootstrapping for training\n",
    "        device: Device to use for computation\n",
    "        class_names: Names of the classes (optional)\n",
    "        \n",
    "    Returns:\n",
    "        ensemble: Trained ensemble model\n",
    "        metrics: Evaluation metrics\n",
    "        figures: Dictionary of visualization figures\n",
    "    \"\"\"\n",
    "    # Initialize wandb for the ensemble experiment\n",
    "    wandb.init(project=\"ALLSTARS_ensemble\", name=\"ensemble_experiment\", reinit=True)\n",
    "    \n",
    "    # Log ensemble configuration\n",
    "    wandb.config.update({\n",
    "        \"num_models\": num_models,\n",
    "        \"bootstrap\": bootstrap,\n",
    "        **model_args,\n",
    "        **train_args\n",
    "    })\n",
    "    \n",
    "    # Initialize ensemble\n",
    "    ensemble = EnsembleStarClassifier(\n",
    "        model_class=model_class,\n",
    "        model_args=model_args,\n",
    "        num_models=num_models,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Train ensemble\n",
    "    ensemble.train(\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader,\n",
    "        train_function=train_function,\n",
    "        train_args=train_args,\n",
    "        bootstrap=bootstrap\n",
    "    )\n",
    "    \n",
    "    # Evaluate ensemble\n",
    "    metrics, mean_probs, std_probs, y_true = ensemble.evaluate(\n",
    "        test_loader=test_loader,\n",
    "        return_predictions=True\n",
    "    )\n",
    "    \n",
    "    # Log evaluation metrics\n",
    "    wandb.log(metrics)\n",
    "    \n",
    "    # Create visualizations\n",
    "    figures = {}\n",
    "    \n",
    "    # Uncertainty visualization\n",
    "    uncertainty_fig = ensemble.visualize_uncertainty(\n",
    "        mean_probs=mean_probs,\n",
    "        std_probs=std_probs,\n",
    "        y_true=y_true,\n",
    "        num_classes=10,\n",
    "        class_names=class_names\n",
    "    )\n",
    "    figures['uncertainty'] = uncertainty_fig\n",
    "    \n",
    "    # Error analysis\n",
    "    error_fig = ensemble.analyze_errors(\n",
    "        mean_probs=mean_probs,\n",
    "        std_probs=std_probs,\n",
    "        y_true=y_true\n",
    "    )\n",
    "    figures['error_analysis'] = error_fig\n",
    "    \n",
    "    # Calibration curve\n",
    "    calibration_fig = ensemble.calibration_curve(\n",
    "        mean_probs=mean_probs,\n",
    "        std_probs=std_probs,\n",
    "        y_true=y_true\n",
    "    )\n",
    "    figures['calibration'] = calibration_fig\n",
    "    \n",
    "    # Uncertainty threshold analysis\n",
    "    threshold_fig, threshold_metrics = ensemble.uncertainty_threshold(\n",
    "        mean_probs=mean_probs,\n",
    "        std_probs=std_probs,\n",
    "        y_true=y_true\n",
    "    )\n",
    "    figures['threshold_analysis'] = threshold_fig\n",
    "    \n",
    "    # Selective prediction\n",
    "    selective_fig = ensemble.selective_prediction(\n",
    "        mean_probs=mean_probs,\n",
    "        std_probs=std_probs,\n",
    "        y_true=y_true\n",
    "    )\n",
    "    figures['selective_prediction'] = selective_fig\n",
    "    \n",
    "    # Log figures\n",
    "    for name, fig in figures.items():\n",
    "        wandb.log({name: wandb.Image(fig)})\n",
    "        plt.close(fig)\n",
    "    \n",
    "    # Finish wandb run\n",
    "    wandb.finish()\n",
    "    \n",
    "    return ensemble, metrics, figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Claude implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "class DeepEnsemble:\n",
    "    \"\"\"\n",
    "    Deep Ensemble implementation for uncertainty quantification in multi-label classification.\n",
    "    Trains multiple instances of the same model with different random initializations.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        model_class, \n",
    "        model_args, \n",
    "        num_models=5, \n",
    "        device='cuda'\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the deep ensemble.\n",
    "        \n",
    "        Args:\n",
    "            model_class: The model class to instantiate (e.g., StarClassifierFusion)\n",
    "            model_args: Dictionary of arguments to pass to the model constructor\n",
    "            num_models: Number of models in the ensemble\n",
    "            device: Device to run the models on ('cuda' or 'cpu')\n",
    "        \"\"\"\n",
    "        self.model_class = model_class\n",
    "        self.model_args = model_args\n",
    "        self.num_models = num_models\n",
    "        self.device = device\n",
    "        self.models = []\n",
    "        \n",
    "        # Initialize models with different random seeds\n",
    "        for i in range(num_models):\n",
    "            # Set seed for reproducibility but different for each model\n",
    "            torch.manual_seed(42 + i)\n",
    "            np.random.seed(42 + i)\n",
    "            \n",
    "            # Create model instance\n",
    "            model = model_class(**model_args).to(device)\n",
    "            self.models.append(model)\n",
    "    \n",
    "    def train(\n",
    "        self, \n",
    "        train_loader, \n",
    "        val_loader, \n",
    "        test_loader=None, \n",
    "        num_epochs=100, \n",
    "        lr=1e-4, \n",
    "        max_patience=20,\n",
    "        scheduler_type='OneCycleLR',\n",
    "        log_to_wandb=True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Train all models in the ensemble.\n",
    "        \n",
    "        Args:\n",
    "            train_loader: DataLoader for training data\n",
    "            val_loader: DataLoader for validation data\n",
    "            test_loader: DataLoader for test data (optional)\n",
    "            num_epochs: Maximum number of epochs to train\n",
    "            lr: Learning rate\n",
    "            max_patience: Maximum patience for early stopping\n",
    "            scheduler_type: Type of learning rate scheduler ('OneCycleLR' or 'ReduceLROnPlateau')\n",
    "            log_to_wandb: Whether to log training progress to wandb\n",
    "            \n",
    "        Returns:\n",
    "            List of trained models\n",
    "        \"\"\"\n",
    "        import wandb\n",
    "\n",
    "        for model_idx, model in enumerate(self.models):\n",
    "            print(f\"\\n----- Training Ensemble Model {model_idx+1}/{self.num_models} -----\\n\")\n",
    "            \n",
    "            # Initialize a new wandb run for each model\n",
    "            if log_to_wandb:\n",
    "                run = wandb.init(\n",
    "                    project=\"ALLSTARS_ensemble\", \n",
    "                    name=f\"model_{model_idx}\",\n",
    "                    group=\"ensemble_training\",\n",
    "                    config={\n",
    "                        **self.model_args,\n",
    "                        \"model_idx\": model_idx,\n",
    "                        \"num_models\": self.num_models,\n",
    "                        \"lr\": lr,\n",
    "                        \"max_patience\": max_patience,\n",
    "                        \"scheduler_type\": scheduler_type,\n",
    "                        \"num_epochs\": num_epochs\n",
    "                    },\n",
    "                    reinit=True\n",
    "                )\n",
    "            \n",
    "            optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "            \n",
    "            # Configure the scheduler\n",
    "            if scheduler_type == 'OneCycleLR':\n",
    "                scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "                    optimizer, \n",
    "                    max_lr=lr,\n",
    "                    epochs=num_epochs, \n",
    "                    steps_per_epoch=len(train_loader)\n",
    "                )\n",
    "            else:  # ReduceLROnPlateau\n",
    "                scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                    optimizer, \n",
    "                    mode='min', \n",
    "                    factor=0.5, \n",
    "                    patience=int(max_patience / 5)\n",
    "                )\n",
    "            \n",
    "            # We assume the datasets are MultiModalBalancedMultiLabelDataset\n",
    "            # that returns (X_spectra, X_gaia, y)\n",
    "            all_labels = []\n",
    "            for _, _, y_batch in train_loader:\n",
    "                all_labels.extend(y_batch.cpu().numpy())\n",
    "            \n",
    "            # Calculate class weights to handle imbalanced classes\n",
    "            class_weights = self._calculate_class_weights(np.array(all_labels))\n",
    "            class_weights = torch.tensor(class_weights, dtype=torch.float).to(self.device)\n",
    "            criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights)\n",
    "            \n",
    "            best_val_loss = float('inf')\n",
    "            patience = max_patience\n",
    "            best_model_state = None\n",
    "\n",
    "            # Training loop\n",
    "            for epoch in range(num_epochs):\n",
    "                # Resample training data for balanced batches\n",
    "                train_loader.dataset.re_sample()\n",
    "\n",
    "                # Recompute class weights based on the new sampling\n",
    "                all_labels = []\n",
    "                for _, _, y_batch in train_loader:\n",
    "                    all_labels.extend(y_batch.cpu().numpy())\n",
    "                class_weights = self._calculate_class_weights(np.array(all_labels))\n",
    "                class_weights = torch.tensor(class_weights, dtype=torch.float).to(self.device)\n",
    "                criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights)\n",
    "\n",
    "                # --- Training Phase ---\n",
    "                model.train()\n",
    "                train_loss, train_acc = 0.0, 0.0\n",
    "                for X_spc, X_ga, y_batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n",
    "                    X_spc, X_ga, y_batch = X_spc.to(self.device), X_ga.to(self.device), y_batch.to(self.device)\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(X_spc, X_ga)\n",
    "                    loss = criterion(outputs, y_batch)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    train_loss += loss.item() * X_spc.size(0)\n",
    "                    predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                    correct = (predicted == y_batch).float()\n",
    "                    train_acc += correct.mean(dim=1).sum().item()\n",
    "                \n",
    "                train_loss /= len(train_loader.dataset)\n",
    "                train_acc /= len(train_loader.dataset)\n",
    "\n",
    "                # --- Validation Phase ---\n",
    "                model.eval()\n",
    "                val_loss, val_acc = 0.0, 0.0\n",
    "                with torch.no_grad():\n",
    "                    for X_spc, X_ga, y_batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n",
    "                        X_spc, X_ga, y_batch = X_spc.to(self.device), X_ga.to(self.device), y_batch.to(self.device)\n",
    "                        outputs = model(X_spc, X_ga)\n",
    "                        loss = criterion(outputs, y_batch)\n",
    "                        val_loss += loss.item() * X_spc.size(0)\n",
    "                        predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                        correct = (predicted == y_batch).float()\n",
    "                        val_acc += correct.mean(dim=1).sum().item()\n",
    "                \n",
    "                val_loss /= len(val_loader.dataset)\n",
    "                val_acc /= len(val_loader.dataset)\n",
    "\n",
    "                # --- Test Phase (if provided) ---\n",
    "                test_metrics = {}\n",
    "                if test_loader is not None:\n",
    "                    test_loss, test_acc = 0.0, 0.0\n",
    "                    y_true, y_pred = [], []\n",
    "                    with torch.no_grad():\n",
    "                        for X_spc, X_ga, y_batch in tqdm(test_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Testing\"):\n",
    "                            X_spc, X_ga, y_batch = X_spc.to(self.device), X_ga.to(self.device), y_batch.to(self.device)\n",
    "                            outputs = model(X_spc, X_ga)\n",
    "                            loss = criterion(outputs, y_batch)\n",
    "                            test_loss += loss.item() * X_spc.size(0)\n",
    "                            \n",
    "                            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                            correct = (predicted == y_batch).float()\n",
    "                            test_acc += correct.mean(dim=1).sum().item()\n",
    "\n",
    "                            y_true.extend(y_batch.cpu().numpy())\n",
    "                            y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "                    test_loss /= len(test_loader.dataset)\n",
    "                    test_acc /= len(test_loader.dataset)\n",
    "                    test_metrics = self._calculate_metrics(np.array(y_true), np.array(y_pred))\n",
    "                    test_metrics.update({\n",
    "                        \"test_loss\": test_loss,\n",
    "                        \"test_acc\": test_acc,\n",
    "                    })\n",
    "\n",
    "                # Logging\n",
    "                if log_to_wandb:\n",
    "                    log_data = {\n",
    "                        \"epoch\": epoch,\n",
    "                        \"train_loss\": train_loss,\n",
    "                        \"val_loss\": val_loss,\n",
    "                        \"train_acc\": train_acc,\n",
    "                        \"val_acc\": val_acc,\n",
    "                        \"lr\": self._get_lr(optimizer)\n",
    "                    }\n",
    "                    log_data.update(test_metrics)\n",
    "                    wandb.log(log_data)\n",
    "\n",
    "                # Update learning rate scheduler\n",
    "                if scheduler_type == 'OneCycleLR':\n",
    "                    scheduler.step()\n",
    "                else:  # ReduceLROnPlateau\n",
    "                    scheduler.step(val_loss)\n",
    "\n",
    "                # Early stopping\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    patience = max_patience\n",
    "                    best_model_state = model.state_dict().copy()\n",
    "                    if log_to_wandb:\n",
    "                        wandb.run.summary[\"best_val_loss\"] = best_val_loss\n",
    "                else:\n",
    "                    patience -= 1\n",
    "                    if patience <= 0:\n",
    "                        print(\"Early stopping triggered.\")\n",
    "                        break\n",
    "\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs} - \"\n",
    "                      f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "                      f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "            # Load the best model state\n",
    "            if best_model_state is not None:\n",
    "                model.load_state_dict(best_model_state)\n",
    "            \n",
    "            # Close wandb run\n",
    "            if log_to_wandb:\n",
    "                wandb.finish()\n",
    "\n",
    "        return self.models\n",
    "    \n",
    "    def predict(self, loader, return_individual=False):\n",
    "        \"\"\"\n",
    "        Make predictions with the ensemble.\n",
    "        \n",
    "        Args:\n",
    "            loader: DataLoader for the data to predict\n",
    "            return_individual: Whether to return predictions from individual models\n",
    "            \n",
    "        Returns:\n",
    "            mean_probs: Mean probability across all models\n",
    "            std_probs: Standard deviation of probabilities (uncertainty measure)\n",
    "            individual_probs: (Optional) Predictions from each individual model\n",
    "        \"\"\"\n",
    "        all_probs = []\n",
    "        \n",
    "        for model in self.models:\n",
    "            model.eval()\n",
    "            model_probs = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for X_spc, X_ga, _ in loader:\n",
    "                    X_spc, X_ga = X_spc.to(self.device), X_ga.to(self.device)\n",
    "                    outputs = model(X_spc, X_ga)\n",
    "                    probs = torch.sigmoid(outputs).cpu().numpy()\n",
    "                    model_probs.append(probs)\n",
    "            \n",
    "            # Concatenate batches\n",
    "            model_probs = np.concatenate(model_probs, axis=0)\n",
    "            all_probs.append(model_probs)\n",
    "        \n",
    "        # Stack along a new axis to get shape (num_models, num_samples, num_classes)\n",
    "        all_probs = np.stack(all_probs, axis=0)\n",
    "        \n",
    "        # Calculate mean and std across models (axis=0)\n",
    "        mean_probs = np.mean(all_probs, axis=0)\n",
    "        std_probs = np.std(all_probs, axis=0)\n",
    "        \n",
    "        if return_individual:\n",
    "            return mean_probs, std_probs, all_probs\n",
    "        else:\n",
    "            return mean_probs, std_probs\n",
    "    \n",
    "    def predict_sample(self, X_spectra, X_gaia):\n",
    "        \"\"\"\n",
    "        Make predictions for a single sample.\n",
    "        \n",
    "        Args:\n",
    "            X_spectra: Spectral features (tensor)\n",
    "            X_gaia: Gaia features (tensor)\n",
    "            \n",
    "        Returns:\n",
    "            mean_probs: Mean probability across all models\n",
    "            std_probs: Standard deviation of probabilities (uncertainty measure)\n",
    "            all_probs: Predictions from each individual model\n",
    "        \"\"\"\n",
    "        # Ensure inputs are tensors with batch dimension\n",
    "        if len(X_spectra.shape) == 1:\n",
    "            X_spectra = X_spectra.unsqueeze(0)\n",
    "        if len(X_gaia.shape) == 1:\n",
    "            X_gaia = X_gaia.unsqueeze(0)\n",
    "        \n",
    "        X_spectra = X_spectra.to(self.device)\n",
    "        X_gaia = X_gaia.to(self.device)\n",
    "        \n",
    "        all_probs = []\n",
    "        for model in self.models:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                outputs = model(X_spectra, X_gaia)\n",
    "                probs = torch.sigmoid(outputs).cpu().numpy()\n",
    "                all_probs.append(probs)\n",
    "        \n",
    "        # Stack to get shape (num_models, 1, num_classes)\n",
    "        all_probs = np.stack(all_probs, axis=0)\n",
    "        \n",
    "        # Calculate mean and std across models\n",
    "        mean_probs = np.mean(all_probs, axis=0)\n",
    "        std_probs = np.std(all_probs, axis=0)\n",
    "        \n",
    "        return mean_probs[0], std_probs[0], all_probs[:, 0, :]\n",
    "    \n",
    "    def save_models(self, path_prefix):\n",
    "        \"\"\"\n",
    "        Save all models in the ensemble.\n",
    "        \n",
    "        Args:\n",
    "            path_prefix: Path prefix for saving models\n",
    "        \"\"\"\n",
    "        for i, model in enumerate(self.models):\n",
    "            torch.save(model.state_dict(), f\"{path_prefix}_model_{i}.pth\")\n",
    "    \n",
    "    def load_models(self, path_prefix):\n",
    "        \"\"\"\n",
    "        Load all models in the ensemble.\n",
    "        \n",
    "        Args:\n",
    "            path_prefix: Path prefix for loading models\n",
    "        \"\"\"\n",
    "        self.models = []\n",
    "        for i in range(self.num_models):\n",
    "            model = self.model_class(**self.model_args).to(self.device)\n",
    "            model.load_state_dict(torch.load(f\"{path_prefix}_model_{i}.pth\"))\n",
    "            model.eval()\n",
    "            self.models.append(model)\n",
    "    \n",
    "    def _calculate_class_weights(self, y):\n",
    "        \"\"\"Calculate class weights for handling imbalanced classes.\"\"\"\n",
    "        if y.ndim > 1:  \n",
    "            class_counts = np.sum(y, axis=0)  \n",
    "        else:\n",
    "            class_counts = np.bincount(y)\n",
    "\n",
    "        total_samples = y.shape[0] if y.ndim > 1 else len(y)\n",
    "        class_counts = np.where(class_counts == 0, 1, class_counts)  # Prevent division by zero\n",
    "        class_weights = total_samples / (len(class_counts) * class_counts)\n",
    "        \n",
    "        return class_weights\n",
    "    \n",
    "    def _calculate_metrics(self, y_true, y_pred):\n",
    "        \"\"\"Calculate evaluation metrics for multi-label classification.\"\"\"\n",
    "        from sklearn.metrics import f1_score, precision_score, recall_score, hamming_loss\n",
    "        \n",
    "        metrics = {\n",
    "            \"micro_f1\": f1_score(y_true, y_pred, average='micro'),\n",
    "            \"macro_f1\": f1_score(y_true, y_pred, average='macro'),\n",
    "            \"weighted_f1\": f1_score(y_true, y_pred, average='weighted'),\n",
    "            \"micro_precision\": precision_score(y_true, y_pred, average='micro', zero_division=1),\n",
    "            \"macro_precision\": precision_score(y_true, y_pred, average='macro', zero_division=1),\n",
    "            \"weighted_precision\": precision_score(y_true, y_pred, average='weighted', zero_division=1),\n",
    "            \"micro_recall\": recall_score(y_true, y_pred, average='micro'),\n",
    "            \"macro_recall\": recall_score(y_true, y_pred, average='macro'),\n",
    "            \"weighted_recall\": recall_score(y_true, y_pred, average='weighted'),\n",
    "            \"hamming_loss\": hamming_loss(y_true, y_pred)\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def _get_lr(self, optimizer):\n",
    "        \"\"\"Get current learning rate from optimizer.\"\"\"\n",
    "        for param_group in optimizer.param_groups:\n",
    "            return param_group['lr']\n",
    "\n",
    "class UncertaintyVisualizer:\n",
    "    \"\"\"\n",
    "    Utility class for visualizing uncertainty in multi-label classification.\n",
    "    \"\"\"\n",
    "    def __init__(self, class_names):\n",
    "        \"\"\"\n",
    "        Initialize the visualizer.\n",
    "        \n",
    "        Args:\n",
    "            class_names: List of class names\n",
    "        \"\"\"\n",
    "        self.class_names = class_names\n",
    "    \n",
    "    def plot_prediction_with_uncertainty(self, mean_probs, std_probs, true_labels=None, \n",
    "                                         threshold=0.5, top_k=10, figsize=(12, 8)):\n",
    "        \"\"\"\n",
    "        Plot prediction probabilities with uncertainty bars.\n",
    "        \n",
    "        Args:\n",
    "            mean_probs: Mean probabilities from ensemble\n",
    "            std_probs: Standard deviation of probabilities\n",
    "            true_labels: True labels (optional)\n",
    "            threshold: Decision threshold for positive prediction\n",
    "            top_k: Number of top predictions to show\n",
    "            figsize: Figure size\n",
    "            \n",
    "        Returns:\n",
    "            matplotlib figure\n",
    "        \"\"\"\n",
    "        # Get indices of top k predictions by mean probability\n",
    "        top_indices = np.argsort(mean_probs)[::-1][:top_k]\n",
    "        \n",
    "        # Extract top k values\n",
    "        top_means = mean_probs[top_indices]\n",
    "        top_stds = std_probs[top_indices]\n",
    "        top_classes = [self.class_names[i] for i in top_indices]\n",
    "        \n",
    "        # Create figure\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        \n",
    "        # Create horizontal bar chart with error bars\n",
    "        y_pos = np.arange(len(top_classes))\n",
    "        bar_colors = ['green' if prob >= threshold else 'red' for prob in top_means]\n",
    "        \n",
    "        # Plot bars\n",
    "        bars = ax.barh(y_pos, top_means, xerr=top_stds, align='center', \n",
    "                     alpha=0.7, color=bar_colors, capsize=5)\n",
    "        \n",
    "        # Add true labels if provided\n",
    "        if true_labels is not None:\n",
    "            true_label_indices = np.where(true_labels == 1)[0]\n",
    "            true_class_names = [self.class_names[i] for i in true_label_indices]\n",
    "            \n",
    "            # Mark true labels in the plot\n",
    "            for i, class_name in enumerate(top_classes):\n",
    "                if class_name in true_class_names:\n",
    "                    ax.get_children()[i].set_edgecolor('blue')\n",
    "                    ax.get_children()[i].set_linewidth(2)\n",
    "        \n",
    "        # Add threshold line\n",
    "        ax.axvline(x=threshold, color='gray', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Customize plot\n",
    "        ax.set_yticks(y_pos)\n",
    "        ax.set_yticklabels(top_classes)\n",
    "        ax.invert_yaxis()  # labels read top-to-bottom\n",
    "        ax.set_xlabel('Probability')\n",
    "        ax.set_title('Prediction with Uncertainty')\n",
    "        \n",
    "        # Add gridlines\n",
    "        ax.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Add legend\n",
    "        from matplotlib.patches import Patch\n",
    "        legend_elements = [\n",
    "            Patch(facecolor='green', edgecolor='black', label='Above threshold'),\n",
    "            Patch(facecolor='red', edgecolor='black', label='Below threshold')\n",
    "        ]\n",
    "        if true_labels is not None:\n",
    "            legend_elements.append(Patch(facecolor='white', edgecolor='blue', linewidth=2, label='True label'))\n",
    "        \n",
    "        ax.legend(handles=legend_elements, loc='lower right')\n",
    "        \n",
    "        # Add text annotations for probabilities\n",
    "        for i, (mean, std) in enumerate(zip(top_means, top_stds)):\n",
    "            ax.text(mean + 0.02, i, f'{mean:.2f} ± {std:.2f}', va='center')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "    \n",
    "    def plot_uncertainty_distribution(self, std_probs, predictions, figsize=(10, 6)):\n",
    "        \"\"\"\n",
    "        Plot histogram of uncertainty (standard deviation) distribution.\n",
    "        \n",
    "        Args:\n",
    "            std_probs: Standard deviation of probabilities (shape: num_samples x num_classes)\n",
    "            predictions: Binary predictions (shape: num_samples x num_classes)\n",
    "            figsize: Figure size\n",
    "            \n",
    "        Returns:\n",
    "            matplotlib figure\n",
    "        \"\"\"\n",
    "        # Flatten predictions and standard deviations\n",
    "        std_probs_flat = std_probs.flatten()\n",
    "        predictions_flat = predictions.flatten()\n",
    "        \n",
    "        # Create figure\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        \n",
    "        # Plot histograms\n",
    "        ax.hist(std_probs_flat, bins=50, alpha=0.5, label='All predictions')\n",
    "        ax.hist(std_probs_flat[predictions_flat == 1], bins=50, alpha=0.5, label='Positive predictions')\n",
    "        \n",
    "        # Customize plot\n",
    "        ax.set_xlabel('Uncertainty (Standard Deviation)')\n",
    "        ax.set_ylabel('Count')\n",
    "        ax.set_title('Distribution of Prediction Uncertainty')\n",
    "        ax.legend()\n",
    "        ax.grid(linestyle='--', alpha=0.7)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "    \n",
    "    def plot_uncertainty_vs_error(self, mean_probs, std_probs, true_labels, figsize=(10, 6)):\n",
    "        \"\"\"\n",
    "        Scatter plot of uncertainty vs prediction error.\n",
    "        \n",
    "        Args:\n",
    "            mean_probs: Mean probabilities from ensemble\n",
    "            std_probs: Standard deviation of probabilities\n",
    "            true_labels: True labels\n",
    "            figsize: Figure size\n",
    "            \n",
    "        Returns:\n",
    "            matplotlib figure\n",
    "        \"\"\"\n",
    "        # Calculate errors (absolute difference between mean probability and true label)\n",
    "        errors = np.abs(mean_probs - true_labels)\n",
    "        \n",
    "        # Flatten arrays\n",
    "        std_probs_flat = std_probs.flatten()\n",
    "        errors_flat = errors.flatten()\n",
    "        \n",
    "        # Create figure\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        \n",
    "        # Create scatter plot\n",
    "        scatter = ax.scatter(std_probs_flat, errors_flat, alpha=0.5, s=10)\n",
    "        \n",
    "        # Add trend line\n",
    "        from scipy.stats import linregress\n",
    "        slope, intercept, r_value, p_value, std_err = linregress(std_probs_flat, errors_flat)\n",
    "        x = np.linspace(std_probs_flat.min(), std_probs_flat.max(), 100)\n",
    "        y = slope * x + intercept\n",
    "        ax.plot(x, y, color='red', linestyle='--')\n",
    "        \n",
    "        # Add correlation coefficient\n",
    "        ax.text(0.95, 0.05, f'Correlation: {r_value:.3f}', transform=ax.transAxes, \n",
    "                ha='right', va='bottom', bbox=dict(facecolor='white', alpha=0.5))\n",
    "        \n",
    "        # Customize plot\n",
    "        ax.set_xlabel('Uncertainty (Standard Deviation)')\n",
    "        ax.set_ylabel('Prediction Error')\n",
    "        ax.set_title('Uncertainty vs Prediction Error')\n",
    "        ax.grid(linestyle='--', alpha=0.7)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "    \n",
    "    def plot_calibration_curve(self, mean_probs, std_probs, true_labels, n_bins=10, figsize=(10, 6)):\n",
    "        \"\"\"\n",
    "        Plot calibration curve to assess the quality of predicted probabilities.\n",
    "        \n",
    "        Args:\n",
    "            mean_probs: Mean probabilities from ensemble\n",
    "            std_probs: Standard deviation of probabilities (for coloring points)\n",
    "            true_labels: True labels\n",
    "            n_bins: Number of bins for the calibration curve\n",
    "            figsize: Figure size\n",
    "            \n",
    "        Returns:\n",
    "            matplotlib figure\n",
    "        \"\"\"\n",
    "        # Flatten arrays\n",
    "        mean_probs_flat = mean_probs.flatten()\n",
    "        std_probs_flat = std_probs.flatten()\n",
    "        true_labels_flat = true_labels.flatten()\n",
    "        \n",
    "        # Remove NaN values if any\n",
    "        valid_indices = ~np.isnan(mean_probs_flat) & ~np.isnan(true_labels_flat)\n",
    "        mean_probs_flat = mean_probs_flat[valid_indices]\n",
    "        std_probs_flat = std_probs_flat[valid_indices]\n",
    "        true_labels_flat = true_labels_flat[valid_indices]\n",
    "        \n",
    "        # Create bins and calculate calibration metrics\n",
    "        bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "        binned = np.digitize(mean_probs_flat, bins) - 1\n",
    "        bin_accs = np.zeros(n_bins)\n",
    "        bin_confs = np.zeros(n_bins)\n",
    "        bin_sizes = np.zeros(n_bins)\n",
    "        bin_uncerts = np.zeros(n_bins)\n",
    "        \n",
    "        for bin_idx in range(n_bins):\n",
    "            bin_mask = binned == bin_idx\n",
    "            if np.sum(bin_mask) > 0:\n",
    "                bin_sizes[bin_idx] = np.sum(bin_mask)\n",
    "                bin_accs[bin_idx] = np.mean(true_labels_flat[bin_mask])\n",
    "                bin_confs[bin_idx] = np.mean(mean_probs_flat[bin_mask])\n",
    "                bin_uncerts[bin_idx] = np.mean(std_probs_flat[bin_mask])\n",
    "        \n",
    "        # Calculate the expected calibration error (ECE)\n",
    "        ece = np.sum(bin_sizes / len(mean_probs_flat) * np.abs(bin_accs - bin_confs))\n",
    "        \n",
    "        # Create figure\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        \n",
    "        # Plot the calibration curve\n",
    "        ax.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfectly calibrated')\n",
    "        \n",
    "        # Color points by uncertainty\n",
    "        sc = ax.scatter(bin_confs, bin_accs, \n",
    "                      s=bin_sizes / np.sum(bin_sizes) * 2000,  # Size proportional to bin size\n",
    "                      c=bin_uncerts, cmap='viridis', alpha=0.8, \n",
    "                      linewidths=1, edgecolors='black')\n",
    "        \n",
    "        # Add colorbar for uncertainty\n",
    "        cbar = plt.colorbar(sc, ax=ax)\n",
    "        cbar.set_label('Mean uncertainty (std)')\n",
    "        \n",
    "        # Customize plot\n",
    "        ax.set_xlabel('Mean predicted probability')\n",
    "        ax.set_ylabel('Fraction of positives (accuracy)')\n",
    "        ax.set_xlim([0, 1])\n",
    "        ax.set_ylim([0, 1])\n",
    "        ax.set_title(f'Calibration Curve (ECE: {ece:.3f})')\n",
    "        ax.grid(linestyle='--', alpha=0.7)\n",
    "        ax.legend(loc='lower right')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Training Ensemble Model 1/5 -----\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ensemble_member_1</strong> at: <a href='https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_ensemble/runs/mtwjdp0q' target=\"_blank\">https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_ensemble/runs/mtwjdp0q</a><br> View project at: <a href='https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_ensemble' target=\"_blank\">https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_ensemble</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250303_115714-mtwjdp0q/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/joao/Documents/Star-Classifier/wandb/run-20250303_121712-tj3yvkl3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_ensemble/runs/tj3yvkl3' target=\"_blank\">model_0</a></strong> to <a href='https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_ensemble' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_ensemble' target=\"_blank\">https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_ensemble</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_ensemble/runs/tj3yvkl3' target=\"_blank\">https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_ensemble/runs/tj3yvkl3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 - Training: 100%|██████████| 15/15 [00:35<00:00,  2.36s/it]\n",
      "Epoch 1/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 31.59it/s]\n",
      "Epoch 1/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 34.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 - Train Loss: 0.5282, Train Acc: 0.7654, Val Loss: 0.3791, Val Acc: 0.9306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/200 - Training: 100%|██████████| 15/15 [00:02<00:00,  6.78it/s]\n",
      "Epoch 2/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 31.56it/s]\n",
      "Epoch 2/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 33.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/200 - Train Loss: 0.3296, Train Acc: 0.9550, Val Loss: 0.2726, Val Acc: 0.9647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.64it/s]\n",
      "Epoch 3/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 32.77it/s]\n",
      "Epoch 3/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 33.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/200 - Train Loss: 0.2537, Train Acc: 0.9638, Val Loss: 0.2222, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.61it/s]\n",
      "Epoch 4/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 30.43it/s]\n",
      "Epoch 4/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 33.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/200 - Train Loss: 0.2155, Train Acc: 0.9639, Val Loss: 0.1940, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.01it/s]\n",
      "Epoch 5/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 31.81it/s]\n",
      "Epoch 5/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 33.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/200 - Train Loss: 0.1929, Train Acc: 0.9639, Val Loss: 0.1762, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.46it/s]\n",
      "Epoch 6/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 31.85it/s]\n",
      "Epoch 6/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 33.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/200 - Train Loss: 0.1785, Train Acc: 0.9638, Val Loss: 0.1644, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.28it/s]\n",
      "Epoch 7/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 30.33it/s]\n",
      "Epoch 7/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 32.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/200 - Train Loss: 0.1687, Train Acc: 0.9638, Val Loss: 0.1558, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.52it/s]\n",
      "Epoch 8/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 31.27it/s]\n",
      "Epoch 8/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 34.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/200 - Train Loss: 0.1612, Train Acc: 0.9639, Val Loss: 0.1493, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.70it/s]\n",
      "Epoch 9/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 30.47it/s]\n",
      "Epoch 9/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 32.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/200 - Train Loss: 0.1553, Train Acc: 0.9638, Val Loss: 0.1437, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.61it/s]\n",
      "Epoch 10/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 31.62it/s]\n",
      "Epoch 10/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 30.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/200 - Train Loss: 0.1502, Train Acc: 0.9639, Val Loss: 0.1390, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.39it/s]\n",
      "Epoch 11/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 31.56it/s]\n",
      "Epoch 11/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 32.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/200 - Train Loss: 0.1459, Train Acc: 0.9639, Val Loss: 0.1347, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.17it/s]\n",
      "Epoch 12/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 31.34it/s]\n",
      "Epoch 12/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 32.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/200 - Train Loss: 0.1418, Train Acc: 0.9638, Val Loss: 0.1309, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.62it/s]\n",
      "Epoch 13/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 31.80it/s]\n",
      "Epoch 13/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 32.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/200 - Train Loss: 0.1381, Train Acc: 0.9640, Val Loss: 0.1271, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.50it/s]\n",
      "Epoch 14/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 32.05it/s]\n",
      "Epoch 14/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 32.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/200 - Train Loss: 0.1346, Train Acc: 0.9638, Val Loss: 0.1238, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.47it/s]\n",
      "Epoch 15/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 32.08it/s]\n",
      "Epoch 15/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 32.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/200 - Train Loss: 0.1314, Train Acc: 0.9637, Val Loss: 0.1206, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.56it/s]\n",
      "Epoch 16/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 32.09it/s]\n",
      "Epoch 16/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 33.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/200 - Train Loss: 0.1283, Train Acc: 0.9638, Val Loss: 0.1177, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.63it/s]\n",
      "Epoch 17/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 32.26it/s]\n",
      "Epoch 17/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 32.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/200 - Train Loss: 0.1256, Train Acc: 0.9640, Val Loss: 0.1149, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.59it/s]\n",
      "Epoch 18/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 31.86it/s]\n",
      "Epoch 18/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 34.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/200 - Train Loss: 0.1228, Train Acc: 0.9640, Val Loss: 0.1124, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  9.19it/s]\n",
      "Epoch 19/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 33.84it/s]\n",
      "Epoch 19/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 34.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/200 - Train Loss: 0.1201, Train Acc: 0.9639, Val Loss: 0.1098, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  9.09it/s]\n",
      "Epoch 20/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 31.80it/s]\n",
      "Epoch 20/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 32.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/200 - Train Loss: 0.1178, Train Acc: 0.9640, Val Loss: 0.1076, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.37it/s]\n",
      "Epoch 21/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 31.26it/s]\n",
      "Epoch 21/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 32.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/200 - Train Loss: 0.1155, Train Acc: 0.9639, Val Loss: 0.1053, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.04it/s]\n",
      "Epoch 22/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 31.11it/s]\n",
      "Epoch 22/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 31.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/200 - Train Loss: 0.1132, Train Acc: 0.9639, Val Loss: 0.1034, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  7.62it/s]\n",
      "Epoch 23/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 30.49it/s]\n",
      "Epoch 23/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 30.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/200 - Train Loss: 0.1111, Train Acc: 0.9639, Val Loss: 0.1014, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  7.85it/s]\n",
      "Epoch 24/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 29.98it/s]\n",
      "Epoch 24/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 32.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/200 - Train Loss: 0.1092, Train Acc: 0.9638, Val Loss: 0.0995, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.26it/s]\n",
      "Epoch 25/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 33.24it/s]\n",
      "Epoch 25/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 35.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/200 - Train Loss: 0.1072, Train Acc: 0.9638, Val Loss: 0.0977, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.49it/s]\n",
      "Epoch 26/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 32.25it/s]\n",
      "Epoch 26/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 35.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/200 - Train Loss: 0.1053, Train Acc: 0.9640, Val Loss: 0.0964, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  9.15it/s]\n",
      "Epoch 27/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 33.77it/s]\n",
      "Epoch 27/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 35.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/200 - Train Loss: 0.1036, Train Acc: 0.9638, Val Loss: 0.0946, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  9.15it/s]\n",
      "Epoch 28/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 33.91it/s]\n",
      "Epoch 28/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 35.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/200 - Train Loss: 0.1018, Train Acc: 0.9639, Val Loss: 0.0930, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.13it/s]\n",
      "Epoch 29/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 30.85it/s]\n",
      "Epoch 29/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 32.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/200 - Train Loss: 0.1002, Train Acc: 0.9639, Val Loss: 0.0918, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.33it/s]\n",
      "Epoch 30/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 33.14it/s]\n",
      "Epoch 30/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 31.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/200 - Train Loss: 0.0987, Train Acc: 0.9639, Val Loss: 0.0903, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.31it/s]\n",
      "Epoch 31/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 33.19it/s]\n",
      "Epoch 31/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 32.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/200 - Train Loss: 0.0972, Train Acc: 0.9640, Val Loss: 0.0891, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.36it/s]\n",
      "Epoch 32/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 31.48it/s]\n",
      "Epoch 32/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 34.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/200 - Train Loss: 0.0956, Train Acc: 0.9640, Val Loss: 0.0878, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.46it/s]\n",
      "Epoch 33/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 31.83it/s]\n",
      "Epoch 33/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 31.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/200 - Train Loss: 0.0942, Train Acc: 0.9640, Val Loss: 0.0866, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.58it/s]\n",
      "Epoch 34/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 31.09it/s]\n",
      "Epoch 34/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 30.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/200 - Train Loss: 0.0929, Train Acc: 0.9640, Val Loss: 0.0856, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.63it/s]\n",
      "Epoch 35/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 32.00it/s]\n",
      "Epoch 35/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 33.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/200 - Train Loss: 0.0915, Train Acc: 0.9639, Val Loss: 0.0843, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.64it/s]\n",
      "Epoch 36/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 32.09it/s]\n",
      "Epoch 36/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 33.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/200 - Train Loss: 0.0901, Train Acc: 0.9639, Val Loss: 0.0830, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.55it/s]\n",
      "Epoch 37/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 32.02it/s]\n",
      "Epoch 37/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 33.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/200 - Train Loss: 0.0889, Train Acc: 0.9639, Val Loss: 0.0824, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.58it/s]\n",
      "Epoch 38/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 31.86it/s]\n",
      "Epoch 38/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 33.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/200 - Train Loss: 0.0877, Train Acc: 0.9639, Val Loss: 0.0812, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.58it/s]\n",
      "Epoch 39/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 32.26it/s]\n",
      "Epoch 39/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 33.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/200 - Train Loss: 0.0864, Train Acc: 0.9639, Val Loss: 0.0805, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.55it/s]\n",
      "Epoch 40/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 32.05it/s]\n",
      "Epoch 40/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 33.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/200 - Train Loss: 0.0851, Train Acc: 0.9640, Val Loss: 0.0792, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.58it/s]\n",
      "Epoch 41/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 30.78it/s]\n",
      "Epoch 41/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 32.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/200 - Train Loss: 0.0841, Train Acc: 0.9639, Val Loss: 0.0784, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.54it/s]\n",
      "Epoch 42/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 31.90it/s]\n",
      "Epoch 42/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 33.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/200 - Train Loss: 0.0828, Train Acc: 0.9641, Val Loss: 0.0776, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.59it/s]\n",
      "Epoch 43/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 32.18it/s]\n",
      "Epoch 43/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 33.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/200 - Train Loss: 0.0817, Train Acc: 0.9640, Val Loss: 0.0765, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.57it/s]\n",
      "Epoch 44/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 31.74it/s]\n",
      "Epoch 44/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 30.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/200 - Train Loss: 0.0805, Train Acc: 0.9640, Val Loss: 0.0756, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.50it/s]\n",
      "Epoch 45/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 31.85it/s]\n",
      "Epoch 45/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 33.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/200 - Train Loss: 0.0794, Train Acc: 0.9640, Val Loss: 0.0749, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.59it/s]\n",
      "Epoch 46/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 31.94it/s]\n",
      "Epoch 46/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 33.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/200 - Train Loss: 0.0785, Train Acc: 0.9640, Val Loss: 0.0740, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.59it/s]\n",
      "Epoch 47/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 31.82it/s]\n",
      "Epoch 47/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 33.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/200 - Train Loss: 0.0779, Train Acc: 0.9641, Val Loss: 0.0739, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.60it/s]\n",
      "Epoch 48/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 32.01it/s]\n",
      "Epoch 48/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 33.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/200 - Train Loss: 0.0766, Train Acc: 0.9640, Val Loss: 0.0727, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.55it/s]\n",
      "Epoch 49/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 32.00it/s]\n",
      "Epoch 49/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 33.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/200 - Train Loss: 0.0758, Train Acc: 0.9641, Val Loss: 0.0719, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.59it/s]\n",
      "Epoch 50/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 31.87it/s]\n",
      "Epoch 50/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 33.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/200 - Train Loss: 0.0746, Train Acc: 0.9642, Val Loss: 0.0708, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.59it/s]\n",
      "Epoch 51/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 31.89it/s]\n",
      "Epoch 51/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 33.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/200 - Train Loss: 0.0737, Train Acc: 0.9641, Val Loss: 0.0705, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.57it/s]\n",
      "Epoch 52/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 32.06it/s]\n",
      "Epoch 52/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 33.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/200 - Train Loss: 0.0724, Train Acc: 0.9640, Val Loss: 0.0695, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.48it/s]\n",
      "Epoch 53/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 32.17it/s]\n",
      "Epoch 53/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 33.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/200 - Train Loss: 0.0716, Train Acc: 0.9643, Val Loss: 0.0689, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.61it/s]\n",
      "Epoch 54/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 32.03it/s]\n",
      "Epoch 54/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 33.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/200 - Train Loss: 0.0705, Train Acc: 0.9642, Val Loss: 0.0682, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.54it/s]\n",
      "Epoch 55/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 31.72it/s]\n",
      "Epoch 55/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 33.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/200 - Train Loss: 0.0696, Train Acc: 0.9643, Val Loss: 0.0681, Val Acc: 0.9649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.62it/s]\n",
      "Epoch 56/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 32.06it/s]\n",
      "Epoch 56/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 33.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/200 - Train Loss: 0.0687, Train Acc: 0.9643, Val Loss: 0.0667, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.55it/s]\n",
      "Epoch 57/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 31.72it/s]\n",
      "Epoch 57/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 33.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/200 - Train Loss: 0.0678, Train Acc: 0.9644, Val Loss: 0.0663, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.27it/s]\n",
      "Epoch 58/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 31.76it/s]\n",
      "Epoch 58/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 33.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/200 - Train Loss: 0.0670, Train Acc: 0.9644, Val Loss: 0.0660, Val Acc: 0.9649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.58it/s]\n",
      "Epoch 59/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 31.96it/s]\n",
      "Epoch 59/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 33.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/200 - Train Loss: 0.0660, Train Acc: 0.9645, Val Loss: 0.0646, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.56it/s]\n",
      "Epoch 60/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 31.92it/s]\n",
      "Epoch 60/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 33.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/200 - Train Loss: 0.0651, Train Acc: 0.9645, Val Loss: 0.0640, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.57it/s]\n",
      "Epoch 61/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 31.85it/s]\n",
      "Epoch 61/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 33.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/200 - Train Loss: 0.0641, Train Acc: 0.9645, Val Loss: 0.0636, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.60it/s]\n",
      "Epoch 62/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 31.92it/s]\n",
      "Epoch 62/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 33.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/200 - Train Loss: 0.0636, Train Acc: 0.9646, Val Loss: 0.0629, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  7.86it/s]\n",
      "Epoch 63/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 29.60it/s]\n",
      "Epoch 63/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 33.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/200 - Train Loss: 0.0627, Train Acc: 0.9645, Val Loss: 0.0618, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.38it/s]\n",
      "Epoch 64/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 31.83it/s]\n",
      "Epoch 64/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 32.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/200 - Train Loss: 0.0620, Train Acc: 0.9647, Val Loss: 0.0615, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.49it/s]\n",
      "Epoch 65/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 28.39it/s]\n",
      "Epoch 65/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 32.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/200 - Train Loss: 0.0608, Train Acc: 0.9646, Val Loss: 0.0610, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  7.90it/s]\n",
      "Epoch 66/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 29.34it/s]\n",
      "Epoch 66/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 31.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/200 - Train Loss: 0.0602, Train Acc: 0.9648, Val Loss: 0.0603, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  7.57it/s]\n",
      "Epoch 67/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 28.34it/s]\n",
      "Epoch 67/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 29.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/200 - Train Loss: 0.0593, Train Acc: 0.9649, Val Loss: 0.0598, Val Acc: 0.9651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68/200 - Training: 100%|██████████| 15/15 [00:02<00:00,  7.45it/s]\n",
      "Epoch 68/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 29.38it/s]\n",
      "Epoch 68/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 31.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/200 - Train Loss: 0.0588, Train Acc: 0.9649, Val Loss: 0.0594, Val Acc: 0.9652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  7.79it/s]\n",
      "Epoch 69/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 28.60it/s]\n",
      "Epoch 69/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 30.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/200 - Train Loss: 0.0578, Train Acc: 0.9651, Val Loss: 0.0585, Val Acc: 0.9654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 70/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  7.75it/s]\n",
      "Epoch 70/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 28.62it/s]\n",
      "Epoch 70/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 30.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/200 - Train Loss: 0.0572, Train Acc: 0.9652, Val Loss: 0.0580, Val Acc: 0.9652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 71/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  7.72it/s]\n",
      "Epoch 71/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 29.26it/s]\n",
      "Epoch 71/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 30.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/200 - Train Loss: 0.0564, Train Acc: 0.9652, Val Loss: 0.0576, Val Acc: 0.9654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 72/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  7.82it/s]\n",
      "Epoch 72/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 28.96it/s]\n",
      "Epoch 72/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 30.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/200 - Train Loss: 0.0559, Train Acc: 0.9655, Val Loss: 0.0570, Val Acc: 0.9654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 73/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  7.79it/s]\n",
      "Epoch 73/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 28.90it/s]\n",
      "Epoch 73/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 30.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/200 - Train Loss: 0.0549, Train Acc: 0.9654, Val Loss: 0.0567, Val Acc: 0.9654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 74/200 - Training: 100%|██████████| 15/15 [00:02<00:00,  7.46it/s]\n",
      "Epoch 74/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 28.82it/s]\n",
      "Epoch 74/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 27.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/200 - Train Loss: 0.0546, Train Acc: 0.9655, Val Loss: 0.0561, Val Acc: 0.9660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 75/200 - Training: 100%|██████████| 15/15 [00:02<00:00,  6.99it/s]\n",
      "Epoch 75/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 25.39it/s]\n",
      "Epoch 75/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 26.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/200 - Train Loss: 0.0539, Train Acc: 0.9657, Val Loss: 0.0555, Val Acc: 0.9653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 76/200 - Training: 100%|██████████| 15/15 [00:02<00:00,  6.85it/s]\n",
      "Epoch 76/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 25.45it/s]\n",
      "Epoch 76/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 26.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/200 - Train Loss: 0.0530, Train Acc: 0.9658, Val Loss: 0.0551, Val Acc: 0.9659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 77/200 - Training: 100%|██████████| 15/15 [00:02<00:00,  6.83it/s]\n",
      "Epoch 77/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 28.20it/s]\n",
      "Epoch 77/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 28.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/200 - Train Loss: 0.0522, Train Acc: 0.9660, Val Loss: 0.0547, Val Acc: 0.9658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 78/200 - Training: 100%|██████████| 15/15 [00:02<00:00,  7.45it/s]\n",
      "Epoch 78/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 26.88it/s]\n",
      "Epoch 78/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 28.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/200 - Train Loss: 0.0515, Train Acc: 0.9663, Val Loss: 0.0541, Val Acc: 0.9658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 79/200 - Training: 100%|██████████| 15/15 [00:02<00:00,  7.13it/s]\n",
      "Epoch 79/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 27.17it/s]\n",
      "Epoch 79/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 27.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/200 - Train Loss: 0.0510, Train Acc: 0.9664, Val Loss: 0.0535, Val Acc: 0.9663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 80/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  7.51it/s]\n",
      "Epoch 80/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 26.25it/s]\n",
      "Epoch 80/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 28.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/200 - Train Loss: 0.0502, Train Acc: 0.9666, Val Loss: 0.0531, Val Acc: 0.9660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 81/200 - Training: 100%|██████████| 15/15 [00:02<00:00,  7.25it/s]\n",
      "Epoch 81/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 26.18it/s]\n",
      "Epoch 81/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 29.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/200 - Train Loss: 0.0497, Train Acc: 0.9671, Val Loss: 0.0525, Val Acc: 0.9662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 82/200 - Training: 100%|██████████| 15/15 [00:02<00:00,  7.28it/s]\n",
      "Epoch 82/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 27.63it/s]\n",
      "Epoch 82/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 27.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/200 - Train Loss: 0.0495, Train Acc: 0.9668, Val Loss: 0.0522, Val Acc: 0.9669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 83/200 - Training: 100%|██████████| 15/15 [00:02<00:00,  7.06it/s]\n",
      "Epoch 83/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 25.29it/s]\n",
      "Epoch 83/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 26.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/200 - Train Loss: 0.0486, Train Acc: 0.9671, Val Loss: 0.0518, Val Acc: 0.9660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 84/200 - Training: 100%|██████████| 15/15 [00:02<00:00,  6.83it/s]\n",
      "Epoch 84/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 25.46it/s]\n",
      "Epoch 84/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 26.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/200 - Train Loss: 0.0483, Train Acc: 0.9670, Val Loss: 0.0513, Val Acc: 0.9666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 85/200 - Training: 100%|██████████| 15/15 [00:02<00:00,  6.98it/s]\n",
      "Epoch 85/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 25.17it/s]\n",
      "Epoch 85/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 26.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/200 - Train Loss: 0.0475, Train Acc: 0.9673, Val Loss: 0.0509, Val Acc: 0.9669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 86/200 - Training: 100%|██████████| 15/15 [00:02<00:00,  7.08it/s]\n",
      "Epoch 86/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 26.02it/s]\n",
      "Epoch 86/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 28.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/200 - Train Loss: 0.0466, Train Acc: 0.9678, Val Loss: 0.0506, Val Acc: 0.9665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 87/200 - Training: 100%|██████████| 15/15 [00:02<00:00,  7.18it/s]\n",
      "Epoch 87/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 28.46it/s]\n",
      "Epoch 87/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 30.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/200 - Train Loss: 0.0461, Train Acc: 0.9679, Val Loss: 0.0499, Val Acc: 0.9664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 88/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  7.76it/s]\n",
      "Epoch 88/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 28.93it/s]\n",
      "Epoch 88/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 30.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/200 - Train Loss: 0.0453, Train Acc: 0.9689, Val Loss: 0.0499, Val Acc: 0.9666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 89/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  7.78it/s]\n",
      "Epoch 89/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 29.07it/s]\n",
      "Epoch 89/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 30.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/200 - Train Loss: 0.0447, Train Acc: 0.9680, Val Loss: 0.0496, Val Acc: 0.9670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 90/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  7.73it/s]\n",
      "Epoch 90/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 29.01it/s]\n",
      "Epoch 90/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 29.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/200 - Train Loss: 0.0445, Train Acc: 0.9686, Val Loss: 0.0491, Val Acc: 0.9667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 91/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  7.77it/s]\n",
      "Epoch 91/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 28.73it/s]\n",
      "Epoch 91/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 30.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91/200 - Train Loss: 0.0438, Train Acc: 0.9687, Val Loss: 0.0485, Val Acc: 0.9675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 92/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  7.66it/s]\n",
      "Epoch 92/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 29.49it/s]\n",
      "Epoch 92/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 30.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92/200 - Train Loss: 0.0434, Train Acc: 0.9685, Val Loss: 0.0481, Val Acc: 0.9678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 93/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  7.89it/s]\n",
      "Epoch 93/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 29.40it/s]\n",
      "Epoch 93/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 30.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/200 - Train Loss: 0.0426, Train Acc: 0.9692, Val Loss: 0.0477, Val Acc: 0.9676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 94/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  7.87it/s]\n",
      "Epoch 94/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 29.43it/s]\n",
      "Epoch 94/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 30.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/200 - Train Loss: 0.0423, Train Acc: 0.9694, Val Loss: 0.0474, Val Acc: 0.9677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 95/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  7.97it/s]\n",
      "Epoch 95/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 29.50it/s]\n",
      "Epoch 95/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 30.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/200 - Train Loss: 0.0420, Train Acc: 0.9692, Val Loss: 0.0469, Val Acc: 0.9680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 96/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  7.90it/s]\n",
      "Epoch 96/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 29.61it/s]\n",
      "Epoch 96/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 30.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/200 - Train Loss: 0.0408, Train Acc: 0.9696, Val Loss: 0.0466, Val Acc: 0.9684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 97/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  7.92it/s]\n",
      "Epoch 97/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 29.41it/s]\n",
      "Epoch 97/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 30.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/200 - Train Loss: 0.0409, Train Acc: 0.9696, Val Loss: 0.0463, Val Acc: 0.9678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 98/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  7.91it/s]\n",
      "Epoch 98/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 29.52it/s]\n",
      "Epoch 98/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 30.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/200 - Train Loss: 0.0404, Train Acc: 0.9702, Val Loss: 0.0464, Val Acc: 0.9679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 99/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  7.96it/s]\n",
      "Epoch 99/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 29.39it/s]\n",
      "Epoch 99/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 30.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/200 - Train Loss: 0.0402, Train Acc: 0.9699, Val Loss: 0.0463, Val Acc: 0.9681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 100/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  7.91it/s]\n",
      "Epoch 100/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 29.54it/s]\n",
      "Epoch 100/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 30.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/200 - Train Loss: 0.0400, Train Acc: 0.9699, Val Loss: 0.0463, Val Acc: 0.9681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 101/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  7.94it/s]\n",
      "Epoch 101/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 30.12it/s]\n",
      "Epoch 101/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 31.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101/200 - Train Loss: 0.0399, Train Acc: 0.9701, Val Loss: 0.0454, Val Acc: 0.9688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 102/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  7.58it/s]\n",
      "Epoch 102/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 30.11it/s]\n",
      "Epoch 102/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 31.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 102/200 - Train Loss: 0.0388, Train Acc: 0.9704, Val Loss: 0.0450, Val Acc: 0.9684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 103/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.13it/s]\n",
      "Epoch 103/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 30.32it/s]\n",
      "Epoch 103/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 31.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103/200 - Train Loss: 0.0385, Train Acc: 0.9708, Val Loss: 0.0446, Val Acc: 0.9682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 104/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  7.95it/s]\n",
      "Epoch 104/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 30.35it/s]\n",
      "Epoch 104/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 31.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104/200 - Train Loss: 0.0385, Train Acc: 0.9705, Val Loss: 0.0446, Val Acc: 0.9697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 105/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.11it/s]\n",
      "Epoch 105/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 30.34it/s]\n",
      "Epoch 105/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 31.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105/200 - Train Loss: 0.0374, Train Acc: 0.9710, Val Loss: 0.0443, Val Acc: 0.9687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 106/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.08it/s]\n",
      "Epoch 106/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 30.39it/s]\n",
      "Epoch 106/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 31.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 106/200 - Train Loss: 0.0374, Train Acc: 0.9713, Val Loss: 0.0442, Val Acc: 0.9684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 107/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.04it/s]\n",
      "Epoch 107/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 29.92it/s]\n",
      "Epoch 107/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 30.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107/200 - Train Loss: 0.0369, Train Acc: 0.9710, Val Loss: 0.0440, Val Acc: 0.9689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 108/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  7.99it/s]\n",
      "Epoch 108/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 29.84it/s]\n",
      "Epoch 108/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 31.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/200 - Train Loss: 0.0372, Train Acc: 0.9711, Val Loss: 0.0435, Val Acc: 0.9692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 109/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  7.85it/s]\n",
      "Epoch 109/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 29.59it/s]\n",
      "Epoch 109/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 30.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 109/200 - Train Loss: 0.0357, Train Acc: 0.9721, Val Loss: 0.0430, Val Acc: 0.9693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 110/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  7.91it/s]\n",
      "Epoch 110/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 29.62it/s]\n",
      "Epoch 110/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 30.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 110/200 - Train Loss: 0.0349, Train Acc: 0.9717, Val Loss: 0.0427, Val Acc: 0.9693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 111/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  7.85it/s]\n",
      "Epoch 111/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 29.35it/s]\n",
      "Epoch 111/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 30.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/200 - Train Loss: 0.0347, Train Acc: 0.9726, Val Loss: 0.0423, Val Acc: 0.9695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 112/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  7.88it/s]\n",
      "Epoch 112/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 29.37it/s]\n",
      "Epoch 112/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 30.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/200 - Train Loss: 0.0346, Train Acc: 0.9724, Val Loss: 0.0419, Val Acc: 0.9698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 113/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  7.90it/s]\n",
      "Epoch 113/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 29.37it/s]\n",
      "Epoch 113/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 31.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113/200 - Train Loss: 0.0336, Train Acc: 0.9729, Val Loss: 0.0421, Val Acc: 0.9697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 114/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  7.88it/s]\n",
      "Epoch 114/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 29.49it/s]\n",
      "Epoch 114/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 30.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114/200 - Train Loss: 0.0337, Train Acc: 0.9727, Val Loss: 0.0416, Val Acc: 0.9695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 115/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  7.83it/s]\n",
      "Epoch 115/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 29.29it/s]\n",
      "Epoch 115/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 30.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/200 - Train Loss: 0.0331, Train Acc: 0.9728, Val Loss: 0.0415, Val Acc: 0.9695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 116/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  7.88it/s]\n",
      "Epoch 116/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 29.44it/s]\n",
      "Epoch 116/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 30.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 116/200 - Train Loss: 0.0332, Train Acc: 0.9725, Val Loss: 0.0415, Val Acc: 0.9699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 117/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  7.88it/s]\n",
      "Epoch 117/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 29.28it/s]\n",
      "Epoch 117/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 30.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117/200 - Train Loss: 0.0326, Train Acc: 0.9732, Val Loss: 0.0411, Val Acc: 0.9699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 118/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  7.87it/s]\n",
      "Epoch 118/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 29.41it/s]\n",
      "Epoch 118/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 30.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/200 - Train Loss: 0.0329, Train Acc: 0.9730, Val Loss: 0.0409, Val Acc: 0.9709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 119/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  7.96it/s]\n",
      "Epoch 119/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 29.53it/s]\n",
      "Epoch 119/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 31.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 119/200 - Train Loss: 0.0321, Train Acc: 0.9737, Val Loss: 0.0408, Val Acc: 0.9705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 120/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.13it/s]\n",
      "Epoch 120/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 29.98it/s]\n",
      "Epoch 120/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 32.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/200 - Train Loss: 0.0332, Train Acc: 0.9727, Val Loss: 0.0403, Val Acc: 0.9703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 121/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.23it/s]\n",
      "Epoch 121/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 31.14it/s]\n",
      "Epoch 121/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 32.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 121/200 - Train Loss: 0.0322, Train Acc: 0.9734, Val Loss: 0.0410, Val Acc: 0.9698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 122/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.27it/s]\n",
      "Epoch 122/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 30.75it/s]\n",
      "Epoch 122/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 31.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 122/200 - Train Loss: 0.0320, Train Acc: 0.9733, Val Loss: 0.0405, Val Acc: 0.9696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 123/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.26it/s]\n",
      "Epoch 123/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 30.93it/s]\n",
      "Epoch 123/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 32.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 123/200 - Train Loss: 0.0315, Train Acc: 0.9742, Val Loss: 0.0400, Val Acc: 0.9705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 124/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.28it/s]\n",
      "Epoch 124/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 30.79it/s]\n",
      "Epoch 124/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 32.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 124/200 - Train Loss: 0.0315, Train Acc: 0.9733, Val Loss: 0.0396, Val Acc: 0.9711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 125/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  7.73it/s]\n",
      "Epoch 125/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 30.82it/s]\n",
      "Epoch 125/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 31.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125/200 - Train Loss: 0.0305, Train Acc: 0.9742, Val Loss: 0.0397, Val Acc: 0.9702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 126/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.15it/s]\n",
      "Epoch 126/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 30.86it/s]\n",
      "Epoch 126/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 30.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 126/200 - Train Loss: 0.0299, Train Acc: 0.9741, Val Loss: 0.0393, Val Acc: 0.9711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 127/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.19it/s]\n",
      "Epoch 127/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 32.99it/s]\n",
      "Epoch 127/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 31.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 127/200 - Train Loss: 0.0305, Train Acc: 0.9746, Val Loss: 0.0394, Val Acc: 0.9704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 128/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.05it/s]\n",
      "Epoch 128/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 29.41it/s]\n",
      "Epoch 128/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 34.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 128/200 - Train Loss: 0.0295, Train Acc: 0.9745, Val Loss: 0.0390, Val Acc: 0.9711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 129/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.13it/s]\n",
      "Epoch 129/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 32.54it/s]\n",
      "Epoch 129/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 34.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 129/200 - Train Loss: 0.0301, Train Acc: 0.9745, Val Loss: 0.0390, Val Acc: 0.9708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 130/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.66it/s]\n",
      "Epoch 130/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 33.02it/s]\n",
      "Epoch 130/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 34.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 130/200 - Train Loss: 0.0292, Train Acc: 0.9752, Val Loss: 0.0390, Val Acc: 0.9713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 131/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.39it/s]\n",
      "Epoch 131/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 30.67it/s]\n",
      "Epoch 131/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 34.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 131/200 - Train Loss: 0.0294, Train Acc: 0.9749, Val Loss: 0.0391, Val Acc: 0.9710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 132/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.47it/s]\n",
      "Epoch 132/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 33.17it/s]\n",
      "Epoch 132/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 34.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 132/200 - Train Loss: 0.0291, Train Acc: 0.9750, Val Loss: 0.0386, Val Acc: 0.9709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 133/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.70it/s]\n",
      "Epoch 133/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 32.91it/s]\n",
      "Epoch 133/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 34.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 133/200 - Train Loss: 0.0289, Train Acc: 0.9747, Val Loss: 0.0382, Val Acc: 0.9710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 134/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.53it/s]\n",
      "Epoch 134/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 31.81it/s]\n",
      "Epoch 134/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 34.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 134/200 - Train Loss: 0.0284, Train Acc: 0.9756, Val Loss: 0.0386, Val Acc: 0.9720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 135/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.67it/s]\n",
      "Epoch 135/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 32.96it/s]\n",
      "Epoch 135/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 31.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 135/200 - Train Loss: 0.0275, Train Acc: 0.9765, Val Loss: 0.0377, Val Acc: 0.9717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 136/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.85it/s]\n",
      "Epoch 136/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 33.03it/s]\n",
      "Epoch 136/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 34.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 136/200 - Train Loss: 0.0281, Train Acc: 0.9754, Val Loss: 0.0378, Val Acc: 0.9720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 137/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.71it/s]\n",
      "Epoch 137/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 32.88it/s]\n",
      "Epoch 137/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 34.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 137/200 - Train Loss: 0.0274, Train Acc: 0.9767, Val Loss: 0.0377, Val Acc: 0.9722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 138/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.81it/s]\n",
      "Epoch 138/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 33.12it/s]\n",
      "Epoch 138/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 34.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 138/200 - Train Loss: 0.0277, Train Acc: 0.9764, Val Loss: 0.0377, Val Acc: 0.9722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 139/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.51it/s]\n",
      "Epoch 139/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 28.81it/s]\n",
      "Epoch 139/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 30.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 139/200 - Train Loss: 0.0279, Train Acc: 0.9767, Val Loss: 0.0376, Val Acc: 0.9723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 140/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.26it/s]\n",
      "Epoch 140/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 32.84it/s]\n",
      "Epoch 140/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 31.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 140/200 - Train Loss: 0.0274, Train Acc: 0.9759, Val Loss: 0.0381, Val Acc: 0.9728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 141/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.37it/s]\n",
      "Epoch 141/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 29.87it/s]\n",
      "Epoch 141/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 31.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 141/200 - Train Loss: 0.0278, Train Acc: 0.9764, Val Loss: 0.0378, Val Acc: 0.9736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 142/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.68it/s]\n",
      "Epoch 142/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 33.05it/s]\n",
      "Epoch 142/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 34.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142/200 - Train Loss: 0.0273, Train Acc: 0.9766, Val Loss: 0.0380, Val Acc: 0.9726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 143/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.82it/s]\n",
      "Epoch 143/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 33.18it/s]\n",
      "Epoch 143/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 34.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 143/200 - Train Loss: 0.0275, Train Acc: 0.9763, Val Loss: 0.0376, Val Acc: 0.9729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 144/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.46it/s]\n",
      "Epoch 144/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 30.75it/s]\n",
      "Epoch 144/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 34.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 144/200 - Train Loss: 0.0275, Train Acc: 0.9765, Val Loss: 0.0377, Val Acc: 0.9726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 145/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.34it/s]\n",
      "Epoch 145/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 33.04it/s]\n",
      "Epoch 145/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 34.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145/200 - Train Loss: 0.0275, Train Acc: 0.9766, Val Loss: 0.0377, Val Acc: 0.9731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 146/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.37it/s]\n",
      "Epoch 146/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 31.42it/s]\n",
      "Epoch 146/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 31.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 146/200 - Train Loss: 0.0264, Train Acc: 0.9773, Val Loss: 0.0368, Val Acc: 0.9727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 147/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.39it/s]\n",
      "Epoch 147/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 32.38it/s]\n",
      "Epoch 147/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 34.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 147/200 - Train Loss: 0.0264, Train Acc: 0.9776, Val Loss: 0.0368, Val Acc: 0.9746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 148/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.85it/s]\n",
      "Epoch 148/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 32.98it/s]\n",
      "Epoch 148/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 34.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 148/200 - Train Loss: 0.0256, Train Acc: 0.9781, Val Loss: 0.0363, Val Acc: 0.9736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 149/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.83it/s]\n",
      "Epoch 149/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 33.24it/s]\n",
      "Epoch 149/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 34.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 149/200 - Train Loss: 0.0251, Train Acc: 0.9783, Val Loss: 0.0364, Val Acc: 0.9738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 150/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.61it/s]\n",
      "Epoch 150/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 31.30it/s]\n",
      "Epoch 150/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 34.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150/200 - Train Loss: 0.0256, Train Acc: 0.9782, Val Loss: 0.0365, Val Acc: 0.9735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 151/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.75it/s]\n",
      "Epoch 151/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 33.20it/s]\n",
      "Epoch 151/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 31.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 151/200 - Train Loss: 0.0251, Train Acc: 0.9781, Val Loss: 0.0361, Val Acc: 0.9739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 152/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.83it/s]\n",
      "Epoch 152/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 32.44it/s]\n",
      "Epoch 152/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 32.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 152/200 - Train Loss: 0.0245, Train Acc: 0.9789, Val Loss: 0.0365, Val Acc: 0.9742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 153/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.53it/s]\n",
      "Epoch 153/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 33.25it/s]\n",
      "Epoch 153/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 35.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 153/200 - Train Loss: 0.0242, Train Acc: 0.9797, Val Loss: 0.0357, Val Acc: 0.9742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 154/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.51it/s]\n",
      "Epoch 154/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 31.45it/s]\n",
      "Epoch 154/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 34.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 154/200 - Train Loss: 0.0244, Train Acc: 0.9784, Val Loss: 0.0363, Val Acc: 0.9745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 155/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.32it/s]\n",
      "Epoch 155/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 30.69it/s]\n",
      "Epoch 155/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 34.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/200 - Train Loss: 0.0249, Train Acc: 0.9783, Val Loss: 0.0364, Val Acc: 0.9749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 156/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.71it/s]\n",
      "Epoch 156/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 33.30it/s]\n",
      "Epoch 156/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 34.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 156/200 - Train Loss: 0.0233, Train Acc: 0.9804, Val Loss: 0.0358, Val Acc: 0.9745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 157/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.82it/s]\n",
      "Epoch 157/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 33.32it/s]\n",
      "Epoch 157/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 33.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 157/200 - Train Loss: 0.0240, Train Acc: 0.9796, Val Loss: 0.0371, Val Acc: 0.9742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 158/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.63it/s]\n",
      "Epoch 158/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 28.41it/s]\n",
      "Epoch 158/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 34.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 158/200 - Train Loss: 0.0239, Train Acc: 0.9795, Val Loss: 0.0358, Val Acc: 0.9755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 159/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.87it/s]\n",
      "Epoch 159/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 33.15it/s]\n",
      "Epoch 159/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 32.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 159/200 - Train Loss: 0.0235, Train Acc: 0.9799, Val Loss: 0.0362, Val Acc: 0.9747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 160/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.85it/s]\n",
      "Epoch 160/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 32.30it/s]\n",
      "Epoch 160/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 34.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 160/200 - Train Loss: 0.0236, Train Acc: 0.9804, Val Loss: 0.0364, Val Acc: 0.9757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 161/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.72it/s]\n",
      "Epoch 161/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 33.32it/s]\n",
      "Epoch 161/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 33.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161/200 - Train Loss: 0.0232, Train Acc: 0.9805, Val Loss: 0.0362, Val Acc: 0.9749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 162/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.04it/s]\n",
      "Epoch 162/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 30.56it/s]\n",
      "Epoch 162/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 32.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 162/200 - Train Loss: 0.0242, Train Acc: 0.9796, Val Loss: 0.0360, Val Acc: 0.9763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 163/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.51it/s]\n",
      "Epoch 163/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 33.00it/s]\n",
      "Epoch 163/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 35.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 163/200 - Train Loss: 0.0230, Train Acc: 0.9812, Val Loss: 0.0363, Val Acc: 0.9748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 164/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.09it/s]\n",
      "Epoch 164/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 32.38it/s]\n",
      "Epoch 164/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 34.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 164/200 - Train Loss: 0.0231, Train Acc: 0.9806, Val Loss: 0.0359, Val Acc: 0.9761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 165/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.70it/s]\n",
      "Epoch 165/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 32.95it/s]\n",
      "Epoch 165/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 34.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 165/200 - Train Loss: 0.0228, Train Acc: 0.9811, Val Loss: 0.0354, Val Acc: 0.9752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 166/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.73it/s]\n",
      "Epoch 166/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 32.50it/s]\n",
      "Epoch 166/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 34.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 166/200 - Train Loss: 0.0218, Train Acc: 0.9810, Val Loss: 0.0354, Val Acc: 0.9765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 167/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.61it/s]\n",
      "Epoch 167/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 33.23it/s]\n",
      "Epoch 167/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 34.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 167/200 - Train Loss: 0.0220, Train Acc: 0.9820, Val Loss: 0.0354, Val Acc: 0.9764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 168/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.84it/s]\n",
      "Epoch 168/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 33.13it/s]\n",
      "Epoch 168/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 34.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 168/200 - Train Loss: 0.0233, Train Acc: 0.9809, Val Loss: 0.0352, Val Acc: 0.9768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 169/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.86it/s]\n",
      "Epoch 169/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 33.01it/s]\n",
      "Epoch 169/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 34.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 169/200 - Train Loss: 0.0224, Train Acc: 0.9810, Val Loss: 0.0352, Val Acc: 0.9760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 170/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.66it/s]\n",
      "Epoch 170/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 33.41it/s]\n",
      "Epoch 170/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 34.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170/200 - Train Loss: 0.0234, Train Acc: 0.9811, Val Loss: 0.0367, Val Acc: 0.9755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 171/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.60it/s]\n",
      "Epoch 171/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 32.72it/s]\n",
      "Epoch 171/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 34.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 171/200 - Train Loss: 0.0231, Train Acc: 0.9803, Val Loss: 0.0353, Val Acc: 0.9757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 172/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.84it/s]\n",
      "Epoch 172/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 33.32it/s]\n",
      "Epoch 172/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 33.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172/200 - Train Loss: 0.0228, Train Acc: 0.9811, Val Loss: 0.0353, Val Acc: 0.9762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 173/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  7.92it/s]\n",
      "Epoch 173/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 30.07it/s]\n",
      "Epoch 173/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 31.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 173/200 - Train Loss: 0.0237, Train Acc: 0.9801, Val Loss: 0.0366, Val Acc: 0.9762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 174/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.71it/s]\n",
      "Epoch 174/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 29.58it/s]\n",
      "Epoch 174/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 34.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 174/200 - Train Loss: 0.0247, Train Acc: 0.9796, Val Loss: 0.0354, Val Acc: 0.9762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 175/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.87it/s]\n",
      "Epoch 175/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 33.11it/s]\n",
      "Epoch 175/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 34.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 175/200 - Train Loss: 0.0254, Train Acc: 0.9797, Val Loss: 0.0359, Val Acc: 0.9753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 176/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.27it/s]\n",
      "Epoch 176/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 30.68it/s]\n",
      "Epoch 176/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 32.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 176/200 - Train Loss: 0.0235, Train Acc: 0.9817, Val Loss: 0.0357, Val Acc: 0.9756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 177/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.19it/s]\n",
      "Epoch 177/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 29.55it/s]\n",
      "Epoch 177/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 31.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 177/200 - Train Loss: 0.0231, Train Acc: 0.9798, Val Loss: 0.0355, Val Acc: 0.9763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 178/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.83it/s]\n",
      "Epoch 178/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 33.08it/s]\n",
      "Epoch 178/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 33.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 178/200 - Train Loss: 0.0236, Train Acc: 0.9807, Val Loss: 0.0365, Val Acc: 0.9746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 179/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.86it/s]\n",
      "Epoch 179/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 33.03it/s]\n",
      "Epoch 179/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 31.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 179/200 - Train Loss: 0.0224, Train Acc: 0.9809, Val Loss: 0.0351, Val Acc: 0.9766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 180/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.79it/s]\n",
      "Epoch 180/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 33.13it/s]\n",
      "Epoch 180/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 34.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 180/200 - Train Loss: 0.0222, Train Acc: 0.9807, Val Loss: 0.0349, Val Acc: 0.9765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 181/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.66it/s]\n",
      "Epoch 181/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 31.66it/s]\n",
      "Epoch 181/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 32.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181/200 - Train Loss: 0.0216, Train Acc: 0.9821, Val Loss: 0.0347, Val Acc: 0.9765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 182/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.89it/s]\n",
      "Epoch 182/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 33.17it/s]\n",
      "Epoch 182/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 34.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 182/200 - Train Loss: 0.0219, Train Acc: 0.9815, Val Loss: 0.0348, Val Acc: 0.9763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 183/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.83it/s]\n",
      "Epoch 183/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 33.14it/s]\n",
      "Epoch 183/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 34.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 183/200 - Train Loss: 0.0210, Train Acc: 0.9821, Val Loss: 0.0349, Val Acc: 0.9764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 184/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.84it/s]\n",
      "Epoch 184/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 33.04it/s]\n",
      "Epoch 184/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 34.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 184/200 - Train Loss: 0.0208, Train Acc: 0.9825, Val Loss: 0.0352, Val Acc: 0.9779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 185/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.83it/s]\n",
      "Epoch 185/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 33.27it/s]\n",
      "Epoch 185/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 34.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 185/200 - Train Loss: 0.0208, Train Acc: 0.9823, Val Loss: 0.0346, Val Acc: 0.9773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 186/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.81it/s]\n",
      "Epoch 186/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 32.23it/s]\n",
      "Epoch 186/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 35.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 186/200 - Train Loss: 0.0201, Train Acc: 0.9829, Val Loss: 0.0348, Val Acc: 0.9764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 187/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.88it/s]\n",
      "Epoch 187/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 33.25it/s]\n",
      "Epoch 187/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 34.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 187/200 - Train Loss: 0.0214, Train Acc: 0.9814, Val Loss: 0.0353, Val Acc: 0.9769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 188/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.81it/s]\n",
      "Epoch 188/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 33.01it/s]\n",
      "Epoch 188/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 34.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 188/200 - Train Loss: 0.0214, Train Acc: 0.9826, Val Loss: 0.0358, Val Acc: 0.9767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 189/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.87it/s]\n",
      "Epoch 189/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 33.09it/s]\n",
      "Epoch 189/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 34.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 189/200 - Train Loss: 0.0201, Train Acc: 0.9830, Val Loss: 0.0353, Val Acc: 0.9771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 190/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.78it/s]\n",
      "Epoch 190/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 32.82it/s]\n",
      "Epoch 190/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 34.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 190/200 - Train Loss: 0.0217, Train Acc: 0.9821, Val Loss: 0.0355, Val Acc: 0.9768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 191/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.85it/s]\n",
      "Epoch 191/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 33.15it/s]\n",
      "Epoch 191/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 34.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 191/200 - Train Loss: 0.0207, Train Acc: 0.9822, Val Loss: 0.0347, Val Acc: 0.9773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 192/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.87it/s]\n",
      "Epoch 192/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 33.08it/s]\n",
      "Epoch 192/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 35.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 192/200 - Train Loss: 0.0205, Train Acc: 0.9826, Val Loss: 0.0343, Val Acc: 0.9785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 193/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.84it/s]\n",
      "Epoch 193/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 33.17it/s]\n",
      "Epoch 193/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 34.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 193/200 - Train Loss: 0.0204, Train Acc: 0.9831, Val Loss: 0.0345, Val Acc: 0.9786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 194/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.83it/s]\n",
      "Epoch 194/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 33.01it/s]\n",
      "Epoch 194/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 34.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 194/200 - Train Loss: 0.0200, Train Acc: 0.9837, Val Loss: 0.0353, Val Acc: 0.9772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 195/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.84it/s]\n",
      "Epoch 195/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 33.18it/s]\n",
      "Epoch 195/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 34.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 195/200 - Train Loss: 0.0206, Train Acc: 0.9827, Val Loss: 0.0348, Val Acc: 0.9772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 196/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.60it/s]\n",
      "Epoch 196/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 32.22it/s]\n",
      "Epoch 196/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 33.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 196/200 - Train Loss: 0.0201, Train Acc: 0.9829, Val Loss: 0.0347, Val Acc: 0.9773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 197/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.60it/s]\n",
      "Epoch 197/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 31.80it/s]\n",
      "Epoch 197/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 32.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 197/200 - Train Loss: 0.0201, Train Acc: 0.9832, Val Loss: 0.0353, Val Acc: 0.9767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 198/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.40it/s]\n",
      "Epoch 198/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 30.74it/s]\n",
      "Epoch 198/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 32.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 198/200 - Train Loss: 0.0199, Train Acc: 0.9833, Val Loss: 0.0347, Val Acc: 0.9780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 199/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.51it/s]\n",
      "Epoch 199/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 31.47it/s]\n",
      "Epoch 199/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 32.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199/200 - Train Loss: 0.0198, Train Acc: 0.9828, Val Loss: 0.0347, Val Acc: 0.9777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 200/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.54it/s]\n",
      "Epoch 200/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 32.03it/s]\n",
      "Epoch 200/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 32.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200/200 - Train Loss: 0.0191, Train Acc: 0.9839, Val Loss: 0.0349, Val Acc: 0.9774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇▇██████</td></tr><tr><td>hamming_loss</td><td>████████████████▇▇▇▇▆▆▅▅▅▅▅▄▅▅▃▂▂▂▁▂▁▁▁▁</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▃▃▃▃▃▄▄▅▅▅▅▆▆▆▆▆▇▇▇█</td></tr><tr><td>macro_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▇▇████</td></tr><tr><td>macro_precision</td><td>▁███████████████▇▇██▇▇▇▇▇▇▇▇▆▇▆▆▆▇▆▆▆▆▆▆</td></tr><tr><td>macro_recall</td><td>▂▁▁▁▁▁▁▁▁▁▁▁▂▂▂▄▄▄▄▄▅▅▅▅▅▆▅▆▆▇▆▆▆▇███▇██</td></tr><tr><td>micro_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▂▃▂▂▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▇▇▇▇▇█▇██</td></tr><tr><td>micro_precision</td><td>██████████▁▁▆▁▆▅▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>micro_recall</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▃▃▃▄▄▄▄▅▅▆▆▆▆▇█▇▇██</td></tr><tr><td>test_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▃▃▄▄▄▄▄▅▅▄▅▆▇▇▇▆█▇██</td></tr><tr><td>test_loss</td><td>█▆▅▅▅▄▄▄▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▃▃▃▃▄▅▅▅▅▅▆▆▇▆▇▇▇▇▇▇▇█▇██</td></tr><tr><td>train_loss</td><td>█▇▇▇▆▅▅▅▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▆▆▆▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█████████</td></tr><tr><td>val_loss</td><td>█▅▅▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>weighted_f1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▃▃▃▃▄▄▄▄▄▄▄▅▅▅▇▇▇▇▇█▇███</td></tr><tr><td>weighted_precision</td><td>████████████████████▇▇▆▄▄▃▄▃▃▂▃▃▂▁▁▁▂▂▁▂</td></tr><tr><td>weighted_recall</td><td>▂▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▃▃▃▃▄▄▄▅▅▆▆▆▆▆▇▇▇█▇█████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_loss</td><td>0.03434</td></tr><tr><td>epoch</td><td>199</td></tr><tr><td>hamming_loss</td><td>0.02262</td></tr><tr><td>lr</td><td>4e-05</td></tr><tr><td>macro_f1</td><td>0.40395</td></tr><tr><td>macro_precision</td><td>0.81217</td></tr><tr><td>macro_recall</td><td>0.33485</td></tr><tr><td>micro_f1</td><td>0.56873</td></tr><tr><td>micro_precision</td><td>0.86506</td></tr><tr><td>micro_recall</td><td>0.42362</td></tr><tr><td>test_acc</td><td>0.97738</td></tr><tr><td>test_loss</td><td>0.04959</td></tr><tr><td>train_acc</td><td>0.98388</td></tr><tr><td>train_loss</td><td>0.01914</td></tr><tr><td>val_acc</td><td>0.97743</td></tr><tr><td>val_loss</td><td>0.03491</td></tr><tr><td>weighted_f1</td><td>0.53969</td></tr><tr><td>weighted_precision</td><td>0.87189</td></tr><tr><td>weighted_recall</td><td>0.42362</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">model_0</strong> at: <a href='https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_ensemble/runs/tj3yvkl3' target=\"_blank\">https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_ensemble/runs/tj3yvkl3</a><br> View project at: <a href='https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_ensemble' target=\"_blank\">https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_ensemble</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250303_121712-tj3yvkl3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjoaocsgalmeida\u001b[0m (\u001b[33mjoaocsgalmeida-university-of-southampton\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Training Ensemble Model 2/5 -----\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/joao/Documents/Star-Classifier/wandb/run-20250303_122643-yx75i3ne</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_ensemble/runs/yx75i3ne' target=\"_blank\">model_1</a></strong> to <a href='https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_ensemble' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_ensemble' target=\"_blank\">https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_ensemble</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_ensemble/runs/yx75i3ne' target=\"_blank\">https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_ensemble/runs/yx75i3ne</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.47it/s]\n",
      "Epoch 1/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 31.45it/s]\n",
      "Epoch 1/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 32.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 - Train Loss: 0.5215, Train Acc: 0.7864, Val Loss: 0.3781, Val Acc: 0.9531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.54it/s]\n",
      "Epoch 2/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 32.00it/s]\n",
      "Epoch 2/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 32.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/200 - Train Loss: 0.3298, Train Acc: 0.9616, Val Loss: 0.2757, Val Acc: 0.9649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.54it/s]\n",
      "Epoch 3/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 31.55it/s]\n",
      "Epoch 3/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 32.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/200 - Train Loss: 0.2587, Train Acc: 0.9638, Val Loss: 0.2283, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.54it/s]\n",
      "Epoch 4/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 31.94it/s]\n",
      "Epoch 4/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 32.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/200 - Train Loss: 0.2225, Train Acc: 0.9638, Val Loss: 0.2015, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.51it/s]\n",
      "Epoch 5/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 31.84it/s]\n",
      "Epoch 5/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 33.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/200 - Train Loss: 0.2011, Train Acc: 0.9638, Val Loss: 0.1843, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.56it/s]\n",
      "Epoch 6/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 31.87it/s]\n",
      "Epoch 6/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 32.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/200 - Train Loss: 0.1870, Train Acc: 0.9639, Val Loss: 0.1725, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.54it/s]\n",
      "Epoch 7/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 31.94it/s]\n",
      "Epoch 7/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 32.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/200 - Train Loss: 0.1769, Train Acc: 0.9639, Val Loss: 0.1635, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  7.98it/s]\n",
      "Epoch 8/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 26.55it/s]\n",
      "Epoch 8/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 29.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/200 - Train Loss: 0.1690, Train Acc: 0.9639, Val Loss: 0.1565, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.12it/s]\n",
      "Epoch 9/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 31.82it/s]\n",
      "Epoch 9/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 30.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/200 - Train Loss: 0.1626, Train Acc: 0.9638, Val Loss: 0.1506, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.01it/s]\n",
      "Epoch 10/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 31.55it/s]\n",
      "Epoch 10/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 32.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/200 - Train Loss: 0.1572, Train Acc: 0.9639, Val Loss: 0.1454, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.52it/s]\n",
      "Epoch 11/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 31.53it/s]\n",
      "Epoch 11/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 31.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/200 - Train Loss: 0.1524, Train Acc: 0.9639, Val Loss: 0.1408, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.39it/s]\n",
      "Epoch 12/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 26.64it/s]\n",
      "Epoch 12/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 29.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/200 - Train Loss: 0.1479, Train Acc: 0.9638, Val Loss: 0.1363, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/200 - Training: 100%|██████████| 15/15 [00:02<00:00,  7.40it/s]\n",
      "Epoch 13/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 28.55it/s]\n",
      "Epoch 13/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 28.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/200 - Train Loss: 0.1439, Train Acc: 0.9639, Val Loss: 0.1324, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.08it/s]\n",
      "Epoch 14/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 31.78it/s]\n",
      "Epoch 14/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 29.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/200 - Train Loss: 0.1401, Train Acc: 0.9639, Val Loss: 0.1287, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  7.85it/s]\n",
      "Epoch 15/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 27.31it/s]\n",
      "Epoch 15/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 28.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/200 - Train Loss: 0.1365, Train Acc: 0.9640, Val Loss: 0.1254, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  7.99it/s]\n",
      "Epoch 16/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 31.63it/s]\n",
      "Epoch 16/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 32.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/200 - Train Loss: 0.1332, Train Acc: 0.9638, Val Loss: 0.1219, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.50it/s]\n",
      "Epoch 17/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 31.81it/s]\n",
      "Epoch 17/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 32.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/200 - Train Loss: 0.1301, Train Acc: 0.9639, Val Loss: 0.1189, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.38it/s]\n",
      "Epoch 18/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 31.47it/s]\n",
      "Epoch 18/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 32.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/200 - Train Loss: 0.1271, Train Acc: 0.9639, Val Loss: 0.1161, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.41it/s]\n",
      "Epoch 19/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 30.77it/s]\n",
      "Epoch 19/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 34.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/200 - Train Loss: 0.1243, Train Acc: 0.9637, Val Loss: 0.1132, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.67it/s]\n",
      "Epoch 20/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 30.24it/s]\n",
      "Epoch 20/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 32.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/200 - Train Loss: 0.1216, Train Acc: 0.9638, Val Loss: 0.1108, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.54it/s]\n",
      "Epoch 21/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 33.21it/s]\n",
      "Epoch 21/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 35.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/200 - Train Loss: 0.1191, Train Acc: 0.9639, Val Loss: 0.1084, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.56it/s]\n",
      "Epoch 22/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 30.00it/s]\n",
      "Epoch 22/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 32.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/200 - Train Loss: 0.1167, Train Acc: 0.9638, Val Loss: 0.1061, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.72it/s]\n",
      "Epoch 23/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 33.48it/s]\n",
      "Epoch 23/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 30.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/200 - Train Loss: 0.1144, Train Acc: 0.9639, Val Loss: 0.1041, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.32it/s]\n",
      "Epoch 24/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 33.25it/s]\n",
      "Epoch 24/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 31.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/200 - Train Loss: 0.1122, Train Acc: 0.9638, Val Loss: 0.1019, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.23it/s]\n",
      "Epoch 25/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 33.30it/s]\n",
      "Epoch 25/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 32.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/200 - Train Loss: 0.1102, Train Acc: 0.9640, Val Loss: 0.1002, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.45it/s]\n",
      "Epoch 26/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 30.05it/s]\n",
      "Epoch 26/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 35.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/200 - Train Loss: 0.1082, Train Acc: 0.9638, Val Loss: 0.0982, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.45it/s]\n",
      "Epoch 27/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 29.37it/s]\n",
      "Epoch 27/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 35.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/200 - Train Loss: 0.1063, Train Acc: 0.9639, Val Loss: 0.0966, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.73it/s]\n",
      "Epoch 28/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 33.57it/s]\n",
      "Epoch 28/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 34.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/200 - Train Loss: 0.1044, Train Acc: 0.9638, Val Loss: 0.0949, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.70it/s]\n",
      "Epoch 29/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 33.43it/s]\n",
      "Epoch 29/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 35.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/200 - Train Loss: 0.1028, Train Acc: 0.9640, Val Loss: 0.0934, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.09it/s]\n",
      "Epoch 30/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 29.88it/s]\n",
      "Epoch 30/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 32.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/200 - Train Loss: 0.1009, Train Acc: 0.9639, Val Loss: 0.0920, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.37it/s]\n",
      "Epoch 31/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 31.72it/s]\n",
      "Epoch 31/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 34.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/200 - Train Loss: 0.0993, Train Acc: 0.9639, Val Loss: 0.0904, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.88it/s]\n",
      "Epoch 32/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 33.47it/s]\n",
      "Epoch 32/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 30.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/200 - Train Loss: 0.0977, Train Acc: 0.9640, Val Loss: 0.0893, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.15it/s]\n",
      "Epoch 33/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 28.36it/s]\n",
      "Epoch 33/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 34.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/200 - Train Loss: 0.0961, Train Acc: 0.9640, Val Loss: 0.0880, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.11it/s]\n",
      "Epoch 34/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 30.56it/s]\n",
      "Epoch 34/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 31.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/200 - Train Loss: 0.0945, Train Acc: 0.9639, Val Loss: 0.0867, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  7.97it/s]\n",
      "Epoch 35/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 30.31it/s]\n",
      "Epoch 35/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 31.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/200 - Train Loss: 0.0931, Train Acc: 0.9639, Val Loss: 0.0857, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.01it/s]\n",
      "Epoch 36/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 29.90it/s]\n",
      "Epoch 36/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 33.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/200 - Train Loss: 0.0918, Train Acc: 0.9640, Val Loss: 0.0844, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.31it/s]\n",
      "Epoch 37/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 33.10it/s]\n",
      "Epoch 37/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 32.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/200 - Train Loss: 0.0902, Train Acc: 0.9640, Val Loss: 0.0835, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.47it/s]\n",
      "Epoch 38/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 32.92it/s]\n",
      "Epoch 38/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 35.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/200 - Train Loss: 0.0888, Train Acc: 0.9640, Val Loss: 0.0825, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.67it/s]\n",
      "Epoch 39/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 33.11it/s]\n",
      "Epoch 39/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 34.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/200 - Train Loss: 0.0874, Train Acc: 0.9639, Val Loss: 0.0812, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.53it/s]\n",
      "Epoch 40/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 33.39it/s]\n",
      "Epoch 40/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 30.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/200 - Train Loss: 0.0860, Train Acc: 0.9640, Val Loss: 0.0801, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.69it/s]\n",
      "Epoch 41/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 33.49it/s]\n",
      "Epoch 41/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 35.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/200 - Train Loss: 0.0846, Train Acc: 0.9640, Val Loss: 0.0793, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  9.25it/s]\n",
      "Epoch 42/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 34.23it/s]\n",
      "Epoch 42/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 35.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/200 - Train Loss: 0.0834, Train Acc: 0.9640, Val Loss: 0.0783, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.44it/s]\n",
      "Epoch 43/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 34.23it/s]\n",
      "Epoch 43/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 33.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/200 - Train Loss: 0.0824, Train Acc: 0.9639, Val Loss: 0.0774, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.30it/s]\n",
      "Epoch 44/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 33.91it/s]\n",
      "Epoch 44/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 35.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/200 - Train Loss: 0.0812, Train Acc: 0.9640, Val Loss: 0.0765, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.64it/s]\n",
      "Epoch 45/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 32.25it/s]\n",
      "Epoch 45/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 31.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/200 - Train Loss: 0.0797, Train Acc: 0.9639, Val Loss: 0.0752, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.89it/s]\n",
      "Epoch 46/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 34.19it/s]\n",
      "Epoch 46/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 34.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/200 - Train Loss: 0.0785, Train Acc: 0.9640, Val Loss: 0.0746, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.78it/s]\n",
      "Epoch 47/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 31.64it/s]\n",
      "Epoch 47/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 33.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/200 - Train Loss: 0.0773, Train Acc: 0.9640, Val Loss: 0.0740, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  9.18it/s]\n",
      "Epoch 48/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 33.75it/s]\n",
      "Epoch 48/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 35.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/200 - Train Loss: 0.0764, Train Acc: 0.9640, Val Loss: 0.0733, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  9.10it/s]\n",
      "Epoch 49/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 34.33it/s]\n",
      "Epoch 49/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 35.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/200 - Train Loss: 0.0758, Train Acc: 0.9641, Val Loss: 0.0720, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.74it/s]\n",
      "Epoch 50/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 33.13it/s]\n",
      "Epoch 50/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 31.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/200 - Train Loss: 0.0744, Train Acc: 0.9641, Val Loss: 0.0717, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.98it/s]\n",
      "Epoch 51/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 32.98it/s]\n",
      "Epoch 51/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 35.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/200 - Train Loss: 0.0732, Train Acc: 0.9643, Val Loss: 0.0708, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.91it/s]\n",
      "Epoch 52/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 31.81it/s]\n",
      "Epoch 52/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 33.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/200 - Train Loss: 0.0723, Train Acc: 0.9642, Val Loss: 0.0703, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  9.12it/s]\n",
      "Epoch 53/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 34.32it/s]\n",
      "Epoch 53/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 35.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/200 - Train Loss: 0.0713, Train Acc: 0.9642, Val Loss: 0.0693, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.21it/s]\n",
      "Epoch 54/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 29.82it/s]\n",
      "Epoch 54/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 29.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/200 - Train Loss: 0.0701, Train Acc: 0.9642, Val Loss: 0.0685, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  7.94it/s]\n",
      "Epoch 55/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 29.99it/s]\n",
      "Epoch 55/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 31.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/200 - Train Loss: 0.0694, Train Acc: 0.9642, Val Loss: 0.0677, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.34it/s]\n",
      "Epoch 56/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 28.61it/s]\n",
      "Epoch 56/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 29.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/200 - Train Loss: 0.0683, Train Acc: 0.9643, Val Loss: 0.0667, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  7.71it/s]\n",
      "Epoch 57/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 32.65it/s]\n",
      "Epoch 57/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 33.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/200 - Train Loss: 0.0674, Train Acc: 0.9644, Val Loss: 0.0663, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.68it/s]\n",
      "Epoch 58/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 32.58it/s]\n",
      "Epoch 58/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 33.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/200 - Train Loss: 0.0665, Train Acc: 0.9645, Val Loss: 0.0655, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.74it/s]\n",
      "Epoch 59/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 33.77it/s]\n",
      "Epoch 59/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 35.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/200 - Train Loss: 0.0657, Train Acc: 0.9646, Val Loss: 0.0650, Val Acc: 0.9651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.48it/s]\n",
      "Epoch 60/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 33.89it/s]\n",
      "Epoch 60/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 27.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/200 - Train Loss: 0.0650, Train Acc: 0.9644, Val Loss: 0.0644, Val Acc: 0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.44it/s]\n",
      "Epoch 61/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 31.22it/s]\n",
      "Epoch 61/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 32.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/200 - Train Loss: 0.0640, Train Acc: 0.9646, Val Loss: 0.0638, Val Acc: 0.9651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.47it/s]\n",
      "Epoch 62/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 31.32it/s]\n",
      "Epoch 62/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 32.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/200 - Train Loss: 0.0631, Train Acc: 0.9646, Val Loss: 0.0630, Val Acc: 0.9651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.49it/s]\n",
      "Epoch 63/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 32.00it/s]\n",
      "Epoch 63/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 32.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/200 - Train Loss: 0.0622, Train Acc: 0.9648, Val Loss: 0.0623, Val Acc: 0.9651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.50it/s]\n",
      "Epoch 64/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 32.03it/s]\n",
      "Epoch 64/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 32.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/200 - Train Loss: 0.0615, Train Acc: 0.9647, Val Loss: 0.0616, Val Acc: 0.9651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.55it/s]\n",
      "Epoch 65/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 31.80it/s]\n",
      "Epoch 65/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 32.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/200 - Train Loss: 0.0609, Train Acc: 0.9648, Val Loss: 0.0608, Val Acc: 0.9652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.47it/s]\n",
      "Epoch 66/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 32.12it/s]\n",
      "Epoch 66/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 31.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/200 - Train Loss: 0.0601, Train Acc: 0.9650, Val Loss: 0.0604, Val Acc: 0.9651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.33it/s]\n",
      "Epoch 67/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 31.62it/s]\n",
      "Epoch 67/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 32.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/200 - Train Loss: 0.0594, Train Acc: 0.9652, Val Loss: 0.0602, Val Acc: 0.9651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.52it/s]\n",
      "Epoch 68/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 32.13it/s]\n",
      "Epoch 68/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 32.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/200 - Train Loss: 0.0587, Train Acc: 0.9651, Val Loss: 0.0598, Val Acc: 0.9652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.53it/s]\n",
      "Epoch 69/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 32.08it/s]\n",
      "Epoch 69/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 32.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/200 - Train Loss: 0.0581, Train Acc: 0.9651, Val Loss: 0.0587, Val Acc: 0.9653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 70/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.41it/s]\n",
      "Epoch 70/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 31.07it/s]\n",
      "Epoch 70/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 32.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/200 - Train Loss: 0.0573, Train Acc: 0.9653, Val Loss: 0.0583, Val Acc: 0.9654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 71/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.50it/s]\n",
      "Epoch 71/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 31.95it/s]\n",
      "Epoch 71/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 32.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/200 - Train Loss: 0.0561, Train Acc: 0.9653, Val Loss: 0.0575, Val Acc: 0.9655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 72/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.54it/s]\n",
      "Epoch 72/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 31.86it/s]\n",
      "Epoch 72/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 32.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/200 - Train Loss: 0.0557, Train Acc: 0.9657, Val Loss: 0.0574, Val Acc: 0.9655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 73/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.54it/s]\n",
      "Epoch 73/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 31.92it/s]\n",
      "Epoch 73/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 32.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/200 - Train Loss: 0.0549, Train Acc: 0.9660, Val Loss: 0.0569, Val Acc: 0.9655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 74/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.90it/s]\n",
      "Epoch 74/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 33.85it/s]\n",
      "Epoch 74/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 34.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/200 - Train Loss: 0.0543, Train Acc: 0.9659, Val Loss: 0.0563, Val Acc: 0.9661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 75/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.71it/s]\n",
      "Epoch 75/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 31.98it/s]\n",
      "Epoch 75/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 32.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/200 - Train Loss: 0.0537, Train Acc: 0.9660, Val Loss: 0.0557, Val Acc: 0.9660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 76/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.68it/s]\n",
      "Epoch 76/200 - Validation: 100%|██████████| 12/12 [00:00<00:00, 34.96it/s]\n",
      "Epoch 76/200 - Testing: 100%|██████████| 13/13 [00:00<00:00, 34.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/200 - Train Loss: 0.0531, Train Acc: 0.9662, Val Loss: 0.0555, Val Acc: 0.9657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 77/200 - Training: 100%|██████████| 15/15 [00:01<00:00,  8.61it/s]\n",
      "Epoch 77/200 - Validation:   0%|          | 0/12 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 51\u001b[0m\n\u001b[1;32m     40\u001b[0m ensemble \u001b[38;5;241m=\u001b[39m DeepEnsemble(\n\u001b[1;32m     41\u001b[0m     model_class\u001b[38;5;241m=\u001b[39mStarClassifierFusion,\n\u001b[1;32m     42\u001b[0m     model_args\u001b[38;5;241m=\u001b[39mmodel_args,\n\u001b[1;32m     43\u001b[0m     num_models\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,  \u001b[38;5;66;03m# Number of models in the ensemble\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[1;32m     45\u001b[0m )\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Load your data loaders as you did in the original code\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# ...\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Train the ensemble\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m trained_models \u001b[38;5;241m=\u001b[39m \u001b[43mensemble\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_patience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mOneCycleLR\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_to_wandb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     60\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Save the ensemble models\u001b[39;00m\n\u001b[1;32m     63\u001b[0m ensemble\u001b[38;5;241m.\u001b[39msave_models(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensemble_mamba_v1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[12], line 165\u001b[0m, in \u001b[0;36mDeepEnsemble.train\u001b[0;34m(self, train_loader, val_loader, test_loader, num_epochs, lr, max_patience, scheduler_type, log_to_wandb)\u001b[0m\n\u001b[1;32m    163\u001b[0m val_loss, val_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 165\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mX_spc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_ga\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEpoch \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m - Validation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_spc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_ga\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mX_spc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_ga\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_spc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_ga\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m~/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m~/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:212\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m--> 212\u001b[0m         \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[1;32m    214\u001b[0m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:155\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m~/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py:272\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    270\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    271\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import wandb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Import your model and the ensemble implementation\n",
    "#from your_model_file import StarClassifierFusion, MultiModalBalancedMultiLabelDataset\n",
    "#from ensemble_uncertainty import DeepEnsemble, UncertaintyVisualizer\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    d_model_spectra = 256\n",
    "    d_model_gaia = 256  # tiny\n",
    "    num_classes = 55\n",
    "    input_dim_spectra = 3647\n",
    "    input_dim_gaia = 18\n",
    "    n_layers = 12\n",
    "    lr = 2.5e-4\n",
    "    patience = 600\n",
    "    num_epochs = 200\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Initialize wandb\n",
    "    wandb.init(project=\"ALLSTARS_ensemble_uncertainty\")\n",
    "    \n",
    "    # Model arguments\n",
    "    model_args = {\n",
    "        \"d_model_spectra\": d_model_spectra,\n",
    "        \"d_model_gaia\": d_model_gaia,\n",
    "        \"num_classes\": num_classes,\n",
    "        \"input_dim_spectra\": input_dim_spectra,\n",
    "        \"input_dim_gaia\": input_dim_gaia,\n",
    "        \"n_layers\": n_layers,\n",
    "        \"use_cross_attention\": True,\n",
    "        \"n_cross_attn_heads\": 8\n",
    "    }\n",
    "    \n",
    "    # Create deep ensemble with 5 models\n",
    "    ensemble = DeepEnsemble(\n",
    "        model_class=StarClassifierFusion,\n",
    "        model_args=model_args,\n",
    "        num_models=5,  # Number of models in the ensemble\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Load your data loaders as you did in the original code\n",
    "    # ...\n",
    "    \n",
    "    # Train the ensemble\n",
    "    trained_models = ensemble.train(\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader,\n",
    "        num_epochs=num_epochs,\n",
    "        lr=lr,\n",
    "        max_patience=patience,\n",
    "        scheduler_type='OneCycleLR',\n",
    "        log_to_wandb=True\n",
    "    )\n",
    "    \n",
    "    # Save the ensemble models\n",
    "    ensemble.save_models(\"ensemble_mamba_v1\")\n",
    "    \n",
    "    # Evaluate the ensemble on test set\n",
    "    mean_probs, std_probs = ensemble.predict(test_loader)\n",
    "    \n",
    "    # Convert mean probabilities to binary predictions\n",
    "    threshold = 0.5\n",
    "    predictions = (mean_probs >= threshold).astype(float)\n",
    "    \n",
    "    # Get true labels from test set\n",
    "    true_labels = []\n",
    "    for _, _, y_batch in test_loader:\n",
    "        true_labels.extend(y_batch.cpu().numpy())\n",
    "    true_labels = np.array(true_labels)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "    metrics = {\n",
    "        \"micro_f1\": f1_score(true_labels, predictions, average='micro'),\n",
    "        \"macro_f1\": f1_score(true_labels, predictions, average='macro'),\n",
    "        \"weighted_f1\": f1_score(true_labels, predictions, average='weighted'),\n",
    "        \"micro_precision\": precision_score(true_labels, predictions, average='micro', zero_division=1),\n",
    "        \"macro_precision\": precision_score(true_labels, predictions, average='macro', zero_division=1),\n",
    "        \"micro_recall\": recall_score(true_labels, predictions, average='micro'),\n",
    "        \"macro_recall\": recall_score(true_labels, predictions, average='macro')\n",
    "    }\n",
    "    \n",
    "    # Log metrics to wandb\n",
    "    wandb.log(metrics)\n",
    "    \n",
    "    # Initialize visualizer with class names\n",
    "    class_names = [f\"Class_{i}\" for i in range(num_classes)]  # Replace with actual class names\n",
    "    visualizer = UncertaintyVisualizer(class_names)\n",
    "    \n",
    "    # Visualize uncertainty for a sample\n",
    "    sample_idx = 0  # Choose any sample\n",
    "    fig = visualizer.plot_prediction_with_uncertainty(\n",
    "        mean_probs[sample_idx], \n",
    "        std_probs[sample_idx], \n",
    "        true_labels[sample_idx],\n",
    "        threshold=0.5\n",
    "    )\n",
    "    wandb.log({\"sample_prediction\": wandb.Image(fig)})\n",
    "    \n",
    "    # Visualize uncertainty distribution\n",
    "    fig = visualizer.plot_uncertainty_distribution(std_probs, predictions)\n",
    "    wandb.log({\"uncertainty_distribution\": wandb.Image(fig)})\n",
    "    \n",
    "    # Visualize uncertainty vs error\n",
    "    fig = visualizer.plot_uncertainty_vs_error(mean_probs, std_probs, true_labels)\n",
    "    wandb.log({\"uncertainty_vs_error\": wandb.Image(fig)})\n",
    "    \n",
    "    # Visualize calibration curve\n",
    "    fig = visualizer.plot_calibration_curve(mean_probs, std_probs, true_labels)\n",
    "    wandb.log({\"calibration_curve\": wandb.Image(fig)})\n",
    "    \n",
    "    # Analysis: Find high uncertainty predictions\n",
    "    flat_std = std_probs.flatten()\n",
    "    high_uncertainty_threshold = np.percentile(flat_std, 95)  # Top 5% most uncertain\n",
    "    \n",
    "    high_uncertainty_mask = std_probs >= high_uncertainty_threshold\n",
    "    num_high_uncertainty = np.sum(high_uncertainty_mask)\n",
    "    \n",
    "    print(f\"Number of high uncertainty predictions: {num_high_uncertainty}\")\n",
    "    print(f\"Average accuracy for high uncertainty predictions: {np.mean((predictions[high_uncertainty_mask] == true_labels[high_uncertainty_mask]).astype(float))}\")\n",
    "    print(f\"Average accuracy for other predictions: {np.mean((predictions[~high_uncertainty_mask] == true_labels[~high_uncertainty_mask]).astype(float))}\")\n",
    "    \n",
    "    wandb.finish()\n",
    "\n",
    "\n",
    "# For inference on new data\n",
    "def predict_with_uncertainty(ensemble, X_spectra, X_gaia, class_names, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Make predictions with uncertainty estimation on new data.\n",
    "    \n",
    "    Args:\n",
    "        ensemble: Trained DeepEnsemble\n",
    "        X_spectra: Spectral features (tensor)\n",
    "        X_gaia: Gaia features (tensor)\n",
    "        class_names: List of class names\n",
    "        threshold: Decision threshold for positive predictions\n",
    "        \n",
    "    Returns:\n",
    "        predictions: Binary predictions\n",
    "        mean_probs: Mean probabilities\n",
    "        uncertainties: Standard deviations of probabilities\n",
    "        visualization: Matplotlib figure of predictions with uncertainty\n",
    "    \"\"\"\n",
    "    # Get predictions with uncertainty\n",
    "    mean_probs, std_probs, all_model_probs = ensemble.predict_sample(X_spectra, X_gaia)\n",
    "    \n",
    "    # Convert to binary predictions\n",
    "    predictions = (mean_probs >= threshold).astype(float)\n",
    "    \n",
    "    # Visualize\n",
    "    visualizer = UncertaintyVisualizer(class_names)\n",
    "    fig = visualizer.plot_prediction_with_uncertainty(\n",
    "        mean_probs, std_probs, threshold=threshold\n",
    "    )\n",
    "    \n",
    "    return predictions, mean_probs, std_probs, fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory efficient implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from mamba_ssm import Mamba2\n",
    "\n",
    "class MemoryEfficientMamba(nn.Module):\n",
    "    \"\"\"\n",
    "    Memory-efficient wrapper for Mamba2 with gradient checkpointing and \n",
    "    optional parameter quantization.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_state=16, d_conv=4, expand=2, use_checkpoint=True):\n",
    "        super().__init__()\n",
    "        self.mamba = Mamba2(\n",
    "            d_model=d_model,\n",
    "            d_state=d_state,\n",
    "            d_conv=d_conv,\n",
    "            expand=expand\n",
    "        )\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.use_checkpoint and self.training:\n",
    "            return checkpoint(self.mamba, x)\n",
    "        else:\n",
    "            return self.mamba(x)\n",
    "\n",
    "class MemoryEfficientStarClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Memory-efficient version of StarClassifierFusion with various\n",
    "    optimizations to reduce VRAM usage.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model_spectra,\n",
    "        d_model_gaia,\n",
    "        num_classes,\n",
    "        input_dim_spectra,\n",
    "        input_dim_gaia,\n",
    "        n_layers=6,\n",
    "        use_cross_attention=True,\n",
    "        n_cross_attn_heads=8,\n",
    "        d_state=16,  # Reduced from 256 to save memory\n",
    "        d_conv=4,\n",
    "        expand=2,\n",
    "        use_checkpoint=True,\n",
    "        activation_checkpointing=True,\n",
    "        use_half_precision=True,\n",
    "        sequential_processing=True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "        self.activation_checkpointing = activation_checkpointing\n",
    "        self.sequential_processing = sequential_processing\n",
    "        \n",
    "        # Use lower precision\n",
    "        self.dtype = torch.float16 if use_half_precision else torch.float32\n",
    "\n",
    "        # Input projection layers\n",
    "        self.input_proj_spectra = nn.Linear(input_dim_spectra, d_model_spectra)\n",
    "        self.input_proj_gaia = nn.Linear(input_dim_gaia, d_model_gaia)\n",
    "        \n",
    "        # Memory-efficient Mamba layers\n",
    "        self.mamba_spectra_layers = nn.ModuleList([\n",
    "            MemoryEfficientMamba(\n",
    "                d_model=d_model_spectra,\n",
    "                d_state=d_state,\n",
    "                d_conv=d_conv,\n",
    "                expand=expand,\n",
    "                use_checkpoint=activation_checkpointing\n",
    "            ) for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        self.mamba_gaia_layers = nn.ModuleList([\n",
    "            MemoryEfficientMamba(\n",
    "                d_model=d_model_gaia,\n",
    "                d_state=d_state,\n",
    "                d_conv=d_conv,\n",
    "                expand=expand,\n",
    "                use_checkpoint=activation_checkpointing\n",
    "            ) for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "        # Cross-attention (optional)\n",
    "        self.use_cross_attention = use_cross_attention\n",
    "        if use_cross_attention:\n",
    "            self.cross_attn_block_spectra = self._create_cross_attn_block(d_model_spectra, n_heads=n_cross_attn_heads)\n",
    "            self.cross_attn_block_gaia = self._create_cross_attn_block(d_model_gaia, n_heads=n_cross_attn_heads)\n",
    "\n",
    "        # Final classifier\n",
    "        fusion_dim = d_model_spectra + d_model_gaia\n",
    "        self.layer_norm = nn.LayerNorm(fusion_dim)\n",
    "        self.classifier = nn.Linear(fusion_dim, num_classes)\n",
    "    \n",
    "    def _create_cross_attn_block(self, d_model, n_heads):\n",
    "        \"\"\"Creates a cross-attention block with optional gradient checkpointing.\"\"\"\n",
    "        class CrossAttentionBlock(nn.Module):\n",
    "            def __init__(self, d_model, n_heads):\n",
    "                super().__init__()\n",
    "                self.cross_attn = nn.MultiheadAttention(\n",
    "                    embed_dim=d_model, \n",
    "                    num_heads=n_heads, \n",
    "                    batch_first=True\n",
    "                )\n",
    "                self.norm1 = nn.LayerNorm(d_model)\n",
    "                \n",
    "                self.ffn = nn.Sequential(\n",
    "                    nn.Linear(d_model, 4 * d_model),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(4 * d_model, d_model)\n",
    "                )\n",
    "                self.norm2 = nn.LayerNorm(d_model)\n",
    "                \n",
    "            def forward(self, x_q, x_kv):\n",
    "                # Cross-attention\n",
    "                attn_output, _ = self.cross_attn(query=x_q, key=x_kv, value=x_kv)\n",
    "                x = self.norm1(x_q + attn_output)\n",
    "                \n",
    "                # Feed forward\n",
    "                ffn_out = self.ffn(x)\n",
    "                x = self.norm2(x + ffn_out)\n",
    "                \n",
    "                return x\n",
    "        \n",
    "        block = CrossAttentionBlock(d_model, n_heads)\n",
    "        \n",
    "        # Wrap with gradient checkpointing if requested\n",
    "        if self.activation_checkpointing:\n",
    "            def forward_with_checkpoint(module, x_q, x_kv):\n",
    "                def custom_forward(x_q, x_kv):\n",
    "                    return module(x_q, x_kv)\n",
    "                return checkpoint(custom_forward, x_q, x_kv)\n",
    "            \n",
    "            class CheckpointedCrossAttention(nn.Module):\n",
    "                def __init__(self, block):\n",
    "                    super().__init__()\n",
    "                    self.block = block\n",
    "                \n",
    "                def forward(self, x_q, x_kv):\n",
    "                    return forward_with_checkpoint(self.block, x_q, x_kv)\n",
    "            \n",
    "            return CheckpointedCrossAttention(block)\n",
    "        else:\n",
    "            return block\n",
    "    \n",
    "    def _process_mamba_layers(self, x, layers):\n",
    "        \"\"\"Process input through Mamba layers, optionally sequentially to save memory.\"\"\"\n",
    "        if self.sequential_processing:\n",
    "            for layer in layers:\n",
    "                x = layer(x)\n",
    "        else:\n",
    "            # Process all layers at once (uses more memory but faster)\n",
    "            for layer in layers:\n",
    "                x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x_spectra, x_gaia):\n",
    "        # Convert to half precision if requested\n",
    "        if hasattr(self, 'dtype') and self.dtype == torch.float16:\n",
    "            x_spectra = x_spectra.half()\n",
    "            x_gaia = x_gaia.half()\n",
    "        \n",
    "        # Project inputs\n",
    "        x_spectra = self.input_proj_spectra(x_spectra)\n",
    "        x_gaia = self.input_proj_gaia(x_gaia)\n",
    "        \n",
    "        # Add sequence dimension if needed\n",
    "        if len(x_spectra.shape) == 2:\n",
    "            x_spectra = x_spectra.unsqueeze(1)\n",
    "        if len(x_gaia.shape) == 2:\n",
    "            x_gaia = x_gaia.unsqueeze(1)\n",
    "        \n",
    "        # Process through Mamba layers\n",
    "        x_spectra = self._process_mamba_layers(x_spectra, self.mamba_spectra_layers)\n",
    "        x_gaia = self._process_mamba_layers(x_gaia, self.mamba_gaia_layers)\n",
    "        \n",
    "        # Optional cross-attention\n",
    "        if self.use_cross_attention:\n",
    "            x_spectra_fused = self.cross_attn_block_spectra(x_spectra, x_gaia)\n",
    "            x_gaia_fused = self.cross_attn_block_gaia(x_gaia, x_spectra)\n",
    "            x_spectra = x_spectra_fused\n",
    "            x_gaia = x_gaia_fused\n",
    "        \n",
    "        # Pool across sequence dimension\n",
    "        x_spectra = x_spectra.mean(dim=1)\n",
    "        x_gaia = x_gaia.mean(dim=1)\n",
    "        \n",
    "        # Concatenate\n",
    "        x_fused = torch.cat([x_spectra, x_gaia], dim=-1)\n",
    "        \n",
    "        # Final classification\n",
    "        x_fused = self.layer_norm(x_fused)\n",
    "        logits = self.classifier(x_fused)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "class MemoryEfficientEnsemble:\n",
    "    \"\"\"\n",
    "    Memory-efficient implementation of ensemble for uncertainty quantification.\n",
    "    Only keeps one model in VRAM at a time and uses quantization and other optimizations.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        model_class, \n",
    "        model_args, \n",
    "        num_models=5, \n",
    "        device='cuda',\n",
    "        low_memory_mode=True,\n",
    "        quantize_models=True,\n",
    "        offload_optimizer=True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the memory-efficient ensemble.\n",
    "        \n",
    "        Args:\n",
    "            model_class: Model class to instantiate\n",
    "            model_args: Arguments for model initialization\n",
    "            num_models: Number of models in ensemble\n",
    "            device: Device to use\n",
    "            low_memory_mode: Whether to use memory optimizations\n",
    "            quantize_models: Whether to quantize model weights\n",
    "            offload_optimizer: Whether to offload optimizer states to CPU\n",
    "        \"\"\"\n",
    "        self.model_class = model_class\n",
    "        self.model_args = model_args\n",
    "        self.num_models = num_models\n",
    "        self.device = device\n",
    "        self.low_memory_mode = low_memory_mode\n",
    "        self.quantize_models = quantize_models\n",
    "        self.offload_optimizer = offload_optimizer\n",
    "        \n",
    "        # Storage for model states (not actual models)\n",
    "        self.model_states = [None] * num_models\n",
    "        \n",
    "        # Create only one model instance to save memory\n",
    "        self.active_model = self._create_model(0)\n",
    "        self.active_model_idx = 0\n",
    "    \n",
    "    def _create_model(self, model_idx):\n",
    "        \"\"\"Create a new model instance with appropriate seed.\"\"\"\n",
    "        torch.manual_seed(42 + model_idx)\n",
    "        np.random.seed(42 + model_idx)\n",
    "        model = self.model_class(**self.model_args).to(self.device)\n",
    "        \n",
    "        if self.quantize_models and hasattr(torch.quantization, 'quantize_dynamic'):\n",
    "            # Only quantize during inference, not training\n",
    "            model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "            model = torch.quantization.prepare(model)\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def _load_model_state(self, model_idx):\n",
    "        \"\"\"Load a specific model's state into the active model.\"\"\"\n",
    "        if self.model_states[model_idx] is not None:\n",
    "            self.active_model.load_state_dict(self.model_states[model_idx])\n",
    "            self.active_model_idx = model_idx\n",
    "    \n",
    "    def train(\n",
    "        self, \n",
    "        train_loader, \n",
    "        val_loader, \n",
    "        test_loader=None, \n",
    "        num_epochs=100, \n",
    "        lr=1e-4, \n",
    "        max_patience=20,\n",
    "        scheduler_type='OneCycleLR',\n",
    "        log_to_wandb=True,\n",
    "        save_checkpoints=True,\n",
    "        checkpoint_dir='checkpoints'\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Train all models in the ensemble sequentially to save memory.\n",
    "        \"\"\"\n",
    "        import os\n",
    "        import wandb\n",
    "        from tqdm import tqdm\n",
    "        import torch.optim as optim\n",
    "        \n",
    "        if save_checkpoints and not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        \n",
    "        for model_idx in range(self.num_models):\n",
    "            print(f\"\\n----- Training Ensemble Model {model_idx+1}/{self.num_models} -----\\n\")\n",
    "            \n",
    "            # Initialize wandb for this model\n",
    "            if log_to_wandb:\n",
    "                run = wandb.init(\n",
    "                    project=\"ALLSTARS_memory_efficient_ensemble\", \n",
    "                    name=f\"model_{model_idx}\",\n",
    "                    group=\"memory_efficient_training\",\n",
    "                    config={\n",
    "                        **self.model_args,\n",
    "                        \"model_idx\": model_idx,\n",
    "                        \"num_models\": self.num_models,\n",
    "                        \"lr\": lr,\n",
    "                        \"max_patience\": max_patience,\n",
    "                        \"scheduler_type\": scheduler_type,\n",
    "                        \"num_epochs\": num_epochs,\n",
    "                        \"low_memory_mode\": self.low_memory_mode,\n",
    "                        \"quantize_models\": self.quantize_models,\n",
    "                        \"offload_optimizer\": self.offload_optimizer\n",
    "                    },\n",
    "                    reinit=True\n",
    "                )\n",
    "            \n",
    "            # Create or load model\n",
    "            if model_idx == 0 and self.active_model_idx == 0:\n",
    "                # Use existing model for first run\n",
    "                model = self.active_model\n",
    "            else:\n",
    "                # Create new model with appropriate seed for subsequent runs\n",
    "                model = self._create_model(model_idx)\n",
    "                self.active_model = model\n",
    "                self.active_model_idx = model_idx\n",
    "            \n",
    "            # Move model to device\n",
    "            model = model.to(self.device)\n",
    "            \n",
    "            # Create optimizer with memory-efficient options\n",
    "            if self.offload_optimizer and hasattr(optim, 'SGD'):\n",
    "                # SGD uses less memory than Adam/AdamW\n",
    "                optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "            else:\n",
    "                optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "            \n",
    "            # Configure the scheduler\n",
    "            if scheduler_type == 'OneCycleLR':\n",
    "                scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "                    optimizer, \n",
    "                    max_lr=lr,\n",
    "                    epochs=num_epochs, \n",
    "                    steps_per_epoch=len(train_loader)\n",
    "                )\n",
    "            else:  # ReduceLROnPlateau\n",
    "                scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                    optimizer, \n",
    "                    mode='min', \n",
    "                    factor=0.5, \n",
    "                    patience=int(max_patience / 5)\n",
    "                )\n",
    "            \n",
    "            # Calculate class weights for imbalanced classes\n",
    "            all_labels = []\n",
    "            for _, _, y_batch in train_loader:\n",
    "                all_labels.extend(y_batch.cpu().numpy())\n",
    "            \n",
    "            class_weights = self._calculate_class_weights(np.array(all_labels))\n",
    "            class_weights = torch.tensor(class_weights, dtype=torch.float).to(self.device)\n",
    "            criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights)\n",
    "            \n",
    "            best_val_loss = float('inf')\n",
    "            patience = max_patience\n",
    "            best_model_state = None\n",
    "\n",
    "            # Training loop\n",
    "            for epoch in range(num_epochs):\n",
    "                # Resample training data for balanced batches\n",
    "                train_loader.dataset.re_sample()\n",
    "\n",
    "                # Recompute class weights based on the new sampling\n",
    "                all_labels = []\n",
    "                for _, _, y_batch in train_loader:\n",
    "                    all_labels.extend(y_batch.cpu().numpy())\n",
    "                class_weights = self._calculate_class_weights(np.array(all_labels))\n",
    "                class_weights = torch.tensor(class_weights, dtype=torch.float).to(self.device)\n",
    "                criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights)\n",
    "\n",
    "                # --- Training Phase ---\n",
    "                model.train()\n",
    "                train_loss, train_acc = 0.0, 0.0\n",
    "                \n",
    "                # Use mixed precision training to save memory\n",
    "                scaler = torch.cuda.amp.GradScaler() if hasattr(torch.cuda, 'amp') else None\n",
    "                \n",
    "                for X_spc, X_ga, y_batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n",
    "                    X_spc, X_ga, y_batch = X_spc.to(self.device), X_ga.to(self.device), y_batch.to(self.device)\n",
    "                    \n",
    "                    # Clear memory before forward pass\n",
    "                    optimizer.zero_grad(set_to_none=True)\n",
    "                    \n",
    "                    # Use mixed precision for forward pass if available\n",
    "                    if scaler:\n",
    "                        with torch.cuda.amp.autocast():\n",
    "                            outputs = model(X_spc, X_ga)\n",
    "                            loss = criterion(outputs, y_batch)\n",
    "                        \n",
    "                        # Mixed precision backward pass\n",
    "                        scaler.scale(loss).backward()\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "                    else:\n",
    "                        outputs = model(X_spc, X_ga)\n",
    "                        loss = criterion(outputs, y_batch)\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                    train_loss += loss.item() * X_spc.size(0)\n",
    "                    predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                    correct = (predicted == y_batch).float()\n",
    "                    train_acc += correct.mean(dim=1).sum().item()\n",
    "                    \n",
    "                    # Free up memory\n",
    "                    del X_spc, X_ga, y_batch, outputs, loss, predicted, correct\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "                train_loss /= len(train_loader.dataset)\n",
    "                train_acc /= len(train_loader.dataset)\n",
    "\n",
    "                # --- Validation Phase ---\n",
    "                model.eval()\n",
    "                val_loss, val_acc = 0.0, 0.0\n",
    "                with torch.no_grad():\n",
    "                    for X_spc, X_ga, y_batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n",
    "                        X_spc, X_ga, y_batch = X_spc.to(self.device), X_ga.to(self.device), y_batch.to(self.device)\n",
    "                        \n",
    "                        # Use mixed precision for inference if available\n",
    "                        if hasattr(torch.cuda, 'amp'):\n",
    "                            with torch.cuda.amp.autocast():\n",
    "                                outputs = model(X_spc, X_ga)\n",
    "                                loss = criterion(outputs, y_batch)\n",
    "                        else:\n",
    "                            outputs = model(X_spc, X_ga)\n",
    "                            loss = criterion(outputs, y_batch)\n",
    "                        \n",
    "                        val_loss += loss.item() * X_spc.size(0)\n",
    "                        predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                        correct = (predicted == y_batch).float()\n",
    "                        val_acc += correct.mean(dim=1).sum().item()\n",
    "                        \n",
    "                        # Free up memory\n",
    "                        del X_spc, X_ga, y_batch, outputs, loss, predicted, correct\n",
    "                        torch.cuda.empty_cache()\n",
    "                \n",
    "                val_loss /= len(val_loader.dataset)\n",
    "                val_acc /= len(val_loader.dataset)\n",
    "\n",
    "                # --- Test Phase (if provided) ---\n",
    "                test_metrics = {}\n",
    "                if test_loader is not None:\n",
    "                    test_loss, test_acc = 0.0, 0.0\n",
    "                    y_true, y_pred = [], []\n",
    "                    with torch.no_grad():\n",
    "                        for X_spc, X_ga, y_batch in tqdm(test_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Testing\"):\n",
    "                            X_spc, X_ga, y_batch = X_spc.to(self.device), X_ga.to(self.device), y_batch.to(self.device)\n",
    "                            \n",
    "                            # Use mixed precision for inference if available\n",
    "                            if hasattr(torch.cuda, 'amp'):\n",
    "                                with torch.cuda.amp.autocast():\n",
    "                                    outputs = model(X_spc, X_ga)\n",
    "                                    loss = criterion(outputs, y_batch)\n",
    "                            else:\n",
    "                                outputs = model(X_spc, X_ga)\n",
    "                                loss = criterion(outputs, y_batch)\n",
    "                            \n",
    "                            test_loss += loss.item() * X_spc.size(0)\n",
    "                            \n",
    "                            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                            correct = (predicted == y_batch).float()\n",
    "                            test_acc += correct.mean(dim=1).sum().item()\n",
    "\n",
    "                            # Collect for metrics calculation\n",
    "                            y_true.extend(y_batch.cpu().numpy())\n",
    "                            y_pred.extend(predicted.cpu().numpy())\n",
    "                            \n",
    "                            # Free up memory\n",
    "                            del X_spc, X_ga, y_batch, outputs, loss, predicted, correct\n",
    "                            torch.cuda.empty_cache()\n",
    "\n",
    "                    test_loss /= len(test_loader.dataset)\n",
    "                    test_acc /= len(test_loader.dataset)\n",
    "                    \n",
    "                    # Calculate metrics on CPU to save GPU memory\n",
    "                    test_metrics = self._calculate_metrics(np.array(y_true), np.array(y_pred))\n",
    "                    test_metrics.update({\n",
    "                        \"test_loss\": test_loss,\n",
    "                        \"test_acc\": test_acc,\n",
    "                    })\n",
    "\n",
    "                # Logging\n",
    "                if log_to_wandb:\n",
    "                    log_data = {\n",
    "                        \"epoch\": epoch,\n",
    "                        \"train_loss\": train_loss,\n",
    "                        \"val_loss\": val_loss,\n",
    "                        \"train_acc\": train_acc,\n",
    "                        \"val_acc\": val_acc,\n",
    "                        \"lr\": self._get_lr(optimizer)\n",
    "                    }\n",
    "                    log_data.update(test_metrics)\n",
    "                    wandb.log(log_data)\n",
    "\n",
    "                # Update learning rate scheduler\n",
    "                if scheduler_type == 'OneCycleLR':\n",
    "                    scheduler.step()\n",
    "                else:  # ReduceLROnPlateau\n",
    "                    scheduler.step(val_loss)\n",
    "\n",
    "                # Early stopping and model saving\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    patience = max_patience\n",
    "                    \n",
    "                    # Save best model state (in memory or to disk)\n",
    "                    if self.low_memory_mode and save_checkpoints:\n",
    "                        # Save to disk\n",
    "                        torch.save(\n",
    "                            model.state_dict(), \n",
    "                            os.path.join(checkpoint_dir, f\"model_{model_idx}_best.pth\")\n",
    "                        )\n",
    "                    else:\n",
    "                        # Save in memory\n",
    "                        best_model_state = model.state_dict().copy()\n",
    "                    \n",
    "                    if log_to_wandb:\n",
    "                        wandb.run.summary[\"best_val_loss\"] = best_val_loss\n",
    "                else:\n",
    "                    patience -= 1\n",
    "                    if patience <= 0:\n",
    "                        print(\"Early stopping triggered.\")\n",
    "                        break\n",
    "\n",
    "                # Periodic checkpoint saving\n",
    "                if save_checkpoints and (epoch + 1) % 10 == 0:\n",
    "                    torch.save(\n",
    "                        model.state_dict(), \n",
    "                        os.path.join(checkpoint_dir, f\"model_{model_idx}_epoch_{epoch+1}.pth\")\n",
    "                    )\n",
    "\n",
    "            # Load the best model state\n",
    "            if self.low_memory_mode and save_checkpoints:\n",
    "                # Load from disk\n",
    "                model.load_state_dict(\n",
    "                    torch.load(os.path.join(checkpoint_dir, f\"model_{model_idx}_best.pth\"))\n",
    "                )\n",
    "            elif best_model_state is not None:\n",
    "                # Load from memory\n",
    "                model.load_state_dict(best_model_state)\n",
    "            \n",
    "            # Store the model state\n",
    "            self.model_states[model_idx] = model.state_dict().copy()\n",
    "            \n",
    "            # Save final model\n",
    "            if save_checkpoints:\n",
    "                torch.save(\n",
    "                    model.state_dict(),\n",
    "                    os.path.join(checkpoint_dir, f\"model_{model_idx}_final.pth\")\n",
    "                )\n",
    "            \n",
    "            # Close wandb run\n",
    "            if log_to_wandb:\n",
    "                wandb.finish()\n",
    "            \n",
    "            # Free up memory before next model\n",
    "            if model_idx < self.num_models - 1:\n",
    "                del model, optimizer, scheduler\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        return self.model_states\n",
    "    \n",
    "    def predict(self, loader, return_individual=False, batch_size=None):\n",
    "        \"\"\"\n",
    "        Make predictions with the ensemble, one model at a time to save memory.\n",
    "        \n",
    "        Args:\n",
    "            loader: DataLoader for the data to predict\n",
    "            return_individual: Whether to return predictions from individual models\n",
    "            batch_size: Optional batch size override for loader\n",
    "            \n",
    "        Returns:\n",
    "            mean_probs: Mean probability across all models\n",
    "            std_probs: Standard deviation of probabilities (uncertainty measure)\n",
    "            individual_probs: (Optional) Predictions from each individual model\n",
    "        \"\"\"\n",
    "        # Create a new dataloader with specified batch size if needed\n",
    "        if batch_size is not None and batch_size != loader.batch_size:\n",
    "            from torch.utils.data import DataLoader\n",
    "            new_loader = DataLoader(\n",
    "                loader.dataset, \n",
    "                batch_size=batch_size, \n",
    "                shuffle=False,\n",
    "                num_workers=loader.num_workers\n",
    "            )\n",
    "            loader = new_loader\n",
    "        \n",
    "        # Get sample dimensions from first batch\n",
    "        for X_spc, X_ga, y in loader:\n",
    "            num_classes = y.shape[1]\n",
    "            break\n",
    "        \n",
    "        # Get total number of samples\n",
    "        num_samples = len(loader.dataset)\n",
    "        \n",
    "        # Storage for predictions\n",
    "        all_probs = []\n",
    "        \n",
    "        # Process one model at a time\n",
    "        for model_idx in range(self.num_models):\n",
    "            print(f\"Making predictions with model {model_idx+1}/{self.num_models}\")\n",
    "            \n",
    "            # Load model state\n",
    "            self._load_model_state(model_idx)\n",
    "            self.active_model.eval()\n",
    "            \n",
    "            # Storage for this model's predictions\n",
    "            model_probs = np.zeros((num_samples, num_classes))\n",
    "            \n",
    "            # Batch indices\n",
    "            start_idx = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for X_spc, X_ga, _ in loader:\n",
    "                    batch_size = X_spc.shape[0]\n",
    "                    \n",
    "                    # Move to device\n",
    "                    X_spc, X_ga = X_spc.to(self.device), X_ga.to(self.device)\n",
    "                    \n",
    "                    # Use mixed precision if available\n",
    "                    if hasattr(torch.cuda, 'amp'):\n",
    "                        with torch.cuda.amp.autocast():\n",
    "                            outputs = self.active_model(X_spc, X_ga)\n",
    "                    else:\n",
    "                        outputs = self.active_model(X_spc, X_ga)\n",
    "                    \n",
    "                    # Get probabilities\n",
    "                    probs = torch.sigmoid(outputs).cpu().numpy()\n",
    "                    \n",
    "                    # Store predictions\n",
    "                    end_idx = start_idx + batch_size\n",
    "                    model_probs[start_idx:end_idx] = probs\n",
    "                    start_idx = end_idx\n",
    "                    \n",
    "                    # Free up memory\n",
    "                    del X_spc, X_ga, outputs, probs\n",
    "                    torch.cuda.empty_cache()\n",
    "            \n",
    "            # Add to ensemble predictions\n",
    "            all_probs.append(model_probs)\n",
    "        \n",
    "        # Stack along a new axis to get shape (num_models, num_samples, num_classes)\n",
    "        all_probs = np.stack(all_probs, axis=0)\n",
    "        \n",
    "        # Calculate mean and std across models (axis=0)\n",
    "        mean_probs = np.mean(all_probs, axis=0)\n",
    "        std_probs = np.std(all_probs, axis=0)\n",
    "        \n",
    "        if return_individual:\n",
    "            return mean_probs, std_probs, all_probs\n",
    "        else:\n",
    "            return mean_probs, std_probs\n",
    "    \n",
    "    def save_models(self, path_prefix):\n",
    "        \"\"\"\n",
    "        Save all model states.\n",
    "        \n",
    "        Args:\n",
    "            path_prefix: Path prefix for saving models\n",
    "        \"\"\"\n",
    "        for i, state in enumerate(self.model_states):\n",
    "            if state is not None:\n",
    "                torch.save(state, f\"{path_prefix}_model_{i}.pth\")\n",
    "    \n",
    "    def load_models(self, path_prefix):\n",
    "        \"\"\"\n",
    "        Load all model states.\n",
    "        \n",
    "        Args:\n",
    "            path_prefix: Path prefix for loading models\n",
    "        \"\"\"\n",
    "        self.model_states = []\n",
    "        for i in range(self.num_models):\n",
    "            state = torch.load(f\"{path_prefix}_model_{i}.pth\", map_location='cpu')\n",
    "            self.model_states.append(state)\n",
    "    \n",
    "    def _calculate_class_weights(self, y):\n",
    "        \"\"\"Calculate class weights for handling imbalanced classes.\"\"\"\n",
    "        if y.ndim > 1:  \n",
    "            class_counts = np.sum(y, axis=0)  \n",
    "        else:\n",
    "            class_counts = np.bincount(y)\n",
    "\n",
    "        total_samples = y.shape[0] if y.ndim > 1 else len(y)\n",
    "        class_counts = np.where(class_counts == 0, 1, class_counts)  # Prevent division by zero\n",
    "        class_weights = total_samples / (len(class_counts) * class_counts)\n",
    "        \n",
    "        return class_weights\n",
    "    \n",
    "    def _calculate_metrics(self, y_true, y_pred):\n",
    "        \"\"\"Calculate evaluation metrics for multi-label classification.\"\"\"\n",
    "        from sklearn.metrics import f1_score, precision_score, recall_score, hamming_loss\n",
    "        \n",
    "        metrics = {\n",
    "            \"micro_f1\": f1_score(y_true, y_pred, average='micro'),\n",
    "            \"macro_f1\": f1_score(y_true, y_pred, average='macro'),\n",
    "            \"weighted_f1\": f1_score(y_true, y_pred, average='weighted'),\n",
    "            \"micro_precision\": precision_score(y_true, y_pred, average='micro', zero_division=1),\n",
    "            \"macro_precision\": precision_score(y_true, y_pred, average='macro', zero_division=1),\n",
    "            \"weighted_precision\": precision_score(y_true, y_pred, average='weighted', zero_division=1),\n",
    "            \"micro_recall\": recall_score(y_true, y_pred, average='micro'),\n",
    "            \"macro_recall\": recall_score(y_true, y_pred, average='macro'),\n",
    "            \"weighted_recall\": recall_score(y_true, y_pred, average='weighted'),\n",
    "            \"hamming_loss\": hamming_loss(y_true, y_pred)\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def _get_lr(self, optimizer):\n",
    "        \"\"\"Get current learning rate from optimizer.\"\"\"\n",
    "        for param_group in optimizer.param_groups:\n",
    "            return param_group['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Initial memory usage:\n",
      "GPU memory allocated: 0.00 MB\n",
      "GPU memory reserved: 0.00 MB\n",
      "\n",
      "Creating memory-efficient model...\n",
      "\n",
      "Memory usage after creating model:\n",
      "GPU memory allocated: 2738.05 MB\n",
      "GPU memory reserved: 2770.00 MB\n",
      "\n",
      "Model parameters size: 1369.01 MB\n",
      "Number of parameters: 717,756,727\n",
      "\n",
      "Creating memory-efficient ensemble...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/ao/quantization/observer.py:229: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "_add_observer_ only works with cpu or single-device CUDA modules, but got devices {device(type='cuda', index=0), device(type='cpu')}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 77\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Create ensemble\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCreating memory-efficient ensemble...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 77\u001b[0m ensemble \u001b[38;5;241m=\u001b[39m \u001b[43mMemoryEfficientEnsemble\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMemoryEfficientStarClassifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCONFIG\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_models\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlow_memory_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantize_models\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43moffload_optimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     85\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Print ensemble memory usage\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMemory usage after creating ensemble:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 236\u001b[0m, in \u001b[0;36mMemoryEfficientEnsemble.__init__\u001b[0;34m(self, model_class, model_args, num_models, device, low_memory_mode, quantize_models, offload_optimizer)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_states \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m num_models\n\u001b[1;32m    235\u001b[0m \u001b[38;5;66;03m# Create only one model instance to save memory\u001b[39;00m\n\u001b[0;32m--> 236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_model_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[0;32mIn[6], line 248\u001b[0m, in \u001b[0;36mMemoryEfficientEnsemble._create_model\u001b[0;34m(self, model_idx)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquantize_models \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39mquantization, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquantize_dynamic\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;66;03m# Only quantize during inference, not training\u001b[39;00m\n\u001b[1;32m    247\u001b[0m     model\u001b[38;5;241m.\u001b[39mqconfig \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mquantization\u001b[38;5;241m.\u001b[39mget_default_qconfig(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfbgemm\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 248\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/ao/quantization/quantize.py:396\u001b[0m, in \u001b[0;36mprepare\u001b[0;34m(model, inplace, allow_list, observer_non_leaf_module_list, prepare_custom_config_dict)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28mhasattr\u001b[39m(m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m m\u001b[38;5;241m.\u001b[39mqconfig \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mmodules()):\n\u001b[1;32m    390\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    391\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of the submodule got qconfig applied. Make sure you \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    392\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassed correct configuration through `qconfig_dict` or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    393\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mby assigning the `.qconfig` attribute directly on submodules\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    394\u001b[0m     )\n\u001b[0;32m--> 396\u001b[0m \u001b[43m_add_observer_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqconfig_propagation_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobserver_non_leaf_module_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_module_class_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_module_class_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/ao/quantization/quantize.py:276\u001b[0m, in \u001b[0;36m_add_observer_\u001b[0;34m(module, qconfig_propagation_list, non_leaf_module_list, device, custom_module_class_mapping)\u001b[0m\n\u001b[1;32m    274\u001b[0m             insert_activation_post_process(observed_child)\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 276\u001b[0m         \u001b[43m_add_observer_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m            \u001b[49m\u001b[43mqconfig_propagation_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnon_leaf_module_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcustom_module_class_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;66;03m# Insert observers only for leaf nodes, note that this observer is for\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;66;03m# the output of the module, for input QuantStub will observe them\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    287\u001b[0m     has_no_children_ignoring_parametrizations(module)\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mSequential)\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m type_before_parametrizations(module) \u001b[38;5;129;01min\u001b[39;00m qconfig_propagation_list\n\u001b[1;32m    290\u001b[0m ):\n",
      "File \u001b[0;32m~/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/ao/quantization/quantize.py:276\u001b[0m, in \u001b[0;36m_add_observer_\u001b[0;34m(module, qconfig_propagation_list, non_leaf_module_list, device, custom_module_class_mapping)\u001b[0m\n\u001b[1;32m    274\u001b[0m             insert_activation_post_process(observed_child)\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 276\u001b[0m         \u001b[43m_add_observer_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchild\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m            \u001b[49m\u001b[43mqconfig_propagation_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnon_leaf_module_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcustom_module_class_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;66;03m# Insert observers only for leaf nodes, note that this observer is for\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;66;03m# the output of the module, for input QuantStub will observe them\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    287\u001b[0m     has_no_children_ignoring_parametrizations(module)\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mSequential)\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m type_before_parametrizations(module) \u001b[38;5;129;01min\u001b[39;00m qconfig_propagation_list\n\u001b[1;32m    290\u001b[0m ):\n",
      "File \u001b[0;32m~/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/ao/quantization/quantize.py:269\u001b[0m, in \u001b[0;36m_add_observer_\u001b[0;34m(module, qconfig_propagation_list, non_leaf_module_list, device, custom_module_class_mapping)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[1;32m    263\u001b[0m     needs_observation(child)\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m type_before_parametrizations(child) \u001b[38;5;129;01min\u001b[39;00m custom_module_class_mapping\n\u001b[1;32m    265\u001b[0m ):\n\u001b[1;32m    266\u001b[0m     observed_class \u001b[38;5;241m=\u001b[39m custom_module_class_mapping[\n\u001b[1;32m    267\u001b[0m         type_before_parametrizations(child)\n\u001b[1;32m    268\u001b[0m     ]\n\u001b[0;32m--> 269\u001b[0m     observed_child \u001b[38;5;241m=\u001b[39m \u001b[43mobserved_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_float\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(module, name, observed_child)\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;66;03m# TODO: These are the modules that cannot be observed\u001b[39;00m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;66;03m#       Once there are more, we should move them to a separate list\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/ao/nn/quantizable/modules/activation.py:186\u001b[0m, in \u001b[0;36mMultiheadAttention.from_float\u001b[0;34m(cls, other)\u001b[0m\n\u001b[1;32m    184\u001b[0m observed\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# Explicit prepare\u001b[39;00m\n\u001b[0;32m--> 186\u001b[0m observed \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mao\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m observed\n",
      "File \u001b[0;32m~/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/ao/quantization/quantize.py:396\u001b[0m, in \u001b[0;36mprepare\u001b[0;34m(model, inplace, allow_list, observer_non_leaf_module_list, prepare_custom_config_dict)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28mhasattr\u001b[39m(m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m m\u001b[38;5;241m.\u001b[39mqconfig \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mmodules()):\n\u001b[1;32m    390\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    391\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of the submodule got qconfig applied. Make sure you \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    392\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassed correct configuration through `qconfig_dict` or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    393\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mby assigning the `.qconfig` attribute directly on submodules\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    394\u001b[0m     )\n\u001b[0;32m--> 396\u001b[0m \u001b[43m_add_observer_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqconfig_propagation_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobserver_non_leaf_module_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_module_class_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_module_class_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/ao/quantization/quantize.py:199\u001b[0m, in \u001b[0;36m_add_observer_\u001b[0;34m(module, qconfig_propagation_list, non_leaf_module_list, device, custom_module_class_mapping)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    197\u001b[0m     devices \u001b[38;5;241m=\u001b[39m _get_unique_devices_(module)\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m--> 199\u001b[0m         \u001b[38;5;28mlen\u001b[39m(devices) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    200\u001b[0m     ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_add_observer_ only works with cpu or single-device CUDA modules, but got devices \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevices\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    201\u001b[0m     device \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(devices)) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(devices) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_activation_post_process\u001b[39m(qconfig, device, special_act_post_process\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "\u001b[0;31mAssertionError\u001b[0m: _add_observer_ only works with cpu or single-device CUDA modules, but got devices {device(type='cuda', index=0), device(type='cpu')}"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "#from memory_optimization import MemoryEfficientStarClassifier, MemoryEfficientEnsemble\n",
    "\n",
    "# Track memory usage\n",
    "def print_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "        print(f\"GPU memory reserved: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")\n",
    "        # Force garbage collection\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# Example config with higher embedding dimensions\n",
    "CONFIG = {\n",
    "    \"d_model_spectra\": 2048,  # Doubled from original 2048\n",
    "    \"d_model_gaia\": 2048,     # Doubled from original 2048\n",
    "    \"num_classes\": 55,\n",
    "    \"input_dim_spectra\": 3647,\n",
    "    \"input_dim_gaia\": 18,\n",
    "    \"n_layers\": 12,\n",
    "    \"d_state\": 16,           # Reduced from 256 to save memory\n",
    "    \"d_conv\": 4,\n",
    "    \"expand\": 2,\n",
    "    \"use_cross_attention\": True,\n",
    "    \"n_cross_attn_heads\": 8,\n",
    "    \"use_checkpoint\": True,\n",
    "    \"activation_checkpointing\": True,\n",
    "    \"use_half_precision\": True,\n",
    "    \"sequential_processing\": True\n",
    "}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set device\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Print initial memory usage\n",
    "    print(\"\\nInitial memory usage:\")\n",
    "    print_gpu_memory()\n",
    "    \n",
    "    # Create memory-efficient model\n",
    "    print(\"\\nCreating memory-efficient model...\")\n",
    "    model = MemoryEfficientStarClassifier(\n",
    "        d_model_spectra=CONFIG[\"d_model_spectra\"],\n",
    "        d_model_gaia=CONFIG[\"d_model_gaia\"],\n",
    "        num_classes=CONFIG[\"num_classes\"],\n",
    "        input_dim_spectra=CONFIG[\"input_dim_spectra\"],\n",
    "        input_dim_gaia=CONFIG[\"input_dim_gaia\"],\n",
    "        n_layers=CONFIG[\"n_layers\"],\n",
    "        d_state=CONFIG[\"d_state\"],\n",
    "        d_conv=CONFIG[\"d_conv\"],\n",
    "        expand=CONFIG[\"expand\"],\n",
    "        use_cross_attention=CONFIG[\"use_cross_attention\"],\n",
    "        n_cross_attn_heads=CONFIG[\"n_cross_attn_heads\"],\n",
    "        use_checkpoint=CONFIG[\"use_checkpoint\"],\n",
    "        activation_checkpointing=CONFIG[\"activation_checkpointing\"],\n",
    "        use_half_precision=CONFIG[\"use_half_precision\"],\n",
    "        sequential_processing=CONFIG[\"sequential_processing\"]\n",
    "    ).to(device)\n",
    "    \n",
    "    # Print model memory usage\n",
    "    print(\"\\nMemory usage after creating model:\")\n",
    "    print_gpu_memory()\n",
    "    \n",
    "    # Print model size information\n",
    "    param_size = sum(p.nelement() * (2 if CONFIG[\"use_half_precision\"] else 4) for p in model.parameters()) / (1024**2)\n",
    "    print(f\"\\nModel parameters size: {param_size:.2f} MB\")\n",
    "    print(f\"Number of parameters: {sum(p.nelement() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # Create ensemble\n",
    "    print(\"\\nCreating memory-efficient ensemble...\")\n",
    "    ensemble = MemoryEfficientEnsemble(\n",
    "        model_class=MemoryEfficientStarClassifier,\n",
    "        model_args=CONFIG,\n",
    "        num_models=5,\n",
    "        device=device,\n",
    "        low_memory_mode=True,\n",
    "        quantize_models=True,\n",
    "        offload_optimizer=True\n",
    "    )\n",
    "    \n",
    "    # Print ensemble memory usage\n",
    "    print(\"\\nMemory usage after creating ensemble:\")\n",
    "    print_gpu_memory()\n",
    "    \n",
    "    # Generate some random data for demonstration\n",
    "    print(\"\\nGenerating random data...\")\n",
    "    batch_size = 32  # Use a smaller batch size to save memory\n",
    "    \n",
    "    # Create smaller mock datasets for demonstration\n",
    "    class MockDataset:\n",
    "        def __init__(self, num_samples=1000):\n",
    "            self.X_spectra = torch.randn(num_samples, CONFIG[\"input_dim_spectra\"])\n",
    "            self.X_gaia = torch.randn(num_samples, CONFIG[\"input_dim_gaia\"])\n",
    "            self.y = torch.randint(0, 2, (num_samples, CONFIG[\"num_classes\"])).float()\n",
    "            self.indices = list(range(num_samples))\n",
    "        \n",
    "        def __len__(self):\n",
    "            return len(self.indices)\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            idx = self.indices[idx]\n",
    "            return self.X_spectra[idx], self.X_gaia[idx], self.y[idx]\n",
    "        \n",
    "        def re_sample(self):\n",
    "            np.random.shuffle(self.indices)\n",
    "    \n",
    "    # Create mock datasets\n",
    "    train_dataset = MockDataset(num_samples=1000)\n",
    "    val_dataset = MockDataset(num_samples=200)\n",
    "    test_dataset = MockDataset(num_samples=200)\n",
    "    \n",
    "    # Create data loaders with smaller batches\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    print(\"\\nMemory usage after creating data loaders:\")\n",
    "    print_gpu_memory()\n",
    "    \n",
    "    # Run a test forward pass\n",
    "    print(\"\\nRunning test forward pass...\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X_spectra, X_gaia, _ in train_loader:\n",
    "            X_spectra = X_spectra.to(device)\n",
    "            X_gaia = X_gaia.to(device)\n",
    "            outputs = model(X_spectra, X_gaia)\n",
    "            break\n",
    "    \n",
    "    print(\"\\nMemory usage after forward pass:\")\n",
    "    print_gpu_memory()\n",
    "    \n",
    "    # Clean up\n",
    "    del outputs, X_spectra, X_gaia\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    print(\"\\nMemory usage after cleanup:\")\n",
    "    print_gpu_memory()\n",
    "    \n",
    "    # Demonstrating memory-efficient training (just a couple steps)\n",
    "    print(\"\\nDemonstrating memory-efficient training (small subset)...\")\n",
    "    # Only train for a few steps to demonstrate\n",
    "    train_subset = [next(iter(train_loader)) for _ in range(2)]\n",
    "    val_subset = [next(iter(val_loader))]\n",
    "    \n",
    "    # Create minimal loaders\n",
    "    class MinimalLoader:\n",
    "        def __init__(self, data):\n",
    "            self.data = data\n",
    "            self.dataset = MockDataset(len(data))  # Just for compatibility\n",
    "        def __iter__(self):\n",
    "            return iter(self.data)\n",
    "        def __len__(self):\n",
    "            return len(self.data)\n",
    "    \n",
    "    mini_train_loader = MinimalLoader(train_subset)\n",
    "    mini_val_loader = MinimalLoader(val_subset)\n",
    "    \n",
    "    # Train for just a couple steps\n",
    "    ensemble.train(\n",
    "        train_loader=mini_train_loader,\n",
    "        val_loader=mini_val_loader,\n",
    "        num_epochs=2,\n",
    "        lr=1e-4,\n",
    "        max_patience=5,\n",
    "        log_to_wandb=False,\n",
    "        save_checkpoints=True,\n",
    "        checkpoint_dir='memory_efficient_checkpoints'\n",
    "    )\n",
    "    \n",
    "    print(\"\\nMemory usage after training:\")\n",
    "    print_gpu_memory()\n",
    "    \n",
    "    # Save and load models\n",
    "    if not os.path.exists('memory_efficient_models'):\n",
    "        os.makedirs('memory_efficient_models')\n",
    "    \n",
    "    print(\"\\nSaving models...\")\n",
    "    ensemble.save_models(\"memory_efficient_models/ensemble\")\n",
    "    \n",
    "    print(\"\\nLoading models...\")\n",
    "    ensemble.load_models(\"memory_efficient_models/ensemble\")\n",
    "    \n",
    "    print(\"\\nMemory usage after loading models:\")\n",
    "    print_gpu_memory()\n",
    "    \n",
    "    # Make predictions\n",
    "    print(\"\\nMaking predictions with ensemble...\")\n",
    "    mean_probs, std_probs = ensemble.predict(\n",
    "        mini_val_loader, \n",
    "        batch_size=8  # Use even smaller batch size for prediction\n",
    "    )\n",
    "    \n",
    "    print(\"\\nMemory usage after prediction:\")\n",
    "    print_gpu_memory()\n",
    "    \n",
    "    print(\"\\nPrediction shape:\", mean_probs.shape)\n",
    "    print(\"Uncertainty shape:\", std_probs.shape)\n",
    "    \n",
    "    print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import pandas as pd\n",
    "\n",
    "class EnsembleDashboard:\n",
    "    \"\"\"\n",
    "    Interactive dashboard for exploring ensemble predictions and uncertainty.\n",
    "    \"\"\"\n",
    "    def __init__(self, ensemble, test_dataset, class_names, batch_size=32):\n",
    "        \"\"\"\n",
    "        Initialize the dashboard.\n",
    "        \n",
    "        Args:\n",
    "            ensemble: Trained DeepEnsemble instance\n",
    "            test_dataset: Dataset containing test samples\n",
    "            class_names: List of class names\n",
    "            batch_size: Batch size for processing test samples\n",
    "        \"\"\"\n",
    "        self.ensemble = ensemble\n",
    "        self.test_dataset = test_dataset\n",
    "        self.class_names = class_names\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Process the entire test set to get predictions\n",
    "        self.prepare_predictions()\n",
    "        \n",
    "        # Create dashboard UI\n",
    "        self.create_dashboard()\n",
    "    \n",
    "    def prepare_predictions(self):\n",
    "        \"\"\"Process all test samples to get predictions with uncertainty.\"\"\"\n",
    "        test_loader = DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "        \n",
    "        # Get predictions with uncertainty\n",
    "        self.mean_probs, self.std_probs = self.ensemble.predict(test_loader)\n",
    "        \n",
    "        # Get true labels\n",
    "        self.true_labels = []\n",
    "        for _, _, y in test_loader:\n",
    "            self.true_labels.append(y.numpy())\n",
    "        self.true_labels = np.concatenate(self.true_labels, axis=0)\n",
    "        \n",
    "        # Calculate binary predictions using default threshold\n",
    "        self.threshold = 0.5\n",
    "        self.predictions = (self.mean_probs >= self.threshold).astype(float)\n",
    "        \n",
    "        # Calculate sample-level uncertainty metrics\n",
    "        self.sample_uncertainty = np.mean(self.std_probs, axis=1)\n",
    "        self.sample_error = np.mean(np.abs(self.predictions - self.true_labels), axis=1)\n",
    "        \n",
    "        # Calculate class-level metrics\n",
    "        self.class_accuracy = np.mean((self.predictions == self.true_labels), axis=0)\n",
    "        self.class_uncertainty = np.mean(self.std_probs, axis=0)\n",
    "    \n",
    "    def create_dashboard(self):\n",
    "        \"\"\"Create the interactive dashboard.\"\"\"\n",
    "        # Create sample selection slider\n",
    "        self.sample_slider = widgets.IntSlider(\n",
    "            value=0,\n",
    "            min=0,\n",
    "            max=len(self.test_dataset) - 1,\n",
    "            step=1,\n",
    "            description='Sample:',\n",
    "            continuous_update=False\n",
    "        )\n",
    "        \n",
    "        # Create threshold slider\n",
    "        self.threshold_slider = widgets.FloatSlider(\n",
    "            value=0.5,\n",
    "            min=0.1,\n",
    "            max=0.9,\n",
    "            step=0.05,\n",
    "            description='Threshold:',\n",
    "            continuous_update=False\n",
    "        )\n",
    "        \n",
    "        # Create view selection tabs\n",
    "        self.tab = widgets.Tab()\n",
    "        self.tab.children = [\n",
    "            widgets.VBox([widgets.Label('Sample Prediction')]),\n",
    "            widgets.VBox([widgets.Label('Class Performance')]),\n",
    "            widgets.VBox([widgets.Label('Uncertainty Analysis')]),\n",
    "            widgets.VBox([widgets.Label('Calibration')])\n",
    "        ]\n",
    "        self.tab.set_title(0, 'Sample View')\n",
    "        self.tab.set_title(1, 'Class View')\n",
    "        self.tab.set_title(2, 'Uncertainty')\n",
    "        self.tab.set_title(3, 'Calibration')\n",
    "        \n",
    "        # Create output area\n",
    "        self.output = widgets.Output()\n",
    "        \n",
    "        # Create update button\n",
    "        self.update_button = widgets.Button(\n",
    "            description='Update View',\n",
    "            button_style='primary',\n",
    "            tooltip='Click to update the visualization'\n",
    "        )\n",
    "        self.update_button.on_click(self.update_view)\n",
    "        \n",
    "        # Create filter for high uncertainty\n",
    "        self.uncertainty_filter = widgets.Checkbox(\n",
    "            value=False,\n",
    "            description='Show high uncertainty samples only',\n",
    "            tooltip='Filter to show only samples with high uncertainty'\n",
    "        )\n",
    "        \n",
    "        # Create export button\n",
    "        self.export_button = widgets.Button(\n",
    "            description='Export Results',\n",
    "            button_style='info',\n",
    "            tooltip='Export predictions and uncertainty to CSV'\n",
    "        )\n",
    "        self.export_button.on_click(self.export_results)\n",
    "        \n",
    "        # Arrange widgets\n",
    "        controls = widgets.VBox([\n",
    "            self.sample_slider, \n",
    "            self.threshold_slider,\n",
    "            self.uncertainty_filter,\n",
    "            widgets.HBox([self.update_button, self.export_button])\n",
    "        ])\n",
    "        \n",
    "        # Create dashboard layout\n",
    "        dashboard = widgets.VBox([\n",
    "            widgets.HTML('<h2>Ensemble Uncertainty Dashboard</h2>'),\n",
    "            controls,\n",
    "            self.tab,\n",
    "            self.output\n",
    "        ])\n",
    "        \n",
    "        # Display dashboard\n",
    "        display(dashboard)\n",
    "        \n",
    "        # Initial update\n",
    "        self.update_view(None)\n",
    "    \n",
    "    def update_view(self, _):\n",
    "        \"\"\"Update the visualization based on current settings.\"\"\"\n",
    "        with self.output:\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            # Get current values\n",
    "            sample_idx = self.sample_slider.value\n",
    "            threshold = self.threshold_slider.value\n",
    "            \n",
    "            # Update predictions if threshold changed\n",
    "            if threshold != self.threshold:\n",
    "                self.threshold = threshold\n",
    "                self.predictions = (self.mean_probs >= self.threshold).astype(float)\n",
    "                self.sample_error = np.mean(np.abs(self.predictions - self.true_labels), axis=1)\n",
    "                self.class_accuracy = np.mean((self.predictions == self.true_labels), axis=0)\n",
    "            \n",
    "            # Filter for high uncertainty if selected\n",
    "            if self.uncertainty_filter.value:\n",
    "                high_uncertainty_threshold = np.percentile(self.sample_uncertainty, 80)\n",
    "                high_uncertainty_indices = np.where(self.sample_uncertainty >= high_uncertainty_threshold)[0]\n",
    "                if len(high_uncertainty_indices) > 0:\n",
    "                    sample_idx = high_uncertainty_indices[sample_idx % len(high_uncertainty_indices)]\n",
    "                    self.sample_slider.value = sample_idx\n",
    "            \n",
    "            # Get current tab index\n",
    "            tab_idx = self.tab.selected_index\n",
    "            \n",
    "            # Sample View\n",
    "            if tab_idx == 0:\n",
    "                # Get a single sample\n",
    "                X_spectra, X_gaia, y_true = self.test_dataset[sample_idx]\n",
    "                \n",
    "                # Get predictions for this sample from all models\n",
    "                mean_prob, std_prob, all_probs = self.ensemble.predict_sample(X_spectra, X_gaia)\n",
    "                \n",
    "                fig = plt.figure(figsize=(15, 10))\n",
    "                \n",
    "                # Create a 2x2 subplot grid\n",
    "                gs = fig.add_gridspec(2, 2)\n",
    "                \n",
    "                # Prediction with uncertainty for top classes\n",
    "                ax1 = fig.add_subplot(gs[0, :])\n",
    "                self._plot_sample_prediction(ax1, mean_prob, std_prob, y_true.numpy(), threshold)\n",
    "                \n",
    "                # Individual model predictions\n",
    "                ax2 = fig.add_subplot(gs[1, 0])\n",
    "                self._plot_model_disagreement(ax2, all_probs, mean_prob, y_true.numpy(), threshold)\n",
    "                \n",
    "                # Sample metadata or additional info\n",
    "                ax3 = fig.add_subplot(gs[1, 1])\n",
    "                self._plot_sample_metadata(ax3, sample_idx, mean_prob, std_prob, y_true.numpy())\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            \n",
    "            # Class View\n",
    "            elif tab_idx == 1:\n",
    "                fig = plt.figure(figsize=(15, 10))\n",
    "                \n",
    "                # Create a 2x2 subplot grid\n",
    "                gs = fig.add_gridspec(2, 2)\n",
    "                \n",
    "                # Class accuracy vs uncertainty\n",
    "                ax1 = fig.add_subplot(gs[0, 0])\n",
    "                self._plot_class_accuracy_vs_uncertainty(ax1)\n",
    "                \n",
    "                # Top/bottom classes by uncertainty\n",
    "                ax2 = fig.add_subplot(gs[0, 1])\n",
    "                self._plot_top_bottom_uncertain_classes(ax2)\n",
    "                \n",
    "                # Confusion matrix for a few selected classes\n",
    "                ax3 = fig.add_subplot(gs[1, :])\n",
    "                self._plot_class_prediction_distribution(ax3)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            \n",
    "            # Uncertainty Analysis\n",
    "            elif tab_idx == 2:\n",
    "                fig = plt.figure(figsize=(15, 10))\n",
    "                \n",
    "                # Create a 2x2 subplot grid\n",
    "                gs = fig.add_gridspec(2, 2)\n",
    "                \n",
    "                # Uncertainty distribution\n",
    "                ax1 = fig.add_subplot(gs[0, 0])\n",
    "                self._plot_uncertainty_distribution(ax1)\n",
    "                \n",
    "                # Uncertainty vs error\n",
    "                ax2 = fig.add_subplot(gs[0, 1])\n",
    "                self._plot_uncertainty_vs_error(ax2)\n",
    "                \n",
    "                # High uncertainty samples\n",
    "                ax3 = fig.add_subplot(gs[1, 0])\n",
    "                self._plot_high_uncertainty_analysis(ax3)\n",
    "                \n",
    "                # Low uncertainty samples\n",
    "                ax4 = fig.add_subplot(gs[1, 1])\n",
    "                self._plot_low_uncertainty_analysis(ax4)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            \n",
    "            # Calibration\n",
    "            elif tab_idx == 3:\n",
    "                fig = plt.figure(figsize=(15, 10))\n",
    "                \n",
    "                # Create a 2x2 subplot grid\n",
    "                gs = fig.add_gridspec(2, 2)\n",
    "                \n",
    "                # Calibration curve\n",
    "                ax1 = fig.add_subplot(gs[0, :])\n",
    "                self._plot_calibration_curve(ax1)\n",
    "                \n",
    "                # Reliability diagram\n",
    "                ax2 = fig.add_subplot(gs[1, 0])\n",
    "                self._plot_reliability_diagram(ax2)\n",
    "                \n",
    "                # Calibration by uncertainty level\n",
    "                ax3 = fig.add_subplot(gs[1, 1])\n",
    "                self._plot_calibration_by_uncertainty(ax3)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "    \n",
    "    def _plot_sample_prediction(self, ax, mean_prob, std_prob, true_label, threshold):\n",
    "        \"\"\"Plot prediction with uncertainty bars for a sample.\"\"\"\n",
    "        # Get indices of top k predictions by mean probability\n",
    "        top_k = 10\n",
    "        top_indices = np.argsort(mean_prob)[::-1][:top_k]\n",
    "        \n",
    "        # Extract top k values\n",
    "        top_means = mean_prob[top_indices]\n",
    "        top_stds = std_prob[top_indices]\n",
    "        top_classes = [self.class_names[i] for i in top_indices]\n",
    "        \n",
    "        # Create horizontal bar chart with error bars\n",
    "        y_pos = np.arange(len(top_classes))\n",
    "        bar_colors = ['green' if prob >= threshold else 'red' for prob in top_means]\n",
    "        \n",
    "        # Plot bars\n",
    "        ax.barh(y_pos, top_means, xerr=top_stds, align='center', \n",
    "               alpha=0.7, color=bar_colors, capsize=5)\n",
    "        \n",
    "        # Add true labels markers\n",
    "        true_indices = np.where(true_label == 1)[0]\n",
    "        for i, idx in enumerate(top_indices):\n",
    "            if idx in true_indices:\n",
    "                ax.get_children()[i].set_edgecolor('blue')\n",
    "                ax.get_children()[i].set_linewidth(2)\n",
    "        \n",
    "        # Add threshold line\n",
    "        ax.axvline(x=threshold, color='gray', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Customize plot\n",
    "        ax.set_yticks(y_pos)\n",
    "        ax.set_yticklabels(top_classes)\n",
    "        ax.invert_yaxis()  # labels read top-to-bottom\n",
    "        ax.set_xlabel('Probability')\n",
    "        ax.set_title('Prediction with Uncertainty')\n",
    "        \n",
    "        # Add gridlines\n",
    "        ax.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Add legend\n",
    "        from matplotlib.patches import Patch\n",
    "        legend_elements = [\n",
    "            Patch(facecolor='green', edgecolor='black', label='Above threshold'),\n",
    "            Patch(facecolor='red', edgecolor='black', label='Below threshold'),\n",
    "            Patch(facecolor='white', edgecolor='blue', linewidth=2, label='True label')\n",
    "        ]\n",
    "        \n",
    "        ax.legend(handles=legend_elements, loc='lower right')\n",
    "        \n",
    "        # Add text annotations for probabilities\n",
    "        for i, (mean, std) in enumerate(zip(top_means, top_stds)):\n",
    "            ax.text(mean + 0.02, i, f'{mean:.2f} ± {std:.2f}', va='center')\n",
    "    \n",
    "    def _plot_model_disagreement(self, ax, all_probs, mean_prob, true_label, threshold):\n",
    "        \"\"\"Plot individual model predictions to show disagreement.\"\"\"\n",
    "        # Get top classes\n",
    "        top_k = 5\n",
    "        top_indices = np.argsort(mean_prob)[::-1][:top_k]\n",
    "        top_classes = [self.class_names[i] for i in top_indices]\n",
    "        \n",
    "        # Prepare data for plotting\n",
    "        model_indices = np.arange(all_probs.shape[0])\n",
    "        \n",
    "        # Create a grouped bar chart\n",
    "        bar_width = 0.8 / top_k\n",
    "        for i, class_idx in enumerate(top_indices):\n",
    "            probs = all_probs[:, class_idx]\n",
    "            pos = model_indices - 0.4 + i * bar_width\n",
    "            bars = ax.bar(pos, probs, bar_width, alpha=0.7,\n",
    "                        label=f\"{top_classes[i]}\")\n",
    "            \n",
    "            # Highlight true labels\n",
    "            if class_idx in np.where(true_label == 1)[0]:\n",
    "                for bar in bars:\n",
    "                    bar.set_edgecolor('blue')\n",
    "                    bar.set_linewidth(1.5)\n",
    "        \n",
    "        # Add threshold line\n",
    "        ax.axhline(y=threshold, color='black', linestyle='--', alpha=0.5)\n",
    "        \n",
    "        # Customize plot\n",
    "        ax.set_ylabel('Probability')\n",
    "        ax.set_xlabel('Model')\n",
    "        ax.set_title('Individual Model Predictions')\n",
    "        ax.set_xticks(model_indices)\n",
    "        ax.set_xticklabels([f'Model {i+1}' for i in model_indices])\n",
    "        ax.legend(loc='upper right')\n",
    "        ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    def _plot_sample_metadata(self, ax, sample_idx, mean_prob, std_prob, true_label):\n",
    "        \"\"\"Plot metadata and summary statistics for the selected sample.\"\"\"\n",
    "        # Get uncertainty and error metrics\n",
    "        uncertainty = np.mean(std_prob)\n",
    "        \n",
    "        # Predicted classes (above threshold)\n",
    "        threshold = self.threshold\n",
    "        pred_indices = np.where(mean_prob >= threshold)[0]\n",
    "        pred_classes = [self.class_names[i] for i in pred_indices]\n",
    "        \n",
    "        # True classes\n",
    "        true_indices = np.where(true_label == 1)[0]\n",
    "        true_classes = [self.class_names[i] for i in true_indices]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        if len(true_indices) > 0:\n",
    "            # True positives, false positives, false negatives\n",
    "            tp = np.sum((mean_prob >= threshold) & (true_label == 1))\n",
    "            fp = np.sum((mean_prob >= threshold) & (true_label == 0))\n",
    "            fn = np.sum((mean_prob < threshold) & (true_label == 1))\n",
    "            \n",
    "            # Precision, recall, F1\n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            \n",
    "            # Accuracy\n",
    "            accuracy = np.mean((mean_prob >= threshold).astype(int) == true_label)\n",
    "        else:\n",
    "            precision, recall, f1, accuracy = 0, 0, 0, 0\n",
    "        \n",
    "        # Clear the axis for text display\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Create a text summary\n",
    "        text = f\"Sample #{sample_idx}\\n\\n\"\n",
    "        text += f\"Average Uncertainty: {uncertainty:.3f}\\n\"\n",
    "        text += f\"Decision Threshold: {threshold:.2f}\\n\\n\"\n",
    "        \n",
    "        text += f\"Performance Metrics:\\n\"\n",
    "        text += f\"Accuracy: {accuracy:.3f}\\n\"\n",
    "        text += f\"Precision: {precision:.3f}\\n\"\n",
    "        text += f\"Recall: {recall:.3f}\\n\"\n",
    "        text += f\"F1 Score: {f1:.3f}\\n\\n\"\n",
    "        \n",
    "        text += f\"Predicted Classes ({len(pred_classes)}):\\n\"\n",
    "        text += \", \".join(pred_classes[:5])\n",
    "        if len(pred_classes) > 5:\n",
    "            text += f\" ... ({len(pred_classes) - 5} more)\"\n",
    "        text += \"\\n\\n\"\n",
    "        \n",
    "        text += f\"True Classes ({len(true_classes)}):\\n\"\n",
    "        text += \", \".join(true_classes[:5])\n",
    "        if len(true_classes) > 5:\n",
    "            text += f\" ... ({len(true_classes) - 5} more)\"\n",
    "        \n",
    "        # Display the text\n",
    "        ax.text(0, 1, text, fontsize=10, va='top', linespacing=1.5)\n",
    "        ax.set_title('Sample Information')\n",
    "    \n",
    "    def _plot_class_accuracy_vs_uncertainty(self, ax):\n",
    "        \"\"\"Plot class accuracy vs. uncertainty scatter plot.\"\"\"\n",
    "        # Get class-level accuracy and uncertainty\n",
    "        class_accuracy = np.mean((self.predictions == self.true_labels), axis=0)\n",
    "        class_uncertainty = np.mean(self.std_probs, axis=0)\n",
    "        \n",
    "        # Create scatter plot\n",
    "        scatter = ax.scatter(class_uncertainty, class_accuracy, alpha=0.7, \n",
    "                          s=50, c=np.sum(self.true_labels, axis=0), cmap='viridis')\n",
    "        \n",
    "        # Add correlation line\n",
    "        from scipy.stats import linregress\n",
    "        slope, intercept, r_value, _, _ = linregress(class_uncertainty, class_accuracy)\n",
    "        x = np.linspace(min(class_uncertainty), max(class_uncertainty), 100)\n",
    "        y = slope * x + intercept\n",
    "        ax.plot(x, y, 'r--', alpha=0.7)\n",
    "        \n",
    "        # Add correlation text\n",
    "        ax.text(0.05, 0.05, f'r = {r_value:.3f}', transform=ax.transAxes,\n",
    "              bbox=dict(facecolor='white', alpha=0.7))\n",
    "        \n",
    "        # Add class labels for outliers (most uncertain or least accurate)\n",
    "        outliers = np.argsort(class_uncertainty)[-5:]  # Top 5 most uncertain\n",
    "        for i in outliers:\n",
    "            ax.annotate(self.class_names[i], \n",
    "                      (class_uncertainty[i], class_accuracy[i]),\n",
    "                      xytext=(5, 5), textcoords='offset points')\n",
    "        \n",
    "        # Add colorbar to indicate class frequency\n",
    "        cbar = plt.colorbar(scatter, ax=ax)\n",
    "        cbar.set_label('Class Frequency')\n",
    "        \n",
    "        # Customize plot\n",
    "        ax.set_xlabel('Average Uncertainty')\n",
    "        ax.set_ylabel('Class Accuracy')\n",
    "        ax.set_title('Class Accuracy vs. Uncertainty')\n",
    "        ax.grid(alpha=0.3)\n",
    "    \n",
    "    def _plot_top_bottom_uncertain_classes(self, ax):\n",
    "        \"\"\"Plot top and bottom classes by uncertainty.\"\"\"\n",
    "        # Sort classes by uncertainty\n",
    "        class_uncertainty = np.mean(self.std_probs, axis=0)\n",
    "        sorted_indices = np.argsort(class_uncertainty)\n",
    "        \n",
    "        # Get top and bottom 5 classes\n",
    "        bottom_indices = sorted_indices[:5]  # Least uncertain\n",
    "        top_indices = sorted_indices[-5:][::-1]  # Most uncertain\n",
    "        \n",
    "        # Combine and prepare for plotting\n",
    "        all_indices = np.concatenate([top_indices, bottom_indices])\n",
    "        all_classes = [self.class_names[i] for i in all_indices]\n",
    "        all_uncertainties = class_uncertainty[all_indices]\n",
    "        all_accuracies = self.class_accuracy[all_indices]\n",
    "        \n",
    "        # Create colors for bars\n",
    "        colors = ['red'] * 5 + ['green'] * 5\n",
    "        \n",
    "        # Plot horizontal bars for uncertainty\n",
    "        y_pos = np.arange(len(all_classes))\n",
    "        bars = ax.barh(y_pos, all_uncertainties, align='center', color=colors, alpha=0.7)\n",
    "        \n",
    "        # Add accuracy as text\n",
    "        for i, acc in enumerate(all_accuracies):\n",
    "            ax.text(all_uncertainties[i] + 0.005, i, f'Acc: {acc:.2f}', va='center')\n",
    "        \n",
    "        # Customize plot\n",
    "        ax.set_yticks(y_pos)\n",
    "        ax.set_yticklabels(all_classes)\n",
    "        ax.invert_yaxis()  # Labels read top-to-bottom\n",
    "        ax.set_xlabel('Average Uncertainty')\n",
    "        ax.set_title('Most and Least Uncertain Classes')\n",
    "        ax.grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        # Add legend\n",
    "        from matplotlib.patches import Patch\n",
    "        legend_elements = [\n",
    "            Patch(facecolor='red', label='High Uncertainty'),\n",
    "            Patch(facecolor='green', label='Low Uncertainty')\n",
    "        ]\n",
    "        ax.legend(handles=legend_elements, loc='lower right')\n",
    "    \n",
    "    def _plot_class_prediction_distribution(self, ax):\n",
    "        \"\"\"Plot distribution of predictions by class.\"\"\"\n",
    "        # Select a few representative classes\n",
    "        class_uncertainty = np.mean(self.std_probs, axis=0)\n",
    "        \n",
    "        # Pick classes with varying uncertainty levels\n",
    "        sorted_by_uncertainty = np.argsort(class_uncertainty)\n",
    "        num_classes = len(sorted_by_uncertainty)\n",
    "        \n",
    "        # Choose 5 classes spread across the uncertainty spectrum\n",
    "        indices = [\n",
    "            sorted_by_uncertainty[0],  # Lowest uncertainty\n",
    "            sorted_by_uncertainty[num_classes // 4],\n",
    "            sorted_by_uncertainty[num_classes // 2],\n",
    "            sorted_by_uncertainty[3 * num_classes // 4],\n",
    "            sorted_by_uncertainty[-1]  # Highest uncertainty\n",
    "        ]\n",
    "        \n",
    "        selected_classes = [self.class_names[i] for i in indices]\n",
    "        \n",
    "        # For each selected class, compute the distribution of predicted probabilities\n",
    "        # separated for true positive and true negative samples\n",
    "        data = []\n",
    "        for i, class_idx in enumerate(indices):\n",
    "            # Get positive and negative samples for this class\n",
    "            pos_mask = self.true_labels[:, class_idx] == 1\n",
    "            neg_mask = ~pos_mask\n",
    "            \n",
    "            if np.sum(pos_mask) > 0:\n",
    "                # Positive samples\n",
    "                pos_probs = self.mean_probs[pos_mask, class_idx]\n",
    "                pos_uncertainties = self.std_probs[pos_mask, class_idx]\n",
    "                \n",
    "                # Create violin plot data\n",
    "                pos_data = {\n",
    "                    'Class': selected_classes[i],\n",
    "                    'Type': 'True Positive',\n",
    "                    'Probability': pos_probs,\n",
    "                    'Uncertainty': pos_uncertainties\n",
    "                }\n",
    "                data.append(pos_data)\n",
    "            \n",
    "            if np.sum(neg_mask) > 0:\n",
    "                # Negative samples\n",
    "                neg_probs = self.mean_probs[neg_mask, class_idx]\n",
    "                neg_uncertainties = self.std_probs[neg_mask, class_idx]\n",
    "                \n",
    "                # Create violin plot data\n",
    "                neg_data = {\n",
    "                    'Class': selected_classes[i],\n",
    "                    'Type': 'True Negative',\n",
    "                    'Probability': neg_probs,\n",
    "                    'Uncertainty': neg_uncertainties\n",
    "                }\n",
    "                data.append(neg_data)\n",
    "        \n",
    "        # Create violin plots\n",
    "        for i, d in enumerate(data):\n",
    "            # Position on x-axis\n",
    "            x_pos = i\n",
    "            \n",
    "            # Create violin plot\n",
    "            violin_parts = ax.violinplot(d['Probability'], positions=[x_pos], widths=0.8,\n",
    "                                       showmeans=False, showmedians=True, showextrema=True)\n",
    "            \n",
    "            # Color by type (TP or TN)\n",
    "            color = 'blue' if d['Type'] == 'True Positive' else 'red'\n",
    "            for pc in violin_parts['bodies']:\n",
    "                pc.set_facecolor(color)\n",
    "                pc.set_alpha(0.7)\n",
    "        \n",
    "        # Add threshold line\n",
    "        ax.axhline(y=self.threshold, color='black', linestyle='--', alpha=0.5)\n",
    "        \n",
    "        # Customize plot\n",
    "        ax.set_xticks(range(len(data)))\n",
    "        ax.set_xticklabels([f\"{d['Class']}\\n({d['Type']})\" for d in data], rotation=45, ha='right')\n",
    "        ax.set_ylabel('Predicted Probability')\n",
    "        ax.set_title('Distribution of Predicted Probabilities by Class')\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Add uncertainty annotations\n",
    "        for i, d in enumerate(data):\n",
    "            mean_uncertainty = np.mean(d['Uncertainty'])\n",
    "            ax.text(i, 0.05, f'Unc: {mean_uncertainty:.3f}', ha='center',\n",
    "                  bbox=dict(facecolor='white', alpha=0.7))\n",
    "    \n",
    "    def _plot_uncertainty_distribution(self, ax):\n",
    "        \"\"\"Plot the distribution of uncertainty values.\"\"\"\n",
    "        # Plot histogram of sample-level uncertainty\n",
    "        ax.hist(self.sample_uncertainty, bins=30, alpha=0.7, color='blue')\n",
    "        \n",
    "        # Mark vertical line for median\n",
    "        median_uncertainty = np.median(self.sample_uncertainty)\n",
    "        ax.axvline(x=median_uncertainty, color='red', linestyle='--', alpha=0.7)\n",
    "        ax.text(median_uncertainty + 0.001, 0.9 * ax.get_ylim()[1], \n",
    "              f'Median: {median_uncertainty:.3f}', color='red')\n",
    "        \n",
    "        # Customize plot\n",
    "        ax.set_xlabel('Sample Uncertainty (mean std across classes)')\n",
    "        ax.set_ylabel('Count')\n",
    "        ax.set_title('Distribution of Uncertainty Values')\n",
    "        ax.grid(alpha=0.3)\n",
    "    \n",
    "    def _plot_uncertainty_vs_error(self, ax):\n",
    "        \"\"\"Plot uncertainty vs. error scatter plot.\"\"\"\n",
    "        # Create scatter plot\n",
    "        scatter = ax.scatter(self.sample_uncertainty, self.sample_error, \n",
    "                          alpha=0.3, s=10, c=np.sum(self.true_labels, axis=1), cmap='viridis')\n",
    "        \n",
    "        # Add correlation line\n",
    "        from scipy.stats import linregress\n",
    "        slope, intercept, r_value, _, _ = linregress(self.sample_uncertainty, self.sample_error)\n",
    "        x = np.linspace(min(self.sample_uncertainty), max(self.sample_uncertainty), 100)\n",
    "        y = slope * x + intercept\n",
    "        ax.plot(x, y, 'r--', alpha=0.7)\n",
    "        \n",
    "        # Add correlation text\n",
    "        ax.text(0.05, 0.95, f'r = {r_value:.3f}', transform=ax.transAxes,\n",
    "              bbox=dict(facecolor='white', alpha=0.7))\n",
    "        \n",
    "        # Customize plot\n",
    "        ax.set_xlabel('Uncertainty')\n",
    "        ax.set_ylabel('Error Rate')\n",
    "        ax.set_title('Uncertainty vs. Error Correlation')\n",
    "        ax.grid(alpha=0.3)\n",
    "        \n",
    "        # Add colorbar\n",
    "        cbar = plt.colorbar(scatter, ax=ax)\n",
    "        cbar.set_label('Number of True Classes')\n",
    "    \n",
    "    def _plot_high_uncertainty_analysis(self, ax):\n",
    "        \"\"\"Analyze predictions with high uncertainty.\"\"\"\n",
    "        # Get high uncertainty threshold (e.g., top 10%)\n",
    "        high_threshold = np.percentile(self.sample_uncertainty, 90)\n",
    "        high_mask = self.sample_uncertainty >= high_threshold\n",
    "        \n",
    "        # Count of samples with high uncertainty\n",
    "        num_high = np.sum(high_mask)\n",
    "        \n",
    "        # Average error for high uncertainty samples\n",
    "        high_error = np.mean(self.sample_error[high_mask]) if num_high > 0 else 0\n",
    "        \n",
    "        # Average error for other samples\n",
    "        other_error = np.mean(self.sample_error[~high_mask]) if np.sum(~high_mask) > 0 else 0\n",
    "        \n",
    "        # Error ratio (high uncertainty vs other)\n",
    "        error_ratio = high_error / other_error if other_error > 0 else 0\n",
    "        \n",
    "        # Distribution of high uncertainty predictions by class\n",
    "        high_uncertain_class_counts = np.sum(self.predictions[high_mask], axis=0)\n",
    "        \n",
    "        # Get top classes with high uncertainty predictions\n",
    "        top_indices = np.argsort(high_uncertain_class_counts)[::-1][:10]\n",
    "        top_classes = [self.class_names[i] for i in top_indices]\n",
    "        top_counts = high_uncertain_class_counts[top_indices]\n",
    "        \n",
    "        # Plot bar chart of top classes with high uncertainty\n",
    "        y_pos = np.arange(len(top_classes))\n",
    "        bars = ax.barh(y_pos, top_counts, align='center', alpha=0.7)\n",
    "        \n",
    "        # Add text annotations for error rates by class\n",
    "        for i, idx in enumerate(top_indices):\n",
    "            # Calculate error rate for this class in high uncertainty samples\n",
    "            class_preds = self.predictions[high_mask, idx]\n",
    "            class_true = self.true_labels[high_mask, idx]\n",
    "            class_error = np.mean(np.abs(class_preds - class_true)) if len(class_preds) > 0 else 0\n",
    "            \n",
    "            # Add text annotation\n",
    "            ax.text(top_counts[i] + 0.5, i, f'Error: {class_error:.2f}', va='center')\n",
    "        \n",
    "        # Customize plot\n",
    "        ax.set_yticks(y_pos)\n",
    "        ax.set_yticklabels(top_classes)\n",
    "        ax.invert_yaxis()  # Labels read top-to-bottom\n",
    "        ax.set_xlabel('Count')\n",
    "        \n",
    "        # Add summary text at the top\n",
    "        title = (f'High Uncertainty Samples (>{high_threshold:.3f})\\n'\n",
    "                f'Count: {num_high} ({num_high/len(self.sample_uncertainty):.1%})\\n'\n",
    "                f'Error: {high_error:.3f} vs {other_error:.3f} ({error_ratio:.1f}x higher)')\n",
    "        ax.set_title(title)\n",
    "        ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    def _plot_low_uncertainty_analysis(self, ax):\n",
    "        \"\"\"Analyze predictions with low uncertainty.\"\"\"\n",
    "        # Get low uncertainty threshold (e.g., bottom 10%)\n",
    "        low_threshold = np.percentile(self.sample_uncertainty, 10)\n",
    "        low_mask = self.sample_uncertainty <= low_threshold\n",
    "        \n",
    "        # Count of samples with low uncertainty\n",
    "        num_low = np.sum(low_mask)\n",
    "        \n",
    "        # Average error for low uncertainty samples\n",
    "        low_error = np.mean(self.sample_error[low_mask]) if num_low > 0 else 0\n",
    "        \n",
    "        # Average error for other samples\n",
    "        other_error = np.mean(self.sample_error[~low_mask]) if np.sum(~low_mask) > 0 else 0\n",
    "        \n",
    "        # Error ratio (low uncertainty vs other)\n",
    "        error_ratio = low_error / other_error if other_error > 0 else 0\n",
    "        \n",
    "        # Distribution of low uncertainty predictions by class\n",
    "        low_uncertain_class_counts = np.sum(self.predictions[low_mask], axis=0)\n",
    "        \n",
    "        # Get top classes with low uncertainty predictions\n",
    "        top_indices = np.argsort(low_uncertain_class_counts)[::-1][:10]\n",
    "        top_classes = [self.class_names[i] for i in top_indices]\n",
    "        top_counts = low_uncertain_class_counts[top_indices]\n",
    "        \n",
    "        # Plot bar chart of top classes with low uncertainty\n",
    "        y_pos = np.arange(len(top_classes))\n",
    "        bars = ax.barh(y_pos, top_counts, align='center', alpha=0.7, color='green')\n",
    "        \n",
    "        # Add text annotations for error rates by class\n",
    "        for i, idx in enumerate(top_indices):\n",
    "            # Calculate error rate for this class in low uncertainty samples\n",
    "            class_preds = self.predictions[low_mask, idx]\n",
    "            class_true = self.true_labels[low_mask, idx]\n",
    "            class_error = np.mean(np.abs(class_preds - class_true)) if len(class_preds) > 0 else 0\n",
    "            \n",
    "            # Add text annotation\n",
    "            ax.text(top_counts[i] + 0.5, i, f'Error: {class_error:.2f}', va='center')\n",
    "        \n",
    "        # Customize plot\n",
    "        ax.set_yticks(y_pos)\n",
    "        ax.set_yticklabels(top_classes)\n",
    "        ax.invert_yaxis()  # Labels read top-to-bottom\n",
    "        ax.set_xlabel('Count')\n",
    "        \n",
    "        # Add summary text at the top\n",
    "        title = (f'Low Uncertainty Samples (<{low_threshold:.3f})\\n'\n",
    "                f'Count: {num_low} ({num_low/len(self.sample_uncertainty):.1%})\\n'\n",
    "                f'Error: {low_error:.3f} vs {other_error:.3f} ({error_ratio:.1f}x lower)')\n",
    "        ax.set_title(title)\n",
    "        ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    def _plot_calibration_curve(self, ax):\n",
    "        \"\"\"Plot calibration curve to assess the quality of predicted probabilities.\"\"\"\n",
    "        # Flatten arrays\n",
    "        mean_probs_flat = self.mean_probs.flatten()\n",
    "        std_probs_flat = self.std_probs.flatten()\n",
    "        true_labels_flat = self.true_labels.flatten()\n",
    "        \n",
    "        # Remove NaN values if any\n",
    "        valid_indices = ~np.isnan(mean_probs_flat) & ~np.isnan(true_labels_flat)\n",
    "        mean_probs_flat = mean_probs_flat[valid_indices]\n",
    "        std_probs_flat = std_probs_flat[valid_indices]\n",
    "        true_labels_flat = true_labels_flat[valid_indices]\n",
    "        \n",
    "        # Create bins and calculate calibration metrics\n",
    "        n_bins = 10\n",
    "        bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "        binned = np.digitize(mean_probs_flat, bins) - 1\n",
    "        bin_accs = np.zeros(n_bins)\n",
    "        bin_confs = np.zeros(n_bins)\n",
    "        bin_sizes = np.zeros(n_bins)\n",
    "        bin_uncerts = np.zeros(n_bins)\n",
    "        \n",
    "        for bin_idx in range(n_bins):\n",
    "            bin_mask = binned == bin_idx\n",
    "            if np.sum(bin_mask) > 0:\n",
    "                bin_sizes[bin_idx] = np.sum(bin_mask)\n",
    "                bin_accs[bin_idx] = np.mean(true_labels_flat[bin_mask])\n",
    "                bin_confs[bin_idx] = np.mean(mean_probs_flat[bin_mask])\n",
    "                bin_uncerts[bin_idx] = np.mean(std_probs_flat[bin_mask])\n",
    "        \n",
    "        # Calculate the expected calibration error (ECE)\n",
    "        ece = np.sum(bin_sizes / len(mean_probs_flat) * np.abs(bin_accs - bin_confs))\n",
    "        \n",
    "        # Plot the calibration curve\n",
    "        ax.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfectly calibrated')\n",
    "        \n",
    "        # Color points by uncertainty\n",
    "        sc = ax.scatter(bin_confs, bin_accs, \n",
    "                      s=bin_sizes / np.sum(bin_sizes) * 2000,  # Size proportional to bin size\n",
    "                      c=bin_uncerts, cmap='viridis', alpha=0.8, \n",
    "                      linewidths=1, edgecolors='black')\n",
    "        \n",
    "        # Add colorbar for uncertainty\n",
    "        cbar = plt.colorbar(sc, ax=ax)\n",
    "        cbar.set_label('Mean uncertainty (std)')\n",
    "        \n",
    "        # Customize plot\n",
    "        ax.set_xlabel('Mean predicted probability')\n",
    "        ax.set_ylabel('Fraction of positives (accuracy)')\n",
    "        ax.set_xlim([0, 1])\n",
    "        ax.set_ylim([0, 1])\n",
    "        ax.set_title(f'Calibration Curve (ECE: {ece:.3f})')\n",
    "        ax.grid(linestyle='--', alpha=0.7)\n",
    "        ax.legend(loc='lower right')\n",
    "    \n",
    "    def _plot_reliability_diagram(self, ax):\n",
    "        \"\"\"Plot reliability diagram showing calibration for each bin.\"\"\"\n",
    "        # Flatten arrays\n",
    "        mean_probs_flat = self.mean_probs.flatten()\n",
    "        true_labels_flat = self.true_labels.flatten()\n",
    "        \n",
    "        # Remove NaN values if any\n",
    "        valid_indices = ~np.isnan(mean_probs_flat) & ~np.isnan(true_labels_flat)\n",
    "        mean_probs_flat = mean_probs_flat[valid_indices]\n",
    "        true_labels_flat = true_labels_flat[valid_indices]\n",
    "        \n",
    "        # Create bins and calculate calibration metrics\n",
    "        n_bins = 10\n",
    "        bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "        binned = np.digitize(mean_probs_flat, bins) - 1\n",
    "        bin_accs = np.zeros(n_bins)\n",
    "        bin_confs = np.zeros(n_bins)\n",
    "        bin_sizes = np.zeros(n_bins)\n",
    "        \n",
    "        for bin_idx in range(n_bins):\n",
    "            bin_mask = binned == bin_idx\n",
    "            if np.sum(bin_mask) > 0:\n",
    "                bin_sizes[bin_idx] = np.sum(bin_mask)\n",
    "                bin_accs[bin_idx] = np.mean(true_labels_flat[bin_mask])\n",
    "                bin_confs[bin_idx] = np.mean(mean_probs_flat[bin_mask])\n",
    "        \n",
    "        # Create the reliability diagram\n",
    "        width = 1.0 / n_bins\n",
    "        bin_edges = np.linspace(0, 1, n_bins + 1)\n",
    "        bin_centers = bin_edges[:-1] + width / 2\n",
    "        \n",
    "        # Plot the observed frequency in each bin\n",
    "        ax.bar(bin_centers, bin_accs, width=width, alpha=0.7, color='blue', \n",
    "             label='Observed frequency')\n",
    "        \n",
    "        # Plot the gap between observed and predicted\n",
    "        gaps = bin_accs - bin_confs\n",
    "        ax.bar(bin_centers, gaps, bottom=bin_confs, width=width, alpha=0.7, \n",
    "             color='red' if np.sum(gaps < 0) > np.sum(gaps >= 0) else 'green', \n",
    "             label='Gap')\n",
    "        \n",
    "        # Add perfect calibration line\n",
    "        ax.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfect calibration')\n",
    "        \n",
    "        # Customize plot\n",
    "        ax.set_xlabel('Predicted probability')\n",
    "        ax.set_ylabel('Observed frequency')\n",
    "        ax.set_xlim([0, 1])\n",
    "        ax.set_ylim([0, 1])\n",
    "        ax.set_title('Reliability Diagram')\n",
    "        ax.grid(alpha=0.3)\n",
    "        ax.legend(loc='upper left')\n",
    "        \n",
    "        # Add bin size annotations\n",
    "        for i, (center, size) in enumerate(zip(bin_centers, bin_sizes)):\n",
    "            ax.text(center, 0.05, f'{int(size)}', ha='center', \n",
    "                  bbox=dict(facecolor='white', alpha=0.7))\n",
    "    \n",
    "    def _plot_calibration_by_uncertainty(self, ax):\n",
    "        \"\"\"Plot calibration for different uncertainty levels.\"\"\"\n",
    "        # Sort samples by uncertainty\n",
    "        sorted_indices = np.argsort(self.sample_uncertainty)\n",
    "        n_samples = len(sorted_indices)\n",
    "        \n",
    "        # Create bins with equal number of samples\n",
    "        n_bins = 5\n",
    "        bin_size = n_samples // n_bins\n",
    "        \n",
    "        # Calculate calibration metrics for each bin\n",
    "        bin_eces = []\n",
    "        bin_uncerts = []\n",
    "        \n",
    "        for i in range(n_bins):\n",
    "            # Get indices for this bin\n",
    "            start_idx = i * bin_size\n",
    "            end_idx = (i + 1) * bin_size if i < n_bins - 1 else n_samples\n",
    "            bin_indices = sorted_indices[start_idx:end_idx]\n",
    "            \n",
    "            # Get mean uncertainty for this bin\n",
    "            bin_uncert = np.mean(self.sample_uncertainty[bin_indices])\n",
    "            bin_uncerts.append(bin_uncert)\n",
    "            \n",
    "            # Calculate ECE for this bin\n",
    "            bin_probs = self.mean_probs[bin_indices].flatten()\n",
    "            bin_labels = self.true_labels[bin_indices].flatten()\n",
    "            \n",
    "            # Create sub-bins for ECE calculation\n",
    "            n_sub_bins = 10\n",
    "            sub_bins = np.linspace(0.0, 1.0, n_sub_bins + 1)\n",
    "            binned = np.digitize(bin_probs, sub_bins) - 1\n",
    "            \n",
    "            # Calculate ECE\n",
    "            ece = 0\n",
    "            for sub_bin_idx in range(n_sub_bins):\n",
    "                sub_bin_mask = binned == sub_bin_idx\n",
    "                if np.sum(sub_bin_mask) > 0:\n",
    "                    sub_bin_size = np.sum(sub_bin_mask)\n",
    "                    sub_bin_acc = np.mean(bin_labels[sub_bin_mask])\n",
    "                    sub_bin_conf = np.mean(bin_probs[sub_bin_mask])\n",
    "                    ece += (sub_bin_size / len(bin_probs)) * np.abs(sub_bin_acc - sub_bin_conf)\n",
    "            \n",
    "            bin_eces.append(ece)\n",
    "        \n",
    "        # Plot bar chart of ECE by uncertainty bin\n",
    "        bars = ax.bar(range(n_bins), bin_eces, alpha=0.7)\n",
    "        \n",
    "        # Add text annotations for mean uncertainty\n",
    "        for i, (ece, uncert) in enumerate(zip(bin_eces, bin_uncerts)):\n",
    "            ax.text(i, ece + 0.005, f'Unc: {uncert:.3f}', ha='center')\n",
    "        \n",
    "        # Customize plot\n",
    "        ax.set_xticks(range(n_bins))\n",
    "        ax.set_xticklabels([f'Bin {i+1}' for i in range(n_bins)])\n",
    "        ax.set_ylabel('Expected Calibration Error (ECE)')\n",
    "        ax.set_title('Calibration Error by Uncertainty Level')\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    def export_results(self, _):\n",
    "        \"\"\"Export prediction results and uncertainty metrics to CSV.\"\"\"\n",
    "        with self.output:\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            # Gather results\n",
    "            results = {\n",
    "                'sample_idx': np.arange(len(self.mean_probs)),\n",
    "                'uncertainty': self.sample_uncertainty,\n",
    "                'error_rate': self.sample_error\n",
    "            }\n",
    "            \n",
    "            # Add binary predictions for each class\n",
    "            for i, class_name in enumerate(self.class_names):\n",
    "                results[f'pred_{class_name}'] = self.predictions[:, i]\n",
    "                results[f'true_{class_name}'] = self.true_labels[:, i]\n",
    "                results[f'prob_{class_name}'] = self.mean_probs[:, i]\n",
    "                results[f'uncert_{class_name}'] = self.std_probs[:, i]\n",
    "            \n",
    "            # Create DataFrame\n",
    "            df = pd.DataFrame(results)\n",
    "            \n",
    "            # Save to CSV\n",
    "            filename = 'ensemble_predictions.csv'\n",
    "            df.to_csv(filename, index=False)\n",
    "            \n",
    "            print(f\"Results exported to {filename}\")\n",
    "            print(f\"DataFrame shape: {df.shape}\")\n",
    "            print(\"\\nFirst few rows:\")\n",
    "            display(df.head())\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load saved ensemble\n",
    "    ensemble = DeepEnsemble(\n",
    "        model_class=StarClassifierFusion,\n",
    "        model_args={\n",
    "            \"d_model_spectra\": 2048,\n",
    "            \"d_model_gaia\": 2048,\n",
    "            \"num_classes\": 55,\n",
    "            \"input_dim_spectra\": 3647,\n",
    "            \"input_dim_gaia\": 18,\n",
    "            \"n_layers\": 12,\n",
    "            \"use_cross_attention\": True,\n",
    "            \"n_cross_attn_heads\": 8\n",
    "        },\n",
    "        num_models=5,\n",
    "        device='cuda'\n",
    "    )\n",
    "    \n",
    "    # Load saved models\n",
    "    ensemble.load_models(\"ensemble_mamba_v1\")\n",
    "    \n",
    "    # Initialize class names (replace with actual names)\n",
    "    class_names = [f\"Class_{i}\" for i in range(55)]\n",
    "    \n",
    "    # Create dashboard with test dataset\n",
    "    dashboard = EnsembleDashboard(\n",
    "        ensemble=ensemble,\n",
    "        test_dataset=test_dataset,\n",
    "        class_names=class_names\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
