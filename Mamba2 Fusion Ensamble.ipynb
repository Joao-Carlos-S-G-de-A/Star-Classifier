{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import wandb\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, hamming_loss, roc_auc_score\n",
    "import gc\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mamba_ssm import Mamba2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'StarClassifierFusion' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 764\u001b[0m\n\u001b[1;32m    755\u001b[0m train_args \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    756\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m: num_epochs,\n\u001b[1;32m    757\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m: lr,\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_patience\u001b[39m\u001b[38;5;124m\"\u001b[39m: patience,\n\u001b[1;32m    759\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m\"\u001b[39m: device\n\u001b[1;32m    760\u001b[0m }\n\u001b[1;32m    762\u001b[0m \u001b[38;5;66;03m# Train and evaluate ensemble\u001b[39;00m\n\u001b[1;32m    763\u001b[0m ensemble, metrics, figures \u001b[38;5;241m=\u001b[39m train_and_evaluate_ensemble(\n\u001b[0;32m--> 764\u001b[0m     model_class\u001b[38;5;241m=\u001b[39m\u001b[43mStarClassifierFusion\u001b[49m,\n\u001b[1;32m    765\u001b[0m     model_args\u001b[38;5;241m=\u001b[39mmodel_args,\n\u001b[1;32m    766\u001b[0m     train_loader\u001b[38;5;241m=\u001b[39mtrain_loader,\n\u001b[1;32m    767\u001b[0m     val_loader\u001b[38;5;241m=\u001b[39mval_loader,\n\u001b[1;32m    768\u001b[0m     test_loader\u001b[38;5;241m=\u001b[39mtest_loader,\n\u001b[1;32m    769\u001b[0m     train_function\u001b[38;5;241m=\u001b[39mtrain_model_fusion,\n\u001b[1;32m    770\u001b[0m     train_args\u001b[38;5;241m=\u001b[39mtrain_args,\n\u001b[1;32m    771\u001b[0m     num_models\u001b[38;5;241m=\u001b[39mnum_models,\n\u001b[1;32m    772\u001b[0m     bootstrap\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    773\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[1;32m    774\u001b[0m     class_names\u001b[38;5;241m=\u001b[39mclasses\n\u001b[1;32m    775\u001b[0m )\n\u001b[1;32m    777\u001b[0m \u001b[38;5;66;03m# Save ensemble model\u001b[39;00m\n\u001b[1;32m    778\u001b[0m torch\u001b[38;5;241m.\u001b[39msave({\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_args\u001b[39m\u001b[38;5;124m\"\u001b[39m: model_args,\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_args\u001b[39m\u001b[38;5;124m\"\u001b[39m: train_args,\n\u001b[1;32m    781\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_models\u001b[39m\u001b[38;5;124m\"\u001b[39m: num_models,\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m\"\u001b[39m: [model\u001b[38;5;241m.\u001b[39mstate_dict() \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m ensemble\u001b[38;5;241m.\u001b[39mmodels]\n\u001b[1;32m    783\u001b[0m }, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensemble_model.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'StarClassifierFusion' is not defined"
     ]
    }
   ],
   "source": [
    "class EnsembleStarClassifier:\n",
    "    \"\"\"\n",
    "    Ensemble of StarClassifierFusion models with uncertainty quantification.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_class,\n",
    "        model_args,\n",
    "        num_models=5,\n",
    "        device='cuda'\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model_class: The model class to use (StarClassifierFusion)\n",
    "            model_args: Dictionary of arguments to pass to the model constructor\n",
    "            num_models: Number of models in the ensemble\n",
    "            device: Device to use for computation\n",
    "        \"\"\"\n",
    "        self.num_models = num_models\n",
    "        self.device = device\n",
    "        self.models = []\n",
    "        \n",
    "        # Initialize models with different random initializations\n",
    "        for i in range(num_models):\n",
    "            model = model_class(**model_args)\n",
    "            model.to(device)\n",
    "            self.models.append(model)\n",
    "            \n",
    "    def train(\n",
    "        self,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        test_loader,\n",
    "        train_function,\n",
    "        train_args,\n",
    "        bootstrap=True,\n",
    "        random_seed_offset=0\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Train each model in the ensemble.\n",
    "        \n",
    "        Args:\n",
    "            train_loader: DataLoader for training data\n",
    "            val_loader: DataLoader for validation data\n",
    "            test_loader: DataLoader for test data\n",
    "            train_function: Function to train a single model\n",
    "            train_args: Dictionary of arguments to pass to train_function\n",
    "            bootstrap: Whether to use bootstrapping for training\n",
    "            random_seed_offset: Offset for random seeds\n",
    "        \"\"\"\n",
    "        trained_models = []\n",
    "        \n",
    "        for i in range(self.num_models):\n",
    "            print(f\"Training model {i+1}/{self.num_models}\")\n",
    "            \n",
    "            # Set different random seed for each model\n",
    "            seed = random_seed_offset + i\n",
    "            torch.manual_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "            \n",
    "            if bootstrap:\n",
    "                # Create bootstrapped dataset\n",
    "                bootstrap_train_loader = self._create_bootstrap_loader(train_loader)\n",
    "                curr_train_loader = bootstrap_train_loader\n",
    "            else:\n",
    "                curr_train_loader = train_loader\n",
    "            \n",
    "            # Initialize a new model for this ensemble member\n",
    "            model = copy.deepcopy(self.models[i])\n",
    "            \n",
    "            # Create a new wandb run for this model\n",
    "            run_name = f\"ensemble_member_{i+1}\"\n",
    "            wandb.init(project=\"ALLSTARS_ensemble\", name=run_name, group=\"ensemble_training\", reinit=True)\n",
    "            \n",
    "            # Log ensemble member info\n",
    "            wandb.config.update({\n",
    "                \"ensemble_member\": i+1,\n",
    "                \"num_models\": self.num_models,\n",
    "                \"bootstrap\": bootstrap,\n",
    "                \"random_seed\": seed\n",
    "            })\n",
    "            \n",
    "            # Train the model\n",
    "            trained_model = train_function(\n",
    "                model=model,\n",
    "                train_loader=curr_train_loader,\n",
    "                val_loader=val_loader,\n",
    "                test_loader=test_loader,\n",
    "                **train_args\n",
    "            )\n",
    "            \n",
    "            # Save the trained model\n",
    "            trained_models.append(trained_model)\n",
    "            \n",
    "            # Save model checkpoint\n",
    "            torch.save(trained_model.state_dict(), f\"ensemble_model_{i+1}.pth\")\n",
    "            \n",
    "            # Finish wandb run\n",
    "            wandb.finish()\n",
    "        \n",
    "        self.models = trained_models\n",
    "        return trained_models\n",
    "    \n",
    "    def _create_bootstrap_loader(self, dataloader):\n",
    "        \"\"\"\n",
    "        Create a bootstrapped version of a dataloader.\n",
    "        \n",
    "        Args:\n",
    "            dataloader: Original DataLoader\n",
    "            \n",
    "        Returns:\n",
    "            DataLoader with bootstrapped samples\n",
    "        \"\"\"\n",
    "        dataset = dataloader.dataset\n",
    "        n_samples = len(dataset)\n",
    "        \n",
    "        # Generate bootstrap indices (sampling with replacement)\n",
    "        bootstrap_indices = np.random.choice(n_samples, size=n_samples, replace=True)\n",
    "        \n",
    "        # Create a subset dataset with the bootstrapped indices\n",
    "        bootstrap_dataset = torch.utils.data.Subset(dataset, bootstrap_indices)\n",
    "        \n",
    "        # Create a new dataloader with the bootstrapped dataset\n",
    "        bootstrap_loader = DataLoader(\n",
    "            bootstrap_dataset,\n",
    "            batch_size=dataloader.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=dataloader.num_workers if hasattr(dataloader, 'num_workers') else 0\n",
    "        )\n",
    "        \n",
    "        return bootstrap_loader\n",
    "    \n",
    "    def predict(self, X_spectra, X_gaia, return_individual=False):\n",
    "        \"\"\"\n",
    "        Generate predictions from the ensemble.\n",
    "        \n",
    "        Args:\n",
    "            X_spectra: Spectral features tensor\n",
    "            X_gaia: Gaia features tensor\n",
    "            return_individual: Whether to return individual model predictions\n",
    "            \n",
    "        Returns:\n",
    "            mean_probs: Mean probabilities across ensemble\n",
    "            std_probs: Standard deviation of probabilities (uncertainty)\n",
    "            individual_probs: Individual model probabilities (if return_individual=True)\n",
    "        \"\"\"\n",
    "        # Ensure inputs are on the correct device\n",
    "        X_spectra = X_spectra.to(self.device)\n",
    "        X_gaia = X_gaia.to(self.device)\n",
    "        \n",
    "        all_probs = []\n",
    "        \n",
    "        # Get predictions from each model\n",
    "        for model in self.models:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                logits = model(X_spectra, X_gaia)\n",
    "                probs = torch.sigmoid(logits)\n",
    "                all_probs.append(probs.cpu().numpy())\n",
    "        \n",
    "        # Stack predictions\n",
    "        all_probs = np.stack(all_probs)\n",
    "        \n",
    "        # Calculate mean and standard deviation\n",
    "        mean_probs = np.mean(all_probs, axis=0)\n",
    "        std_probs = np.std(all_probs, axis=0)\n",
    "        \n",
    "        if return_individual:\n",
    "            return mean_probs, std_probs, all_probs\n",
    "        else:\n",
    "            return mean_probs, std_probs\n",
    "    \n",
    "    def evaluate(self, test_loader, threshold=0.5, return_predictions=False):\n",
    "        \"\"\"\n",
    "        Evaluate the ensemble on a test set.\n",
    "        \n",
    "        Args:\n",
    "            test_loader: DataLoader for test data\n",
    "            threshold: Classification threshold\n",
    "            return_predictions: Whether to return predictions\n",
    "            \n",
    "        Returns:\n",
    "            metrics: Dictionary of evaluation metrics\n",
    "            mean_probs: Mean probabilities (if return_predictions=True)\n",
    "            std_probs: Standard deviation of probabilities (if return_predictions=True)\n",
    "            y_true: True labels (if return_predictions=True)\n",
    "        \"\"\"\n",
    "        all_mean_probs = []\n",
    "        all_std_probs = []\n",
    "        all_y_true = []\n",
    "        \n",
    "        # Generate predictions for each batch\n",
    "        for X_spectra, X_gaia, y_batch in test_loader:\n",
    "            X_spectra, X_gaia = X_spectra.to(self.device), X_gaia.to(self.device)\n",
    "            \n",
    "            # Get ensemble predictions\n",
    "            mean_probs, std_probs = self.predict(X_spectra, X_gaia)\n",
    "            \n",
    "            all_mean_probs.extend(mean_probs)\n",
    "            all_std_probs.extend(std_probs)\n",
    "            all_y_true.extend(y_batch.cpu().numpy())\n",
    "        \n",
    "        # Convert to numpy arrays\n",
    "        mean_probs = np.array(all_mean_probs)\n",
    "        std_probs = np.array(all_std_probs)\n",
    "        y_true = np.array(all_y_true)\n",
    "        \n",
    "        # Make binary predictions\n",
    "        y_pred = (mean_probs > threshold).astype(float)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            \"micro_f1\": f1_score(y_true, y_pred, average='micro'),\n",
    "            \"macro_f1\": f1_score(y_true, y_pred, average='macro'),\n",
    "            \"weighted_f1\": f1_score(y_true, y_pred, average='weighted'),\n",
    "            \"micro_precision\": precision_score(y_true, y_pred, average='micro', zero_division=1),\n",
    "            \"macro_precision\": precision_score(y_true, y_pred, average='macro', zero_division=1),\n",
    "            \"weighted_precision\": precision_score(y_true, y_pred, average='weighted', zero_division=1),\n",
    "            \"micro_recall\": recall_score(y_true, y_pred, average='micro'),\n",
    "            \"macro_recall\": recall_score(y_true, y_pred, average='macro'),\n",
    "            \"weighted_recall\": recall_score(y_true, y_pred, average='weighted'),\n",
    "            \"hamming_loss\": hamming_loss(y_true, y_pred),\n",
    "            \"mean_uncertainty\": np.mean(std_probs),\n",
    "            \"median_uncertainty\": np.median(std_probs),\n",
    "            \"max_uncertainty\": np.max(std_probs)\n",
    "        }\n",
    "        \n",
    "        # Try to calculate ROC AUC if possible\n",
    "        try:\n",
    "            metrics[\"roc_auc\"] = roc_auc_score(y_true, mean_probs, average='macro', multi_class='ovr')\n",
    "        except:\n",
    "            metrics[\"roc_auc\"] = None\n",
    "        \n",
    "        if return_predictions:\n",
    "            return metrics, mean_probs, std_probs, y_true\n",
    "        else:\n",
    "            return metrics\n",
    "    \n",
    "    def visualize_uncertainty(self, mean_probs, std_probs, y_true, num_classes=10, class_names=None):\n",
    "        \"\"\"\n",
    "        Visualize uncertainty for selected classes.\n",
    "        \n",
    "        Args:\n",
    "            mean_probs: Mean probabilities from ensemble\n",
    "            std_probs: Standard deviation of probabilities (uncertainty)\n",
    "            y_true: True labels\n",
    "            num_classes: Number of classes to visualize\n",
    "            class_names: List of class names\n",
    "        \"\"\"\n",
    "        n_classes = y_true.shape[1]\n",
    "        \n",
    "        # Select a subset of classes to visualize\n",
    "        classes_to_plot = np.random.choice(n_classes, min(num_classes, n_classes), replace=False)\n",
    "        \n",
    "        # Create figure\n",
    "        fig, axes = plt.subplots(len(classes_to_plot), 1, figsize=(10, 3 * len(classes_to_plot)))\n",
    "        \n",
    "        if len(classes_to_plot) == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for i, class_idx in enumerate(classes_to_plot):\n",
    "            ax = axes[i]\n",
    "            \n",
    "            # Get probabilities, uncertainties, and true labels for this class\n",
    "            probs = mean_probs[:, class_idx]\n",
    "            uncertainties = std_probs[:, class_idx]\n",
    "            true_labels = y_true[:, class_idx]\n",
    "            \n",
    "            # Create scatter plot\n",
    "            scatter = ax.scatter(probs, uncertainties, c=true_labels, cmap='coolwarm', alpha=0.6)\n",
    "            \n",
    "            # Add colorbar\n",
    "            cbar = plt.colorbar(scatter, ax=ax)\n",
    "            cbar.set_label('True Label')\n",
    "            \n",
    "            # Set class label\n",
    "            if class_names is not None:\n",
    "                class_label = class_names[class_idx]\n",
    "            else:\n",
    "                class_label = f\"Class {class_idx}\"\n",
    "            \n",
    "            ax.set_xlabel('Predicted Probability')\n",
    "            ax.set_ylabel('Uncertainty (Std. Dev.)')\n",
    "            ax.set_title(f'Uncertainty vs. Prediction for {class_label}')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add threshold line\n",
    "            ax.axvline(x=0.5, color='gray', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "    \n",
    "    def analyze_errors(self, mean_probs, std_probs, y_true, threshold=0.5):\n",
    "        \"\"\"\n",
    "        Analyze relationship between prediction errors and uncertainty.\n",
    "        \n",
    "        Args:\n",
    "            mean_probs: Mean probabilities from ensemble\n",
    "            std_probs: Standard deviation of probabilities (uncertainty)\n",
    "            y_true: True labels\n",
    "            threshold: Classification threshold\n",
    "            \n",
    "        Returns:\n",
    "            fig: Matplotlib figure\n",
    "        \"\"\"\n",
    "        # Make binary predictions\n",
    "        y_pred = (mean_probs > threshold).astype(float)\n",
    "        \n",
    "        # Calculate error\n",
    "        errors = np.abs(y_true - mean_probs)\n",
    "        \n",
    "        # Flatten arrays\n",
    "        flat_errors = errors.flatten()\n",
    "        flat_uncertainty = std_probs.flatten()\n",
    "        \n",
    "        # Create bins for uncertainty\n",
    "        n_bins = 20\n",
    "        bins = np.linspace(np.min(flat_uncertainty), np.max(flat_uncertainty), n_bins+1)\n",
    "        bin_indices = np.digitize(flat_uncertainty, bins) - 1\n",
    "        \n",
    "        # Calculate mean error for each bin\n",
    "        bin_mean_errors = np.zeros(n_bins)\n",
    "        bin_counts = np.zeros(n_bins)\n",
    "        \n",
    "        for i in range(len(flat_errors)):\n",
    "            bin_idx = bin_indices[i]\n",
    "            if bin_idx >= 0 and bin_idx < n_bins:\n",
    "                bin_mean_errors[bin_idx] += flat_errors[i]\n",
    "                bin_counts[bin_idx] += 1\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        valid_bins = bin_counts > 0\n",
    "        bin_mean_errors[valid_bins] /= bin_counts[valid_bins]\n",
    "        \n",
    "        # Create figure\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        \n",
    "        # Plot mean error vs. uncertainty\n",
    "        bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "        ax.plot(bin_centers, bin_mean_errors, 'o-', markersize=8)\n",
    "        \n",
    "        # Fit linear regression\n",
    "        valid_x = bin_centers[valid_bins]\n",
    "        valid_y = bin_mean_errors[valid_bins]\n",
    "        \n",
    "        if len(valid_x) > 1:\n",
    "            from sklearn.linear_model import LinearRegression\n",
    "            reg = LinearRegression().fit(valid_x.reshape(-1, 1), valid_y)\n",
    "            x_range = np.linspace(np.min(valid_x), np.max(valid_x), 100)\n",
    "            y_pred = reg.predict(x_range.reshape(-1, 1))\n",
    "            ax.plot(x_range, y_pred, 'r--', linewidth=2, \n",
    "                    label=f'Slope: {reg.coef_[0]:.4f}, RÂ²: {reg.score(valid_x.reshape(-1, 1), valid_y):.4f}')\n",
    "            ax.legend()\n",
    "        \n",
    "        ax.set_xlabel('Uncertainty (Std. Dev.)')\n",
    "        ax.set_ylabel('Mean Absolute Error')\n",
    "        ax.set_title('Relationship Between Uncertainty and Prediction Error')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        return fig\n",
    "    \n",
    "    def calibration_curve(self, mean_probs, std_probs, y_true, n_bins=10):\n",
    "        \"\"\"\n",
    "        Plot calibration curve to analyze if predicted probabilities match observed frequencies.\n",
    "        \n",
    "        Args:\n",
    "            mean_probs: Mean probabilities from ensemble\n",
    "            std_probs: Standard deviation of probabilities (uncertainty)\n",
    "            y_true: True labels\n",
    "            n_bins: Number of bins for calibration curve\n",
    "            \n",
    "        Returns:\n",
    "            fig: Matplotlib figure\n",
    "        \"\"\"\n",
    "        # Flatten arrays\n",
    "        flat_probs = mean_probs.flatten()\n",
    "        flat_true = y_true.flatten()\n",
    "        flat_uncertainty = std_probs.flatten()\n",
    "        \n",
    "        # Create bins for probabilities\n",
    "        bins = np.linspace(0, 1, n_bins+1)\n",
    "        bin_indices = np.digitize(flat_probs, bins) - 1\n",
    "        \n",
    "        # Calculate observed frequency and mean predicted probability for each bin\n",
    "        bin_obs_freq = np.zeros(n_bins)\n",
    "        bin_pred_prob = np.zeros(n_bins)\n",
    "        bin_uncertainty = np.zeros(n_bins)\n",
    "        bin_counts = np.zeros(n_bins)\n",
    "        \n",
    "        for i in range(len(flat_probs)):\n",
    "            bin_idx = bin_indices[i]\n",
    "            if bin_idx >= 0 and bin_idx < n_bins:\n",
    "                bin_obs_freq[bin_idx] += flat_true[i]\n",
    "                bin_pred_prob[bin_idx] += flat_probs[i]\n",
    "                bin_uncertainty[bin_idx] += flat_uncertainty[i]\n",
    "                bin_counts[bin_idx] += 1\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        valid_bins = bin_counts > 0\n",
    "        bin_obs_freq[valid_bins] /= bin_counts[valid_bins]\n",
    "        bin_pred_prob[valid_bins] /= bin_counts[valid_bins]\n",
    "        bin_uncertainty[valid_bins] /= bin_counts[valid_bins]\n",
    "        \n",
    "        # Create figure\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        \n",
    "        # Plot calibration curve\n",
    "        bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "        ax.plot(bin_centers, bin_obs_freq, 'o-', markersize=8, label='Calibration Curve')\n",
    "        \n",
    "        # Plot perfect calibration\n",
    "        ax.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')\n",
    "        \n",
    "        # Plot uncertainties\n",
    "        ax2 = ax.twinx()\n",
    "        ax2.bar(bin_centers, bin_uncertainty, alpha=0.2, width=1/n_bins, color='r', label='Mean Uncertainty')\n",
    "        ax2.set_ylabel('Mean Uncertainty (Std. Dev.)', color='r')\n",
    "        ax2.tick_params(axis='y', labelcolor='r')\n",
    "        \n",
    "        # Add labels\n",
    "        ax.set_xlabel('Mean Predicted Probability')\n",
    "        ax.set_ylabel('Observed Frequency')\n",
    "        ax.set_title('Calibration Curve with Uncertainty')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add legends\n",
    "        lines1, labels1 = ax.get_legend_handles_labels()\n",
    "        lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "        ax.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "        \n",
    "        return fig\n",
    "    \n",
    "    def uncertainty_threshold(self, mean_probs, std_probs, y_true, threshold=0.5, uncertainty_percentiles=None):\n",
    "        \"\"\"\n",
    "        Analyze performance at different uncertainty thresholds.\n",
    "        \n",
    "        Args:\n",
    "            mean_probs: Mean probabilities from ensemble\n",
    "            std_probs: Standard deviation of probabilities (uncertainty)\n",
    "            y_true: True labels\n",
    "            threshold: Classification threshold\n",
    "            uncertainty_percentiles: List of uncertainty percentiles to evaluate\n",
    "            \n",
    "        Returns:\n",
    "            fig: Matplotlib figure\n",
    "            metrics: Dictionary of metrics at different uncertainty thresholds\n",
    "        \"\"\"\n",
    "        if uncertainty_percentiles is None:\n",
    "            uncertainty_percentiles = [0, 25, 50, 75, 90, 95]\n",
    "        \n",
    "        # Flatten arrays for uncertainty analysis\n",
    "        flat_uncertainty = std_probs.flatten()\n",
    "        \n",
    "        # Calculate uncertainty thresholds\n",
    "        uncertainty_thresholds = [np.percentile(flat_uncertainty, p) for p in uncertainty_percentiles]\n",
    "        \n",
    "        # Calculate metrics at different uncertainty thresholds\n",
    "        metrics = []\n",
    "        coverage = []\n",
    "        \n",
    "        for unc_thresh in uncertainty_thresholds:\n",
    "            # Create mask for samples below uncertainty threshold\n",
    "            mask = np.max(std_probs, axis=1) <= unc_thresh\n",
    "            \n",
    "            # Skip if no samples meet the criteria\n",
    "            if np.sum(mask) == 0:\n",
    "                metrics.append(None)\n",
    "                coverage.append(0)\n",
    "                continue\n",
    "            \n",
    "            # Filter predictions and true labels\n",
    "            filtered_probs = mean_probs[mask]\n",
    "            filtered_true = y_true[mask]\n",
    "            \n",
    "            # Make binary predictions\n",
    "            filtered_pred = (filtered_probs > threshold).astype(float)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            current_metrics = {\n",
    "                \"micro_f1\": f1_score(filtered_true, filtered_pred, average='micro'),\n",
    "                \"macro_f1\": f1_score(filtered_true, filtered_pred, average='macro'),\n",
    "                \"weighted_f1\": f1_score(filtered_true, filtered_pred, average='weighted'),\n",
    "                \"hamming_loss\": hamming_loss(filtered_true, filtered_pred),\n",
    "            }\n",
    "            \n",
    "            metrics.append(current_metrics)\n",
    "            coverage.append(np.mean(mask))\n",
    "        \n",
    "        # Create figure\n",
    "        fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "        \n",
    "        # Plot F1 score\n",
    "        f1_scores = [m[\"micro_f1\"] if m is not None else 0 for m in metrics]\n",
    "        ax1.plot(uncertainty_percentiles, f1_scores, 'bo-', label='Micro F1 Score')\n",
    "        \n",
    "        # Plot macro F1 score\n",
    "        macro_f1_scores = [m[\"macro_f1\"] if m is not None else 0 for m in metrics]\n",
    "        ax1.plot(uncertainty_percentiles, macro_f1_scores, 'go-', label='Macro F1 Score')\n",
    "        \n",
    "        # Plot coverage\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.plot(uncertainty_percentiles, coverage, 'r--', label='Data Coverage')\n",
    "        ax2.set_ylabel('Data Coverage', color='r')\n",
    "        ax2.tick_params(axis='y', labelcolor='r')\n",
    "        \n",
    "        # Add labels\n",
    "        ax1.set_xlabel('Uncertainty Percentile Threshold')\n",
    "        ax1.set_ylabel('F1 Score')\n",
    "        ax1.set_title('Performance vs. Uncertainty Threshold')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add legend\n",
    "        lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "        lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "        ax1.legend(lines1 + lines2, labels1 + labels2, loc='center right')\n",
    "        \n",
    "        return fig, {\"metrics\": metrics, \"coverage\": coverage, \"percentiles\": uncertainty_percentiles}\n",
    "    \n",
    "    def selective_prediction(self, mean_probs, std_probs, y_true, threshold=0.5):\n",
    "        \"\"\"\n",
    "        Perform selective prediction analysis.\n",
    "        \n",
    "        Args:\n",
    "            mean_probs: Mean probabilities from ensemble\n",
    "            std_probs: Standard deviation of probabilities (uncertainty)\n",
    "            y_true: True labels\n",
    "            threshold: Classification threshold\n",
    "            \n",
    "        Returns:\n",
    "            fig: Matplotlib figure\n",
    "        \"\"\"\n",
    "        # Calculate max uncertainty for each sample\n",
    "        max_uncertainties = np.max(std_probs, axis=1)\n",
    "        \n",
    "        # Sort samples by uncertainty\n",
    "        sorted_indices = np.argsort(max_uncertainties)\n",
    "        \n",
    "        # Initialize lists for storing results\n",
    "        coverages = []\n",
    "        f1_scores = []\n",
    "        \n",
    "        # Calculate metrics at different coverage levels\n",
    "        coverage_steps = np.linspace(0.1, 1.0, 10)\n",
    "        \n",
    "        for coverage in coverage_steps:\n",
    "            # Select top-k% most certain predictions\n",
    "            k = int(len(sorted_indices) * coverage)\n",
    "            selected_indices = sorted_indices[:k]\n",
    "            \n",
    "            # Filter predictions and true labels\n",
    "            selected_probs = mean_probs[selected_indices]\n",
    "            selected_true = y_true[selected_indices]\n",
    "            \n",
    "            # Make binary predictions\n",
    "            selected_pred = (selected_probs > threshold).astype(float)\n",
    "            \n",
    "            # Calculate F1 score\n",
    "            f1 = f1_score(selected_true, selected_pred, average='micro')\n",
    "            \n",
    "            coverages.append(coverage)\n",
    "            f1_scores.append(f1)\n",
    "        \n",
    "        # Create figure\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        \n",
    "        # Plot F1 score vs. coverage\n",
    "        ax.plot(coverages, f1_scores, 'bo-', markersize=8)\n",
    "        \n",
    "        # Add area under curve\n",
    "        ax.fill_between(coverages, 0, f1_scores, alpha=0.2)\n",
    "        \n",
    "        # Add labels\n",
    "        ax.set_xlabel('Coverage (Fraction of Data)')\n",
    "        ax.set_ylabel('Micro F1 Score')\n",
    "        ax.set_title('Selective Prediction: Performance vs. Coverage')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add area under the curve value\n",
    "        auc = np.trapz(f1_scores, coverages)\n",
    "        ax.text(0.05, 0.95, f'AUC: {auc:.4f}', transform=ax.transAxes, \n",
    "                fontsize=12, verticalalignment='top', bbox=dict(boxstyle='round', alpha=0.1))\n",
    "        \n",
    "        return fig\n",
    "\n",
    "# Function to run ensemble training and evaluation\n",
    "def train_and_evaluate_ensemble(\n",
    "    model_class,\n",
    "    model_args,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    test_loader,\n",
    "    train_function,\n",
    "    train_args,\n",
    "    num_models=5,\n",
    "    bootstrap=True,\n",
    "    device='cuda',\n",
    "    class_names=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Train and evaluate an ensemble model.\n",
    "    \n",
    "    Args:\n",
    "        model_class: The model class to use\n",
    "        model_args: Arguments for model initialization\n",
    "        train_loader: DataLoader for training data\n",
    "        val_loader: DataLoader for validation data\n",
    "        test_loader: DataLoader for test data\n",
    "        train_function: Function to train a single model\n",
    "        train_args: Arguments for the training function\n",
    "        num_models: Number of models in the ensemble\n",
    "        bootstrap: Whether to use bootstrapping for training\n",
    "        device: Device to use for computation\n",
    "        class_names: Names of the classes (optional)\n",
    "        \n",
    "    Returns:\n",
    "        ensemble: Trained ensemble model\n",
    "        metrics: Evaluation metrics\n",
    "        figures: Dictionary of visualization figures\n",
    "    \"\"\"\n",
    "    # Initialize wandb for the ensemble experiment\n",
    "    wandb.init(project=\"ALLSTARS_ensemble\", name=\"ensemble_experiment\", reinit=True)\n",
    "    \n",
    "    # Log ensemble configuration\n",
    "    wandb.config.update({\n",
    "        \"num_models\": num_models,\n",
    "        \"bootstrap\": bootstrap,\n",
    "        **model_args,\n",
    "        **train_args\n",
    "    })\n",
    "    \n",
    "    # Initialize ensemble\n",
    "    ensemble = EnsembleStarClassifier(\n",
    "        model_class=model_class,\n",
    "        model_args=model_args,\n",
    "        num_models=num_models,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Train ensemble\n",
    "    ensemble.train(\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader,\n",
    "        train_function=train_function,\n",
    "        train_args=train_args,\n",
    "        bootstrap=bootstrap\n",
    "    )\n",
    "    \n",
    "    # Evaluate ensemble\n",
    "    metrics, mean_probs, std_probs, y_true = ensemble.evaluate(\n",
    "        test_loader=test_loader,\n",
    "        return_predictions=True\n",
    "    )\n",
    "    \n",
    "    # Log evaluation metrics\n",
    "    wandb.log(metrics)\n",
    "    \n",
    "    # Create visualizations\n",
    "    figures = {}\n",
    "    \n",
    "    # Uncertainty visualization\n",
    "    uncertainty_fig = ensemble.visualize_uncertainty(\n",
    "        mean_probs=mean_probs,\n",
    "        std_probs=std_probs,\n",
    "        y_true=y_true,\n",
    "        num_classes=10,\n",
    "        class_names=class_names\n",
    "    )\n",
    "    figures['uncertainty'] = uncertainty_fig\n",
    "    \n",
    "    # Error analysis\n",
    "    error_fig = ensemble.analyze_errors(\n",
    "        mean_probs=mean_probs,\n",
    "        std_probs=std_probs,\n",
    "        y_true=y_true\n",
    "    )\n",
    "    figures['error_analysis'] = error_fig\n",
    "    \n",
    "    # Calibration curve\n",
    "    calibration_fig = ensemble.calibration_curve(\n",
    "        mean_probs=mean_probs,\n",
    "        std_probs=std_probs,\n",
    "        y_true=y_true\n",
    "    )\n",
    "    figures['calibration'] = calibration_fig\n",
    "    \n",
    "    # Uncertainty threshold analysis\n",
    "    threshold_fig, threshold_metrics = ensemble.uncertainty_threshold(\n",
    "        mean_probs=mean_probs,\n",
    "        std_probs=std_probs,\n",
    "        y_true=y_true\n",
    "    )\n",
    "    figures['threshold_analysis'] = threshold_fig\n",
    "    \n",
    "    # Selective prediction\n",
    "    selective_fig = ensemble.selective_prediction(\n",
    "        mean_probs=mean_probs,\n",
    "        std_probs=std_probs,\n",
    "        y_true=y_true\n",
    "    )\n",
    "    figures['selective_prediction'] = selective_fig\n",
    "    \n",
    "    # Log figures\n",
    "    for name, fig in figures.items():\n",
    "        wandb.log({name: wandb.Image(fig)})\n",
    "        plt.close(fig)\n",
    "    \n",
    "    # Finish wandb run\n",
    "    wandb.finish()\n",
    "    \n",
    "    return ensemble, metrics, figures\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example configuration for ensemble\n",
    "    num_models = 5\n",
    "    \n",
    "    # Model configuration\n",
    "    d_model_spectra = 2048\n",
    "    d_model_gaia = 2048\n",
    "    num_classes = 55\n",
    "    input_dim_spectra = 3647\n",
    "    input_dim_gaia = 18\n",
    "    n_layers = 12\n",
    "    \n",
    "    # Training configuration\n",
    "    lr = 2.5e-4\n",
    "    patience = 600\n",
    "    num_epochs = 200\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Initialize model arguments\n",
    "    model_args = {\n",
    "        \"d_model_spectra\": d_model_spectra,\n",
    "        \"d_model_gaia\": d_model_gaia,\n",
    "        \"num_classes\": num_classes,\n",
    "        \"input_dim_spectra\": input_dim_spectra,\n",
    "        \"input_dim_gaia\": input_dim_gaia,\n",
    "        \"n_layers\": n_layers,\n",
    "        \"use_cross_attention\": True,\n",
    "        \"n_cross_attn_heads\": 8\n",
    "    }\n",
    "    \n",
    "    # Initialize training arguments\n",
    "    train_args = {\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"lr\": lr,\n",
    "        \"max_patience\": patience,\n",
    "        \"device\": device\n",
    "    }\n",
    "    \n",
    "    # Train and evaluate ensemble\n",
    "    ensemble, metrics, figures = train_and_evaluate_ensemble(\n",
    "        model_class=StarClassifierFusion,\n",
    "        model_args=model_args,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader,\n",
    "        train_function=train_model_fusion,\n",
    "        train_args=train_args,\n",
    "        num_models=num_models,\n",
    "        bootstrap=True,\n",
    "        device=device,\n",
    "        class_names=classes\n",
    "    )\n",
    "    \n",
    "    # Save ensemble model\n",
    "    torch.save({\n",
    "        \"model_args\": model_args,\n",
    "        \"train_args\": train_args,\n",
    "        \"num_models\": num_models,\n",
    "        \"models\": [model.state_dict() for model in ensemble.models]\n",
    "    }, \"ensemble_model.pth\")\n",
    "    \n",
    "    print(\"Ensemble model training and evaluation complete!\")\n",
    "    print(f\"Final metrics: {metrics}\")\n",
    "    \n",
    "    # Generate predictions with uncertainty\n",
    "    # Example:\n",
    "    # mean_probs, std_probs = ensemble.predict(X_test_spectra, X_test_gaia)\n",
    "    # predictions = (mean_probs > 0.5).astype(float)\n",
    "    # uncertainties = std_probs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
