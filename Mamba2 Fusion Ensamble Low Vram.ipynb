{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from mamba_ssm import Mamba2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryEfficientMamba(nn.Module):\n",
    "    \"\"\"\n",
    "    Memory-efficient wrapper for Mamba2 with gradient checkpointing and \n",
    "    optional parameter quantization.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_state=16, d_conv=4, expand=2, use_checkpoint=True):\n",
    "        super().__init__()\n",
    "        self.mamba = Mamba2(\n",
    "            d_model=d_model,\n",
    "            d_state=d_state,\n",
    "            d_conv=d_conv,\n",
    "            expand=expand\n",
    "        )\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.use_checkpoint and self.training:\n",
    "            return checkpoint(self.mamba, x)\n",
    "        else:\n",
    "            return self.mamba(x)\n",
    "\n",
    "class MemoryEfficientStarClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Memory-efficient version of StarClassifierFusion with various\n",
    "    optimizations to reduce VRAM usage.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model_spectra,\n",
    "        d_model_gaia,\n",
    "        num_classes,\n",
    "        input_dim_spectra,\n",
    "        input_dim_gaia,\n",
    "        n_layers=6,\n",
    "        use_cross_attention=True,\n",
    "        n_cross_attn_heads=8,\n",
    "        d_state=16,  # Reduced from 256 to save memory\n",
    "        d_conv=4,\n",
    "        expand=2,\n",
    "        use_checkpoint=True,\n",
    "        activation_checkpointing=True,\n",
    "        use_half_precision=True,\n",
    "        sequential_processing=True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "        self.activation_checkpointing = activation_checkpointing\n",
    "        self.sequential_processing = sequential_processing\n",
    "        \n",
    "        # Use lower precision\n",
    "        self.dtype = torch.float16 if use_half_precision else torch.float32\n",
    "\n",
    "        # Input projection layers\n",
    "        self.input_proj_spectra = nn.Linear(input_dim_spectra, d_model_spectra)\n",
    "        self.input_proj_gaia = nn.Linear(input_dim_gaia, d_model_gaia)\n",
    "        \n",
    "        # Memory-efficient Mamba layers\n",
    "        self.mamba_spectra_layers = nn.ModuleList([\n",
    "            MemoryEfficientMamba(\n",
    "                d_model=d_model_spectra,\n",
    "                d_state=d_state,\n",
    "                d_conv=d_conv,\n",
    "                expand=expand,\n",
    "                use_checkpoint=activation_checkpointing\n",
    "            ) for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        self.mamba_gaia_layers = nn.ModuleList([\n",
    "            MemoryEfficientMamba(\n",
    "                d_model=d_model_gaia,\n",
    "                d_state=d_state,\n",
    "                d_conv=d_conv,\n",
    "                expand=expand,\n",
    "                use_checkpoint=activation_checkpointing\n",
    "            ) for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "        # Cross-attention (optional)\n",
    "        self.use_cross_attention = use_cross_attention\n",
    "        if use_cross_attention:\n",
    "            self.cross_attn_block_spectra = self._create_cross_attn_block(d_model_spectra, n_heads=n_cross_attn_heads)\n",
    "            self.cross_attn_block_gaia = self._create_cross_attn_block(d_model_gaia, n_heads=n_cross_attn_heads)\n",
    "\n",
    "        # Final classifier\n",
    "        fusion_dim = d_model_spectra + d_model_gaia\n",
    "        self.layer_norm = nn.LayerNorm(fusion_dim)\n",
    "        self.classifier = nn.Linear(fusion_dim, num_classes)\n",
    "    \n",
    "    def _create_cross_attn_block(self, d_model, n_heads):\n",
    "        \"\"\"Creates a cross-attention block with optional gradient checkpointing.\"\"\"\n",
    "        class CrossAttentionBlock(nn.Module):\n",
    "            def __init__(self, d_model, n_heads):\n",
    "                super().__init__()\n",
    "                self.cross_attn = nn.MultiheadAttention(\n",
    "                    embed_dim=d_model, \n",
    "                    num_heads=n_heads, \n",
    "                    batch_first=True\n",
    "                )\n",
    "                self.norm1 = nn.LayerNorm(d_model)\n",
    "                \n",
    "                self.ffn = nn.Sequential(\n",
    "                    nn.Linear(d_model, 4 * d_model),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(4 * d_model, d_model)\n",
    "                )\n",
    "                self.norm2 = nn.LayerNorm(d_model)\n",
    "                \n",
    "            def forward(self, x_q, x_kv):\n",
    "                # Cross-attention\n",
    "                attn_output, _ = self.cross_attn(query=x_q, key=x_kv, value=x_kv)\n",
    "                x = self.norm1(x_q + attn_output)\n",
    "                \n",
    "                # Feed forward\n",
    "                ffn_out = self.ffn(x)\n",
    "                x = self.norm2(x + ffn_out)\n",
    "                \n",
    "                return x\n",
    "        \n",
    "        block = CrossAttentionBlock(d_model, n_heads)\n",
    "        \n",
    "        # Wrap with gradient checkpointing if requested\n",
    "        if self.activation_checkpointing:\n",
    "            def forward_with_checkpoint(module, x_q, x_kv):\n",
    "                def custom_forward(x_q, x_kv):\n",
    "                    return module(x_q, x_kv)\n",
    "                return checkpoint(custom_forward, x_q, x_kv)\n",
    "            \n",
    "            class CheckpointedCrossAttention(nn.Module):\n",
    "                def __init__(self, block):\n",
    "                    super().__init__()\n",
    "                    self.block = block\n",
    "                \n",
    "                def forward(self, x_q, x_kv):\n",
    "                    return forward_with_checkpoint(self.block, x_q, x_kv)\n",
    "            \n",
    "            return CheckpointedCrossAttention(block)\n",
    "        else:\n",
    "            return block\n",
    "    \n",
    "    def _process_mamba_layers(self, x, layers):\n",
    "        \"\"\"Process input through Mamba layers, optionally sequentially to save memory.\"\"\"\n",
    "        if self.sequential_processing:\n",
    "            for layer in layers:\n",
    "                x = layer(x)\n",
    "        else:\n",
    "            # Process all layers at once (uses more memory but faster)\n",
    "            for layer in layers:\n",
    "                x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x_spectra, x_gaia):\n",
    "        # Convert to half precision if requested\n",
    "        if hasattr(self, 'dtype') and self.dtype == torch.float16:\n",
    "            x_spectra = x_spectra.half()\n",
    "            x_gaia = x_gaia.half()\n",
    "        \n",
    "        # Project inputs\n",
    "        x_spectra = self.input_proj_spectra(x_spectra)\n",
    "        x_gaia = self.input_proj_gaia(x_gaia)\n",
    "        \n",
    "        # Add sequence dimension if needed\n",
    "        if len(x_spectra.shape) == 2:\n",
    "            x_spectra = x_spectra.unsqueeze(1)\n",
    "        if len(x_gaia.shape) == 2:\n",
    "            x_gaia = x_gaia.unsqueeze(1)\n",
    "        \n",
    "        # Process through Mamba layers\n",
    "        x_spectra = self._process_mamba_layers(x_spectra, self.mamba_spectra_layers)\n",
    "        x_gaia = self._process_mamba_layers(x_gaia, self.mamba_gaia_layers)\n",
    "        \n",
    "        # Optional cross-attention\n",
    "        if self.use_cross_attention:\n",
    "            x_spectra_fused = self.cross_attn_block_spectra(x_spectra, x_gaia)\n",
    "            x_gaia_fused = self.cross_attn_block_gaia(x_gaia, x_spectra)\n",
    "            x_spectra = x_spectra_fused\n",
    "            x_gaia = x_gaia_fused\n",
    "        \n",
    "        # Pool across sequence dimension\n",
    "        x_spectra = x_spectra.mean(dim=1)\n",
    "        x_gaia = x_gaia.mean(dim=1)\n",
    "        \n",
    "        # Concatenate\n",
    "        x_fused = torch.cat([x_spectra, x_gaia], dim=-1)\n",
    "        \n",
    "        # Final classification\n",
    "        x_fused = self.layer_norm(x_fused)\n",
    "        logits = self.classifier(x_fused)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "class UltraMemoryEfficientEnsemble:\n",
    "    \"\"\"\n",
    "    Ultra memory-efficient implementation of ensemble for uncertainty quantification.\n",
    "    Avoids creating multiple models in memory and removes quantization during initialization.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        model_class, \n",
    "        model_args, \n",
    "        num_models=5, \n",
    "        device='cuda',\n",
    "        checkpoint_dir='ensemble_checkpoints'\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the ultra memory-efficient ensemble.\n",
    "        \n",
    "        Args:\n",
    "            model_class: Model class to instantiate\n",
    "            model_args: Arguments for model initialization\n",
    "            num_models: Number of models in ensemble\n",
    "            device: Device to use\n",
    "            checkpoint_dir: Directory to save/load model checkpoints\n",
    "        \"\"\"\n",
    "        self.model_class = model_class\n",
    "        self.model_args = model_args\n",
    "        self.num_models = num_models\n",
    "        self.device = device\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        \n",
    "        # Create directory if it doesn't exist\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        \n",
    "        # We don't create any models during initialization to save memory\n",
    "        self.active_model = None\n",
    "        self.active_model_idx = -1\n",
    "    \n",
    "    def _create_model(self, model_idx, for_inference=False):\n",
    "        \"\"\"Create a new model instance with appropriate seed.\"\"\"\n",
    "        # Set seeds for reproducibility\n",
    "        torch.manual_seed(42 + model_idx)\n",
    "        np.random.seed(42 + model_idx)\n",
    "        \n",
    "        # Create model instance\n",
    "        model = self.model_class(**self.model_args)\n",
    "        \n",
    "        # Move to device and set mode\n",
    "        model = model.to(self.device)\n",
    "        if for_inference:\n",
    "            model.eval()\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def _get_checkpoint_path(self, model_idx):\n",
    "        \"\"\"Get path for model checkpoint.\"\"\"\n",
    "        return os.path.join(self.checkpoint_dir, f\"model_{model_idx}.pt\")\n",
    "    \n",
    "    def _save_model(self, model, model_idx):\n",
    "        \"\"\"Save model to checkpoint.\"\"\"\n",
    "        # Save to disk\n",
    "        checkpoint_path = self._get_checkpoint_path(model_idx)\n",
    "        # Move state_dict to CPU before saving to avoid GPU memory issues\n",
    "        state_dict = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "        torch.save(state_dict, checkpoint_path)\n",
    "    \n",
    "    def _load_model(self, model_idx, for_inference=False):\n",
    "        \"\"\"Load a model for the specified index.\"\"\"\n",
    "        # Clear any existing model to free memory\n",
    "        if self.active_model is not None:\n",
    "            del self.active_model\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "        \n",
    "        # Create a new model instance\n",
    "        model = self._create_model(model_idx, for_inference)\n",
    "        \n",
    "        # Load checkpoint if it exists\n",
    "        checkpoint_path = self._get_checkpoint_path(model_idx)\n",
    "        if os.path.exists(checkpoint_path):\n",
    "            # Load state dict from CPU to save GPU memory\n",
    "            state_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "            model.load_state_dict(state_dict)\n",
    "            \n",
    "            # Move model to device after loading weights\n",
    "            model = model.to(self.device)\n",
    "        \n",
    "        self.active_model = model\n",
    "        self.active_model_idx = model_idx\n",
    "        return model\n",
    "    \n",
    "    def train_single_model(\n",
    "        self, \n",
    "        model_idx,\n",
    "        train_loader, \n",
    "        val_loader, \n",
    "        test_loader=None, \n",
    "        num_epochs=100, \n",
    "        lr=1e-4, \n",
    "        max_patience=20,\n",
    "        scheduler_type='OneCycleLR',\n",
    "        batch_accumulation=1,  # Gradient accumulation steps\n",
    "        log_to_wandb=True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Train a single model in the ensemble.\n",
    "        \n",
    "        Args:\n",
    "            model_idx: Index of the model to train\n",
    "            train_loader: DataLoader for training data\n",
    "            val_loader: DataLoader for validation data\n",
    "            test_loader: DataLoader for test data (optional)\n",
    "            num_epochs: Maximum number of epochs to train\n",
    "            lr: Learning rate\n",
    "            max_patience: Maximum patience for early stopping\n",
    "            scheduler_type: Type of learning rate scheduler ('OneCycleLR' or 'ReduceLROnPlateau')\n",
    "            batch_accumulation: Number of batches to accumulate gradients (to simulate larger batch size)\n",
    "            log_to_wandb: Whether to log training progress to wandb\n",
    "            \n",
    "        Returns:\n",
    "            Trained model (also saved to checkpoint)\n",
    "        \"\"\"\n",
    "        import torch.optim as optim\n",
    "        from tqdm import tqdm\n",
    "        \n",
    "        # Initialize wandb if requested\n",
    "        if log_to_wandb:\n",
    "            try:\n",
    "                import wandb\n",
    "                run = wandb.init(\n",
    "                    project=\"ALLSTARS_ultra_memory_efficient\", \n",
    "                    name=f\"model_{model_idx}\",\n",
    "                    group=\"memory_efficient_training\",\n",
    "                    config={\n",
    "                        **self.model_args,\n",
    "                        \"model_idx\": model_idx,\n",
    "                        \"num_models\": self.num_models,\n",
    "                        \"lr\": lr,\n",
    "                        \"max_patience\": max_patience,\n",
    "                        \"scheduler_type\": scheduler_type,\n",
    "                        \"batch_accumulation\": batch_accumulation,\n",
    "                        \"num_epochs\": num_epochs\n",
    "                    },\n",
    "                    reinit=True\n",
    "                )\n",
    "            except ImportError:\n",
    "                print(\"wandb not installed. Training without logging.\")\n",
    "                log_to_wandb = False\n",
    "        \n",
    "        # Load or create model\n",
    "        model = self._load_model(model_idx, for_inference=False)\n",
    "        model.train()\n",
    "        \n",
    "        # Create optimizer (SGD uses less memory than Adam/AdamW)\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "        \n",
    "        # Configure the scheduler\n",
    "        if scheduler_type == 'OneCycleLR':\n",
    "            scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "                optimizer, \n",
    "                max_lr=lr,\n",
    "                epochs=num_epochs, \n",
    "                steps_per_epoch=len(train_loader) // batch_accumulation\n",
    "            )\n",
    "        else:  # ReduceLROnPlateau\n",
    "            scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer, \n",
    "                mode='min', \n",
    "                factor=0.5, \n",
    "                patience=int(max_patience / 5)\n",
    "            )\n",
    "        \n",
    "        # Calculate class weights for imbalanced classes\n",
    "        all_labels = []\n",
    "        for _, _, y_batch in train_loader:\n",
    "            all_labels.extend(y_batch.cpu().numpy())\n",
    "        \n",
    "        class_weights = self._calculate_class_weights(np.array(all_labels))\n",
    "        class_weights = torch.tensor(class_weights, dtype=torch.float).to(self.device)\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights)\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        patience = max_patience\n",
    "        \n",
    "        # Initialize mixed precision scaler if supported\n",
    "        scaler = torch.cuda.amp.GradScaler() if hasattr(torch.cuda, 'amp') else None\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            # Resample training data if needed\n",
    "            if hasattr(train_loader.dataset, 're_sample'):\n",
    "                train_loader.dataset.re_sample()\n",
    "                \n",
    "                # Recompute class weights if needed\n",
    "                all_labels = []\n",
    "                for _, _, y_batch in train_loader:\n",
    "                    all_labels.extend(y_batch.cpu().numpy())\n",
    "                class_weights = self._calculate_class_weights(np.array(all_labels))\n",
    "                class_weights = torch.tensor(class_weights, dtype=torch.float).to(self.device)\n",
    "                criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights)\n",
    "\n",
    "            # --- Training Phase ---\n",
    "            model.train()\n",
    "            train_loss, train_acc = 0.0, 0.0\n",
    "            batch_count = 0\n",
    "            \n",
    "            for i, (X_spc, X_ga, y_batch) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\")):\n",
    "                X_spc, X_ga, y_batch = X_spc.to(self.device), X_ga.to(self.device), y_batch.to(self.device)\n",
    "                \n",
    "                # Only zero gradients at the start of accumulation cycle\n",
    "                if i % batch_accumulation == 0:\n",
    "                    optimizer.zero_grad(set_to_none=True)\n",
    "                \n",
    "                # Forward pass with mixed precision if available\n",
    "                if scaler:\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        outputs = model(X_spc, X_ga)\n",
    "                        loss = criterion(outputs, y_batch) / batch_accumulation\n",
    "                    \n",
    "                    # Backward pass with scaled gradients\n",
    "                    scaler.scale(loss).backward()\n",
    "                    \n",
    "                    # Step only at the end of accumulation cycle\n",
    "                    if (i + 1) % batch_accumulation == 0 or (i + 1) == len(train_loader):\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "                        if scheduler_type == 'OneCycleLR':\n",
    "                            scheduler.step()\n",
    "                else:\n",
    "                    outputs = model(X_spc, X_ga)\n",
    "                    loss = criterion(outputs, y_batch) / batch_accumulation\n",
    "                    loss.backward()\n",
    "                    \n",
    "                    if (i + 1) % batch_accumulation == 0 or (i + 1) == len(train_loader):\n",
    "                        optimizer.step()\n",
    "                        if scheduler_type == 'OneCycleLR':\n",
    "                            scheduler.step()\n",
    "                \n",
    "                # Calculate metrics\n",
    "                train_loss += loss.item() * batch_accumulation * X_spc.size(0)\n",
    "                predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                correct = (predicted == y_batch).float()\n",
    "                train_acc += correct.mean(dim=1).sum().item()\n",
    "                batch_count += X_spc.size(0)\n",
    "                \n",
    "                # Free up memory\n",
    "                del X_spc, X_ga, y_batch, outputs, loss, predicted, correct\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            train_loss /= batch_count\n",
    "            train_acc /= batch_count\n",
    "\n",
    "            # --- Validation Phase ---\n",
    "            model.eval()\n",
    "            val_loss, val_acc = 0.0, 0.0\n",
    "            val_batch_count = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for X_spc, X_ga, y_batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n",
    "                    X_spc, X_ga, y_batch = X_spc.to(self.device), X_ga.to(self.device), y_batch.to(self.device)\n",
    "                    \n",
    "                    # Forward pass with mixed precision if available\n",
    "                    if scaler:\n",
    "                        with torch.cuda.amp.autocast():\n",
    "                            outputs = model(X_spc, X_ga)\n",
    "                            loss = criterion(outputs, y_batch)\n",
    "                    else:\n",
    "                        outputs = model(X_spc, X_ga)\n",
    "                        loss = criterion(outputs, y_batch)\n",
    "                    \n",
    "                    val_loss += loss.item() * X_spc.size(0)\n",
    "                    predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                    correct = (predicted == y_batch).float()\n",
    "                    val_acc += correct.mean(dim=1).sum().item()\n",
    "                    val_batch_count += X_spc.size(0)\n",
    "                    \n",
    "                    # Free up memory\n",
    "                    del X_spc, X_ga, y_batch, outputs, loss, predicted, correct\n",
    "                    torch.cuda.empty_cache()\n",
    "            \n",
    "            val_loss /= val_batch_count\n",
    "            val_acc /= val_batch_count\n",
    "\n",
    "            # --- Test Phase (if provided) ---\n",
    "            test_metrics = {}\n",
    "            if test_loader is not None:\n",
    "                test_loss, test_acc = 0.0, 0.0\n",
    "                test_batch_count = 0\n",
    "                y_true, y_pred = [], []\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    for X_spc, X_ga, y_batch in tqdm(test_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Testing\"):\n",
    "                        X_spc, X_ga, y_batch = X_spc.to(self.device), X_ga.to(self.device), y_batch.to(self.device)\n",
    "                        \n",
    "                        # Forward pass with mixed precision if available\n",
    "                        if scaler:\n",
    "                            with torch.cuda.amp.autocast():\n",
    "                                outputs = model(X_spc, X_ga)\n",
    "                                loss = criterion(outputs, y_batch)\n",
    "                        else:\n",
    "                            outputs = model(X_spc, X_ga)\n",
    "                            loss = criterion(outputs, y_batch)\n",
    "                        \n",
    "                        test_loss += loss.item() * X_spc.size(0)\n",
    "                        predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                        correct = (predicted == y_batch).float()\n",
    "                        test_acc += correct.mean(dim=1).sum().item()\n",
    "                        test_batch_count += X_spc.size(0)\n",
    "                        \n",
    "                        # Store on CPU to save GPU memory\n",
    "                        y_true.extend(y_batch.cpu().numpy())\n",
    "                        y_pred.extend(predicted.cpu().numpy())\n",
    "                        \n",
    "                        # Free up memory\n",
    "                        del X_spc, X_ga, y_batch, outputs, loss, predicted, correct\n",
    "                        torch.cuda.empty_cache()\n",
    "                \n",
    "                test_loss /= test_batch_count\n",
    "                test_acc /= test_batch_count\n",
    "                \n",
    "                # Calculate metrics on CPU to save GPU memory\n",
    "                test_metrics = self._calculate_metrics(np.array(y_true), np.array(y_pred))\n",
    "                test_metrics.update({\n",
    "                    \"test_loss\": test_loss,\n",
    "                    \"test_acc\": test_acc,\n",
    "                })\n",
    "\n",
    "            # Log metrics\n",
    "            if log_to_wandb:\n",
    "                log_data = {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"train_loss\": train_loss,\n",
    "                    \"val_loss\": val_loss,\n",
    "                    \"train_acc\": train_acc,\n",
    "                    \"val_acc\": val_acc,\n",
    "                    \"lr\": self._get_lr(optimizer)\n",
    "                }\n",
    "                log_data.update(test_metrics)\n",
    "                wandb.log(log_data)\n",
    "            \n",
    "            # Print progress\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} - \"\n",
    "                  f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "            # Update ReduceLROnPlateau scheduler if used\n",
    "            if scheduler_type == 'ReduceLROnPlateau':\n",
    "                scheduler.step(val_loss)\n",
    "\n",
    "            # Early stopping and checkpoint saving\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience = max_patience\n",
    "                \n",
    "                # Save best model\n",
    "                self._save_model(model, model_idx)\n",
    "                \n",
    "                if log_to_wandb:\n",
    "                    wandb.run.summary[\"best_val_loss\"] = best_val_loss\n",
    "            else:\n",
    "                patience -= 1\n",
    "                if patience <= 0:\n",
    "                    print(\"Early stopping triggered.\")\n",
    "                    break\n",
    "            \n",
    "            # Save every 10 epochs as a checkpoint\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                checkpoint_path = os.path.join(self.checkpoint_dir, f\"model_{model_idx}_epoch_{epoch+1}.pt\")\n",
    "                state_dict = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "                torch.save(state_dict, checkpoint_path)\n",
    "        \n",
    "        # Close wandb run if used\n",
    "        if log_to_wandb:\n",
    "            wandb.finish()\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def train(\n",
    "        self,\n",
    "        train_loader, \n",
    "        val_loader, \n",
    "        test_loader=None, \n",
    "        num_epochs=100, \n",
    "        lr=1e-4, \n",
    "        max_patience=20,\n",
    "        scheduler_type='OneCycleLR',\n",
    "        batch_accumulation=1,\n",
    "        log_to_wandb=True\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Train all models in the ensemble.\n",
    "        \n",
    "        Args:\n",
    "            train_loader: DataLoader for training data\n",
    "            val_loader: DataLoader for validation data\n",
    "            test_loader: DataLoader for test data (optional)\n",
    "            num_epochs: Maximum number of epochs to train\n",
    "            lr: Learning rate\n",
    "            max_patience: Maximum patience for early stopping\n",
    "            scheduler_type: Type of learning rate scheduler\n",
    "            batch_accumulation: Number of batches to accumulate gradients\n",
    "            log_to_wandb: Whether to log training progress to wandb\n",
    "            \n",
    "        Returns:\n",
    "            List of trained model paths\n",
    "        \"\"\"\n",
    "        for model_idx in range(self.num_models):\n",
    "            print(f\"\\n----- Training Ensemble Model {model_idx+1}/{self.num_models} -----\\n\")\n",
    "            \n",
    "            # Train this model\n",
    "            self.train_single_model(\n",
    "                model_idx=model_idx,\n",
    "                train_loader=train_loader,\n",
    "                val_loader=val_loader,\n",
    "                test_loader=test_loader,\n",
    "                num_epochs=num_epochs,\n",
    "                lr=lr,\n",
    "                max_patience=max_patience,\n",
    "                scheduler_type=scheduler_type,\n",
    "                batch_accumulation=batch_accumulation,\n",
    "                log_to_wandb=log_to_wandb\n",
    "            )\n",
    "            \n",
    "            # Free memory before moving to next model\n",
    "            if self.active_model is not None:\n",
    "                del self.active_model\n",
    "                self.active_model = None\n",
    "                self.active_model_idx = -1\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "        \n",
    "        # Return paths to all model checkpoints\n",
    "        return [self._get_checkpoint_path(i) for i in range(self.num_models)]\n",
    "    \n",
    "    def predict(self, loader, return_individual=False, micro_batch_size=1):\n",
    "        \"\"\"\n",
    "        Make predictions with the ensemble, one model at a time to save memory.\n",
    "        Uses micro-batching to further reduce memory usage.\n",
    "        \n",
    "        Args:\n",
    "            loader: DataLoader for the data to predict\n",
    "            return_individual: Whether to return predictions from individual models\n",
    "            micro_batch_size: Size of micro-batches to process at once\n",
    "            \n",
    "        Returns:\n",
    "            mean_probs: Mean probability across all models\n",
    "            std_probs: Standard deviation of probabilities (uncertainty measure)\n",
    "            individual_probs: (Optional) Predictions from each individual model\n",
    "        \"\"\"\n",
    "        from tqdm import tqdm\n",
    "        import math\n",
    "        \n",
    "        # Get shapes from first batch\n",
    "        for X_spc, X_ga, y in loader:\n",
    "            num_classes = y.shape[1]\n",
    "            break\n",
    "        \n",
    "        # Total number of samples\n",
    "        num_samples = len(loader.dataset)\n",
    "        \n",
    "        # Create array to store all model outputs (if return_individual)\n",
    "        all_model_outputs = [] if return_individual else None\n",
    "        \n",
    "        # Running sum and sum of squares for mean and std calculation\n",
    "        sum_outputs = np.zeros((num_samples, num_classes))\n",
    "        sum_squared_outputs = np.zeros((num_samples, num_classes))\n",
    "        \n",
    "        # Process each model sequentially\n",
    "        for model_idx in range(self.num_models):\n",
    "            print(f\"Making predictions with model {model_idx+1}/{self.num_models}\")\n",
    "            \n",
    "            # Load model\n",
    "            model = self._load_model(model_idx, for_inference=True)\n",
    "            model.eval()\n",
    "            \n",
    "            # Array to store this model's outputs\n",
    "            model_outputs = np.zeros((num_samples, num_classes))\n",
    "            \n",
    "            # Track current position in the outputs array\n",
    "            sample_idx = 0\n",
    "            \n",
    "            # Process batches\n",
    "            with torch.no_grad():\n",
    "                for X_spc, X_ga, _ in tqdm(loader, desc=f\"Model {model_idx+1}\"):\n",
    "                    batch_size = X_spc.shape[0]\n",
    "                    \n",
    "                    # Process in micro-batches to save memory\n",
    "                    for micro_start in range(0, batch_size, micro_batch_size):\n",
    "                        micro_end = min(micro_start + micro_batch_size, batch_size)\n",
    "                        \n",
    "                        X_spc_micro = X_spc[micro_start:micro_end].to(self.device)\n",
    "                        X_ga_micro = X_ga[micro_start:micro_end].to(self.device)\n",
    "                        \n",
    "                        # Forward pass with mixed precision\n",
    "                        if hasattr(torch.cuda, 'amp'):\n",
    "                            with torch.cuda.amp.autocast():\n",
    "                                outputs = model(X_spc_micro, X_ga_micro)\n",
    "                        else:\n",
    "                            outputs = model(X_spc_micro, X_ga_micro)\n",
    "                        \n",
    "                        # Get probabilities\n",
    "                        probs = torch.sigmoid(outputs).cpu().numpy()\n",
    "                        \n",
    "                        # Store in output array\n",
    "                        start_idx = sample_idx + micro_start\n",
    "                        end_idx = sample_idx + micro_end\n",
    "                        model_outputs[start_idx:end_idx] = probs\n",
    "                        \n",
    "                        # Free memory\n",
    "                        del X_spc_micro, X_ga_micro, outputs, probs\n",
    "                        torch.cuda.empty_cache()\n",
    "                    \n",
    "                    # Update sample index\n",
    "                    sample_idx += batch_size\n",
    "            \n",
    "            # Store model outputs if returning individual predictions\n",
    "            if return_individual:\n",
    "                all_model_outputs.append(model_outputs)\n",
    "            \n",
    "            # Update sums for mean and std calculation\n",
    "            sum_outputs += model_outputs\n",
    "            sum_squared_outputs += model_outputs**2\n",
    "            \n",
    "            # Free memory\n",
    "            del model, model_outputs\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "        \n",
    "        # Calculate mean and std\n",
    "        mean_probs = sum_outputs / self.num_models\n",
    "        \n",
    "        # Calculate standard deviation\n",
    "        variance = (sum_squared_outputs / self.num_models) - (mean_probs**2)\n",
    "        # Clip small negative values that can occur due to numerical issues\n",
    "        variance = np.clip(variance, 0, None)\n",
    "        std_probs = np.sqrt(variance)\n",
    "        \n",
    "        if return_individual:\n",
    "            return mean_probs, std_probs, np.array(all_model_outputs)\n",
    "        else:\n",
    "            return mean_probs, std_probs\n",
    "\n",
    "    def _calculate_class_weights(self, y):\n",
    "        \"\"\"Calculate class weights for handling imbalanced classes.\"\"\"\n",
    "        if y.ndim > 1:  \n",
    "            class_counts = np.sum(y, axis=0)  \n",
    "        else:\n",
    "            class_counts = np.bincount(y)\n",
    "\n",
    "        total_samples = y.shape[0] if y.ndim > 1 else len(y)\n",
    "        class_counts = np.where(class_counts == 0, 1, class_counts)  # Prevent division by zero\n",
    "        class_weights = total_samples / (len(class_counts) * class_counts)\n",
    "        \n",
    "        return class_weights\n",
    "    \n",
    "    def _calculate_metrics(self, y_true, y_pred):\n",
    "        \"\"\"Calculate evaluation metrics for multi-label classification.\"\"\"\n",
    "        from sklearn.metrics import f1_score, precision_score, recall_score, hamming_loss\n",
    "        \n",
    "        metrics = {\n",
    "            \"micro_f1\": f1_score(y_true, y_pred, average='micro'),\n",
    "            \"macro_f1\": f1_score(y_true, y_pred, average='macro'),\n",
    "            \"weighted_f1\": f1_score(y_true, y_pred, average='weighted'),\n",
    "            \"micro_precision\": precision_score(y_true, y_pred, average='micro', zero_division=1),\n",
    "            \"macro_precision\": precision_score(y_true, y_pred, average='macro', zero_division=1),\n",
    "            \"weighted_precision\": precision_score(y_true, y_pred, average='weighted', zero_division=1),\n",
    "            \"micro_recall\": recall_score(y_true, y_pred, average='micro'),\n",
    "            \"macro_recall\": recall_score(y_true, y_pred, average='macro'),\n",
    "            \"weighted_recall\": recall_score(y_true, y_pred, average='weighted'),\n",
    "            \"hamming_loss\": hamming_loss(y_true, y_pred)\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def _get_lr(self, optimizer):\n",
    "        \"\"\"Get current learning rate from optimizer.\"\"\"\n",
    "        for param_group in optimizer.param_groups:\n",
    "            return param_group['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalBalancedMultiLabelDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A balanced multi-label dataset that returns (X_spectra, X_gaia, y).\n",
    "    It uses the same balancing strategy as `BalancedMultiLabelDataset`.\n",
    "    \"\"\"\n",
    "    def __init__(self, X_spectra, X_gaia, y, limit_per_label=201):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X_spectra (torch.Tensor): [num_samples, num_spectra_features]\n",
    "            X_gaia (torch.Tensor): [num_samples, num_gaia_features]\n",
    "            y (torch.Tensor): [num_samples, num_classes], multi-hot labels\n",
    "            limit_per_label (int): limit or target number of samples per label\n",
    "        \"\"\"\n",
    "        self.X_spectra = X_spectra\n",
    "        self.X_gaia = X_gaia\n",
    "        self.y = y\n",
    "        self.limit_per_label = limit_per_label\n",
    "        self.num_classes = y.shape[1]\n",
    "        self.indices = self.balance_classes()\n",
    "        \n",
    "    def balance_classes(self):\n",
    "        indices = []\n",
    "        class_counts = torch.sum(self.y, axis=0)\n",
    "        for cls in range(self.num_classes):\n",
    "            cls_indices = np.where(self.y[:, cls] == 1)[0]\n",
    "            if len(cls_indices) < self.limit_per_label:\n",
    "                if len(cls_indices) == 0:\n",
    "                    # No samples for this class\n",
    "                    continue\n",
    "                extra_indices = np.random.choice(\n",
    "                    cls_indices, self.limit_per_label - len(cls_indices), replace=True\n",
    "                )\n",
    "                cls_indices = np.concatenate([cls_indices, extra_indices])\n",
    "            elif len(cls_indices) > self.limit_per_label:\n",
    "                cls_indices = np.random.choice(cls_indices, self.limit_per_label, replace=False)\n",
    "            indices.extend(cls_indices)\n",
    "        indices = np.unique(indices)\n",
    "        np.random.shuffle(indices)\n",
    "        return indices\n",
    "\n",
    "    def re_sample(self):\n",
    "        self.indices = self.balance_classes()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        index = self.indices[idx]\n",
    "        return (\n",
    "            self.X_spectra[index],  # spectra features\n",
    "            self.X_gaia[index],     # gaia features\n",
    "            self.y[index],          # multi-hot labels\n",
    "        )\n",
    "def calculate_class_weights(y):\n",
    "    if y.ndim > 1:  \n",
    "        class_counts = np.sum(y, axis=0)  \n",
    "    else:\n",
    "        class_counts = np.bincount(y)\n",
    "\n",
    "    total_samples = y.shape[0] if y.ndim > 1 else len(y)\n",
    "    class_counts = np.where(class_counts == 0, 1, class_counts)  # Prevent division by zero\n",
    "    class_weights = total_samples / (len(class_counts) * class_counts)\n",
    "    \n",
    "    return class_weights\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    metrics = {\n",
    "        \"micro_f1\": f1_score(y_true, y_pred, average='micro'),\n",
    "        \"macro_f1\": f1_score(y_true, y_pred, average='macro'),\n",
    "        \"weighted_f1\": f1_score(y_true, y_pred, average='weighted'),\n",
    "        \"micro_precision\": precision_score(y_true, y_pred, average='micro', zero_division=1),\n",
    "        \"macro_precision\": precision_score(y_true, y_pred, average='macro', zero_division=1),\n",
    "        \"weighted_precision\": precision_score(y_true, y_pred, average='weighted', zero_division=1),\n",
    "        \"micro_recall\": recall_score(y_true, y_pred, average='micro'),\n",
    "        \"macro_recall\": recall_score(y_true, y_pred, average='macro'),\n",
    "        \"weighted_recall\": recall_score(y_true, y_pred, average='weighted'),\n",
    "        \"hamming_loss\": hamming_loss(y_true, y_pred)\n",
    "    }\n",
    "    \n",
    "    # Check if there are at least two classes present in y_true\n",
    "    #if len(np.unique(y_true)) > 1:\n",
    "        #metrics[\"roc_auc\"] = roc_auc_score(y_true, y_pred, average='macro', multi_class='ovr')\n",
    "    #else:\n",
    "       # metrics[\"roc_auc\"] = None  # or you can set it to a default value or message\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parallax                   0\n",
      "ra                         0\n",
      "dec                        0\n",
      "ra_error                   0\n",
      "dec_error                  0\n",
      "parallax_error             0\n",
      "pmra                       0\n",
      "pmdec                      0\n",
      "pmra_error                 0\n",
      "pmdec_error                0\n",
      "phot_g_mean_flux           0\n",
      "flagnopllx                 0\n",
      "phot_g_mean_flux_error     0\n",
      "phot_bp_mean_flux          0\n",
      "phot_rp_mean_flux          0\n",
      "phot_bp_mean_flux_error    0\n",
      "phot_rp_mean_flux_error    0\n",
      "flagnoflux                 0\n",
      "dtype: int64\n",
      "parallax                   0\n",
      "ra                         0\n",
      "dec                        0\n",
      "ra_error                   0\n",
      "dec_error                  0\n",
      "parallax_error             0\n",
      "pmra                       0\n",
      "pmdec                      0\n",
      "pmra_error                 0\n",
      "pmdec_error                0\n",
      "phot_g_mean_flux           0\n",
      "flagnopllx                 0\n",
      "phot_g_mean_flux_error     0\n",
      "phot_bp_mean_flux          0\n",
      "phot_rp_mean_flux          0\n",
      "phot_bp_mean_flux_error    0\n",
      "phot_rp_mean_flux_error    0\n",
      "flagnoflux                 0\n",
      "dtype: int64\n",
      "X_train_spectra shape: torch.Size([87134, 3647])\n",
      "X_val_spectra shape: torch.Size([21784, 3647])\n",
      "X_test_spectra shape: torch.Size([27237, 3647])\n",
      "X_train_gaia shape: torch.Size([87134, 18])\n",
      "X_val_gaia shape: torch.Size([21784, 18])\n",
      "X_test_gaia shape: torch.Size([27237, 18])\n",
      "y_train shape: torch.Size([87134, 55])\n",
      "y_val shape: torch.Size([21784, 55])\n",
      "y_test shape: torch.Size([27237, 55])\n",
      "Train dataset: 1871 samples\n",
      "Validation dataset: 1513 samples\n",
      "Test dataset: 1576 samples\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "batch_limit = int(batch_size / 2.5)\n",
    "\n",
    "# Load datasets\n",
    "#X_train_full = pd.read_pickle(\"Pickles/train_data_transformed2.pkl\")\n",
    "#X_test_full = pd.read_pickle(\"Pickles/test_data_transformed.pkl\")\n",
    "# classes = pd.read_pickle(\"Pickles/Updated_list_of_Classes.pkl\")\n",
    "import pickle\n",
    "# Open them in a cross-platform way\n",
    "with open(\"Pickles/Updated_List_of_Classes_ubuntu.pkl\", \"rb\") as f:\n",
    "    classes = pickle.load(f)  # This reads the actual data\n",
    "with open(\"Pickles/train_data_transformed_ubuntu.pkl\", \"rb\") as f:\n",
    "    X_train_full = pickle.load(f)\n",
    "with open(\"Pickles/test_data_transformed_ubuntu.pkl\", \"rb\") as f:\n",
    "    X_test_full = pickle.load(f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Extract labels\n",
    "y_train_full = X_train_full[classes]\n",
    "y_test = X_test_full[classes]\n",
    "\n",
    "# Drop labels from both datasets\n",
    "X_train_full.drop(classes, axis=1, inplace=True)\n",
    "X_test_full.drop(classes, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Columns for spectral data (assuming all remaining columns after removing Gaia are spectra)\n",
    "gaia_columns = [\"parallax\", \"ra\", \"dec\", \"ra_error\", \"dec_error\", \"parallax_error\", \"pmra\", \"pmdec\", \n",
    "                \"pmra_error\", \"pmdec_error\", \"phot_g_mean_flux\", \"flagnopllx\", \"phot_g_mean_flux_error\", \n",
    "                \"phot_bp_mean_flux\", \"phot_rp_mean_flux\", \"phot_bp_mean_flux_error\", \"phot_rp_mean_flux_error\", \n",
    "                \"flagnoflux\"]\n",
    "\n",
    "# Spectra data (everything that is not Gaia-related) and the column 'otype'\n",
    "X_train_spectra = X_train_full.drop(columns={\"otype\", \"obsid\", *gaia_columns})\n",
    "X_test_spectra = X_test_full.drop(columns={\"otype\", \"obsid\", *gaia_columns})\n",
    "\n",
    "# Gaia data (only the selected columns)\n",
    "X_train_gaia = X_train_full[gaia_columns]\n",
    "X_test_gaia = X_test_full[gaia_columns]\n",
    "\n",
    "# Count nans and infs in x_train_gaia\n",
    "print(X_train_gaia.isnull().sum())\n",
    "print(X_train_gaia.isin([np.inf, -np.inf]).sum())\n",
    "\n",
    "\n",
    "# Free up memory\n",
    "del X_train_full, X_test_full\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "# Split training set into training and validation\n",
    "X_train_spectra, X_val_spectra, X_train_gaia, X_val_gaia, y_train, y_val = train_test_split(\n",
    "    X_train_spectra, X_train_gaia, y_train_full, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Free memory\n",
    "del y_train_full\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "# Convert spectra and Gaia data into PyTorch tensors\n",
    "X_train_spectra = torch.tensor(X_train_spectra.values, dtype=torch.float32)\n",
    "X_val_spectra = torch.tensor(X_val_spectra.values, dtype=torch.float32)\n",
    "X_test_spectra = torch.tensor(X_test_spectra.values, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "X_train_gaia = torch.tensor(X_train_gaia.values, dtype=torch.float32)\n",
    "X_val_gaia = torch.tensor(X_val_gaia.values, dtype=torch.float32)\n",
    "X_test_gaia = torch.tensor(X_test_gaia.values, dtype=torch.float32)\n",
    "\n",
    "y_train = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "# Print dataset shapes\n",
    "print(f\"X_train_spectra shape: {X_train_spectra.shape}\")\n",
    "print(f\"X_val_spectra shape: {X_val_spectra.shape}\")\n",
    "print(f\"X_test_spectra shape: {X_test_spectra.shape}\")\n",
    "\n",
    "print(f\"X_train_gaia shape: {X_train_gaia.shape}\")\n",
    "print(f\"X_val_gaia shape: {X_val_gaia.shape}\")\n",
    "print(f\"X_test_gaia shape: {X_test_gaia.shape}\")\n",
    "\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_val shape: {y_val.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "\n",
    "train_dataset = MultiModalBalancedMultiLabelDataset(X_train_spectra, X_train_gaia, y_train, limit_per_label=batch_limit)\n",
    "val_dataset = MultiModalBalancedMultiLabelDataset(X_val_spectra, X_val_gaia, y_val, limit_per_label=batch_limit)\n",
    "test_dataset = MultiModalBalancedMultiLabelDataset(X_test_spectra, X_test_gaia, y_test, limit_per_label=batch_limit)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# print the number of samples in each dataset\n",
    "print(f\"Train dataset: {len(train_dataset)} samples\")\n",
    "print(f\"Validation dataset: {len(val_dataset)} samples\")\n",
    "print(f\"Test dataset: {len(test_dataset)} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Initial memory usage:\n",
      "GPU memory allocated: 0.00 MB\n",
      "GPU memory reserved: 0.00 MB\n",
      "Loading datasets...\n",
      "\n",
      "Creating ultra memory-efficient ensemble...\n",
      "\n",
      "Memory usage after creating ensemble (no models created yet):\n",
      "GPU memory allocated: 0.00 MB\n",
      "GPU memory reserved: 0.00 MB\n",
      "\n",
      "Training a single model from the ensemble...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjoaocsgalmeida\u001b[0m (\u001b[33mjoaocsgalmeida-university-of-southampton\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/joao/Documents/Star-Classifier/wandb/run-20250303_125314-qg7085o6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_ultra_memory_efficient/runs/qg7085o6' target=\"_blank\">model_0</a></strong> to <a href='https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_ultra_memory_efficient' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_ultra_memory_efficient' target=\"_blank\">https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_ultra_memory_efficient</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_ultra_memory_efficient/runs/qg7085o6' target=\"_blank\">https://wandb.ai/joaocsgalmeida-university-of-southampton/ALLSTARS_ultra_memory_efficient/runs/qg7085o6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15099/2606998298.py:375: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler() if hasattr(torch.cuda, 'amp') else None\n",
      "Epoch 1/100 - Training:   0%|          | 0/18 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 1/100 - Training:   6%|▌         | 1/18 [00:20<05:51, 20.65s/it]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 1/100 - Training: 100%|██████████| 18/18 [00:22<00:00,  1.25s/it]\n",
      "Epoch 1/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 1/100 - Validation:  87%|████████▋ | 13/15 [00:00<00:00, 24.53it/s]/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 1/100 - Validation: 100%|██████████| 15/15 [00:01<00:00, 11.31it/s]\n",
      "Epoch 1/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 1/100 - Testing:  94%|█████████▍| 16/17 [00:00<00:00, 24.45it/s]/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 1/100 - Testing: 100%|██████████| 17/17 [00:01<00:00,  8.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - Train Loss: 0.7293, Train Acc: 0.4869, Val Loss: 0.7252, Val Acc: 0.4911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100 - Training:   0%|          | 0/18 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 2/100 - Training: 100%|██████████| 18/18 [00:03<00:00,  4.64it/s]\n",
      "Epoch 2/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 2/100 - Validation: 100%|██████████| 15/15 [00:00<00:00, 19.79it/s]\n",
      "Epoch 2/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 2/100 - Testing: 100%|██████████| 17/17 [00:00<00:00, 20.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100 - Train Loss: 0.7249, Train Acc: 0.4922, Val Loss: 0.7100, Val Acc: 0.5112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100 - Training:   0%|          | 0/18 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 3/100 - Training: 100%|██████████| 18/18 [00:02<00:00,  8.09it/s]\n",
      "Epoch 3/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 3/100 - Validation: 100%|██████████| 15/15 [00:00<00:00, 18.54it/s]\n",
      "Epoch 3/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 3/100 - Testing: 100%|██████████| 17/17 [00:00<00:00, 19.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100 - Train Loss: 0.7018, Train Acc: 0.5230, Val Loss: 0.6836, Val Acc: 0.5504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/100 - Training:   0%|          | 0/19 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 4/100 - Training: 100%|██████████| 19/19 [00:02<00:00,  7.93it/s]\n",
      "Epoch 4/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 4/100 - Validation: 100%|██████████| 15/15 [00:00<00:00, 19.90it/s]\n",
      "Epoch 4/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 4/100 - Testing: 100%|██████████| 17/17 [00:00<00:00, 20.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100 - Train Loss: 0.6641, Train Acc: 0.5836, Val Loss: 0.6438, Val Acc: 0.6073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/100 - Training:   0%|          | 0/19 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 5/100 - Training: 100%|██████████| 19/19 [00:02<00:00,  8.16it/s]\n",
      "Epoch 5/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 5/100 - Validation: 100%|██████████| 15/15 [00:00<00:00, 18.54it/s]\n",
      "Epoch 5/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 5/100 - Testing: 100%|██████████| 17/17 [00:00<00:00, 19.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100 - Train Loss: 0.6261, Train Acc: 0.6333, Val Loss: 0.5923, Val Acc: 0.6835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/100 - Training:   0%|          | 0/19 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 6/100 - Training: 100%|██████████| 19/19 [00:02<00:00,  7.96it/s]\n",
      "Epoch 6/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 6/100 - Validation: 100%|██████████| 15/15 [00:00<00:00, 17.71it/s]\n",
      "Epoch 6/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 6/100 - Testing: 100%|██████████| 17/17 [00:00<00:00, 18.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100 - Train Loss: 0.5697, Train Acc: 0.7186, Val Loss: 0.5342, Val Acc: 0.7650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/100 - Training:   0%|          | 0/19 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 7/100 - Training: 100%|██████████| 19/19 [00:05<00:00,  3.49it/s]\n",
      "Epoch 7/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 7/100 - Validation: 100%|██████████| 15/15 [00:00<00:00, 20.24it/s]\n",
      "Epoch 7/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 7/100 - Testing: 100%|██████████| 17/17 [00:00<00:00, 20.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100 - Train Loss: 0.5114, Train Acc: 0.7950, Val Loss: 0.4745, Val Acc: 0.8387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/100 - Training:   0%|          | 0/19 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 8/100 - Training: 100%|██████████| 19/19 [00:02<00:00,  7.99it/s]\n",
      "Epoch 8/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 8/100 - Validation: 100%|██████████| 15/15 [00:00<00:00, 18.14it/s]\n",
      "Epoch 8/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 8/100 - Testing: 100%|██████████| 17/17 [00:00<00:00, 18.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100 - Train Loss: 0.4507, Train Acc: 0.8609, Val Loss: 0.4181, Val Acc: 0.8923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/100 - Training:   0%|          | 0/19 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 9/100 - Training: 100%|██████████| 19/19 [00:02<00:00,  8.20it/s]\n",
      "Epoch 9/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 9/100 - Validation: 100%|██████████| 15/15 [00:00<00:00, 17.63it/s]\n",
      "Epoch 9/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 9/100 - Testing: 100%|██████████| 17/17 [00:00<00:00, 17.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100 - Train Loss: 0.3944, Train Acc: 0.9094, Val Loss: 0.3678, Val Acc: 0.9288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/100 - Training:   0%|          | 0/19 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 10/100 - Training: 100%|██████████| 19/19 [00:02<00:00,  8.15it/s]\n",
      "Epoch 10/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 10/100 - Validation: 100%|██████████| 15/15 [00:00<00:00, 17.76it/s]\n",
      "Epoch 10/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 10/100 - Testing: 100%|██████████| 17/17 [00:00<00:00, 18.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100 - Train Loss: 0.3491, Train Acc: 0.9367, Val Loss: 0.3243, Val Acc: 0.9484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/100 - Training:   0%|          | 0/19 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 11/100 - Training: 100%|██████████| 19/19 [00:02<00:00,  7.76it/s]\n",
      "Epoch 11/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 11/100 - Validation: 100%|██████████| 15/15 [00:00<00:00, 16.73it/s]\n",
      "Epoch 11/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 11/100 - Testing: 100%|██████████| 17/17 [00:00<00:00, 18.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100 - Train Loss: 0.3075, Train Acc: 0.9490, Val Loss: 0.2875, Val Acc: 0.9572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/100 - Training:   0%|          | 0/18 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 12/100 - Training: 100%|██████████| 18/18 [00:02<00:00,  7.36it/s]\n",
      "Epoch 12/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 12/100 - Validation: 100%|██████████| 15/15 [00:00<00:00, 17.34it/s]\n",
      "Epoch 12/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 12/100 - Testing: 100%|██████████| 17/17 [00:00<00:00, 18.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100 - Train Loss: 0.2789, Train Acc: 0.9571, Val Loss: 0.2575, Val Acc: 0.9607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/100 - Training:   0%|          | 0/19 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 13/100 - Training: 100%|██████████| 19/19 [00:02<00:00,  7.53it/s]\n",
      "Epoch 13/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 13/100 - Validation: 100%|██████████| 15/15 [00:00<00:00, 16.91it/s]\n",
      "Epoch 13/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 13/100 - Testing: 100%|██████████| 17/17 [00:00<00:00, 17.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100 - Train Loss: 0.2492, Train Acc: 0.9605, Val Loss: 0.2331, Val Acc: 0.9625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/100 - Training:   0%|          | 0/19 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 14/100 - Training: 100%|██████████| 19/19 [00:02<00:00,  7.89it/s]\n",
      "Epoch 14/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 14/100 - Validation: 100%|██████████| 15/15 [00:00<00:00, 15.93it/s]\n",
      "Epoch 14/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 14/100 - Testing: 100%|██████████| 17/17 [00:01<00:00, 16.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100 - Train Loss: 0.2210, Train Acc: 0.9616, Val Loss: 0.2127, Val Acc: 0.9629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/100 - Training:   0%|          | 0/19 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 15/100 - Training: 100%|██████████| 19/19 [00:02<00:00,  7.74it/s]\n",
      "Epoch 15/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 15/100 - Validation: 100%|██████████| 15/15 [00:00<00:00, 15.43it/s]\n",
      "Epoch 15/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 15/100 - Testing: 100%|██████████| 17/17 [00:01<00:00, 16.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100 - Train Loss: 0.2046, Train Acc: 0.9621, Val Loss: 0.1968, Val Acc: 0.9631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/100 - Training:   0%|          | 0/19 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 16/100 - Training: 100%|██████████| 19/19 [00:02<00:00,  7.41it/s]\n",
      "Epoch 16/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 16/100 - Validation: 100%|██████████| 15/15 [00:01<00:00, 14.34it/s]\n",
      "Epoch 16/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 16/100 - Testing: 100%|██████████| 17/17 [00:01<00:00, 15.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100 - Train Loss: 0.1910, Train Acc: 0.9622, Val Loss: 0.1836, Val Acc: 0.9634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/100 - Training:   0%|          | 0/18 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 17/100 - Training: 100%|██████████| 18/18 [00:02<00:00,  7.26it/s]\n",
      "Epoch 17/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 17/100 - Validation: 100%|██████████| 15/15 [00:01<00:00, 14.53it/s]\n",
      "Epoch 17/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 17/100 - Testing: 100%|██████████| 17/17 [00:01<00:00, 14.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100 - Train Loss: 0.1792, Train Acc: 0.9620, Val Loss: 0.1726, Val Acc: 0.9636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/100 - Training:   0%|          | 0/19 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 18/100 - Training: 100%|██████████| 19/19 [00:02<00:00,  7.03it/s]\n",
      "Epoch 18/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 18/100 - Validation: 100%|██████████| 15/15 [00:00<00:00, 18.81it/s]\n",
      "Epoch 18/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 18/100 - Testing: 100%|██████████| 17/17 [00:00<00:00, 19.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/100 - Train Loss: 0.1711, Train Acc: 0.9616, Val Loss: 0.1640, Val Acc: 0.9636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/100 - Training:   0%|          | 0/19 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 19/100 - Training: 100%|██████████| 19/19 [00:02<00:00,  8.04it/s]\n",
      "Epoch 19/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 19/100 - Validation: 100%|██████████| 15/15 [00:00<00:00, 16.83it/s]\n",
      "Epoch 19/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 19/100 - Testing: 100%|██████████| 17/17 [00:00<00:00, 17.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/100 - Train Loss: 0.1623, Train Acc: 0.9622, Val Loss: 0.1562, Val Acc: 0.9636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/100 - Training:   0%|          | 0/18 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 20/100 - Training: 100%|██████████| 18/18 [00:02<00:00,  7.79it/s]\n",
      "Epoch 20/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 20/100 - Validation: 100%|██████████| 15/15 [00:00<00:00, 16.60it/s]\n",
      "Epoch 20/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 20/100 - Testing: 100%|██████████| 17/17 [00:00<00:00, 17.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/100 - Train Loss: 0.1512, Train Acc: 0.9622, Val Loss: 0.1500, Val Acc: 0.9637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/100 - Training:   0%|          | 0/18 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 21/100 - Training: 100%|██████████| 18/18 [00:02<00:00,  7.71it/s]\n",
      "Epoch 21/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 21/100 - Validation: 100%|██████████| 15/15 [00:00<00:00, 15.41it/s]\n",
      "Epoch 21/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 21/100 - Testing: 100%|██████████| 17/17 [00:01<00:00, 15.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/100 - Train Loss: 0.1472, Train Acc: 0.9626, Val Loss: 0.1448, Val Acc: 0.9638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/100 - Training:   0%|          | 0/18 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 22/100 - Training: 100%|██████████| 18/18 [00:02<00:00,  7.48it/s]\n",
      "Epoch 22/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 22/100 - Validation: 100%|██████████| 15/15 [00:01<00:00, 14.93it/s]\n",
      "Epoch 22/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 22/100 - Testing: 100%|██████████| 17/17 [00:01<00:00, 15.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/100 - Train Loss: 0.1457, Train Acc: 0.9620, Val Loss: 0.1398, Val Acc: 0.9638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/100 - Training:   0%|          | 0/19 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 23/100 - Training: 100%|██████████| 19/19 [00:02<00:00,  7.30it/s]\n",
      "Epoch 23/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 23/100 - Validation: 100%|██████████| 15/15 [00:00<00:00, 17.60it/s]\n",
      "Epoch 23/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 23/100 - Testing: 100%|██████████| 17/17 [00:00<00:00, 18.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/100 - Train Loss: 0.1383, Train Acc: 0.9624, Val Loss: 0.1362, Val Acc: 0.9638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/100 - Training:   0%|          | 0/19 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 24/100 - Training: 100%|██████████| 19/19 [00:02<00:00,  8.11it/s]\n",
      "Epoch 24/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 24/100 - Validation: 100%|██████████| 15/15 [00:00<00:00, 17.61it/s]\n",
      "Epoch 24/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 24/100 - Testing: 100%|██████████| 17/17 [00:00<00:00, 18.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/100 - Train Loss: 0.1375, Train Acc: 0.9623, Val Loss: 0.1318, Val Acc: 0.9638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/100 - Training:   0%|          | 0/19 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 25/100 - Training: 100%|██████████| 19/19 [00:02<00:00,  8.08it/s]\n",
      "Epoch 25/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 25/100 - Validation: 100%|██████████| 15/15 [00:00<00:00, 17.59it/s]\n",
      "Epoch 25/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 25/100 - Testing: 100%|██████████| 17/17 [00:00<00:00, 18.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/100 - Train Loss: 0.1326, Train Acc: 0.9623, Val Loss: 0.1287, Val Acc: 0.9638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/100 - Training:   0%|          | 0/18 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 26/100 - Training: 100%|██████████| 18/18 [00:02<00:00,  7.87it/s]\n",
      "Epoch 26/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 26/100 - Validation: 100%|██████████| 15/15 [00:00<00:00, 16.21it/s]\n",
      "Epoch 26/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 26/100 - Testing: 100%|██████████| 17/17 [00:00<00:00, 17.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/100 - Train Loss: 0.1272, Train Acc: 0.9621, Val Loss: 0.1256, Val Acc: 0.9638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/100 - Training:   0%|          | 0/18 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 27/100 - Training: 100%|██████████| 18/18 [00:02<00:00,  7.81it/s]\n",
      "Epoch 27/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 27/100 - Validation: 100%|██████████| 15/15 [00:00<00:00, 15.56it/s]\n",
      "Epoch 27/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 27/100 - Testing: 100%|██████████| 17/17 [00:01<00:00, 16.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/100 - Train Loss: 0.1259, Train Acc: 0.9624, Val Loss: 0.1234, Val Acc: 0.9637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/100 - Training:   0%|          | 0/18 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 28/100 - Training: 100%|██████████| 18/18 [00:02<00:00,  8.19it/s]\n",
      "Epoch 28/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 28/100 - Validation: 100%|██████████| 15/15 [00:00<00:00, 16.53it/s]\n",
      "Epoch 28/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 28/100 - Testing: 100%|██████████| 17/17 [00:01<00:00, 16.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/100 - Train Loss: 0.1231, Train Acc: 0.9622, Val Loss: 0.1208, Val Acc: 0.9637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/100 - Training:   0%|          | 0/18 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 29/100 - Training: 100%|██████████| 18/18 [00:02<00:00,  7.56it/s]\n",
      "Epoch 29/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 29/100 - Validation: 100%|██████████| 15/15 [00:00<00:00, 15.05it/s]\n",
      "Epoch 29/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 29/100 - Testing: 100%|██████████| 17/17 [00:01<00:00, 15.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/100 - Train Loss: 0.1210, Train Acc: 0.9625, Val Loss: 0.1192, Val Acc: 0.9637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/100 - Training:   0%|          | 0/19 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 30/100 - Training: 100%|██████████| 19/19 [00:02<00:00,  7.47it/s]\n",
      "Epoch 30/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 30/100 - Validation: 100%|██████████| 15/15 [00:00<00:00, 17.48it/s]\n",
      "Epoch 30/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 30/100 - Testing: 100%|██████████| 17/17 [00:00<00:00, 18.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/100 - Train Loss: 0.1195, Train Acc: 0.9624, Val Loss: 0.1168, Val Acc: 0.9637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/100 - Training:   0%|          | 0/19 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 31/100 - Training: 100%|██████████| 19/19 [00:02<00:00,  8.01it/s]\n",
      "Epoch 31/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 31/100 - Validation: 100%|██████████| 15/15 [00:00<00:00, 16.31it/s]\n",
      "Epoch 31/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 31/100 - Testing: 100%|██████████| 17/17 [00:01<00:00, 16.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/100 - Train Loss: 0.1156, Train Acc: 0.9624, Val Loss: 0.1150, Val Acc: 0.9637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/100 - Training:   0%|          | 0/19 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 32/100 - Training: 100%|██████████| 19/19 [00:02<00:00,  7.85it/s]\n",
      "Epoch 32/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 32/100 - Validation: 100%|██████████| 15/15 [00:00<00:00, 15.85it/s]\n",
      "Epoch 32/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 32/100 - Testing: 100%|██████████| 17/17 [00:01<00:00, 16.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/100 - Train Loss: 0.1155, Train Acc: 0.9623, Val Loss: 0.1133, Val Acc: 0.9637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/100 - Training:   0%|          | 0/18 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 33/100 - Training: 100%|██████████| 18/18 [00:02<00:00,  7.69it/s]\n",
      "Epoch 33/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 33/100 - Validation: 100%|██████████| 15/15 [00:00<00:00, 15.47it/s]\n",
      "Epoch 33/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 33/100 - Testing: 100%|██████████| 17/17 [00:01<00:00, 16.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/100 - Train Loss: 0.1142, Train Acc: 0.9624, Val Loss: 0.1122, Val Acc: 0.9637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/100 - Training:   0%|          | 0/19 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 34/100 - Training: 100%|██████████| 19/19 [00:02<00:00,  7.73it/s]\n",
      "Epoch 34/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 34/100 - Validation: 100%|██████████| 15/15 [00:01<00:00, 14.95it/s]\n",
      "Epoch 34/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 34/100 - Testing: 100%|██████████| 17/17 [00:01<00:00, 15.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/100 - Train Loss: 0.1117, Train Acc: 0.9624, Val Loss: 0.1103, Val Acc: 0.9637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/100 - Training:   0%|          | 0/18 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 35/100 - Training: 100%|██████████| 18/18 [00:02<00:00,  7.21it/s]\n",
      "Epoch 35/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 35/100 - Validation: 100%|██████████| 15/15 [00:01<00:00, 13.72it/s]\n",
      "Epoch 35/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 35/100 - Testing: 100%|██████████| 17/17 [00:01<00:00, 15.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/100 - Train Loss: 0.1106, Train Acc: 0.9624, Val Loss: 0.1086, Val Acc: 0.9637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/100 - Training:   0%|          | 0/19 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 36/100 - Training: 100%|██████████| 19/19 [00:02<00:00,  7.07it/s]\n",
      "Epoch 36/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 36/100 - Validation: 100%|██████████| 15/15 [00:00<00:00, 18.20it/s]\n",
      "Epoch 36/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 36/100 - Testing: 100%|██████████| 17/17 [00:00<00:00, 18.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/100 - Train Loss: 0.1102, Train Acc: 0.9618, Val Loss: 0.1075, Val Acc: 0.9637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/100 - Training:   0%|          | 0/18 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 37/100 - Training: 100%|██████████| 18/18 [00:02<00:00,  8.05it/s]\n",
      "Epoch 37/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 37/100 - Validation: 100%|██████████| 15/15 [00:00<00:00, 16.25it/s]\n",
      "Epoch 37/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 37/100 - Testing: 100%|██████████| 17/17 [00:00<00:00, 17.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/100 - Train Loss: 0.1077, Train Acc: 0.9622, Val Loss: 0.1061, Val Acc: 0.9637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/100 - Training:   0%|          | 0/19 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 38/100 - Training: 100%|██████████| 19/19 [00:02<00:00,  7.70it/s]\n",
      "Epoch 38/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 38/100 - Validation: 100%|██████████| 15/15 [00:00<00:00, 15.41it/s]\n",
      "Epoch 38/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 38/100 - Testing: 100%|██████████| 17/17 [00:01<00:00, 16.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/100 - Train Loss: 0.1068, Train Acc: 0.9624, Val Loss: 0.1061, Val Acc: 0.9637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 39/100 - Training:   0%|          | 0/18 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 39/100 - Training: 100%|██████████| 18/18 [00:02<00:00,  7.55it/s]\n",
      "Epoch 39/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 39/100 - Validation: 100%|██████████| 15/15 [00:00<00:00, 15.59it/s]\n",
      "Epoch 39/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 39/100 - Testing: 100%|██████████| 17/17 [00:01<00:00, 16.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/100 - Train Loss: 0.1062, Train Acc: 0.9626, Val Loss: 0.1043, Val Acc: 0.9637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/100 - Training:   0%|          | 0/19 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 40/100 - Training: 100%|██████████| 19/19 [00:02<00:00,  7.84it/s]\n",
      "Epoch 40/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 40/100 - Validation: 100%|██████████| 15/15 [00:00<00:00, 15.87it/s]\n",
      "Epoch 40/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 40/100 - Testing: 100%|██████████| 17/17 [00:01<00:00, 16.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/100 - Train Loss: 0.1041, Train Acc: 0.9621, Val Loss: 0.1032, Val Acc: 0.9637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/100 - Training:   0%|          | 0/19 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 41/100 - Training: 100%|██████████| 19/19 [00:02<00:00,  7.90it/s]\n",
      "Epoch 41/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 41/100 - Validation: 100%|██████████| 15/15 [00:00<00:00, 15.20it/s]\n",
      "Epoch 41/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 41/100 - Testing: 100%|██████████| 17/17 [00:01<00:00, 16.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/100 - Train Loss: 0.1043, Train Acc: 0.9620, Val Loss: 0.1021, Val Acc: 0.9637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/100 - Training:   0%|          | 0/19 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 42/100 - Training: 100%|██████████| 19/19 [00:02<00:00,  7.47it/s]\n",
      "Epoch 42/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 42/100 - Validation: 100%|██████████| 15/15 [00:00<00:00, 15.78it/s]\n",
      "Epoch 42/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 42/100 - Testing: 100%|██████████| 17/17 [00:01<00:00, 16.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/100 - Train Loss: 0.1041, Train Acc: 0.9623, Val Loss: 0.1019, Val Acc: 0.9637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/100 - Training:   0%|          | 0/19 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 43/100 - Training: 100%|██████████| 19/19 [00:02<00:00,  7.96it/s]\n",
      "Epoch 43/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 43/100 - Validation: 100%|██████████| 15/15 [00:00<00:00, 17.59it/s]\n",
      "Epoch 43/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 43/100 - Testing: 100%|██████████| 17/17 [00:00<00:00, 18.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/100 - Train Loss: 0.1026, Train Acc: 0.9621, Val Loss: 0.1008, Val Acc: 0.9637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/100 - Training:   0%|          | 0/19 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 44/100 - Training: 100%|██████████| 19/19 [00:02<00:00,  8.01it/s]\n",
      "Epoch 44/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 44/100 - Validation: 100%|██████████| 15/15 [00:00<00:00, 18.44it/s]\n",
      "Epoch 44/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 44/100 - Testing: 100%|██████████| 17/17 [00:00<00:00, 18.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/100 - Train Loss: 0.1009, Train Acc: 0.9623, Val Loss: 0.1005, Val Acc: 0.9637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/100 - Training:   0%|          | 0/18 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 45/100 - Training: 100%|██████████| 18/18 [00:02<00:00,  7.84it/s]\n",
      "Epoch 45/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 45/100 - Validation: 100%|██████████| 15/15 [00:00<00:00, 17.48it/s]\n",
      "Epoch 45/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 45/100 - Testing: 100%|██████████| 17/17 [00:00<00:00, 17.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/100 - Train Loss: 0.1001, Train Acc: 0.9624, Val Loss: 0.0996, Val Acc: 0.9637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/100 - Training:   0%|          | 0/18 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 46/100 - Training: 100%|██████████| 18/18 [00:02<00:00,  7.83it/s]\n",
      "Epoch 46/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 46/100 - Validation: 100%|██████████| 15/15 [00:00<00:00, 16.12it/s]\n",
      "Epoch 46/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 46/100 - Testing: 100%|██████████| 17/17 [00:00<00:00, 17.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/100 - Train Loss: 0.0998, Train Acc: 0.9623, Val Loss: 0.0983, Val Acc: 0.9637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/100 - Training:   0%|          | 0/19 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 47/100 - Training: 100%|██████████| 19/19 [00:02<00:00,  7.99it/s]\n",
      "Epoch 47/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 47/100 - Validation: 100%|██████████| 15/15 [00:00<00:00, 15.97it/s]\n",
      "Epoch 47/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 47/100 - Testing: 100%|██████████| 17/17 [00:01<00:00, 16.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/100 - Train Loss: 0.0996, Train Acc: 0.9626, Val Loss: 0.0987, Val Acc: 0.9637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 48/100 - Training:   0%|          | 0/18 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 48/100 - Training: 100%|██████████| 18/18 [00:02<00:00,  7.81it/s]\n",
      "Epoch 48/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 48/100 - Validation: 100%|██████████| 15/15 [00:00<00:00, 15.98it/s]\n",
      "Epoch 48/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 48/100 - Testing: 100%|██████████| 17/17 [00:01<00:00, 16.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/100 - Train Loss: 0.0988, Train Acc: 0.9625, Val Loss: 0.0980, Val Acc: 0.9637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/100 - Training:   0%|          | 0/19 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 49/100 - Training: 100%|██████████| 19/19 [00:02<00:00,  7.86it/s]\n",
      "Epoch 49/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 49/100 - Validation: 100%|██████████| 15/15 [00:00<00:00, 15.26it/s]\n",
      "Epoch 49/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 49/100 - Testing: 100%|██████████| 17/17 [00:01<00:00, 15.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/100 - Train Loss: 0.1006, Train Acc: 0.9624, Val Loss: 0.0973, Val Acc: 0.9637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/100 - Training:   0%|          | 0/19 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 50/100 - Training: 100%|██████████| 19/19 [00:02<00:00,  7.58it/s]\n",
      "Epoch 50/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 50/100 - Validation: 100%|██████████| 15/15 [00:00<00:00, 17.81it/s]\n",
      "Epoch 50/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 50/100 - Testing: 100%|██████████| 17/17 [00:00<00:00, 18.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/100 - Train Loss: 0.0979, Train Acc: 0.9621, Val Loss: 0.0967, Val Acc: 0.9637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51/100 - Training:   0%|          | 0/19 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 51/100 - Training: 100%|██████████| 19/19 [00:02<00:00,  8.15it/s]\n",
      "Epoch 51/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 51/100 - Validation: 100%|██████████| 15/15 [00:00<00:00, 16.43it/s]\n",
      "Epoch 51/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 51/100 - Testing: 100%|██████████| 17/17 [00:01<00:00, 16.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/100 - Train Loss: 0.0975, Train Acc: 0.9624, Val Loss: 0.0967, Val Acc: 0.9637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 52/100 - Training:   0%|          | 0/19 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 52/100 - Training: 100%|██████████| 19/19 [00:02<00:00,  8.11it/s]\n",
      "Epoch 52/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 52/100 - Validation: 100%|██████████| 15/15 [00:00<00:00, 16.57it/s]\n",
      "Epoch 52/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 52/100 - Testing: 100%|██████████| 17/17 [00:00<00:00, 17.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/100 - Train Loss: 0.0969, Train Acc: 0.9624, Val Loss: 0.0958, Val Acc: 0.9637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53/100 - Training:   0%|          | 0/18 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 53/100 - Training: 100%|██████████| 18/18 [00:02<00:00,  7.81it/s]\n",
      "Epoch 53/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 53/100 - Validation: 100%|██████████| 15/15 [00:00<00:00, 15.63it/s]\n",
      "Epoch 53/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 53/100 - Testing: 100%|██████████| 17/17 [00:01<00:00, 16.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/100 - Train Loss: 0.0975, Train Acc: 0.9626, Val Loss: 0.0958, Val Acc: 0.9637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 54/100 - Training:   0%|          | 0/19 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 54/100 - Training: 100%|██████████| 19/19 [00:02<00:00,  8.05it/s]\n",
      "Epoch 54/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 54/100 - Validation: 100%|██████████| 15/15 [00:00<00:00, 15.65it/s]\n",
      "Epoch 54/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 54/100 - Testing: 100%|██████████| 17/17 [00:01<00:00, 16.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/100 - Train Loss: 0.0959, Train Acc: 0.9620, Val Loss: 0.0948, Val Acc: 0.9637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55/100 - Training:   0%|          | 0/18 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 55/100 - Training: 100%|██████████| 18/18 [00:02<00:00,  7.38it/s]\n",
      "Epoch 55/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 55/100 - Validation: 100%|██████████| 15/15 [00:00<00:00, 15.32it/s]\n",
      "Epoch 55/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 55/100 - Testing: 100%|██████████| 17/17 [00:01<00:00, 15.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/100 - Train Loss: 0.0956, Train Acc: 0.9624, Val Loss: 0.0947, Val Acc: 0.9637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56/100 - Training:   0%|          | 0/19 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 56/100 - Training: 100%|██████████| 19/19 [00:02<00:00,  7.45it/s]\n",
      "Epoch 56/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 56/100 - Validation: 100%|██████████| 15/15 [00:01<00:00, 13.89it/s]\n",
      "Epoch 56/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 56/100 - Testing: 100%|██████████| 17/17 [00:01<00:00, 15.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/100 - Train Loss: 0.0949, Train Acc: 0.9621, Val Loss: 0.0942, Val Acc: 0.9637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57/100 - Training:   0%|          | 0/19 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 57/100 - Training: 100%|██████████| 19/19 [00:02<00:00,  7.84it/s]\n",
      "Epoch 57/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 57/100 - Validation: 100%|██████████| 15/15 [00:00<00:00, 16.50it/s]\n",
      "Epoch 57/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 57/100 - Testing: 100%|██████████| 17/17 [00:00<00:00, 17.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100 - Train Loss: 0.0954, Train Acc: 0.9626, Val Loss: 0.0948, Val Acc: 0.9637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58/100 - Training:   0%|          | 0/19 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 58/100 - Training: 100%|██████████| 19/19 [00:02<00:00,  8.00it/s]\n",
      "Epoch 58/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 58/100 - Validation: 100%|██████████| 15/15 [00:00<00:00, 16.92it/s]\n",
      "Epoch 58/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 58/100 - Testing: 100%|██████████| 17/17 [00:00<00:00, 17.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100 - Train Loss: 0.0946, Train Acc: 0.9621, Val Loss: 0.0944, Val Acc: 0.9637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59/100 - Training:   0%|          | 0/19 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 59/100 - Training: 100%|██████████| 19/19 [00:02<00:00,  8.25it/s]\n",
      "Epoch 59/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 59/100 - Validation: 100%|██████████| 15/15 [00:00<00:00, 17.09it/s]\n",
      "Epoch 59/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 59/100 - Testing: 100%|██████████| 17/17 [00:00<00:00, 17.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100 - Train Loss: 0.0956, Train Acc: 0.9621, Val Loss: 0.0937, Val Acc: 0.9637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60/100 - Training:   0%|          | 0/18 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 60/100 - Training: 100%|██████████| 18/18 [00:02<00:00,  7.90it/s]\n",
      "Epoch 60/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 60/100 - Validation: 100%|██████████| 15/15 [00:00<00:00, 16.93it/s]\n",
      "Epoch 60/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 60/100 - Testing: 100%|██████████| 17/17 [00:01<00:00, 16.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/100 - Train Loss: 0.0943, Train Acc: 0.9623, Val Loss: 0.0928, Val Acc: 0.9637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61/100 - Training:   0%|          | 0/18 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 61/100 - Training: 100%|██████████| 18/18 [00:02<00:00,  7.73it/s]\n",
      "Epoch 61/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 61/100 - Validation: 100%|██████████| 15/15 [00:00<00:00, 15.42it/s]\n",
      "Epoch 61/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 61/100 - Testing: 100%|██████████| 17/17 [00:01<00:00, 16.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/100 - Train Loss: 0.0943, Train Acc: 0.9623, Val Loss: 0.0928, Val Acc: 0.9637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 62/100 - Training:   0%|          | 0/19 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 62/100 - Training: 100%|██████████| 19/19 [00:02<00:00,  7.66it/s]\n",
      "Epoch 62/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 62/100 - Validation: 100%|██████████| 15/15 [00:00<00:00, 15.62it/s]\n",
      "Epoch 62/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 62/100 - Testing: 100%|██████████| 17/17 [00:01<00:00, 16.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/100 - Train Loss: 0.0938, Train Acc: 0.9623, Val Loss: 0.0926, Val Acc: 0.9637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63/100 - Training:   0%|          | 0/19 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 63/100 - Training: 100%|██████████| 19/19 [00:02<00:00,  7.72it/s]\n",
      "Epoch 63/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 63/100 - Validation: 100%|██████████| 15/15 [00:01<00:00, 14.56it/s]\n",
      "Epoch 63/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 63/100 - Testing: 100%|██████████| 17/17 [00:01<00:00, 15.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/100 - Train Loss: 0.0938, Train Acc: 0.9626, Val Loss: 0.0928, Val Acc: 0.9637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 64/100 - Training:   0%|          | 0/19 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 64/100 - Training: 100%|██████████| 19/19 [00:02<00:00,  7.65it/s]\n",
      "Epoch 64/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 64/100 - Validation: 100%|██████████| 15/15 [00:01<00:00, 14.35it/s]\n",
      "Epoch 64/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 64/100 - Testing: 100%|██████████| 17/17 [00:01<00:00, 15.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/100 - Train Loss: 0.0935, Train Acc: 0.9621, Val Loss: 0.0921, Val Acc: 0.9637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65/100 - Training:   0%|          | 0/18 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 65/100 - Training: 100%|██████████| 18/18 [00:02<00:00,  7.65it/s]\n",
      "Epoch 65/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 65/100 - Validation: 100%|██████████| 15/15 [00:00<00:00, 15.12it/s]\n",
      "Epoch 65/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 65/100 - Testing: 100%|██████████| 17/17 [00:01<00:00, 15.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/100 - Train Loss: 0.0937, Train Acc: 0.9621, Val Loss: 0.0918, Val Acc: 0.9637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66/100 - Training:   0%|          | 0/19 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 66/100 - Training: 100%|██████████| 19/19 [00:02<00:00,  7.63it/s]\n",
      "Epoch 66/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 66/100 - Validation: 100%|██████████| 15/15 [00:01<00:00, 14.53it/s]\n",
      "Epoch 66/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 66/100 - Testing: 100%|██████████| 17/17 [00:01<00:00, 15.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/100 - Train Loss: 0.0927, Train Acc: 0.9623, Val Loss: 0.0921, Val Acc: 0.9637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 67/100 - Training:   0%|          | 0/18 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 67/100 - Training: 100%|██████████| 18/18 [00:02<00:00,  7.17it/s]\n",
      "Epoch 67/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 67/100 - Validation: 100%|██████████| 15/15 [00:01<00:00, 13.95it/s]\n",
      "Epoch 67/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 67/100 - Testing: 100%|██████████| 17/17 [00:01<00:00, 14.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/100 - Train Loss: 0.0929, Train Acc: 0.9625, Val Loss: 0.0920, Val Acc: 0.9637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 68/100 - Training:   0%|          | 0/19 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 68/100 - Training: 100%|██████████| 19/19 [00:02<00:00,  7.51it/s]\n",
      "Epoch 68/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 68/100 - Validation: 100%|██████████| 15/15 [00:01<00:00, 13.76it/s]\n",
      "Epoch 68/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 68/100 - Testing: 100%|██████████| 17/17 [00:01<00:00, 14.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/100 - Train Loss: 0.0936, Train Acc: 0.9622, Val Loss: 0.0918, Val Acc: 0.9637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69/100 - Training:   0%|          | 0/18 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 69/100 - Training: 100%|██████████| 18/18 [00:02<00:00,  6.96it/s]\n",
      "Epoch 69/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 69/100 - Validation: 100%|██████████| 15/15 [00:01<00:00, 14.46it/s]\n",
      "Epoch 69/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 69/100 - Testing: 100%|██████████| 17/17 [00:01<00:00, 14.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/100 - Train Loss: 0.0936, Train Acc: 0.9625, Val Loss: 0.0920, Val Acc: 0.9637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 70/100 - Training:   0%|          | 0/19 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 70/100 - Training: 100%|██████████| 19/19 [00:02<00:00,  7.48it/s]\n",
      "Epoch 70/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 70/100 - Validation: 100%|██████████| 15/15 [00:01<00:00, 14.24it/s]\n",
      "Epoch 70/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 70/100 - Testing: 100%|██████████| 17/17 [00:01<00:00, 15.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/100 - Train Loss: 0.0927, Train Acc: 0.9620, Val Loss: 0.0914, Val Acc: 0.9637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 71/100 - Training:   0%|          | 0/19 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 71/100 - Training: 100%|██████████| 19/19 [00:02<00:00,  7.38it/s]\n",
      "Epoch 71/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 71/100 - Validation: 100%|██████████| 15/15 [00:01<00:00, 14.08it/s]\n",
      "Epoch 71/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 71/100 - Testing: 100%|██████████| 17/17 [00:01<00:00, 15.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/100 - Train Loss: 0.0923, Train Acc: 0.9621, Val Loss: 0.0915, Val Acc: 0.9637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 72/100 - Training:   0%|          | 0/18 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 72/100 - Training: 100%|██████████| 18/18 [00:02<00:00,  7.16it/s]\n",
      "Epoch 72/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 72/100 - Validation: 100%|██████████| 15/15 [00:01<00:00, 13.58it/s]\n",
      "Epoch 72/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 72/100 - Testing: 100%|██████████| 17/17 [00:01<00:00, 14.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/100 - Train Loss: 0.0918, Train Acc: 0.9622, Val Loss: 0.0915, Val Acc: 0.9637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 73/100 - Training:   0%|          | 0/19 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 73/100 - Training: 100%|██████████| 19/19 [00:02<00:00,  6.89it/s]\n",
      "Epoch 73/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 73/100 - Validation: 100%|██████████| 15/15 [00:01<00:00, 14.50it/s]\n",
      "Epoch 73/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 73/100 - Testing: 100%|██████████| 17/17 [00:01<00:00, 14.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/100 - Train Loss: 0.0929, Train Acc: 0.9622, Val Loss: 0.0911, Val Acc: 0.9637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 74/100 - Training:   0%|          | 0/19 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 74/100 - Training: 100%|██████████| 19/19 [00:02<00:00,  7.35it/s]\n",
      "Epoch 74/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 74/100 - Validation: 100%|██████████| 15/15 [00:01<00:00, 14.39it/s]\n",
      "Epoch 74/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 74/100 - Testing: 100%|██████████| 17/17 [00:01<00:00, 15.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/100 - Train Loss: 0.0922, Train Acc: 0.9623, Val Loss: 0.0909, Val Acc: 0.9637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 75/100 - Training:   0%|          | 0/19 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 75/100 - Training: 100%|██████████| 19/19 [00:02<00:00,  7.19it/s]\n",
      "Epoch 75/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 75/100 - Validation: 100%|██████████| 15/15 [00:01<00:00, 13.94it/s]\n",
      "Epoch 75/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 75/100 - Testing: 100%|██████████| 17/17 [00:01<00:00, 14.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/100 - Train Loss: 0.0920, Train Acc: 0.9622, Val Loss: 0.0914, Val Acc: 0.9637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 76/100 - Training:   0%|          | 0/18 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 76/100 - Training: 100%|██████████| 18/18 [00:02<00:00,  7.16it/s]\n",
      "Epoch 76/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 76/100 - Validation: 100%|██████████| 15/15 [00:01<00:00, 14.22it/s]\n",
      "Epoch 76/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 76/100 - Testing: 100%|██████████| 17/17 [00:01<00:00, 14.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/100 - Train Loss: 0.0925, Train Acc: 0.9624, Val Loss: 0.0912, Val Acc: 0.9637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 77/100 - Training:   0%|          | 0/19 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 77/100 - Training: 100%|██████████| 19/19 [00:02<00:00,  7.69it/s]\n",
      "Epoch 77/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 77/100 - Validation: 100%|██████████| 15/15 [00:01<00:00, 13.89it/s]\n",
      "Epoch 77/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 77/100 - Testing: 100%|██████████| 17/17 [00:01<00:00, 14.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/100 - Train Loss: 0.0924, Train Acc: 0.9623, Val Loss: 0.0918, Val Acc: 0.9637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 78/100 - Training:   0%|          | 0/18 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 78/100 - Training: 100%|██████████| 18/18 [00:02<00:00,  7.02it/s]\n",
      "Epoch 78/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 78/100 - Validation: 100%|██████████| 15/15 [00:01<00:00, 14.52it/s]\n",
      "Epoch 78/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 78/100 - Testing: 100%|██████████| 17/17 [00:01<00:00, 14.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/100 - Train Loss: 0.0921, Train Acc: 0.9624, Val Loss: 0.0909, Val Acc: 0.9637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 79/100 - Training:   0%|          | 0/19 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 79/100 - Training: 100%|██████████| 19/19 [00:02<00:00,  7.45it/s]\n",
      "Epoch 79/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 79/100 - Validation: 100%|██████████| 15/15 [00:01<00:00, 13.77it/s]\n",
      "Epoch 79/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 79/100 - Testing: 100%|██████████| 17/17 [00:01<00:00, 14.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/100 - Train Loss: 0.0923, Train Acc: 0.9625, Val Loss: 0.0917, Val Acc: 0.9637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 80/100 - Training:   0%|          | 0/19 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 80/100 - Training: 100%|██████████| 19/19 [00:02<00:00,  7.55it/s]\n",
      "Epoch 80/100 - Validation:   0%|          | 0/15 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:453: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 80/100 - Validation: 100%|██████████| 15/15 [00:01<00:00, 13.70it/s]\n",
      "Epoch 80/100 - Testing:   0%|          | 0/17 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:486: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 80/100 - Testing: 100%|██████████| 17/17 [00:01<00:00, 14.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100 - Train Loss: 0.0933, Train Acc: 0.9621, Val Loss: 0.0911, Val Acc: 0.9637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 81/100 - Training:   0%|          | 0/19 [00:00<?, ?it/s]/tmp/ipykernel_15099/2606998298.py:405: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/joao/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "Epoch 81/100 - Training:  16%|█▌        | 3/19 [00:01<00:05,  2.93it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Tried to step 401 times. The specified number of total steps is 400",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 100\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining a single model from the ensemble...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     98\u001b[0m model_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Train the first model\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mensemble\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_single_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_patience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mOneCycleLR\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_accumulation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Accumulate gradients over 4 batches to simulate larger batch\u001b[39;49;00m\n\u001b[1;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_to_wandb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    111\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# Free up memory\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m model\n",
      "Cell \u001b[0;32mIn[6], line 417\u001b[0m, in \u001b[0;36mUltraMemoryEfficientEnsemble.train_single_model\u001b[0;34m(self, model_idx, train_loader, val_loader, test_loader, num_epochs, lr, max_patience, scheduler_type, batch_accumulation, log_to_wandb)\u001b[0m\n\u001b[1;32m    415\u001b[0m         scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m    416\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m scheduler_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOneCycleLR\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 417\u001b[0m             \u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    419\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(X_spc, X_ga)\n",
      "File \u001b[0;32m~/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:241\u001b[0m, in \u001b[0;36mLRScheduler.step\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_epoch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 241\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_lr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    243\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(EPOCH_DEPRECATION_WARNING, \u001b[38;5;167;01mUserWarning\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents/Star-Classifier/myenv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:2153\u001b[0m, in \u001b[0;36mOneCycleLR.get_lr\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2150\u001b[0m step_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_epoch\n\u001b[1;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step_num \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_steps:\n\u001b[0;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2154\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to step \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m times. The specified number of total steps is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_steps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# noqa: UP032\u001b[39;00m\n\u001b[1;32m   2155\u001b[0m     )\n\u001b[1;32m   2157\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[1;32m   2158\u001b[0m     start_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: Tried to step 401 times. The specified number of total steps is 400"
     ]
    }
   ],
   "source": [
    "# Import your model and the ultra memory-efficient ensemble\n",
    "#from ultra_memory_efficient import MemoryEfficientStarClassifier, UltraMemoryEfficientEnsemble\n",
    "#from your_dataset_file import MultiModalBalancedMultiLabelDataset  # Import your dataset class\n",
    "\n",
    "# Function to track memory usage\n",
    "def print_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "        print(f\"GPU memory reserved: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")\n",
    "        # Force garbage collection\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# Example configuration for high-dimensional embeddings\n",
    "CONFIG = {\n",
    "    \"d_model_spectra\": 2048,  # Higher embedding dimension\n",
    "    \"d_model_gaia\": 2048,     # Higher embedding dimension\n",
    "    \"num_classes\": 55,\n",
    "    \"input_dim_spectra\": 3647,\n",
    "    \"input_dim_gaia\": 18,\n",
    "    \"n_layers\": 12,\n",
    "    \"d_state\": 8,            # Reduced state dimension to save memory\n",
    "    \"d_conv\": 4,\n",
    "    \"expand\": 2,\n",
    "    \"use_cross_attention\": True,\n",
    "    \"n_cross_attn_heads\": 8,\n",
    "    \"use_checkpoint\": True,\n",
    "    \"activation_checkpointing\": True,\n",
    "    \"use_half_precision\": True,\n",
    "    \"sequential_processing\": True\n",
    "}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set device\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Print initial memory usage\n",
    "    print(\"\\nInitial memory usage:\")\n",
    "    print_gpu_memory()\n",
    "    \n",
    "    # Load your dataset\n",
    "    # Replace this with your actual dataset loading code\n",
    "    batch_size = 16  # Use smaller batch size to save memory\n",
    "    batch_limit = int(batch_size / 2.5)\n",
    "    \n",
    "    # Load datasets (replace with your actual loading code)\n",
    "    print(\"Loading datasets...\")\n",
    "    train_dataset = MultiModalBalancedMultiLabelDataset(\n",
    "        X_train_spectra, X_train_gaia, y_train, limit_per_label=batch_limit\n",
    "    )\n",
    "    val_dataset = MultiModalBalancedMultiLabelDataset(\n",
    "        X_val_spectra, X_val_gaia, y_val, limit_per_label=batch_limit\n",
    "    )\n",
    "    test_dataset = MultiModalBalancedMultiLabelDataset(\n",
    "        X_test_spectra, X_test_gaia, y_test, limit_per_label=batch_limit\n",
    "    )\n",
    "    \n",
    "    # Create data loaders with smaller batches\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    \n",
    "    # Create ultra memory-efficient ensemble (no models are created yet)\n",
    "    print(\"\\nCreating ultra memory-efficient ensemble...\")\n",
    "    ensemble = UltraMemoryEfficientEnsemble(\n",
    "        model_class=MemoryEfficientStarClassifier,\n",
    "        model_args=CONFIG,\n",
    "        num_models=5,\n",
    "        device=device,\n",
    "        checkpoint_dir='ultra_memory_efficient_models'\n",
    "    )\n",
    "    \n",
    "    # Print memory usage after creating ensemble (should be minimal)\n",
    "    print(\"\\nMemory usage after creating ensemble (no models created yet):\")\n",
    "    print_gpu_memory()\n",
    "    \n",
    "    # ----------------- Option 1: Train the entire ensemble -----------------\n",
    "    # Uncomment this section to train all models in the ensemble\n",
    "    \n",
    "    # print(\"\\nTraining all models in the ensemble...\")\n",
    "    # model_paths = ensemble.train(\n",
    "    #     train_loader=train_loader,\n",
    "    #     val_loader=val_loader,\n",
    "    #     test_loader=test_loader,\n",
    "    #     num_epochs=100,\n",
    "    #     lr=1e-4,\n",
    "    #     max_patience=20,\n",
    "    #     scheduler_type='OneCycleLR',\n",
    "    #     batch_accumulation=4,  # Accumulate gradients over 4 batches (effectively 4x batch size)\n",
    "    #     log_to_wandb=True\n",
    "    # )\n",
    "    \n",
    "    # ----------------- Option 2: Train one model at a time -----------------\n",
    "    # For more control, train models one at a time\n",
    "    \n",
    "    print(\"\\nTraining a single model from the ensemble...\")\n",
    "    model_idx = 0  # Train the first model\n",
    "    \n",
    "    model = ensemble.train_single_model(\n",
    "        model_idx=model_idx,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader,\n",
    "        num_epochs=100,\n",
    "        lr=1e-4,\n",
    "        max_patience=20,\n",
    "        scheduler_type='OneCycleLR',\n",
    "        batch_accumulation=4,  # Accumulate gradients over 4 batches to simulate larger batch\n",
    "        log_to_wandb=True\n",
    "    )\n",
    "    \n",
    "    # Free up memory\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    print(\"\\nMemory usage after training and cleanup:\")\n",
    "    print_gpu_memory()\n",
    "    \n",
    "    # ----------------- Making predictions with the ensemble -----------------\n",
    "    \n",
    "    # Setup for smaller batch inference\n",
    "    inference_batch_size = 8  # Use a very small batch size for inference\n",
    "    test_loader_small = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=inference_batch_size, \n",
    "        shuffle=False,\n",
    "        num_workers=2\n",
    "    )\n",
    "    \n",
    "    print(\"\\nMaking predictions with the trained model(s)...\")\n",
    "    \n",
    "    # Use micro-batch size of 1 for minimal memory usage\n",
    "    mean_probs, std_probs = ensemble.predict(\n",
    "        test_loader_small, \n",
    "        return_individual=False,\n",
    "        micro_batch_size=1\n",
    "    )\n",
    "    \n",
    "    print(\"\\nMemory usage after prediction:\")\n",
    "    print_gpu_memory()\n",
    "    \n",
    "    print(\"\\nPrediction shape:\", mean_probs.shape)\n",
    "    print(\"Uncertainty shape:\", std_probs.shape)\n",
    "    \n",
    "    # ----------------- Analyze results -----------------\n",
    "    \n",
    "    # Convert to binary predictions\n",
    "    threshold = 0.5\n",
    "    predictions = (mean_probs >= threshold).astype(float)\n",
    "    \n",
    "    # Calculate overall accuracy\n",
    "    # Assuming y_test is available and has the same order as predictions\n",
    "    accuracy = np.mean(np.equal(predictions, y_test.numpy()).astype(float))\n",
    "    print(f\"\\nOverall accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Identify high uncertainty predictions\n",
    "    high_uncertainty_threshold = np.percentile(std_probs, 90)  # Top 10% most uncertain\n",
    "    high_uncertainty_mask = std_probs >= high_uncertainty_threshold\n",
    "    \n",
    "    high_uncertainty_count = np.sum(high_uncertainty_mask)\n",
    "    print(f\"\\nHigh uncertainty predictions: {high_uncertainty_count} ({high_uncertainty_count/std_probs.size:.2%})\")\n",
    "    \n",
    "    # Check if high uncertainty correlates with errors\n",
    "    high_uncertainty_errors = np.mean(np.not_equal(\n",
    "        predictions[high_uncertainty_mask], \n",
    "        y_test.numpy()[high_uncertainty_mask]\n",
    "    ).astype(float))\n",
    "    \n",
    "    normal_uncertainty_errors = np.mean(np.not_equal(\n",
    "        predictions[~high_uncertainty_mask], \n",
    "        y_test.numpy()[~high_uncertainty_mask]\n",
    "    ).astype(float))\n",
    "    \n",
    "    print(f\"Error rate for high uncertainty predictions: {high_uncertainty_errors:.4f}\")\n",
    "    print(f\"Error rate for normal predictions: {normal_uncertainty_errors:.4f}\")\n",
    "    print(f\"Ratio: {high_uncertainty_errors/normal_uncertainty_errors:.2f}x\")\n",
    "    \n",
    "    print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assymetric Gaia and Lamost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from mamba_ssm import Mamba2\n",
    "\n",
    "class AsymmetricMemoryEfficientStarClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Memory-efficient version of StarClassifierFusion with asymmetric dimensions\n",
    "    for spectral and Gaia data, allowing for much smaller Gaia embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model_spectra,\n",
    "        d_model_gaia,\n",
    "        num_classes,\n",
    "        input_dim_spectra,\n",
    "        input_dim_gaia,\n",
    "        n_layers=6,\n",
    "        use_cross_attention=True,\n",
    "        n_cross_attn_heads=8,\n",
    "        d_state_spectra=16,\n",
    "        d_state_gaia=8,  # Can be smaller for Gaia\n",
    "        d_conv=4,\n",
    "        expand=2,\n",
    "        use_checkpoint=True,\n",
    "        activation_checkpointing=True,\n",
    "        use_half_precision=True,\n",
    "        sequential_processing=True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "        self.activation_checkpointing = activation_checkpointing\n",
    "        self.sequential_processing = sequential_processing\n",
    "        \n",
    "        # Store dimensions for later use\n",
    "        self.d_model_spectra = d_model_spectra\n",
    "        self.d_model_gaia = d_model_gaia\n",
    "        \n",
    "        # Use lower precision\n",
    "        self.dtype = torch.float16 if use_half_precision else torch.float32\n",
    "\n",
    "        # Input projection layers - project inputs to their respective embedding spaces\n",
    "        self.input_proj_spectra = nn.Linear(input_dim_spectra, d_model_spectra)\n",
    "        self.input_proj_gaia = nn.Linear(input_dim_gaia, d_model_gaia)\n",
    "        \n",
    "        # Memory-efficient Mamba layers for Spectra (higher dimension)\n",
    "        self.mamba_spectra_layers = nn.ModuleList([\n",
    "            self._create_mamba_layer(\n",
    "                d_model=d_model_spectra,\n",
    "                d_state=d_state_spectra,\n",
    "                d_conv=d_conv,\n",
    "                expand=expand\n",
    "            ) for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        # Memory-efficient Mamba layers for Gaia (lower dimension)\n",
    "        self.mamba_gaia_layers = nn.ModuleList([\n",
    "            self._create_mamba_layer(\n",
    "                d_model=d_model_gaia,\n",
    "                d_state=d_state_gaia,\n",
    "                d_conv=d_conv,\n",
    "                expand=expand\n",
    "            ) for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "        # Cross-attention (optional)\n",
    "        self.use_cross_attention = use_cross_attention\n",
    "        if use_cross_attention:\n",
    "            # Adaptation layers for cross-attention with different dimensions\n",
    "            # For Gaia→Spectra attention, we need to project Gaia to match Spectra dimension\n",
    "            self.gaia_to_spectra_proj = nn.Linear(d_model_gaia, d_model_spectra)\n",
    "            \n",
    "            # For Spectra→Gaia attention, we need to project Spectra to match Gaia dimension\n",
    "            self.spectra_to_gaia_proj = nn.Linear(d_model_spectra, d_model_gaia)\n",
    "            \n",
    "            # Create cross-attention blocks\n",
    "            self.cross_attn_block_spectra = self._create_cross_attn_block(\n",
    "                d_model=d_model_spectra, n_heads=n_cross_attn_heads\n",
    "            )\n",
    "            self.cross_attn_block_gaia = self._create_cross_attn_block(\n",
    "                d_model=d_model_gaia, n_heads=n_cross_attn_heads\n",
    "            )\n",
    "\n",
    "        # Final classifier\n",
    "        # Add a projection layer to transform concatenated features to a common fusion dimension\n",
    "        fusion_dim = d_model_spectra + d_model_gaia\n",
    "        self.layer_norm = nn.LayerNorm(fusion_dim)\n",
    "        self.classifier = nn.Linear(fusion_dim, num_classes)\n",
    "    \n",
    "    def _create_mamba_layer(self, d_model, d_state, d_conv, expand):\n",
    "        \"\"\"Create a memory-efficient Mamba layer.\"\"\"\n",
    "        mamba = Mamba2(\n",
    "            d_model=d_model,\n",
    "            d_state=d_state,\n",
    "            d_conv=d_conv,\n",
    "            expand=expand\n",
    "        )\n",
    "        \n",
    "        # Wrap with gradient checkpointing if requested\n",
    "        if self.activation_checkpointing:\n",
    "            return MemoryEfficientMamba(mamba, use_checkpoint=True)\n",
    "        else:\n",
    "            return mamba\n",
    "    \n",
    "    def _create_cross_attn_block(self, d_model, n_heads):\n",
    "        \"\"\"Creates a cross-attention block with optional gradient checkpointing.\"\"\"\n",
    "        class CrossAttentionBlock(nn.Module):\n",
    "            def __init__(self, d_model, n_heads):\n",
    "                super().__init__()\n",
    "                self.cross_attn = nn.MultiheadAttention(\n",
    "                    embed_dim=d_model, \n",
    "                    num_heads=n_heads, \n",
    "                    batch_first=True\n",
    "                )\n",
    "                self.norm1 = nn.LayerNorm(d_model)\n",
    "                \n",
    "                # Smaller FFN to save memory\n",
    "                self.ffn = nn.Sequential(\n",
    "                    nn.Linear(d_model, 2 * d_model),  # Reduced from 4x\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(2 * d_model, d_model)   # Reduced from 4x\n",
    "                )\n",
    "                self.norm2 = nn.LayerNorm(d_model)\n",
    "                \n",
    "            def forward(self, x_q, x_kv):\n",
    "                # Cross-attention\n",
    "                attn_output, _ = self.cross_attn(query=x_q, key=x_kv, value=x_kv)\n",
    "                x = self.norm1(x_q + attn_output)\n",
    "                \n",
    "                # Feed forward\n",
    "                ffn_out = self.ffn(x)\n",
    "                x = self.norm2(x + ffn_out)\n",
    "                \n",
    "                return x\n",
    "        \n",
    "        block = CrossAttentionBlock(d_model, n_heads)\n",
    "        \n",
    "        # Wrap with gradient checkpointing if requested\n",
    "        if self.activation_checkpointing:\n",
    "            def forward_with_checkpoint(module, x_q, x_kv):\n",
    "                def custom_forward(x_q, x_kv):\n",
    "                    return module(x_q, x_kv)\n",
    "                return checkpoint(custom_forward, x_q, x_kv)\n",
    "            \n",
    "            class CheckpointedCrossAttention(nn.Module):\n",
    "                def __init__(self, block):\n",
    "                    super().__init__()\n",
    "                    self.block = block\n",
    "                \n",
    "                def forward(self, x_q, x_kv):\n",
    "                    return forward_with_checkpoint(self.block, x_q, x_kv)\n",
    "            \n",
    "            return CheckpointedCrossAttention(block)\n",
    "        else:\n",
    "            return block\n",
    "    \n",
    "    def _process_mamba_layers(self, x, layers):\n",
    "        \"\"\"Process input through Mamba layers, optionally sequentially to save memory.\"\"\"\n",
    "        if self.sequential_processing:\n",
    "            for layer in layers:\n",
    "                x = layer(x)\n",
    "                # Optional: explicitly delete intermediate activations\n",
    "                torch.cuda.empty_cache()\n",
    "        else:\n",
    "            # Process all layers at once (uses more memory but faster)\n",
    "            for layer in layers:\n",
    "                x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x_spectra, x_gaia):\n",
    "        # Convert to half precision if requested\n",
    "        if hasattr(self, 'dtype') and self.dtype == torch.float16:\n",
    "            x_spectra = x_spectra.half()\n",
    "            x_gaia = x_gaia.half()\n",
    "        \n",
    "        # Project inputs to their respective embedding spaces\n",
    "        x_spectra = self.input_proj_spectra(x_spectra)\n",
    "        x_gaia = self.input_proj_gaia(x_gaia)\n",
    "        \n",
    "        # Add sequence dimension if needed\n",
    "        if len(x_spectra.shape) == 2:\n",
    "            x_spectra = x_spectra.unsqueeze(1)\n",
    "        if len(x_gaia.shape) == 2:\n",
    "            x_gaia = x_gaia.unsqueeze(1)\n",
    "        \n",
    "        # Process through Mamba layers\n",
    "        x_spectra = self._process_mamba_layers(x_spectra, self.mamba_spectra_layers)\n",
    "        x_gaia = self._process_mamba_layers(x_gaia, self.mamba_gaia_layers)\n",
    "        \n",
    "        # Optional cross-attention (with dimension adaptation)\n",
    "        if self.use_cross_attention:\n",
    "            # Project Gaia features to match spectra dimension for spectra's cross-attention\n",
    "            x_gaia_projected = self.gaia_to_spectra_proj(x_gaia)\n",
    "            \n",
    "            # Project Spectra features to match Gaia dimension for Gaia's cross-attention\n",
    "            x_spectra_projected = self.spectra_to_gaia_proj(x_spectra)\n",
    "            \n",
    "            # Cross-attention from spectra -> gaia (using projected Gaia)\n",
    "            x_spectra_fused = self.cross_attn_block_spectra(x_spectra, x_gaia_projected)\n",
    "            \n",
    "            # Cross-attention from gaia -> spectra (using projected Spectra)\n",
    "            x_gaia_fused = self.cross_attn_block_gaia(x_gaia, x_spectra_projected)\n",
    "            \n",
    "            # Update embeddings\n",
    "            x_spectra = x_spectra_fused\n",
    "            x_gaia = x_gaia_fused\n",
    "            \n",
    "            # Free memory\n",
    "            del x_gaia_projected, x_spectra_projected, x_spectra_fused, x_gaia_fused\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Pool across sequence dimension\n",
    "        x_spectra = x_spectra.mean(dim=1)\n",
    "        x_gaia = x_gaia.mean(dim=1)\n",
    "        \n",
    "        # Concatenate (different dimensions are fine for concatenation)\n",
    "        x_fused = torch.cat([x_spectra, x_gaia], dim=-1)\n",
    "        \n",
    "        # Final classification\n",
    "        x_fused = self.layer_norm(x_fused)\n",
    "        logits = self.classifier(x_fused)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "class MemoryEfficientMamba(nn.Module):\n",
    "    \"\"\"\n",
    "    Memory-efficient wrapper for Mamba2 with gradient checkpointing.\n",
    "    \"\"\"\n",
    "    def __init__(self, mamba, use_checkpoint=True):\n",
    "        super().__init__()\n",
    "        self.mamba = mamba\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.use_checkpoint and self.training:\n",
    "            return checkpoint(self.mamba, x)\n",
    "        else:\n",
    "            return self.mamba(x)\n",
    "\n",
    "\n",
    "class UltraMemoryEfficientEnsemble:\n",
    "    \"\"\"\n",
    "    Ultra memory-efficient implementation of ensemble for uncertainty quantification.\n",
    "    Avoids creating multiple models in memory and removes quantization during initialization.\n",
    "    \n",
    "    Note: See ultra-memory-efficient.py for full implementation.\n",
    "    \"\"\"\n",
    "    # This is a placeholder class - use the full implementation from the other file\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating configurations for VRAM usage:\n",
      "--------------------------------------------------------------------------------\n",
      "Configuration: Balanced High-Dim\n",
      "d_model_spectra: 4096, d_model_gaia: 1024\n",
      "Estimated parameters: 1,004,045,312\n",
      "Parameter memory: 1.87 GB\n",
      "Optimizer memory: 3.74 GB\n",
      "Activations memory: 0.00 GB\n",
      "Total estimated VRAM: 5.61 GB\n",
      "--------------------------------------------------------------------------------\n",
      "Configuration: Extreme Asymmetric\n",
      "d_model_spectra: 4096, d_model_gaia: 512\n",
      "Estimated parameters: 956,559,872\n",
      "Parameter memory: 1.78 GB\n",
      "Optimizer memory: 3.56 GB\n",
      "Activations memory: 0.00 GB\n",
      "Total estimated VRAM: 5.35 GB\n",
      "--------------------------------------------------------------------------------\n",
      "Configuration: Extremely Memory Efficient\n",
      "d_model_spectra: 3072, d_model_gaia: 256\n",
      "Estimated parameters: 459,591,936\n",
      "Parameter memory: 0.86 GB\n",
      "Optimizer memory: 1.71 GB\n",
      "Activations memory: 0.00 GB\n",
      "Total estimated VRAM: 2.57 GB\n",
      "--------------------------------------------------------------------------------\n",
      "Configuration: Balanced Medium-Dim\n",
      "d_model_spectra: 2048, d_model_gaia: 512\n",
      "Estimated parameters: 183,517,696\n",
      "Parameter memory: 0.34 GB\n",
      "Optimizer memory: 0.68 GB\n",
      "Activations memory: 0.00 GB\n",
      "Total estimated VRAM: 1.03 GB\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Recommended Configuration:\n",
      "--------------------------------------------------------------------------------\n",
      "Recommended Configuration: Balanced High-Dim\n",
      "d_model_spectra: 4096, d_model_gaia: 1024\n",
      "n_layers: 12\n",
      "Estimated parameters: 1,004,045,312\n",
      "Total estimated VRAM: 5.61 GB\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "#from ultra_memory_efficient import UltraMemoryEfficientEnsemble\n",
    "#from asymmetric_dimensions import AsymmetricMemoryEfficientStarClassifier\n",
    "\n",
    "# Function to estimate model VRAM usage\n",
    "def estimate_vram_usage(config):\n",
    "    # Parameters count estimates\n",
    "    d_model_spectra = config[\"d_model_spectra\"]\n",
    "    d_model_gaia = config[\"d_model_gaia\"]\n",
    "    input_dim_spectra = config[\"input_dim_spectra\"]\n",
    "    input_dim_gaia = config[\"input_dim_gaia\"]\n",
    "    n_layers = config[\"n_layers\"]\n",
    "    num_classes = config[\"num_classes\"]\n",
    "    \n",
    "    # Estimate input projection parameters\n",
    "    input_proj_params = (input_dim_spectra * d_model_spectra) + (input_dim_gaia * d_model_gaia)\n",
    "    \n",
    "    # Estimate Mamba parameters (very rough estimation)\n",
    "    # For each Mamba block: we have the main projection, expand factor, state space, etc.\n",
    "    mamba_params_spectra = n_layers * d_model_spectra * d_model_spectra * 4  # Approximation\n",
    "    mamba_params_gaia = n_layers * d_model_gaia * d_model_gaia * 4  # Approximation\n",
    "    \n",
    "    # Cross-attention parameters\n",
    "    cross_attn_params = 0\n",
    "    if config[\"use_cross_attention\"]:\n",
    "        # Adaptation layers\n",
    "        cross_attn_params += (d_model_spectra * d_model_gaia) * 2\n",
    "        # Attention layers\n",
    "        cross_attn_params += d_model_spectra * d_model_spectra * 3  # Q,K,V projections\n",
    "        cross_attn_params += d_model_gaia * d_model_gaia * 3  # Q,K,V projections\n",
    "        # FFN layers\n",
    "        cross_attn_params += (d_model_spectra * d_model_spectra * 4) + (d_model_gaia * d_model_gaia * 4)\n",
    "    \n",
    "    # Classifier parameters\n",
    "    classifier_params = (d_model_spectra + d_model_gaia) * num_classes\n",
    "    \n",
    "    # Total parameters\n",
    "    total_params = input_proj_params + mamba_params_spectra + mamba_params_gaia + cross_attn_params + classifier_params\n",
    "    \n",
    "    # Estimate VRAM usage (in GB)\n",
    "    # Parameters (4 bytes per param in FP32, 2 bytes in FP16)\n",
    "    param_memory = total_params * (2 if config[\"use_half_precision\"] else 4) / (1024**3)\n",
    "    \n",
    "    # Optimizer states (Adam uses 8 bytes per parameter, SGD 4 bytes)\n",
    "    # We'll assume SGD for memory efficiency\n",
    "    optimizer_memory = total_params * 4 / (1024**3)\n",
    "    \n",
    "    # Activations - this is highly approximate\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    activations_memory = batch_size * (d_model_spectra + d_model_gaia) * 2 / (1024**3)\n",
    "    \n",
    "    # Total VRAM usage\n",
    "    total_vram = param_memory + optimizer_memory + activations_memory\n",
    "    \n",
    "    return {\n",
    "        \"parameters\": int(total_params),\n",
    "        \"param_memory_GB\": param_memory,\n",
    "        \"optimizer_memory_GB\": optimizer_memory,\n",
    "        \"activations_memory_GB\": activations_memory,\n",
    "        \"total_vram_GB\": total_vram\n",
    "    }\n",
    "\n",
    "# Memory-optimized configurations\n",
    "CONFIGS = [\n",
    "    {\n",
    "        \"name\": \"Balanced High-Dim\",\n",
    "        \"d_model_spectra\": 4096,\n",
    "        \"d_model_gaia\": 1024,  # 4x reduction for Gaia\n",
    "        \"num_classes\": 55,\n",
    "        \"input_dim_spectra\": 3647,\n",
    "        \"input_dim_gaia\": 18,\n",
    "        \"n_layers\": 12,\n",
    "        \"d_state_spectra\": 16,\n",
    "        \"d_state_gaia\": 8,\n",
    "        \"d_conv\": 4,\n",
    "        \"expand\": 2,\n",
    "        \"use_cross_attention\": True,\n",
    "        \"n_cross_attn_heads\": 8,\n",
    "        \"use_checkpoint\": True,\n",
    "        \"activation_checkpointing\": True,\n",
    "        \"use_half_precision\": True,\n",
    "        \"sequential_processing\": True,\n",
    "        \"batch_size\": 16\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Extreme Asymmetric\",\n",
    "        \"d_model_spectra\": 4096,\n",
    "        \"d_model_gaia\": 512,   # 8x reduction for Gaia\n",
    "        \"num_classes\": 55,\n",
    "        \"input_dim_spectra\": 3647,\n",
    "        \"input_dim_gaia\": 18,\n",
    "        \"n_layers\": 12,\n",
    "        \"d_state_spectra\": 16,\n",
    "        \"d_state_gaia\": 8,\n",
    "        \"d_conv\": 4,\n",
    "        \"expand\": 2,\n",
    "        \"use_cross_attention\": True,\n",
    "        \"n_cross_attn_heads\": 8,\n",
    "        \"use_checkpoint\": True,\n",
    "        \"activation_checkpointing\": True,\n",
    "        \"use_half_precision\": True,\n",
    "        \"sequential_processing\": True,\n",
    "        \"batch_size\": 32\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Extremely Memory Efficient\",\n",
    "        \"d_model_spectra\": 3072,\n",
    "        \"d_model_gaia\": 256,   # 12x reduction for Gaia\n",
    "        \"num_classes\": 55,\n",
    "        \"input_dim_spectra\": 3647,\n",
    "        \"input_dim_gaia\": 18,\n",
    "        \"n_layers\": 10,        # Reduced layers\n",
    "        \"d_state_spectra\": 8,  # Smaller state\n",
    "        \"d_state_gaia\": 4,     # Tiny state for Gaia\n",
    "        \"d_conv\": 2,           # Smaller conv\n",
    "        \"expand\": 1,           # No expansion\n",
    "        \"use_cross_attention\": True,\n",
    "        \"n_cross_attn_heads\": 4,  # Fewer heads\n",
    "        \"use_checkpoint\": True,\n",
    "        \"activation_checkpointing\": True,\n",
    "        \"use_half_precision\": True,\n",
    "        \"sequential_processing\": True,\n",
    "        \"batch_size\": 32\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Balanced Medium-Dim\",\n",
    "        \"d_model_spectra\": 2048,  # Reduced spectra dim\n",
    "        \"d_model_gaia\": 512,      # 4x reduction for Gaia\n",
    "        \"num_classes\": 55,\n",
    "        \"input_dim_spectra\": 3647,\n",
    "        \"input_dim_gaia\": 18,\n",
    "        \"n_layers\": 8,            # Reduced layers\n",
    "        \"d_state_spectra\": 8,     # Smaller state\n",
    "        \"d_state_gaia\": 4,        # Tiny state for Gaia\n",
    "        \"d_conv\": 2,              # Smaller conv\n",
    "        \"expand\": 2,\n",
    "        \"use_cross_attention\": True,\n",
    "        \"n_cross_attn_heads\": 4,  # Fewer heads\n",
    "        \"use_checkpoint\": True,\n",
    "        \"activation_checkpointing\": True,\n",
    "        \"use_half_precision\": True,\n",
    "        \"sequential_processing\": True,\n",
    "        \"batch_size\": 32\n",
    "    }\n",
    "]\n",
    "\n",
    "# Evaluate each configuration\n",
    "print(\"Evaluating configurations for VRAM usage:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for config in CONFIGS:\n",
    "    vram_usage = estimate_vram_usage(config)\n",
    "    print(f\"Configuration: {config['name']}\")\n",
    "    print(f\"d_model_spectra: {config['d_model_spectra']}, d_model_gaia: {config['d_model_gaia']}\")\n",
    "    print(f\"Estimated parameters: {vram_usage['parameters']:,}\")\n",
    "    print(f\"Parameter memory: {vram_usage['param_memory_GB']:.2f} GB\")\n",
    "    print(f\"Optimizer memory: {vram_usage['optimizer_memory_GB']:.2f} GB\")\n",
    "    print(f\"Activations memory: {vram_usage['activations_memory_GB']:.2f} GB\")\n",
    "    print(f\"Total estimated VRAM: {vram_usage['total_vram_GB']:.2f} GB\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(\"\\nRecommended Configuration:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Choose the best configuration\n",
    "# For a 24 GB GPU, we want to stay under ~20 GB to leave room for system overhead\n",
    "recommended = None\n",
    "for config in CONFIGS:\n",
    "    vram_usage = estimate_vram_usage(config)\n",
    "    if vram_usage['total_vram_GB'] < 20:\n",
    "        if recommended is None or config['d_model_spectra'] > recommended['d_model_spectra']:\n",
    "            recommended = config\n",
    "\n",
    "if recommended:\n",
    "    vram_usage = estimate_vram_usage(recommended)\n",
    "    print(f\"Recommended Configuration: {recommended['name']}\")\n",
    "    print(f\"d_model_spectra: {recommended['d_model_spectra']}, d_model_gaia: {recommended['d_model_gaia']}\")\n",
    "    print(f\"n_layers: {recommended['n_layers']}\")\n",
    "    print(f\"Estimated parameters: {vram_usage['parameters']:,}\")\n",
    "    print(f\"Total estimated VRAM: {vram_usage['total_vram_GB']:.2f} GB\")\n",
    "    print(\"-\" * 80)\n",
    "else:\n",
    "    print(\"No configuration fits within 20 GB VRAM limit.\")\n",
    "    print(\"Consider further reductions in model dimensions.\")\n",
    "\n",
    "# Example usage of the recommended configuration\n",
    "def train_with_recommended_config():\n",
    "    \"\"\"Example of how to train with the recommended configuration.\"\"\"\n",
    "    # Set device\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Create model using recommended config\n",
    "    model = AsymmetricMemoryEfficientStarClassifier(\n",
    "        d_model_spectra=recommended['d_model_spectra'],\n",
    "        d_model_gaia=recommended['d_model_gaia'],\n",
    "        num_classes=recommended['num_classes'],\n",
    "        input_dim_spectra=recommended['input_dim_spectra'],\n",
    "        input_dim_gaia=recommended['input_dim_gaia'],\n",
    "        n_layers=recommended['n_layers'],\n",
    "        d_state_spectra=recommended['d_state_spectra'],\n",
    "        d_state_gaia=recommended['d_state_gaia'],\n",
    "        d_conv=recommended['d_conv'],\n",
    "        expand=recommended['expand'],\n",
    "        use_cross_attention=recommended['use_cross_attention'],\n",
    "        n_cross_attn_heads=recommended['n_cross_attn_heads'],\n",
    "        use_checkpoint=recommended['use_checkpoint'],\n",
    "        activation_checkpointing=recommended['activation_checkpointing'],\n",
    "        use_half_precision=recommended['use_half_precision'],\n",
    "        sequential_processing=recommended['sequential_processing']\n",
    "    )\n",
    "    \n",
    "    # Create ensemble with this model\n",
    "    ensemble = UltraMemoryEfficientEnsemble(\n",
    "        model_class=AsymmetricMemoryEfficientStarClassifier,\n",
    "        model_args=recommended,\n",
    "        num_models=5,\n",
    "        device=device,\n",
    "        checkpoint_dir='asymmetric_ensemble_models'\n",
    "    )\n",
    "    \n",
    "    # Example training code (assuming your datasets are available)\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=recommended['batch_size'], \n",
    "        shuffle=True,\n",
    "        num_workers=2\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=recommended['batch_size'], \n",
    "        shuffle=False,\n",
    "        num_workers=2\n",
    "    )\n",
    "    \n",
    "    # Train one model at a time to maximize memory efficiency\n",
    "    for model_idx in range(5):\n",
    "        ensemble.train_single_model(\n",
    "            model_idx=model_idx,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            num_epochs=100,\n",
    "            lr=1e-4,\n",
    "            batch_accumulation=4,  # Accumulate gradients over 4 batches\n",
    "            scheduler_type='OneCycleLR'\n",
    "        )\n",
    "    \n",
    "    return ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_single_model(\n",
    "    self, \n",
    "    model_idx,\n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    test_loader=None, \n",
    "    num_epochs=100, \n",
    "    lr=1e-4, \n",
    "    max_patience=20,\n",
    "    scheduler_type='OneCycleLR',\n",
    "    batch_accumulation=1,  # Gradient accumulation steps\n",
    "    log_to_wandb=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a single model in the ensemble with fixed scheduler stepping.\n",
    "    \n",
    "    Args:\n",
    "        model_idx: Index of the model to train\n",
    "        train_loader: DataLoader for training data\n",
    "        val_loader: DataLoader for validation data\n",
    "        test_loader: DataLoader for test data (optional)\n",
    "        num_epochs: Maximum number of epochs to train\n",
    "        lr: Learning rate\n",
    "        max_patience: Maximum patience for early stopping\n",
    "        scheduler_type: Type of learning rate scheduler ('OneCycleLR' or 'ReduceLROnPlateau')\n",
    "        batch_accumulation: Number of batches to accumulate gradients \n",
    "        log_to_wandb: Whether to log training progress to wandb\n",
    "        \n",
    "    Returns:\n",
    "        Trained model (also saved to checkpoint)\n",
    "    \"\"\"\n",
    "    import torch.optim as optim\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    # Initialize wandb if requested\n",
    "    if log_to_wandb:\n",
    "        try:\n",
    "            import wandb\n",
    "            run = wandb.init(\n",
    "                project=\"ALLSTARS_ultra_memory_efficient\", \n",
    "                name=f\"model_{model_idx}\",\n",
    "                group=\"memory_efficient_training\",\n",
    "                config={\n",
    "                    **self.model_args,\n",
    "                    \"model_idx\": model_idx,\n",
    "                    \"num_models\": self.num_models,\n",
    "                    \"lr\": lr,\n",
    "                    \"max_patience\": max_patience,\n",
    "                    \"scheduler_type\": scheduler_type,\n",
    "                    \"batch_accumulation\": batch_accumulation,\n",
    "                    \"num_epochs\": num_epochs\n",
    "                },\n",
    "                reinit=True\n",
    "            )\n",
    "        except ImportError:\n",
    "            print(\"wandb not installed. Training without logging.\")\n",
    "            log_to_wandb = False\n",
    "    \n",
    "    # Load or create model\n",
    "    model = self._load_model(model_idx, for_inference=False)\n",
    "    model.train()\n",
    "    \n",
    "    # Create optimizer (SGD uses less memory than Adam/AdamW)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    \n",
    "    # Calculate the actual number of optimization steps that will be performed\n",
    "    # This is crucial for OneCycleLR to prevent the 'stepped too many times' error\n",
    "    # For gradient accumulation, we need to account for the reduced number of steps\n",
    "    effective_steps_per_epoch = (len(train_loader) + batch_accumulation - 1) // batch_accumulation\n",
    "    \n",
    "    # Configure the scheduler\n",
    "    if scheduler_type == 'OneCycleLR':\n",
    "        scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer, \n",
    "            max_lr=lr,\n",
    "            epochs=num_epochs, \n",
    "            steps_per_epoch=effective_steps_per_epoch\n",
    "        )\n",
    "    else:  # ReduceLROnPlateau\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, \n",
    "            mode='min', \n",
    "            factor=0.5, \n",
    "            patience=int(max_patience / 5)\n",
    "        )\n",
    "    \n",
    "    # Calculate class weights for imbalanced classes\n",
    "    all_labels = []\n",
    "    for _, _, y_batch in train_loader:\n",
    "        all_labels.extend(y_batch.cpu().numpy())\n",
    "    \n",
    "    class_weights = self._calculate_class_weights(np.array(all_labels))\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float).to(self.device)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience = max_patience\n",
    "    \n",
    "    # Initialize mixed precision scaler if supported\n",
    "    scaler = torch.cuda.amp.GradScaler() if hasattr(torch.cuda, 'amp') else None\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Resample training data if needed\n",
    "        if hasattr(train_loader.dataset, 're_sample'):\n",
    "            train_loader.dataset.re_sample()\n",
    "            \n",
    "            # Recompute class weights if needed\n",
    "            all_labels = []\n",
    "            for _, _, y_batch in train_loader:\n",
    "                all_labels.extend(y_batch.cpu().numpy())\n",
    "            class_weights = self._calculate_class_weights(np.array(all_labels))\n",
    "            class_weights = torch.tensor(class_weights, dtype=torch.float).to(self.device)\n",
    "            criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights)\n",
    "\n",
    "        # --- Training Phase ---\n",
    "        model.train()\n",
    "        train_loss, train_acc = 0.0, 0.0\n",
    "        batch_count = 0\n",
    "        optimization_steps = 0  # Track actual optimizer steps\n",
    "        \n",
    "        # Reset gradients at the start of epoch\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        for i, (X_spc, X_ga, y_batch) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\")):\n",
    "            X_spc, X_ga, y_batch = X_spc.to(self.device), X_ga.to(self.device), y_batch.to(self.device)\n",
    "            \n",
    "            # Forward pass with mixed precision if available\n",
    "            if scaler:\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model(X_spc, X_ga)\n",
    "                    loss = criterion(outputs, y_batch) / batch_accumulation\n",
    "                \n",
    "                # Backward pass with scaled gradients\n",
    "                scaler.scale(loss).backward()\n",
    "                \n",
    "                # Step only at the end of accumulation cycle or at the end of epoch\n",
    "                if (i + 1) % batch_accumulation == 0 or (i + 1) == len(train_loader):\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                    optimizer.zero_grad(set_to_none=True)\n",
    "                    optimization_steps += 1\n",
    "                    \n",
    "                    # Step the scheduler after optimization\n",
    "                    if scheduler_type == 'OneCycleLR':\n",
    "                        scheduler.step()\n",
    "            else:\n",
    "                outputs = model(X_spc, X_ga)\n",
    "                loss = criterion(outputs, y_batch) / batch_accumulation\n",
    "                loss.backward()\n",
    "                \n",
    "                if (i + 1) % batch_accumulation == 0 or (i + 1) == len(train_loader):\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad(set_to_none=True)\n",
    "                    optimization_steps += 1\n",
    "                    \n",
    "                    # Step the scheduler after optimization\n",
    "                    if scheduler_type == 'OneCycleLR':\n",
    "                        scheduler.step()\n",
    "            \n",
    "            # Calculate metrics\n",
    "            train_loss += loss.item() * batch_accumulation * X_spc.size(0)\n",
    "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            correct = (predicted == y_batch).float()\n",
    "            train_acc += correct.mean(dim=1).sum().item()\n",
    "            batch_count += X_spc.size(0)\n",
    "            \n",
    "            # Free up memory\n",
    "            del X_spc, X_ga, y_batch, outputs, loss, predicted, correct\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        print(f\"Optimization steps this epoch: {optimization_steps}\")\n",
    "        \n",
    "        train_loss /= batch_count\n",
    "        train_acc /= batch_count\n",
    "\n",
    "        # --- Validation Phase ---\n",
    "        model.eval()\n",
    "        val_loss, val_acc = 0.0, 0.0\n",
    "        val_batch_count = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_spc, X_ga, y_batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n",
    "                X_spc, X_ga, y_batch = X_spc.to(self.device), X_ga.to(self.device), y_batch.to(self.device)\n",
    "                \n",
    "                # Forward pass with mixed precision if available\n",
    "                if scaler:\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        outputs = model(X_spc, X_ga)\n",
    "                        loss = criterion(outputs, y_batch)\n",
    "                else:\n",
    "                    outputs = model(X_spc, X_ga)\n",
    "                    loss = criterion(outputs, y_batch)\n",
    "                \n",
    "                val_loss += loss.item() * X_spc.size(0)\n",
    "                predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                correct = (predicted == y_batch).float()\n",
    "                val_acc += correct.mean(dim=1).sum().item()\n",
    "                val_batch_count += X_spc.size(0)\n",
    "                \n",
    "                # Free up memory\n",
    "                del X_spc, X_ga, y_batch, outputs, loss, predicted, correct\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        val_loss /= val_batch_count\n",
    "        val_acc /= val_batch_count\n",
    "\n",
    "        # --- Test Phase (if provided) ---\n",
    "        test_metrics = {}\n",
    "        if test_loader is not None:\n",
    "            test_loss, test_acc = 0.0, 0.0\n",
    "            test_batch_count = 0\n",
    "            y_true, y_pred = [], []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for X_spc, X_ga, y_batch in tqdm(test_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Testing\"):\n",
    "                    X_spc, X_ga, y_batch = X_spc.to(self.device), X_ga.to(self.device), y_batch.to(self.device)\n",
    "                    \n",
    "                    # Forward pass with mixed precision if available\n",
    "                    if scaler:\n",
    "                        with torch.cuda.amp.autocast():\n",
    "                            outputs = model(X_spc, X_ga)\n",
    "                            loss = criterion(outputs, y_batch)\n",
    "                    else:\n",
    "                        outputs = model(X_spc, X_ga)\n",
    "                        loss = criterion(outputs, y_batch)\n",
    "                    \n",
    "                    test_loss += loss.item() * X_spc.size(0)\n",
    "                    predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                    correct = (predicted == y_batch).float()\n",
    "                    test_acc += correct.mean(dim=1).sum().item()\n",
    "                    test_batch_count += X_spc.size(0)\n",
    "                    \n",
    "                    # Store on CPU to save GPU memory\n",
    "                    y_true.extend(y_batch.cpu().numpy())\n",
    "                    y_pred.extend(predicted.cpu().numpy())\n",
    "                    \n",
    "                    # Free up memory\n",
    "                    del X_spc, X_ga, y_batch, outputs, loss, predicted, correct\n",
    "                    torch.cuda.empty_cache()\n",
    "            \n",
    "            test_loss /= test_batch_count\n",
    "            test_acc /= test_batch_count\n",
    "            \n",
    "            # Calculate metrics on CPU to save GPU memory\n",
    "            test_metrics = self._calculate_metrics(np.array(y_true), np.array(y_pred))\n",
    "            test_metrics.update({\n",
    "                \"test_loss\": test_loss,\n",
    "                \"test_acc\": test_acc,\n",
    "            })\n",
    "\n",
    "        # Log metrics\n",
    "        if log_to_wandb:\n",
    "            log_data = {\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\": train_loss,\n",
    "                \"val_loss\": val_loss,\n",
    "                \"train_acc\": train_acc,\n",
    "                \"val_acc\": val_acc,\n",
    "                \"lr\": self._get_lr(optimizer)\n",
    "            }\n",
    "            log_data.update(test_metrics)\n",
    "            wandb.log(log_data)\n",
    "        \n",
    "        # Print progress\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        # Update ReduceLROnPlateau scheduler if used\n",
    "        if scheduler_type == 'ReduceLROnPlateau':\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "        # Early stopping and checkpoint saving\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience = max_patience\n",
    "            \n",
    "            # Save best model\n",
    "            self._save_model(model, model_idx)\n",
    "            \n",
    "            if log_to_wandb:\n",
    "                wandb.run.summary[\"best_val_loss\"] = best_val_loss\n",
    "        else:\n",
    "            patience -= 1\n",
    "            if patience <= 0:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "        \n",
    "        # Save every 10 epochs as a checkpoint\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            checkpoint_path = os.path.join(self.checkpoint_dir, f\"model_{model_idx}_epoch_{epoch+1}.pt\")\n",
    "            state_dict = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "            torch.save(state_dict, checkpoint_path)\n",
    "    \n",
    "    # Close wandb run if used\n",
    "    if log_to_wandb:\n",
    "        wandb.finish()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Ultra-low memory configurations for 24GB GPU ---\n",
    "\n",
    "# Configuration 1: Maximum Asymmetry\n",
    "# Maximizes the spectral embedding dimension while keeping total VRAM usage manageable\n",
    "MAX_ASYMMETRY_CONFIG = {\n",
    "    \"name\": \"Maximum Asymmetry\",\n",
    "    \"d_model_spectra\": 4096,      # Maximum dimension for spectral data\n",
    "    \"d_model_gaia\": 256,          # 16x reduction for Gaia\n",
    "    \"num_classes\": 55,\n",
    "    \"input_dim_spectra\": 3647,\n",
    "    \"input_dim_gaia\": 18,\n",
    "    \"n_layers\": 12,\n",
    "    \"d_state_spectra\": 16,\n",
    "    \"d_state_gaia\": 4,            # Tiny state dimension for Gaia\n",
    "    \"d_conv\": 4,\n",
    "    \"expand\": 2,\n",
    "    \"use_cross_attention\": True,\n",
    "    \"n_cross_attn_heads\": 4,      # Reduced heads\n",
    "    \"use_checkpoint\": True,\n",
    "    \"activation_checkpointing\": True,\n",
    "    \"use_half_precision\": True,\n",
    "    \"sequential_processing\": True,\n",
    "    \"batch_size\": 24,             # Moderate batch size\n",
    "    \"micro_batch_size\": 8,        # Process in smaller chunks during training\n",
    "    \"gradient_accumulation\": 4    # Effective batch size = 24 × 4 = 96\n",
    "}\n",
    "\n",
    "# Configuration 2: Extreme Low Memory\n",
    "# For when you need to train with very large spectral embeddings\n",
    "EXTREME_LOW_MEM_CONFIG = {\n",
    "    \"name\": \"Extreme Low Memory\",\n",
    "    \"d_model_spectra\": 5120,      # Even larger for spectral data\n",
    "    \"d_model_gaia\": 128,          # 40x reduction for Gaia\n",
    "    \"num_classes\": 55,\n",
    "    \"input_dim_spectra\": 3647,\n",
    "    \"input_dim_gaia\": 18,\n",
    "    \"n_layers\": 12,\n",
    "    \"d_state_spectra\": 8,         # Reduced state dimension\n",
    "    \"d_state_gaia\": 2,            # Minimal state dimension\n",
    "    \"d_conv\": 2,                  # Reduced conv\n",
    "    \"expand\": 1,                  # No expansion\n",
    "    \"use_cross_attention\": False, # Remove cross-attention to save memory\n",
    "    \"use_checkpoint\": True,\n",
    "    \"activation_checkpointing\": True,\n",
    "    \"use_half_precision\": True,\n",
    "    \"sequential_processing\": True,\n",
    "    \"batch_size\": 16,\n",
    "    \"micro_batch_size\": 4,\n",
    "    \"gradient_accumulation\": 8    # Effective batch size = 16 × 8 = 128\n",
    "}\n",
    "\n",
    "# Configuration 3: Balanced Performance\n",
    "# Good balance between memory efficiency and model performance\n",
    "BALANCED_CONFIG = {\n",
    "    \"name\": \"Balanced Performance\",\n",
    "    \"d_model_spectra\": 3072,      # Moderate dimension\n",
    "    \"d_model_gaia\": 384,          # 8x reduction\n",
    "    \"num_classes\": 55,\n",
    "    \"input_dim_spectra\": 3647,\n",
    "    \"input_dim_gaia\": 18,\n",
    "    \"n_layers\": 10,               # Slightly reduced layers\n",
    "    \"d_state_spectra\": 16,\n",
    "    \"d_state_gaia\": 4,\n",
    "    \"d_conv\": 4,\n",
    "    \"expand\": 2,\n",
    "    \"use_cross_attention\": True,\n",
    "    \"n_cross_attn_heads\": 6,\n",
    "    \"use_checkpoint\": True,\n",
    "    \"activation_checkpointing\": True,\n",
    "    \"use_half_precision\": True,\n",
    "    \"sequential_processing\": True,\n",
    "    \"batch_size\": 32,\n",
    "    \"micro_batch_size\": 8,        \n",
    "    \"gradient_accumulation\": 2    # Effective batch size = 32 × 2 = 64\n",
    "}\n",
    "\n",
    "# Configuration 4: Production Ensemble\n",
    "# For training multiple ensemble models efficiently\n",
    "PRODUCTION_ENSEMBLE_CONFIG = {\n",
    "    \"name\": \"Production Ensemble\",\n",
    "    \"d_model_spectra\": 2560,      # Further reduced for ensemble training\n",
    "    \"d_model_gaia\": 320,          # 8x reduction\n",
    "    \"num_classes\": 55,\n",
    "    \"input_dim_spectra\": 3647,\n",
    "    \"input_dim_gaia\": 18,\n",
    "    \"n_layers\": 8,                # Reduced layers for faster training\n",
    "    \"d_state_spectra\": 8,\n",
    "    \"d_state_gaia\": 4,\n",
    "    \"d_conv\": 2,\n",
    "    \"expand\": 2,\n",
    "    \"use_cross_attention\": True,\n",
    "    \"n_cross_attn_heads\": 4,\n",
    "    \"use_checkpoint\": True,\n",
    "    \"activation_checkpointing\": True,\n",
    "    \"use_half_precision\": True,\n",
    "    \"sequential_processing\": True,\n",
    "    \"batch_size\": 32,\n",
    "    \"micro_batch_size\": 8,\n",
    "    \"gradient_accumulation\": 2    # Effective batch size = 32 × 2 = 64\n",
    "}\n",
    "\n",
    "def get_memory_efficient_config(available_vram_gb=24, target_spectra_dim=4096):\n",
    "    \"\"\"\n",
    "    Dynamically generate a memory-efficient configuration based on available VRAM\n",
    "    and target spectral dimension.\n",
    "    \n",
    "    Args:\n",
    "        available_vram_gb: Available VRAM in GB\n",
    "        target_spectra_dim: Target embedding dimension for spectral data\n",
    "        \n",
    "    Returns:\n",
    "        Optimized configuration dictionary\n",
    "    \"\"\"\n",
    "    # Base configuration\n",
    "    config = {\n",
    "        \"num_classes\": 55,\n",
    "        \"input_dim_spectra\": 3647,\n",
    "        \"input_dim_gaia\": 18,\n",
    "        \"use_checkpoint\": True,\n",
    "        \"activation_checkpointing\": True,\n",
    "        \"use_half_precision\": True,\n",
    "        \"sequential_processing\": True,\n",
    "    }\n",
    "    \n",
    "    # Scale dimensions based on available VRAM\n",
    "    usable_vram = available_vram_gb * 0.8  # Leave 20% for system overhead\n",
    "    \n",
    "    if usable_vram >= 20:\n",
    "        # High memory scenario\n",
    "        config.update({\n",
    "            \"d_model_spectra\": min(target_spectra_dim, 4096),\n",
    "            \"d_model_gaia\": 256,\n",
    "            \"n_layers\": 12,\n",
    "            \"d_state_spectra\": 16,\n",
    "            \"d_state_gaia\": 4,\n",
    "            \"d_conv\": 4,\n",
    "            \"expand\": 2,\n",
    "            \"use_cross_attention\": True,\n",
    "            \"n_cross_attn_heads\": 8,\n",
    "            \"batch_size\": 32,\n",
    "            \"gradient_accumulation\": 2\n",
    "        })\n",
    "    elif usable_vram >= 16:\n",
    "        # Medium memory scenario\n",
    "        config.update({\n",
    "            \"d_model_spectra\": min(target_spectra_dim, 3072),\n",
    "            \"d_model_gaia\": 256,\n",
    "            \"n_layers\": 10,\n",
    "            \"d_state_spectra\": 8,\n",
    "            \"d_state_gaia\": 4,\n",
    "            \"d_conv\": 2,\n",
    "            \"expand\": 2,\n",
    "            \"use_cross_attention\": True,\n",
    "            \"n_cross_attn_heads\": 4,\n",
    "            \"batch_size\": 24,\n",
    "            \"gradient_accumulation\": 4\n",
    "        })\n",
    "    elif usable_vram >= 12:\n",
    "        # Low memory scenario\n",
    "        config.update({\n",
    "            \"d_model_spectra\": min(target_spectra_dim, 2048),\n",
    "            \"d_model_gaia\": 192,\n",
    "            \"n_layers\": 8,\n",
    "            \"d_state_spectra\": 8,\n",
    "            \"d_state_gaia\": 4,\n",
    "            \"d_conv\": 2,\n",
    "            \"expand\": 1,\n",
    "            \"use_cross_attention\": True,\n",
    "            \"n_cross_attn_heads\": 4,\n",
    "            \"batch_size\": 16,\n",
    "            \"gradient_accumulation\": 4\n",
    "        })\n",
    "    else:\n",
    "        # Very low memory scenario\n",
    "        config.update({\n",
    "            \"d_model_spectra\": min(target_spectra_dim, 1536),\n",
    "            \"d_model_gaia\": 128,\n",
    "            \"n_layers\": 6,\n",
    "            \"d_state_spectra\": 8,\n",
    "            \"d_state_gaia\": 2,\n",
    "            \"d_conv\": 2,\n",
    "            \"expand\": 1,\n",
    "            \"use_cross_attention\": False,\n",
    "            \"batch_size\": 8,\n",
    "            \"gradient_accumulation\": 8\n",
    "        })\n",
    "    \n",
    "    # Calculate ratio between spectral and Gaia dimensions\n",
    "    ratio = config[\"d_model_spectra\"] / config[\"d_model_gaia\"]\n",
    "    config[\"name\"] = f\"Auto-{config['d_model_spectra']}:{config['d_model_gaia']} ({ratio:.1f}x)\"\n",
    "    \n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from ultra_memory_efficient import UltraMemoryEfficientEnsemble\n",
    "#?rom asymmetric_dimensions import AsymmetricMemoryEfficientStarClassifier\n",
    "#from advanced_asymmetric_config import (\n",
    "    MAX_ASYMMETRY_CONFIG, \n",
    "    EXTREME_LOW_MEM_CONFIG, \n",
    "    BALANCED_CONFIG,\n",
    "    PRODUCTION_ENSEMBLE_CONFIG,\n",
    "    get_memory_efficient_config\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h]\n",
      "                             [--config {max_asymmetry,extreme_low_mem,balanced,production,auto}]\n",
      "                             [--target_dim TARGET_DIM] [--epochs EPOCHS]\n",
      "                             [--output_dir OUTPUT_DIR] [--no_wandb]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=/home/joao/.local/share/jupyter/runtime/kernel-v380eaa32613c5cb54b8cb2e37731c87b559a132d2.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import argparse\n",
    "\n",
    "\n",
    "# Define a function to safely clean GPU memory\n",
    "def clean_gpu_memory():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        current_memory = torch.cuda.memory_allocated() / (1024**2)\n",
    "        print(f\"Current GPU memory usage: {current_memory:.2f} MB\")\n",
    "\n",
    "# Function to create and train a model with asymmetric dimensions\n",
    "def train_asymmetric_model(\n",
    "    config,\n",
    "    train_dataset,\n",
    "    val_dataset,\n",
    "    test_dataset=None,\n",
    "    output_dir=\"asymmetric_models\",\n",
    "    num_epochs=100,\n",
    "    log_to_wandb=True,\n",
    "    model_idx=0\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a model with asymmetric embedding dimensions for spectral and Gaia data.\n",
    "    \n",
    "    Args:\n",
    "        config: Configuration dictionary with model parameters\n",
    "        train_dataset: Training dataset\n",
    "        val_dataset: Validation dataset\n",
    "        test_dataset: Test dataset (optional)\n",
    "        output_dir: Directory to save model checkpoints\n",
    "        num_epochs: Number of epochs to train\n",
    "        log_to_wandb: Whether to log to Weights & Biases\n",
    "        model_idx: Model index for ensemble training\n",
    "        \n",
    "    Returns:\n",
    "        Trained model or None if training fails\n",
    "    \"\"\"\n",
    "    # Set device\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    if device == 'cuda':\n",
    "        # Check available GPU memory\n",
    "        total_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "        free_memory = torch.cuda.memory_reserved() / (1024**3)\n",
    "        print(f\"Total GPU memory: {total_memory:.2f} GB\")\n",
    "        print(f\"Reserved GPU memory: {free_memory:.2f} GB\")\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Print configuration\n",
    "    print(f\"\\nTraining model with configuration: {config['name']}\")\n",
    "    print(f\"Spectral dimension: {config['d_model_spectra']}, Gaia dimension: {config['d_model_gaia']}\")\n",
    "    print(f\"Ratio: {config['d_model_spectra'] / config['d_model_gaia']:.1f}x\")\n",
    "    print(f\"Batch size: {config['batch_size']}, Gradient accumulation: {config.get('gradient_accumulation', 1)}\")\n",
    "    print(f\"Effective batch size: {config['batch_size'] * config.get('gradient_accumulation', 1)}\")\n",
    "    \n",
    "    try:\n",
    "        # Create data loaders with appropriate batch sizes\n",
    "        batch_size = config['batch_size']\n",
    "        \n",
    "        # You can modify DataLoader parameters based on your dataset\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=True,\n",
    "            num_workers=2,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            val_dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=False,\n",
    "            num_workers=2,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        test_loader = None\n",
    "        if test_dataset is not None:\n",
    "            test_loader = DataLoader(\n",
    "                test_dataset, \n",
    "                batch_size=batch_size, \n",
    "                shuffle=False,\n",
    "                num_workers=2,\n",
    "                pin_memory=True\n",
    "            )\n",
    "        \n",
    "        # Create checkpoint directory for this specific configuration\n",
    "        config_dir = os.path.join(output_dir, config['name'].replace(':', '_').replace(' ', '_'))\n",
    "        os.makedirs(config_dir, exist_ok=True)\n",
    "        \n",
    "        # Create ensemble with just one model for now\n",
    "        ensemble = UltraMemoryEfficientEnsemble(\n",
    "            model_class=AsymmetricMemoryEfficientStarClassifier,\n",
    "            model_args=config,\n",
    "            num_models=1,  # Just train one model for now\n",
    "            device=device,\n",
    "            checkpoint_dir=config_dir\n",
    "        )\n",
    "        \n",
    "        # Train the model\n",
    "        model = ensemble.train_single_model(\n",
    "            model_idx=model_idx,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            test_loader=test_loader,\n",
    "            num_epochs=num_epochs,\n",
    "            lr=1e-4,\n",
    "            max_patience=20,\n",
    "            scheduler_type='OneCycleLR',\n",
    "            batch_accumulation=config.get('gradient_accumulation', 1),\n",
    "            log_to_wandb=log_to_wandb\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error training model: {e}\")\n",
    "        # Try to clean up memory\n",
    "        clean_gpu_memory()\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='Train a model with asymmetric embedding dimensions')\n",
    "    parser.add_argument('--config', type=str, default='auto', \n",
    "                        choices=['max_asymmetry', 'extreme_low_mem', 'balanced', 'production', 'auto'],\n",
    "                        help='Configuration to use')\n",
    "    parser.add_argument('--target_dim', type=int, default=4096, \n",
    "                        help='Target spectral dimension for auto config')\n",
    "    parser.add_argument('--epochs', type=int, default=100, \n",
    "                        help='Number of epochs to train')\n",
    "    parser.add_argument('--output_dir', type=str, default='asymmetric_models', \n",
    "                        help='Directory to save model checkpoints')\n",
    "    parser.add_argument('--no_wandb', action='store_true', \n",
    "                        help='Disable Weights & Biases logging')\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Load datasets (replace this with your actual loading code)\n",
    "    # Example placeholder:\n",
    "    from your_dataset_module import (\n",
    "        X_train_spectra, X_train_gaia, y_train,\n",
    "        X_val_spectra, X_val_gaia, y_val,\n",
    "        X_test_spectra, X_test_gaia, y_test,\n",
    "        MultiModalBalancedMultiLabelDataset\n",
    "    )\n",
    "    \n",
    "    # Create datasets with appropriate batch limits\n",
    "    # These would be replaced with your actual dataset creation code\n",
    "    train_dataset = MultiModalBalancedMultiLabelDataset(\n",
    "        X_train_spectra, X_train_gaia, y_train, limit_per_label=201\n",
    "    )\n",
    "    val_dataset = MultiModalBalancedMultiLabelDataset(\n",
    "        X_val_spectra, X_val_gaia, y_val, limit_per_label=201\n",
    "    )\n",
    "    test_dataset = MultiModalBalancedMultiLabelDataset(\n",
    "        X_test_spectra, X_test_gaia, y_test, limit_per_label=201\n",
    "    )\n",
    "    \n",
    "    # Select configuration\n",
    "    if args.config == 'max_asymmetry':\n",
    "        config = MAX_ASYMMETRY_CONFIG\n",
    "    elif args.config == 'extreme_low_mem':\n",
    "        config = EXTREME_LOW_MEM_CONFIG\n",
    "    elif args.config == 'balanced':\n",
    "        config = BALANCED_CONFIG\n",
    "    elif args.config == 'production':\n",
    "        config = PRODUCTION_ENSEMBLE_CONFIG\n",
    "    else:  # auto\n",
    "        # Auto-configure based on available VRAM\n",
    "        if torch.cuda.is_available():\n",
    "            vram_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "        else:\n",
    "            vram_gb = 16  # Default assumption\n",
    "        \n",
    "        config = get_memory_efficient_config(\n",
    "            available_vram_gb=vram_gb,\n",
    "            target_spectra_dim=args.target_dim\n",
    "        )\n",
    "    \n",
    "    # Train model\n",
    "    trained_model = train_asymmetric_model(\n",
    "        config=config,\n",
    "        train_dataset=train_dataset,\n",
    "        val_dataset=val_dataset,\n",
    "        test_dataset=test_dataset,\n",
    "        output_dir=args.output_dir,\n",
    "        num_epochs=args.epochs,\n",
    "        log_to_wandb=not args.no_wandb\n",
    "    )\n",
    "    \n",
    "    # Clean up\n",
    "    if trained_model is not None:\n",
    "        del trained_model\n",
    "    \n",
    "    clean_gpu_memory()\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set environment variables for better memory management\n",
    "    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "    \n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
